/data0/xjw/anaconda3/envs/llm311/bin/python: can't open file '/data2/xjw/llm_development/LLaMA-METEOR/train_old.py': [Errno 2] No such file or directory
/data0/xjw/anaconda3/envs/llm311/bin/python: can't open file '/data2/xjw/llm_development/LLaMA-METEOR/train_old.py': [Errno 2] No such file or directory
[2024-06-12 16:05:53,230] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 2) local_rank: 0 (pid: 1118244) of binary: /data0/xjw/anaconda3/envs/llm311/bin/python
Traceback (most recent call last):
  File "/data0/xjw/anaconda3/envs/llm311/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1010, in launch_command
    multi_gpu_launcher(args)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/accelerate/commands/launch.py", line 672, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_old.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-06-12_16:05:53
  host      : amax
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 1118245)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-12_16:05:53
  host      : amax
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 1118244)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
OrderedDict([('hf_auth', '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')])
OrderedDict([('hf_auth', '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')])
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4):  21%|██        | 212/1000 [00:00<00:01, 475.74 examples/s]Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 1011.07 examples/s]Map (num_proc=4):  96%|█████████▌| 962/1000 [00:00<00:00, 1658.00 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1160.19 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
Map (num_proc=4):  22%|██▏       | 219/1000 [00:00<00:01, 460.83 examples/s]Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 984.79 examples/s]Map (num_proc=4):  96%|█████████▌| 955/1000 [00:00<00:00, 1647.86 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1147.22 examples/s]
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.64it/s]Map (num_proc=4):  21%|██        | 211/1000 [00:00<00:01, 490.61 examples/s]Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 1069.49 examples/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.42it/s]Map (num_proc=4):  97%|█████████▋| 969/1000 [00:00<00:00, 1745.69 examples/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.52it/s]
Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1214.17 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4):  20%|█▉        | 199/1000 [00:00<00:02, 400.46 examples/s]Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 953.59 examples/s]Map (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 1278.44 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1564.79 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1066.49 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.77it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.48it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.64it/s]
model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
Load adapter, lora1
Load adapter, lora2
Load adapter, lora3
Load adapter, lora4
Load adapter, lora5
Load adapter, lora6
Load adapter, lora1
Load adapter, lora2
Load adapter, lora3
Load adapter, lora7
Load adapter, lora4
Load adapter, lora5
Load adapter, lora8
Load adapter, lora6
Load adapter, lora9
Load adapter, lora7
Load adapter, lora8
Load adapter, lora10
Load adapter, lora9
Load adapter, lora11
Load adapter, lora10
Load adapter, lora12
Load adapter, lora11
Load adapter, lora13
Load adapter, lora12
Load adapter, lora13
Load adapter, lora14
Load adapter, lora14
Load adapter, lora15
Load adapter, lora15
Load adapter, lora16
Load adapter, lora16
Load adapter, lora17
Load adapter, lora17
Load adapter, lora18
Load adapter, lora18
Load adapter, lora19
Load adapter, lora19
Load adapter, lora20
Load adapter, lora20
Load adapter, lora21
Load adapter, lora21
Load adapter, lora22
Load adapter, lora22
Load adapter, lora23
Load adapter, lora23
Load adapter, lora24
Load adapter, lora24
Load adapter, lora25
Load adapter, lora25
Load adapter, lora26
Load adapter, lora26
Load adapter, lora27
Load adapter, lora27
Load adapter, lora28
Load adapter, lora28
adapter model loaded PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0007,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun12_16-07-11_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
max_steps is given, it will override any value given in num_train_epochs
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
adapter model loaded PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0007,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun12_16-07-11_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
max_steps is given, it will override any value given in num_train_epochs
[2024-06-12 16:09:06,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1118572 closing signal SIGTERM
[2024-06-12 16:09:07,663] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -15) local_rank: 0 (pid: 1118571) of binary: /data0/xjw/anaconda3/envs/llm311/bin/python
Traceback (most recent call last):
  File "/data0/xjw/anaconda3/envs/llm311/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1010, in launch_command
    multi_gpu_launcher(args)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/accelerate/commands/launch.py", line 672, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=========================================================
meteora_train.py FAILED
---------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
---------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-12_16:09:06
  host      : amax
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 1118571)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 1118571
=========================================================
OrderedDict([('hf_auth', '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')])
OrderedDict([('hf_auth', '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')])
OrderedDict([('hf_auth', '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')])
OrderedDict([('hf_auth', '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')])
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 21
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})> 22
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
Map (num_proc=4):  20%|██        | 203/1000 [00:00<00:01, 468.37 examples/s]<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 1062.73 examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})> 22
Map (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 1365.62 examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1185.77 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
Map (num_proc=4):  19%|█▉        | 192/1000 [00:00<00:02, 373.40 examples/s]Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 920.97 examples/s]<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
Map (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 1252.92 examples/s]<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1508.99 examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1030.48 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.07it/s]<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.32it/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.40it/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
Map (num_proc=4):  22%|██▏       | 217/1000 [00:00<00:01, 479.48 examples/s]<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 1012.97 examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Map (num_proc=4):  97%|█████████▋| 970/1000 [00:00<00:00, 1697.99 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1170.83 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4):  22%|██▏       | 220/1000 [00:00<00:01, 448.14 examples/s]Map (num_proc=4):  72%|███████▏  | 718/1000 [00:00<00:00, 1327.22 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1626.84 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1161.98 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.18it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.44it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.52it/s]
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4):  20%|██        | 204/1000 [00:00<00:01, 405.08 examples/s]Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 951.27 examples/s]Map (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 1276.40 examples/s]Map (num_proc=4):  22%|██▏       | 223/1000 [00:00<00:01, 435.68 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1520.64 examples/s]Map (num_proc=4):  45%|████▍     | 446/1000 [00:00<00:00, 812.68 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1041.65 examples/s]
Map (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 1319.92 examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1102.39 examples/s]
model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Load adapter, lora1
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4):  20%|██        | 204/1000 [00:00<00:01, 406.97 examples/s]Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 947.22 examples/s]Map (num_proc=4):  96%|█████████▌| 959/1000 [00:00<00:00, 1645.26 examples/s]Map (num_proc=4):  22%|██▏       | 219/1000 [00:00<00:01, 472.99 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1140.68 examples/s]
Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 984.45 examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Map (num_proc=4):  96%|█████████▌| 956/1000 [00:00<00:00, 1649.00 examples/s]Load adapter, lora2
Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1158.36 examples/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.13it/s]Load adapter, lora3
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.17it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.32it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.05it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.33it/s]
Load adapter, lora4
Load adapter, lora5
Load adapter, lora6
model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
Load adapter, lora7
Load adapter, lora8
Load adapter, lora1
model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
Load adapter, lora2
Load adapter, lora9
Load adapter, lora3
Load adapter, lora4
Load adapter, lora10
Load adapter, lora5
Load adapter, lora6
Load adapter, lora11
Load adapter, lora1
Load adapter, lora7
Load adapter, lora1
Load adapter, lora2
Load adapter, lora2
Load adapter, lora12
Load adapter, lora8
Load adapter, lora3
Load adapter, lora3
Load adapter, lora4
Load adapter, lora4
Load adapter, lora9
Load adapter, lora5
Load adapter, lora5
Load adapter, lora13
Load adapter, lora6
Load adapter, lora6
Load adapter, lora10
Load adapter, lora7
Load adapter, lora7
Load adapter, lora14
Load adapter, lora11
Load adapter, lora8
Load adapter, lora8
Load adapter, lora15
Load adapter, lora9
Load adapter, lora9
Load adapter, lora12
Load adapter, lora10
Load adapter, lora10
Load adapter, lora13
Load adapter, lora16
Load adapter, lora11
Load adapter, lora11
Load adapter, lora14
Load adapter, lora12
Load adapter, lora12
Load adapter, lora17
Load adapter, lora15
Load adapter, lora13
Load adapter, lora13
Load adapter, lora18
Load adapter, lora14
Load adapter, lora16
Load adapter, lora14
Load adapter, lora19
Load adapter, lora15
Load adapter, lora15
Load adapter, lora17
Load adapter, lora16
Load adapter, lora20
Load adapter, lora16
Load adapter, lora18
Load adapter, lora17
Load adapter, lora17
Load adapter, lora21
Load adapter, lora19
Load adapter, lora18
Load adapter, lora18
Load adapter, lora22
Load adapter, lora20
Load adapter, lora19
Load adapter, lora19
Load adapter, lora23
Load adapter, lora21
Load adapter, lora20
Load adapter, lora20
Load adapter, lora22
Load adapter, lora24
Load adapter, lora21
Load adapter, lora21
Load adapter, lora23
Load adapter, lora25
Load adapter, lora22
Load adapter, lora22
Load adapter, lora24
Load adapter, lora26
Load adapter, lora23
Load adapter, lora23
Load adapter, lora25
Load adapter, lora24
Load adapter, lora24
Load adapter, lora27
Load adapter, lora26
Load adapter, lora25
Load adapter, lora25
Load adapter, lora28
Load adapter, lora27
Load adapter, lora26
Load adapter, lora26
Load adapter, lora27
Load adapter, lora28
Load adapter, lora27
adapter model loaded PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0007,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun12_16-09-19_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
max_steps is given, it will override any value given in num_train_epochs
Load adapter, lora28
Load adapter, lora28
adapter model loaded adapter model loaded PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0007,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun12_16-09-19_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0007,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun12_16-09-19_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
max_steps is given, it will override any value given in num_train_epochs
max_steps is given, it will override any value given in num_train_epochs
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
adapter model loaded PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0007,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun12_16-09-19_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
max_steps is given, it will override any value given in num_train_epochs
wandb: Currently logged in as: jingwei-xu-nju (nju-ics). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /data2/xjw/llm_development/LLaMA-METEOR/wandb/run-20240612_161121-u6r23m76
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sea-385
wandb: ⭐️ View project at https://wandb.ai/nju-ics/huggingface
wandb: 🚀 View run at https://wandb.ai/nju-ics/huggingface/runs/u6r23m76
  0%|          | 0/5000 [00:00<?, ?it/s]/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/5000 [00:21<29:31:42, 21.26s/it]                                                   {'loss': 2053.7349, 'grad_norm': 181.0, 'learning_rate': 2.8e-06, 'epoch': 0.0}
  0%|          | 1/5000 [00:21<29:31:42, 21.26s/it]  0%|          | 2/5000 [00:47<33:20:10, 24.01s/it]                                                   {'loss': 2044.0028, 'grad_norm': 162.0, 'learning_rate': 5.6e-06, 'epoch': 0.0}
  0%|          | 2/5000 [00:47<33:20:10, 24.01s/it]  0%|          | 3/5000 [01:05<29:43:22, 21.41s/it]                                                   {'loss': 2057.5078, 'grad_norm': 164.0, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.0}
  0%|          | 3/5000 [01:05<29:43:22, 21.41s/it]  0%|          | 4/5000 [01:28<30:21:02, 21.87s/it]                                                   {'loss': 2046.2229, 'grad_norm': 171.0, 'learning_rate': 1.12e-05, 'epoch': 0.01}
  0%|          | 4/5000 [01:28<30:21:02, 21.87s/it]  0%|          | 5/5000 [01:43<27:07:06, 19.54s/it]                                                   {'loss': 2043.2529, 'grad_norm': 161.0, 'learning_rate': 1.4e-05, 'epoch': 0.01}
  0%|          | 5/5000 [01:43<27:07:06, 19.54s/it]  0%|          | 6/5000 [02:02<26:52:41, 19.38s/it]                                                   {'loss': 2056.4299, 'grad_norm': 154.0, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.01}
  0%|          | 6/5000 [02:02<26:52:41, 19.38s/it]  0%|          | 7/5000 [02:23<27:46:25, 20.03s/it]                                                   {'loss': 2056.98, 'grad_norm': 176.0, 'learning_rate': 1.96e-05, 'epoch': 0.01}
  0%|          | 7/5000 [02:23<27:46:25, 20.03s/it]  0%|          | 8/5000 [02:39<25:45:14, 18.57s/it]                                                   {'loss': 2019.4332, 'grad_norm': 197.0, 'learning_rate': 2.24e-05, 'epoch': 0.01}
  0%|          | 8/5000 [02:39<25:45:14, 18.57s/it]  0%|          | 9/5000 [02:55<24:47:19, 17.88s/it]                                                   {'loss': 2018.2773, 'grad_norm': 196.0, 'learning_rate': 2.52e-05, 'epoch': 0.01}
  0%|          | 9/5000 [02:55<24:47:19, 17.88s/it]  0%|          | 10/5000 [03:10<23:32:39, 16.99s/it]                                                    {'loss': 2001.771, 'grad_norm': 230.0, 'learning_rate': 2.8e-05, 'epoch': 0.01}
  0%|          | 10/5000 [03:10<23:32:39, 16.99s/it]  0%|          | 11/5000 [03:28<23:52:45, 17.23s/it]                                                    {'loss': 2039.1389, 'grad_norm': 192.0, 'learning_rate': 3.0799999999999996e-05, 'epoch': 0.01}
  0%|          | 11/5000 [03:28<23:52:45, 17.23s/it]  0%|          | 12/5000 [03:41<22:09:21, 15.99s/it]                                                    {'loss': 1979.6777, 'grad_norm': 266.0, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.02}
  0%|          | 12/5000 [03:41<22:09:21, 15.99s/it]  0%|          | 13/5000 [03:55<21:17:10, 15.37s/it]                                                    {'loss': 1994.5925, 'grad_norm': 326.0, 'learning_rate': 3.64e-05, 'epoch': 0.02}
  0%|          | 13/5000 [03:55<21:17:10, 15.37s/it]  0%|          | 14/5000 [04:31<29:49:05, 21.53s/it]                                                    {'loss': 1998.7021, 'grad_norm': 272.0, 'learning_rate': 3.92e-05, 'epoch': 0.02}
  0%|          | 14/5000 [04:31<29:49:05, 21.53s/it]  0%|          | 15/5000 [04:59<32:34:24, 23.52s/it]                                                    {'loss': 1998.5715, 'grad_norm': 330.0, 'learning_rate': 4.2e-05, 'epoch': 0.02}
  0%|          | 15/5000 [04:59<32:34:24, 23.52s/it]  0%|          | 16/5000 [05:25<33:35:00, 24.26s/it]                                                    {'loss': 1934.231, 'grad_norm': 402.0, 'learning_rate': 4.48e-05, 'epoch': 0.02}
  0%|          | 16/5000 [05:25<33:35:00, 24.26s/it]  0%|          | 17/5000 [05:38<28:56:27, 20.91s/it]                                                    {'loss': 1899.427, 'grad_norm': 604.0, 'learning_rate': 4.7600000000000005e-05, 'epoch': 0.02}
  0%|          | 17/5000 [05:38<28:56:27, 20.91s/it]  0%|          | 18/5000 [05:51<25:37:19, 18.51s/it]                                                    {'loss': 1868.1082, 'grad_norm': 620.0, 'learning_rate': 5.04e-05, 'epoch': 0.02}
  0%|          | 18/5000 [05:51<25:37:19, 18.51s/it]  0%|          | 19/5000 [06:09<25:28:22, 18.41s/it]                                                    {'loss': 1878.0919, 'grad_norm': 556.0, 'learning_rate': 5.32e-05, 'epoch': 0.02}
  0%|          | 19/5000 [06:09<25:28:22, 18.41s/it]  0%|          | 20/5000 [06:25<24:17:22, 17.56s/it]                                                    {'loss': 1828.1992, 'grad_norm': 708.0, 'learning_rate': 5.6e-05, 'epoch': 0.03}
  0%|          | 20/5000 [06:25<24:17:22, 17.56s/it]  0%|          | 21/5000 [06:40<23:24:10, 16.92s/it]                                                    {'loss': 1799.6311, 'grad_norm': 864.0, 'learning_rate': 5.8800000000000006e-05, 'epoch': 0.03}
  0%|          | 21/5000 [06:40<23:24:10, 16.92s/it]  0%|          | 22/5000 [06:57<23:31:47, 17.02s/it]                                                    {'loss': 1775.0193, 'grad_norm': 1004.0, 'learning_rate': 6.159999999999999e-05, 'epoch': 0.03}
  0%|          | 22/5000 [06:57<23:31:47, 17.02s/it]  0%|          | 23/5000 [07:13<22:55:52, 16.59s/it]                                                    {'loss': 1708.6001, 'grad_norm': 1144.0, 'learning_rate': 6.44e-05, 'epoch': 0.03}
  0%|          | 23/5000 [07:13<22:55:52, 16.59s/it]  0%|          | 24/5000 [07:29<22:28:06, 16.26s/it]                                                    {'loss': 1655.2035, 'grad_norm': 1080.0, 'learning_rate': 6.720000000000001e-05, 'epoch': 0.03}
  0%|          | 24/5000 [07:29<22:28:06, 16.26s/it]  0%|          | 25/5000 [07:46<22:53:59, 16.57s/it]                                                    {'loss': 1647.402, 'grad_norm': 1512.0, 'learning_rate': 7.000000000000001e-05, 'epoch': 0.03}
  0%|          | 25/5000 [07:46<22:53:59, 16.57s/it]  1%|          | 26/5000 [08:03<22:59:13, 16.64s/it]                                                    {'loss': 1521.1797, 'grad_norm': 1128.0, 'learning_rate': 7.28e-05, 'epoch': 0.03}
  1%|          | 26/5000 [08:03<22:59:13, 16.64s/it]  1%|          | 27/5000 [08:17<21:51:46, 15.83s/it]                                                    {'loss': 1442.9675, 'grad_norm': 916.0, 'learning_rate': 7.56e-05, 'epoch': 0.03}
  1%|          | 27/5000 [08:17<21:51:46, 15.83s/it]  1%|          | 28/5000 [08:31<21:10:32, 15.33s/it]                                                    {'loss': 1375.7053, 'grad_norm': 1584.0, 'learning_rate': 7.84e-05, 'epoch': 0.04}
  1%|          | 28/5000 [08:31<21:10:32, 15.33s/it]  1%|          | 29/5000 [08:48<22:00:11, 15.93s/it]                                                    {'loss': 1328.2534, 'grad_norm': 1168.0, 'learning_rate': 8.120000000000001e-05, 'epoch': 0.04}
  1%|          | 29/5000 [08:48<22:00:11, 15.93s/it]  1%|          | 30/5000 [09:00<20:10:27, 14.61s/it]                                                    {'loss': 1257.2319, 'grad_norm': 988.0, 'learning_rate': 8.4e-05, 'epoch': 0.04}
  1%|          | 30/5000 [09:00<20:10:27, 14.61s/it]  1%|          | 31/5000 [09:17<21:09:54, 15.33s/it]                                                    {'loss': 1179.4314, 'grad_norm': 860.0, 'learning_rate': 8.68e-05, 'epoch': 0.04}
  1%|          | 31/5000 [09:17<21:09:54, 15.33s/it]  1%|          | 32/5000 [09:28<19:39:24, 14.24s/it]                                                    {'loss': 1126.1096, 'grad_norm': 1008.0, 'learning_rate': 8.96e-05, 'epoch': 0.04}
  1%|          | 32/5000 [09:28<19:39:24, 14.24s/it]  1%|          | 33/5000 [09:42<19:18:47, 14.00s/it]                                                    {'loss': 1041.8572, 'grad_norm': 924.0, 'learning_rate': 9.240000000000001e-05, 'epoch': 0.04}
  1%|          | 33/5000 [09:42<19:18:47, 14.00s/it]  1%|          | 34/5000 [09:52<17:43:14, 12.85s/it]                                                    {'loss': 996.2354, 'grad_norm': 1016.0, 'learning_rate': 9.520000000000001e-05, 'epoch': 0.04}
  1%|          | 34/5000 [09:52<17:43:14, 12.85s/it]  1%|          | 35/5000 [10:12<20:44:10, 15.04s/it]                                                    {'loss': 907.7849, 'grad_norm': 2024.0, 'learning_rate': 9.800000000000001e-05, 'epoch': 0.04}
  1%|          | 35/5000 [10:12<20:44:10, 15.04s/it]  1%|          | 36/5000 [10:25<19:57:31, 14.47s/it]                                                    {'loss': 861.9077, 'grad_norm': 1072.0, 'learning_rate': 0.0001008, 'epoch': 0.05}
  1%|          | 36/5000 [10:25<19:57:31, 14.47s/it]  1%|          | 37/5000 [10:35<18:09:22, 13.17s/it]                                                    {'loss': 849.8156, 'grad_norm': 1008.0, 'learning_rate': 0.0001036, 'epoch': 0.05}
  1%|          | 37/5000 [10:35<18:09:22, 13.17s/it]  1%|          | 38/5000 [11:00<22:58:21, 16.67s/it]                                                    {'loss': 803.8958, 'grad_norm': 1304.0, 'learning_rate': 0.0001064, 'epoch': 0.05}
  1%|          | 38/5000 [11:00<22:58:21, 16.67s/it]  1%|          | 39/5000 [11:13<21:26:43, 15.56s/it]                                                    {'loss': 750.9426, 'grad_norm': 1000.0, 'learning_rate': 0.0001092, 'epoch': 0.05}
  1%|          | 39/5000 [11:13<21:26:43, 15.56s/it]  1%|          | 40/5000 [11:45<28:22:21, 20.59s/it]                                                    {'loss': 670.8353, 'grad_norm': 1136.0, 'learning_rate': 0.000112, 'epoch': 0.05}
  1%|          | 40/5000 [11:45<28:22:21, 20.59s/it]  1%|          | 41/5000 [12:00<25:46:13, 18.71s/it]                                                    {'loss': 703.5007, 'grad_norm': 1384.0, 'learning_rate': 0.0001148, 'epoch': 0.05}
  1%|          | 41/5000 [12:00<25:46:13, 18.71s/it]  1%|          | 42/5000 [12:12<23:10:03, 16.82s/it]                                                    {'loss': 646.346, 'grad_norm': 968.0, 'learning_rate': 0.00011760000000000001, 'epoch': 0.05}
  1%|          | 42/5000 [12:12<23:10:03, 16.82s/it]  1%|          | 43/5000 [12:26<21:55:17, 15.92s/it]                                                    {'loss': 614.1068, 'grad_norm': 928.0, 'learning_rate': 0.00012039999999999999, 'epoch': 0.05}
  1%|          | 43/5000 [12:26<21:55:17, 15.92s/it]  1%|          | 44/5000 [12:41<21:31:42, 15.64s/it]                                                    {'loss': 553.3309, 'grad_norm': 944.0, 'learning_rate': 0.00012319999999999999, 'epoch': 0.06}
  1%|          | 44/5000 [12:41<21:31:42, 15.64s/it]  1%|          | 45/5000 [13:09<26:40:36, 19.38s/it]                                                    {'loss': 520.2599, 'grad_norm': 1032.0, 'learning_rate': 0.000126, 'epoch': 0.06}
  1%|          | 45/5000 [13:09<26:40:36, 19.38s/it]  1%|          | 46/5000 [13:31<27:38:37, 20.09s/it]                                                    {'loss': 537.3496, 'grad_norm': 828.0, 'learning_rate': 0.0001288, 'epoch': 0.06}
  1%|          | 46/5000 [13:31<27:38:37, 20.09s/it]  1%|          | 47/5000 [13:43<24:28:36, 17.79s/it]                                                    {'loss': 491.4962, 'grad_norm': 940.0, 'learning_rate': 0.0001316, 'epoch': 0.06}
  1%|          | 47/5000 [13:43<24:28:36, 17.79s/it]  1%|          | 48/5000 [13:58<23:14:51, 16.90s/it]                                                    {'loss': 446.3587, 'grad_norm': 1104.0, 'learning_rate': 0.00013440000000000001, 'epoch': 0.06}
  1%|          | 48/5000 [13:58<23:14:51, 16.90s/it]  1%|          | 49/5000 [14:11<21:30:01, 15.63s/it]                                                    {'loss': 525.9469, 'grad_norm': 1480.0, 'learning_rate': 0.0001372, 'epoch': 0.06}
  1%|          | 49/5000 [14:11<21:30:01, 15.63s/it]  1%|          | 50/5000 [14:25<20:49:13, 15.14s/it]                                                    {'loss': 446.5135, 'grad_norm': 1032.0, 'learning_rate': 0.00014000000000000001, 'epoch': 0.06}
  1%|          | 50/5000 [14:25<20:49:13, 15.14s/it]  1%|          | 51/5000 [14:48<24:20:45, 17.71s/it]                                                    {'loss': 456.4223, 'grad_norm': 1104.0, 'learning_rate': 0.0001428, 'epoch': 0.06}
  1%|          | 51/5000 [14:48<24:20:45, 17.71s/it]  1%|          | 52/5000 [15:01<22:22:33, 16.28s/it]                                                    {'loss': 448.4766, 'grad_norm': 1208.0, 'learning_rate': 0.0001456, 'epoch': 0.07}
  1%|          | 52/5000 [15:01<22:22:33, 16.28s/it]  1%|          | 53/5000 [15:16<21:38:10, 15.75s/it]                                                    {'loss': 404.9394, 'grad_norm': 1120.0, 'learning_rate': 0.0001484, 'epoch': 0.07}
  1%|          | 53/5000 [15:16<21:38:10, 15.75s/it]  1%|          | 54/5000 [15:28<20:13:08, 14.72s/it]                                                    {'loss': 405.4988, 'grad_norm': 1200.0, 'learning_rate': 0.0001512, 'epoch': 0.07}
  1%|          | 54/5000 [15:28<20:13:08, 14.72s/it]  1%|          | 55/5000 [15:40<18:57:21, 13.80s/it]                                                    {'loss': 595.4476, 'grad_norm': 3568.0, 'learning_rate': 0.000154, 'epoch': 0.07}
  1%|          | 55/5000 [15:40<18:57:21, 13.80s/it]  1%|          | 56/5000 [16:03<22:47:42, 16.60s/it]                                                    {'loss': 383.9142, 'grad_norm': 1784.0, 'learning_rate': 0.0001568, 'epoch': 0.07}
  1%|          | 56/5000 [16:03<22:47:42, 16.60s/it]  1%|          | 57/5000 [16:13<20:07:17, 14.65s/it]                                                    {'loss': 427.8734, 'grad_norm': 1408.0, 'learning_rate': 0.0001596, 'epoch': 0.07}
  1%|          | 57/5000 [16:13<20:07:17, 14.65s/it]  1%|          | 58/5000 [16:27<19:37:10, 14.29s/it]                                                    {'loss': 383.8142, 'grad_norm': 1104.0, 'learning_rate': 0.00016240000000000002, 'epoch': 0.07}
  1%|          | 58/5000 [16:27<19:37:10, 14.29s/it]  1%|          | 59/5000 [16:38<18:33:59, 13.53s/it]                                                    {'loss': 399.4972, 'grad_norm': 1128.0, 'learning_rate': 0.00016519999999999998, 'epoch': 0.07}
  1%|          | 59/5000 [16:38<18:33:59, 13.53s/it]  1%|          | 60/5000 [16:48<16:49:40, 12.26s/it]                                                    {'loss': 409.3643, 'grad_norm': 1080.0, 'learning_rate': 0.000168, 'epoch': 0.08}
  1%|          | 60/5000 [16:48<16:49:40, 12.26s/it]  1%|          | 61/5000 [17:00<16:50:20, 12.27s/it]                                                    {'loss': 363.3816, 'grad_norm': 1128.0, 'learning_rate': 0.0001708, 'epoch': 0.08}
  1%|          | 61/5000 [17:00<16:50:20, 12.27s/it]  1%|          | 62/5000 [17:14<17:27:01, 12.72s/it]                                                    {'loss': 316.0648, 'grad_norm': 876.0, 'learning_rate': 0.0001736, 'epoch': 0.08}
  1%|          | 62/5000 [17:14<17:27:01, 12.72s/it]  1%|▏         | 63/5000 [17:26<17:12:50, 12.55s/it]                                                    {'loss': 348.9838, 'grad_norm': 1240.0, 'learning_rate': 0.0001764, 'epoch': 0.08}
  1%|▏         | 63/5000 [17:26<17:12:50, 12.55s/it]  1%|▏         | 64/5000 [17:44<19:36:30, 14.30s/it]                                                    {'loss': 296.7612, 'grad_norm': 840.0, 'learning_rate': 0.0001792, 'epoch': 0.08}
  1%|▏         | 64/5000 [17:44<19:36:30, 14.30s/it]  1%|▏         | 65/5000 [17:56<18:31:20, 13.51s/it]                                                    {'loss': 411.544, 'grad_norm': 1928.0, 'learning_rate': 0.000182, 'epoch': 0.08}
  1%|▏         | 65/5000 [17:56<18:31:20, 13.51s/it]  1%|▏         | 66/5000 [18:06<17:06:19, 12.48s/it]                                                    {'loss': 348.1234, 'grad_norm': 1088.0, 'learning_rate': 0.00018480000000000002, 'epoch': 0.08}
  1%|▏         | 66/5000 [18:06<17:06:19, 12.48s/it]  1%|▏         | 67/5000 [18:29<21:34:24, 15.74s/it]                                                    {'loss': 345.4297, 'grad_norm': 1160.0, 'learning_rate': 0.0001876, 'epoch': 0.09}
  1%|▏         | 67/5000 [18:29<21:34:24, 15.74s/it]  1%|▏         | 68/5000 [18:51<24:07:56, 17.61s/it]                                                    {'loss': 324.0066, 'grad_norm': 900.0, 'learning_rate': 0.00019040000000000002, 'epoch': 0.09}
  1%|▏         | 68/5000 [18:51<24:07:56, 17.61s/it]  1%|▏         | 69/5000 [19:07<23:07:14, 16.88s/it]                                                    {'loss': 320.481, 'grad_norm': 964.0, 'learning_rate': 0.0001932, 'epoch': 0.09}
  1%|▏         | 69/5000 [19:07<23:07:14, 16.88s/it]  1%|▏         | 70/5000 [19:17<20:25:51, 14.92s/it]                                                    {'loss': 313.9882, 'grad_norm': 1144.0, 'learning_rate': 0.00019600000000000002, 'epoch': 0.09}
  1%|▏         | 70/5000 [19:17<20:25:51, 14.92s/it]  1%|▏         | 71/5000 [19:31<19:55:31, 14.55s/it]                                                    {'loss': 301.6672, 'grad_norm': 864.0, 'learning_rate': 0.00019879999999999998, 'epoch': 0.09}
  1%|▏         | 71/5000 [19:31<19:55:31, 14.55s/it]  1%|▏         | 72/5000 [19:53<23:14:15, 16.98s/it]                                                    {'loss': 297.5405, 'grad_norm': 992.0, 'learning_rate': 0.0002016, 'epoch': 0.09}
  1%|▏         | 72/5000 [19:53<23:14:15, 16.98s/it]  1%|▏         | 73/5000 [20:10<23:20:29, 17.05s/it]                                                    {'loss': 323.3909, 'grad_norm': 888.0, 'learning_rate': 0.00020439999999999998, 'epoch': 0.09}
  1%|▏         | 73/5000 [20:10<23:20:29, 17.05s/it]  1%|▏         | 74/5000 [20:23<21:27:52, 15.69s/it]                                                    {'loss': 284.8436, 'grad_norm': 900.0, 'learning_rate': 0.0002072, 'epoch': 0.09}
  1%|▏         | 74/5000 [20:23<21:27:52, 15.69s/it]  2%|▏         | 75/5000 [20:40<22:07:26, 16.17s/it]                                                    {'loss': 271.9068, 'grad_norm': 712.0, 'learning_rate': 0.00020999999999999998, 'epoch': 0.1}
  2%|▏         | 75/5000 [20:40<22:07:26, 16.17s/it]  2%|▏         | 76/5000 [20:56<22:03:51, 16.13s/it]                                                    {'loss': 281.6011, 'grad_norm': 680.0, 'learning_rate': 0.0002128, 'epoch': 0.1}
  2%|▏         | 76/5000 [20:56<22:03:51, 16.13s/it]  2%|▏         | 77/5000 [21:09<20:49:48, 15.23s/it]                                                    {'loss': 242.717, 'grad_norm': 768.0, 'learning_rate': 0.00021559999999999998, 'epoch': 0.1}
  2%|▏         | 77/5000 [21:09<20:49:48, 15.23s/it]  2%|▏         | 78/5000 [21:22<19:36:11, 14.34s/it]                                                    {'loss': 251.846, 'grad_norm': 592.0, 'learning_rate': 0.0002184, 'epoch': 0.1}
  2%|▏         | 78/5000 [21:22<19:36:11, 14.34s/it]  2%|▏         | 79/5000 [21:37<19:50:31, 14.52s/it]                                                    {'loss': 251.808, 'grad_norm': 780.0, 'learning_rate': 0.0002212, 'epoch': 0.1}
  2%|▏         | 79/5000 [21:37<19:50:31, 14.52s/it]  2%|▏         | 80/5000 [21:50<19:17:27, 14.12s/it]                                                    {'loss': 298.7079, 'grad_norm': 1168.0, 'learning_rate': 0.000224, 'epoch': 0.1}
  2%|▏         | 80/5000 [21:50<19:17:27, 14.12s/it]  2%|▏         | 81/5000 [22:03<19:03:55, 13.95s/it]                                                    {'loss': 266.2607, 'grad_norm': 684.0, 'learning_rate': 0.0002268, 'epoch': 0.1}
  2%|▏         | 81/5000 [22:03<19:03:55, 13.95s/it]  2%|▏         | 82/5000 [22:21<20:25:32, 14.95s/it]                                                    {'loss': 203.8417, 'grad_norm': 932.0, 'learning_rate': 0.0002296, 'epoch': 0.1}
  2%|▏         | 82/5000 [22:21<20:25:32, 14.95s/it]  2%|▏         | 83/5000 [22:36<20:44:11, 15.18s/it]                                                    {'loss': 255.0615, 'grad_norm': 884.0, 'learning_rate': 0.0002324, 'epoch': 0.11}
  2%|▏         | 83/5000 [22:36<20:44:11, 15.18s/it]  2%|▏         | 84/5000 [22:53<21:12:45, 15.53s/it]                                                    {'loss': 234.3929, 'grad_norm': 1064.0, 'learning_rate': 0.00023520000000000002, 'epoch': 0.11}
  2%|▏         | 84/5000 [22:53<21:12:45, 15.53s/it]  2%|▏         | 85/5000 [23:08<21:16:09, 15.58s/it]                                                    {'loss': 246.5532, 'grad_norm': 860.0, 'learning_rate': 0.000238, 'epoch': 0.11}
  2%|▏         | 85/5000 [23:08<21:16:09, 15.58s/it]  2%|▏         | 86/5000 [23:19<19:23:54, 14.21s/it]                                                    {'loss': 259.731, 'grad_norm': 740.0, 'learning_rate': 0.00024079999999999997, 'epoch': 0.11}
  2%|▏         | 86/5000 [23:19<19:23:54, 14.21s/it]  2%|▏         | 87/5000 [23:41<22:17:09, 16.33s/it]                                                    {'loss': 186.5862, 'grad_norm': 904.0, 'learning_rate': 0.00024359999999999999, 'epoch': 0.11}
  2%|▏         | 87/5000 [23:41<22:17:09, 16.33s/it]  2%|▏         | 88/5000 [23:57<22:22:22, 16.40s/it]                                                    {'loss': 260.0997, 'grad_norm': 1224.0, 'learning_rate': 0.00024639999999999997, 'epoch': 0.11}
  2%|▏         | 88/5000 [23:57<22:22:22, 16.40s/it]  2%|▏         | 89/5000 [24:10<20:53:26, 15.31s/it]                                                    {'loss': 226.4076, 'grad_norm': 1296.0, 'learning_rate': 0.0002492, 'epoch': 0.11}
  2%|▏         | 89/5000 [24:10<20:53:26, 15.31s/it]  2%|▏         | 90/5000 [24:21<19:08:03, 14.03s/it]                                                    {'loss': 242.8185, 'grad_norm': 940.0, 'learning_rate': 0.000252, 'epoch': 0.11}
  2%|▏         | 90/5000 [24:21<19:08:03, 14.03s/it]  2%|▏         | 91/5000 [24:39<20:42:41, 15.19s/it]                                                    {'loss': 226.6375, 'grad_norm': 560.0, 'learning_rate': 0.0002548, 'epoch': 0.12}
  2%|▏         | 91/5000 [24:39<20:42:41, 15.19s/it]  2%|▏         | 92/5000 [24:51<19:21:25, 14.20s/it]                                                    {'loss': 232.7385, 'grad_norm': 820.0, 'learning_rate': 0.0002576, 'epoch': 0.12}
  2%|▏         | 92/5000 [24:51<19:21:25, 14.20s/it]  2%|▏         | 93/5000 [25:05<19:15:39, 14.13s/it]                                                    {'loss': 217.2887, 'grad_norm': 536.0, 'learning_rate': 0.0002604, 'epoch': 0.12}
  2%|▏         | 93/5000 [25:05<19:15:39, 14.13s/it]  2%|▏         | 94/5000 [25:25<21:44:17, 15.95s/it]                                                    {'loss': 212.3031, 'grad_norm': 584.0, 'learning_rate': 0.0002632, 'epoch': 0.12}
  2%|▏         | 94/5000 [25:25<21:44:17, 15.95s/it]  2%|▏         | 95/5000 [25:40<21:18:40, 15.64s/it]                                                    {'loss': 198.3847, 'grad_norm': 2224.0, 'learning_rate': 0.000266, 'epoch': 0.12}
  2%|▏         | 95/5000 [25:40<21:18:40, 15.64s/it]  2%|▏         | 96/5000 [25:54<20:44:48, 15.23s/it]                                                    {'loss': 208.4947, 'grad_norm': 736.0, 'learning_rate': 0.00026880000000000003, 'epoch': 0.12}
  2%|▏         | 96/5000 [25:54<20:44:48, 15.23s/it]  2%|▏         | 97/5000 [26:10<21:06:40, 15.50s/it]                                                    {'loss': 229.6669, 'grad_norm': 1136.0, 'learning_rate': 0.0002716, 'epoch': 0.12}
  2%|▏         | 97/5000 [26:10<21:06:40, 15.50s/it]  2%|▏         | 98/5000 [26:26<21:00:12, 15.42s/it]                                                    {'loss': 205.6406, 'grad_norm': 748.0, 'learning_rate': 0.0002744, 'epoch': 0.12}
  2%|▏         | 98/5000 [26:26<21:00:12, 15.42s/it]  2%|▏         | 99/5000 [26:38<19:42:58, 14.48s/it]                                                    {'loss': 231.4333, 'grad_norm': 1080.0, 'learning_rate': 0.0002772, 'epoch': 0.13}
  2%|▏         | 99/5000 [26:38<19:42:58, 14.48s/it]  2%|▏         | 100/5000 [26:58<22:04:09, 16.21s/it]                                                     {'loss': 275.5608, 'grad_norm': 3504.0, 'learning_rate': 0.00028000000000000003, 'epoch': 0.13}
  2%|▏         | 100/5000 [26:58<22:04:09, 16.21s/it]  2%|▏         | 101/5000 [27:12<21:03:02, 15.47s/it]                                                     {'loss': 189.7067, 'grad_norm': 896.0, 'learning_rate': 0.0002828, 'epoch': 0.13}
  2%|▏         | 101/5000 [27:12<21:03:02, 15.47s/it]  2%|▏         | 102/5000 [27:32<22:51:31, 16.80s/it]                                                     {'loss': 241.2575, 'grad_norm': 840.0, 'learning_rate': 0.0002856, 'epoch': 0.13}
  2%|▏         | 102/5000 [27:32<22:51:31, 16.80s/it]  2%|▏         | 103/5000 [27:47<22:24:07, 16.47s/it]                                                     {'loss': 192.0626, 'grad_norm': 632.0, 'learning_rate': 0.00028839999999999996, 'epoch': 0.13}
  2%|▏         | 103/5000 [27:47<22:24:07, 16.47s/it]  2%|▏         | 104/5000 [27:59<20:19:03, 14.94s/it]                                                     {'loss': 212.5396, 'grad_norm': 788.0, 'learning_rate': 0.0002912, 'epoch': 0.13}
  2%|▏         | 104/5000 [27:59<20:19:03, 14.94s/it]  2%|▏         | 105/5000 [28:22<23:32:13, 17.31s/it]                                                     {'loss': 226.1225, 'grad_norm': 1032.0, 'learning_rate': 0.000294, 'epoch': 0.13}
  2%|▏         | 105/5000 [28:22<23:32:13, 17.31s/it]  2%|▏         | 106/5000 [28:33<21:16:58, 15.66s/it]                                                     {'loss': 275.591, 'grad_norm': 3488.0, 'learning_rate': 0.0002968, 'epoch': 0.13}
  2%|▏         | 106/5000 [28:33<21:16:58, 15.66s/it]  2%|▏         | 107/5000 [28:46<19:50:30, 14.60s/it]                                                     {'loss': 299.848, 'grad_norm': 1376.0, 'learning_rate': 0.00029959999999999996, 'epoch': 0.14}
  2%|▏         | 107/5000 [28:46<19:50:30, 14.60s/it]  2%|▏         | 108/5000 [28:56<18:10:33, 13.38s/it]                                                     {'loss': 193.9501, 'grad_norm': 984.0, 'learning_rate': 0.0003024, 'epoch': 0.14}
  2%|▏         | 108/5000 [28:56<18:10:33, 13.38s/it]  2%|▏         | 109/5000 [29:10<18:23:36, 13.54s/it]                                                     {'loss': 304.486, 'grad_norm': 1960.0, 'learning_rate': 0.0003052, 'epoch': 0.14}
  2%|▏         | 109/5000 [29:10<18:23:36, 13.54s/it]  2%|▏         | 110/5000 [29:23<18:14:49, 13.43s/it]                                                     {'loss': 247.4935, 'grad_norm': 1512.0, 'learning_rate': 0.000308, 'epoch': 0.14}
  2%|▏         | 110/5000 [29:23<18:14:49, 13.43s/it]  2%|▏         | 111/5000 [29:41<19:53:04, 14.64s/it]                                                     {'loss': 216.8601, 'grad_norm': 780.0, 'learning_rate': 0.0003108, 'epoch': 0.14}
  2%|▏         | 111/5000 [29:41<19:53:04, 14.64s/it]  2%|▏         | 112/5000 [29:54<19:18:37, 14.22s/it]                                                     {'loss': 149.1404, 'grad_norm': 462.0, 'learning_rate': 0.0003136, 'epoch': 0.14}
  2%|▏         | 112/5000 [29:54<19:18:37, 14.22s/it]  2%|▏         | 113/5000 [30:13<21:06:56, 15.55s/it]                                                     {'loss': 200.0232, 'grad_norm': 700.0, 'learning_rate': 0.0003164, 'epoch': 0.14}
  2%|▏         | 113/5000 [30:13<21:06:56, 15.55s/it]  2%|▏         | 114/5000 [30:33<23:15:47, 17.14s/it]                                                     {'loss': 173.7126, 'grad_norm': 564.0, 'learning_rate': 0.0003192, 'epoch': 0.14}
  2%|▏         | 114/5000 [30:33<23:15:47, 17.14s/it]  2%|▏         | 115/5000 [30:45<20:50:20, 15.36s/it]                                                     {'loss': 208.4333, 'grad_norm': 592.0, 'learning_rate': 0.000322, 'epoch': 0.15}
  2%|▏         | 115/5000 [30:45<20:50:20, 15.36s/it]  2%|▏         | 116/5000 [31:08<23:57:40, 17.66s/it]                                                     {'loss': 189.7346, 'grad_norm': 552.0, 'learning_rate': 0.00032480000000000003, 'epoch': 0.15}
  2%|▏         | 116/5000 [31:08<23:57:40, 17.66s/it]  2%|▏         | 117/5000 [31:25<23:40:42, 17.46s/it]                                                     {'loss': 162.8902, 'grad_norm': 508.0, 'learning_rate': 0.0003276, 'epoch': 0.15}
  2%|▏         | 117/5000 [31:25<23:40:42, 17.46s/it]  2%|▏         | 118/5000 [31:36<21:14:41, 15.67s/it]                                                     {'loss': 206.625, 'grad_norm': 1200.0, 'learning_rate': 0.00033039999999999995, 'epoch': 0.15}
  2%|▏         | 118/5000 [31:36<21:14:41, 15.67s/it]  2%|▏         | 119/5000 [31:52<21:26:08, 15.81s/it]                                                     {'loss': 190.1773, 'grad_norm': 844.0, 'learning_rate': 0.00033319999999999997, 'epoch': 0.15}
  2%|▏         | 119/5000 [31:52<21:26:08, 15.81s/it]  2%|▏         | 120/5000 [32:05<20:02:36, 14.79s/it]                                                     {'loss': 159.8247, 'grad_norm': 440.0, 'learning_rate': 0.000336, 'epoch': 0.15}
  2%|▏         | 120/5000 [32:05<20:02:36, 14.79s/it]  2%|▏         | 121/5000 [32:23<21:19:02, 15.73s/it]                                                     {'loss': 210.6521, 'grad_norm': 2080.0, 'learning_rate': 0.0003388, 'epoch': 0.15}
  2%|▏         | 121/5000 [32:23<21:19:02, 15.73s/it]  2%|▏         | 122/5000 [32:45<24:07:17, 17.80s/it]                                                     {'loss': 199.5931, 'grad_norm': 676.0, 'learning_rate': 0.0003416, 'epoch': 0.15}
  2%|▏         | 122/5000 [32:45<24:07:17, 17.80s/it]  2%|▏         | 123/5000 [32:58<21:55:26, 16.18s/it]                                                     {'loss': 190.185, 'grad_norm': 3040.0, 'learning_rate': 0.00034439999999999997, 'epoch': 0.16}
  2%|▏         | 123/5000 [32:58<21:55:26, 16.18s/it]  2%|▏         | 124/5000 [33:11<20:36:26, 15.21s/it]                                                     {'loss': 229.6516, 'grad_norm': 9152.0, 'learning_rate': 0.0003472, 'epoch': 0.16}
  2%|▏         | 124/5000 [33:11<20:36:26, 15.21s/it]  2%|▎         | 125/5000 [33:24<19:45:11, 14.59s/it]                                                     {'loss': 213.3076, 'grad_norm': 2192.0, 'learning_rate': 0.00035, 'epoch': 0.16}
  2%|▎         | 125/5000 [33:24<19:45:11, 14.59s/it]  3%|▎         | 126/5000 [33:38<19:46:53, 14.61s/it]                                                     {'loss': 216.4536, 'grad_norm': 1128.0, 'learning_rate': 0.0003528, 'epoch': 0.16}
  3%|▎         | 126/5000 [33:38<19:46:53, 14.61s/it]  3%|▎         | 127/5000 [33:58<21:47:20, 16.10s/it]                                                     {'loss': 283.3956, 'grad_norm': 2544.0, 'learning_rate': 0.0003556, 'epoch': 0.16}
  3%|▎         | 127/5000 [33:58<21:47:20, 16.10s/it]  3%|▎         | 128/5000 [34:19<23:46:30, 17.57s/it]                                                     {'loss': 227.1002, 'grad_norm': 584.0, 'learning_rate': 0.0003584, 'epoch': 0.16}
  3%|▎         | 128/5000 [34:19<23:46:30, 17.57s/it]  3%|▎         | 129/5000 [34:31<21:25:29, 15.83s/it]                                                     {'loss': 245.0932, 'grad_norm': 908.0, 'learning_rate': 0.0003612, 'epoch': 0.16}
  3%|▎         | 129/5000 [34:31<21:25:29, 15.83s/it]  3%|▎         | 130/5000 [34:47<21:26:57, 15.86s/it]                                                     {'loss': 239.5199, 'grad_norm': 580.0, 'learning_rate': 0.000364, 'epoch': 0.17}
  3%|▎         | 130/5000 [34:47<21:26:57, 15.86s/it]  3%|▎         | 131/5000 [34:56<18:50:41, 13.93s/it]                                                     {'loss': 184.1771, 'grad_norm': 442.0, 'learning_rate': 0.0003668, 'epoch': 0.17}
  3%|▎         | 131/5000 [34:56<18:50:41, 13.93s/it]  3%|▎         | 132/5000 [35:11<19:19:50, 14.30s/it]                                                     {'loss': 225.6835, 'grad_norm': 1056.0, 'learning_rate': 0.00036960000000000004, 'epoch': 0.17}
  3%|▎         | 132/5000 [35:11<19:19:50, 14.30s/it]  3%|▎         | 133/5000 [35:23<18:26:09, 13.64s/it]                                                     {'loss': 144.0027, 'grad_norm': 362.0, 'learning_rate': 0.0003724, 'epoch': 0.17}
  3%|▎         | 133/5000 [35:23<18:26:09, 13.64s/it]  3%|▎         | 134/5000 [35:45<21:31:04, 15.92s/it]                                                     {'loss': 228.3735, 'grad_norm': 704.0, 'learning_rate': 0.0003752, 'epoch': 0.17}
  3%|▎         | 134/5000 [35:45<21:31:04, 15.92s/it]  3%|▎         | 135/5000 [35:55<19:12:46, 14.22s/it]                                                     {'loss': 166.918, 'grad_norm': 820.0, 'learning_rate': 0.000378, 'epoch': 0.17}
  3%|▎         | 135/5000 [35:55<19:12:46, 14.22s/it]  3%|▎         | 136/5000 [36:07<18:13:30, 13.49s/it]                                                     {'loss': 242.245, 'grad_norm': 584.0, 'learning_rate': 0.00038080000000000004, 'epoch': 0.17}
  3%|▎         | 136/5000 [36:07<18:13:30, 13.49s/it]  3%|▎         | 137/5000 [36:24<19:49:25, 14.68s/it]                                                     {'loss': 183.651, 'grad_norm': 1160.0, 'learning_rate': 0.0003836, 'epoch': 0.17}
  3%|▎         | 137/5000 [36:24<19:49:25, 14.68s/it]  3%|▎         | 138/5000 [36:37<19:16:03, 14.27s/it]                                                     {'loss': 276.3408, 'grad_norm': 3264.0, 'learning_rate': 0.0003864, 'epoch': 0.18}
  3%|▎         | 138/5000 [36:37<19:16:03, 14.27s/it]  3%|▎         | 139/5000 [36:52<19:36:21, 14.52s/it]                                                     {'loss': 270.266, 'grad_norm': 2240.0, 'learning_rate': 0.00038920000000000003, 'epoch': 0.18}
  3%|▎         | 139/5000 [36:52<19:36:21, 14.52s/it]  3%|▎         | 140/5000 [37:05<18:50:49, 13.96s/it]                                                     {'loss': 227.1761, 'grad_norm': 2096.0, 'learning_rate': 0.00039200000000000004, 'epoch': 0.18}
  3%|▎         | 140/5000 [37:05<18:50:49, 13.96s/it]  3%|▎         | 141/5000 [37:17<17:52:08, 13.24s/it]                                                     {'loss': 176.7151, 'grad_norm': 484.0, 'learning_rate': 0.00039479999999999995, 'epoch': 0.18}
  3%|▎         | 141/5000 [37:17<17:52:08, 13.24s/it]  3%|▎         | 142/5000 [37:34<19:33:22, 14.49s/it]                                                     {'loss': 233.0149, 'grad_norm': 1440.0, 'learning_rate': 0.00039759999999999996, 'epoch': 0.18}
  3%|▎         | 142/5000 [37:34<19:33:22, 14.49s/it]  3%|▎         | 143/5000 [37:45<18:09:24, 13.46s/it]                                                     {'loss': 149.3431, 'grad_norm': 462.0, 'learning_rate': 0.0004004, 'epoch': 0.18}
  3%|▎         | 143/5000 [37:45<18:09:24, 13.46s/it]  3%|▎         | 144/5000 [37:58<18:03:02, 13.38s/it]                                                     {'loss': 200.3452, 'grad_norm': 764.0, 'learning_rate': 0.0004032, 'epoch': 0.18}
  3%|▎         | 144/5000 [37:58<18:03:02, 13.38s/it]  3%|▎         | 145/5000 [38:09<16:58:34, 12.59s/it]                                                     {'loss': 181.6165, 'grad_norm': 1176.0, 'learning_rate': 0.00040599999999999995, 'epoch': 0.18}
  3%|▎         | 145/5000 [38:09<16:58:34, 12.59s/it]  3%|▎         | 146/5000 [38:22<16:54:29, 12.54s/it]                                                     {'loss': 177.8322, 'grad_norm': 1096.0, 'learning_rate': 0.00040879999999999996, 'epoch': 0.19}
  3%|▎         | 146/5000 [38:22<16:54:29, 12.54s/it]  3%|▎         | 147/5000 [38:36<17:43:08, 13.14s/it]                                                     {'loss': 263.4127, 'grad_norm': 10496.0, 'learning_rate': 0.0004116, 'epoch': 0.19}
  3%|▎         | 147/5000 [38:36<17:43:08, 13.14s/it]  3%|▎         | 148/5000 [38:46<16:32:46, 12.28s/it]                                                     {'loss': 183.1047, 'grad_norm': 616.0, 'learning_rate': 0.0004144, 'epoch': 0.19}
  3%|▎         | 148/5000 [38:46<16:32:46, 12.28s/it]  3%|▎         | 149/5000 [39:01<17:42:04, 13.14s/it]                                                     {'loss': 143.4972, 'grad_norm': 1432.0, 'learning_rate': 0.0004172, 'epoch': 0.19}
  3%|▎         | 149/5000 [39:01<17:42:04, 13.14s/it]  3%|▎         | 150/5000 [39:23<20:54:45, 15.52s/it]                                                     {'loss': 162.1413, 'grad_norm': 1904.0, 'learning_rate': 0.00041999999999999996, 'epoch': 0.19}
  3%|▎         | 150/5000 [39:23<20:54:45, 15.52s/it]  3%|▎         | 151/5000 [39:42<22:34:15, 16.76s/it]                                                     {'loss': 116.9874, 'grad_norm': 980.0, 'learning_rate': 0.0004228, 'epoch': 0.19}
  3%|▎         | 151/5000 [39:42<22:34:15, 16.76s/it]  3%|▎         | 152/5000 [39:55<20:47:33, 15.44s/it]                                                     {'loss': 128.7959, 'grad_norm': 298.0, 'learning_rate': 0.0004256, 'epoch': 0.19}
  3%|▎         | 152/5000 [39:55<20:47:33, 15.44s/it]  3%|▎         | 153/5000 [40:12<21:36:10, 16.05s/it]                                                     {'loss': 182.2943, 'grad_norm': 2272.0, 'learning_rate': 0.0004284, 'epoch': 0.19}
  3%|▎         | 153/5000 [40:12<21:36:10, 16.05s/it]  3%|▎         | 154/5000 [40:25<20:12:41, 15.01s/it]                                                     {'loss': 117.4202, 'grad_norm': 5472.0, 'learning_rate': 0.00043119999999999996, 'epoch': 0.2}
  3%|▎         | 154/5000 [40:25<20:12:41, 15.01s/it]  3%|▎         | 155/5000 [40:42<21:14:50, 15.79s/it]                                                     {'loss': 80.5358, 'grad_norm': 508.0, 'learning_rate': 0.000434, 'epoch': 0.2}
  3%|▎         | 155/5000 [40:42<21:14:50, 15.79s/it]  3%|▎         | 156/5000 [40:54<19:36:11, 14.57s/it]                                                     {'loss': 159.7039, 'grad_norm': 728.0, 'learning_rate': 0.0004368, 'epoch': 0.2}
  3%|▎         | 156/5000 [40:54<19:36:11, 14.57s/it]  3%|▎         | 157/5000 [41:14<21:57:18, 16.32s/it]                                                     {'loss': 156.3747, 'grad_norm': 31488.0, 'learning_rate': 0.0004396, 'epoch': 0.2}
  3%|▎         | 157/5000 [41:14<21:57:18, 16.32s/it]  3%|▎         | 158/5000 [41:29<21:11:27, 15.76s/it]                                                     {'loss': 484.6899, 'grad_norm': 8576.0, 'learning_rate': 0.0004424, 'epoch': 0.2}
  3%|▎         | 158/5000 [41:29<21:11:27, 15.76s/it]  3%|▎         | 159/5000 [41:44<20:47:35, 15.46s/it]                                                     {'loss': 181.4797, 'grad_norm': 2208.0, 'learning_rate': 0.0004452, 'epoch': 0.2}
  3%|▎         | 159/5000 [41:44<20:47:35, 15.46s/it]  3%|▎         | 160/5000 [41:57<19:58:07, 14.85s/it]                                                     {'loss': 222.8947, 'grad_norm': 7488.0, 'learning_rate': 0.000448, 'epoch': 0.2}
  3%|▎         | 160/5000 [41:57<19:58:07, 14.85s/it]  3%|▎         | 161/5000 [42:18<22:24:50, 16.68s/it]                                                     {'loss': 197.1564, 'grad_norm': 4192.0, 'learning_rate': 0.0004508, 'epoch': 0.2}
  3%|▎         | 161/5000 [42:18<22:24:50, 16.68s/it]  3%|▎         | 162/5000 [42:29<20:04:53, 14.94s/it]                                                     {'loss': 214.7794, 'grad_norm': 5184.0, 'learning_rate': 0.0004536, 'epoch': 0.21}
  3%|▎         | 162/5000 [42:29<20:04:53, 14.94s/it]  3%|▎         | 163/5000 [42:41<19:04:13, 14.19s/it]                                                     {'loss': 146.2459, 'grad_norm': 1112.0, 'learning_rate': 0.00045640000000000003, 'epoch': 0.21}
  3%|▎         | 163/5000 [42:41<19:04:13, 14.19s/it]  3%|▎         | 164/5000 [42:53<18:00:47, 13.41s/it]                                                     {'loss': 195.0884, 'grad_norm': 1224.0, 'learning_rate': 0.0004592, 'epoch': 0.21}
  3%|▎         | 164/5000 [42:53<18:00:47, 13.41s/it]  3%|▎         | 165/5000 [43:05<17:42:16, 13.18s/it]                                                     {'loss': 356.6307, 'grad_norm': 3024.0, 'learning_rate': 0.000462, 'epoch': 0.21}
  3%|▎         | 165/5000 [43:05<17:42:16, 13.18s/it]  3%|▎         | 166/5000 [43:29<21:56:14, 16.34s/it]                                                     {'loss': 437.1557, 'grad_norm': 2736.0, 'learning_rate': 0.0004648, 'epoch': 0.21}
  3%|▎         | 166/5000 [43:29<21:56:14, 16.34s/it]  3%|▎         | 167/5000 [43:42<20:22:53, 15.18s/it]                                                     {'loss': 209.3892, 'grad_norm': 2544.0, 'learning_rate': 0.00046760000000000004, 'epoch': 0.21}
  3%|▎         | 167/5000 [43:42<20:22:53, 15.18s/it]  3%|▎         | 168/5000 [43:53<18:40:10, 13.91s/it]                                                     {'loss': 188.7611, 'grad_norm': 5984.0, 'learning_rate': 0.00047040000000000005, 'epoch': 0.21}
  3%|▎         | 168/5000 [43:53<18:40:10, 13.91s/it]  3%|▎         | 169/5000 [44:05<18:10:02, 13.54s/it]                                                     {'loss': 257.2363, 'grad_norm': 2624.0, 'learning_rate': 0.0004732, 'epoch': 0.21}
  3%|▎         | 169/5000 [44:05<18:10:02, 13.54s/it]  3%|▎         | 170/5000 [44:19<18:08:34, 13.52s/it]                                                     {'loss': 180.8896, 'grad_norm': 10432.0, 'learning_rate': 0.000476, 'epoch': 0.22}
  3%|▎         | 170/5000 [44:19<18:08:34, 13.52s/it]  3%|▎         | 171/5000 [44:30<17:18:38, 12.91s/it]                                                     {'loss': 252.9923, 'grad_norm': 1648.0, 'learning_rate': 0.00047880000000000004, 'epoch': 0.22}
  3%|▎         | 171/5000 [44:30<17:18:38, 12.91s/it]  3%|▎         | 172/5000 [44:47<18:54:45, 14.10s/it]                                                     {'loss': 190.8834, 'grad_norm': 3264.0, 'learning_rate': 0.00048159999999999994, 'epoch': 0.22}
  3%|▎         | 172/5000 [44:47<18:54:45, 14.10s/it]  3%|▎         | 173/5000 [44:59<18:07:25, 13.52s/it]                                                     {'loss': 176.5257, 'grad_norm': 1488.0, 'learning_rate': 0.00048439999999999996, 'epoch': 0.22}
  3%|▎         | 173/5000 [44:59<18:07:25, 13.52s/it]  3%|▎         | 174/5000 [45:13<18:19:21, 13.67s/it]                                                     {'loss': 153.4971, 'grad_norm': 636.0, 'learning_rate': 0.00048719999999999997, 'epoch': 0.22}
  3%|▎         | 174/5000 [45:13<18:19:21, 13.67s/it]  4%|▎         | 175/5000 [45:28<18:52:42, 14.09s/it]                                                     {'loss': 160.3682, 'grad_norm': 728.0, 'learning_rate': 0.00049, 'epoch': 0.22}
  4%|▎         | 175/5000 [45:28<18:52:42, 14.09s/it]  4%|▎         | 176/5000 [45:41<18:08:15, 13.54s/it]                                                     {'loss': 248.1929, 'grad_norm': 2432.0, 'learning_rate': 0.0004927999999999999, 'epoch': 0.22}
  4%|▎         | 176/5000 [45:41<18:08:15, 13.54s/it]  4%|▎         | 177/5000 [45:55<18:32:10, 13.84s/it]                                                     {'loss': 319.8959, 'grad_norm': 1360.0, 'learning_rate': 0.0004956, 'epoch': 0.22}
  4%|▎         | 177/5000 [45:55<18:32:10, 13.84s/it]  4%|▎         | 178/5000 [46:09<18:40:55, 13.95s/it]                                                     {'loss': 305.8503, 'grad_norm': 2064.0, 'learning_rate': 0.0004984, 'epoch': 0.23}
  4%|▎         | 178/5000 [46:09<18:40:55, 13.95s/it]  4%|▎         | 179/5000 [46:24<18:50:41, 14.07s/it]                                                     {'loss': 313.2853, 'grad_norm': 3264.0, 'learning_rate': 0.0005011999999999999, 'epoch': 0.23}
  4%|▎         | 179/5000 [46:24<18:50:41, 14.07s/it]  4%|▎         | 180/5000 [46:35<17:47:51, 13.29s/it]                                                     {'loss': 341.5287, 'grad_norm': 2528.0, 'learning_rate': 0.000504, 'epoch': 0.23}
  4%|▎         | 180/5000 [46:35<17:47:51, 13.29s/it]  4%|▎         | 181/5000 [46:50<18:25:44, 13.77s/it]                                                     {'loss': 275.5112, 'grad_norm': 4928.0, 'learning_rate': 0.0005068, 'epoch': 0.23}
  4%|▎         | 181/5000 [46:50<18:25:44, 13.77s/it]  4%|▎         | 182/5000 [47:03<17:58:25, 13.43s/it]                                                     {'loss': 167.4116, 'grad_norm': 604.0, 'learning_rate': 0.0005096, 'epoch': 0.23}
  4%|▎         | 182/5000 [47:03<17:58:25, 13.43s/it]  4%|▎         | 183/5000 [47:14<16:54:38, 12.64s/it]                                                     {'loss': 365.4493, 'grad_norm': 1336.0, 'learning_rate': 0.0005124, 'epoch': 0.23}
  4%|▎         | 183/5000 [47:14<16:54:38, 12.64s/it]  4%|▎         | 184/5000 [47:24<16:01:22, 11.98s/it]                                                     {'loss': 272.9206, 'grad_norm': 1552.0, 'learning_rate': 0.0005152, 'epoch': 0.23}
  4%|▎         | 184/5000 [47:24<16:01:22, 11.98s/it]  4%|▎         | 185/5000 [47:41<18:13:36, 13.63s/it]                                                     {'loss': 296.3676, 'grad_norm': 6016.0, 'learning_rate': 0.000518, 'epoch': 0.23}
  4%|▎         | 185/5000 [47:41<18:13:36, 13.63s/it]  4%|▎         | 186/5000 [47:56<18:32:04, 13.86s/it]                                                     {'loss': 261.2029, 'grad_norm': 2720.0, 'learning_rate': 0.0005208, 'epoch': 0.24}
  4%|▎         | 186/5000 [47:56<18:32:04, 13.86s/it]  4%|▎         | 187/5000 [48:10<18:36:27, 13.92s/it]                                                     {'loss': 244.8363, 'grad_norm': 2256.0, 'learning_rate': 0.0005236, 'epoch': 0.24}
  4%|▎         | 187/5000 [48:10<18:36:27, 13.92s/it]  4%|▍         | 188/5000 [48:23<18:05:42, 13.54s/it]                                                     {'loss': 360.7567, 'grad_norm': 1272.0, 'learning_rate': 0.0005264, 'epoch': 0.24}
  4%|▍         | 188/5000 [48:23<18:05:42, 13.54s/it]  4%|▍         | 189/5000 [48:34<17:13:07, 12.88s/it]                                                     {'loss': 282.9423, 'grad_norm': 1056.0, 'learning_rate': 0.0005292, 'epoch': 0.24}
  4%|▍         | 189/5000 [48:34<17:13:07, 12.88s/it]  4%|▍         | 190/5000 [48:47<17:10:47, 12.86s/it]                                                     {'loss': 277.4245, 'grad_norm': 716.0, 'learning_rate': 0.000532, 'epoch': 0.24}
  4%|▍         | 190/5000 [48:47<17:10:47, 12.86s/it]  4%|▍         | 191/5000 [49:01<17:56:13, 13.43s/it]                                                     {'loss': 195.0597, 'grad_norm': 848.0, 'learning_rate': 0.0005348, 'epoch': 0.24}
  4%|▍         | 191/5000 [49:01<17:56:13, 13.43s/it]  4%|▍         | 192/5000 [49:15<17:54:13, 13.41s/it]                                                     {'loss': 259.4298, 'grad_norm': 656.0, 'learning_rate': 0.0005376000000000001, 'epoch': 0.24}
  4%|▍         | 192/5000 [49:15<17:54:13, 13.41s/it]  4%|▍         | 193/5000 [49:32<19:24:19, 14.53s/it]                                                     {'loss': 306.811, 'grad_norm': 4064.0, 'learning_rate': 0.0005404, 'epoch': 0.25}
  4%|▍         | 193/5000 [49:32<19:24:19, 14.53s/it]  4%|▍         | 194/5000 [49:44<18:25:24, 13.80s/it]                                                     {'loss': 263.4623, 'grad_norm': 4896.0, 'learning_rate': 0.0005432, 'epoch': 0.25}
  4%|▍         | 194/5000 [49:44<18:25:24, 13.80s/it]  4%|▍         | 195/5000 [49:57<18:10:03, 13.61s/it]                                                     {'loss': 250.1147, 'grad_norm': 1016.0, 'learning_rate': 0.000546, 'epoch': 0.25}
  4%|▍         | 195/5000 [49:57<18:10:03, 13.61s/it]  4%|▍         | 196/5000 [50:10<17:48:15, 13.34s/it]                                                     {'loss': 326.2816, 'grad_norm': 5888.0, 'learning_rate': 0.0005488, 'epoch': 0.25}
  4%|▍         | 196/5000 [50:10<17:48:15, 13.34s/it]  4%|▍         | 197/5000 [50:24<18:00:00, 13.49s/it]                                                     {'loss': 324.9612, 'grad_norm': 1608.0, 'learning_rate': 0.0005516000000000001, 'epoch': 0.25}
  4%|▍         | 197/5000 [50:24<18:00:00, 13.49s/it]  4%|▍         | 198/5000 [50:38<18:05:48, 13.57s/it]                                                     {'loss': 319.0828, 'grad_norm': 2208.0, 'learning_rate': 0.0005544, 'epoch': 0.25}
  4%|▍         | 198/5000 [50:38<18:05:48, 13.57s/it]  4%|▍         | 199/5000 [50:52<18:28:24, 13.85s/it]                                                     {'loss': 253.833, 'grad_norm': 1504.0, 'learning_rate': 0.0005572, 'epoch': 0.25}
  4%|▍         | 199/5000 [50:52<18:28:24, 13.85s/it]  4%|▍         | 200/5000 [51:05<18:08:24, 13.61s/it]                                                     {'loss': 290.7089, 'grad_norm': 916.0, 'learning_rate': 0.0005600000000000001, 'epoch': 0.25}
  4%|▍         | 200/5000 [51:05<18:08:24, 13.61s/it]  4%|▍         | 201/5000 [51:28<21:46:30, 16.33s/it]                                                     {'loss': 273.5791, 'grad_norm': 804.0, 'learning_rate': 0.0005628, 'epoch': 0.26}
  4%|▍         | 201/5000 [51:28<21:46:30, 16.33s/it]  4%|▍         | 202/5000 [51:38<19:22:46, 14.54s/it]                                                     {'loss': 239.1608, 'grad_norm': 4896.0, 'learning_rate': 0.0005656, 'epoch': 0.26}
  4%|▍         | 202/5000 [51:38<19:22:46, 14.54s/it]  4%|▍         | 203/5000 [51:51<18:52:23, 14.16s/it]                                                     {'loss': 227.1683, 'grad_norm': 1136.0, 'learning_rate': 0.0005684, 'epoch': 0.26}
  4%|▍         | 203/5000 [51:51<18:52:23, 14.16s/it]  4%|▍         | 204/5000 [52:04<18:07:46, 13.61s/it]                                                     {'loss': 219.6333, 'grad_norm': 1192.0, 'learning_rate': 0.0005712, 'epoch': 0.26}
  4%|▍         | 204/5000 [52:04<18:07:46, 13.61s/it]  4%|▍         | 205/5000 [52:17<17:49:03, 13.38s/it]                                                     {'loss': 205.532, 'grad_norm': 896.0, 'learning_rate': 0.000574, 'epoch': 0.26}
  4%|▍         | 205/5000 [52:17<17:49:03, 13.38s/it]  4%|▍         | 206/5000 [52:29<17:29:11, 13.13s/it]                                                     {'loss': 375.232, 'grad_norm': 3680.0, 'learning_rate': 0.0005767999999999999, 'epoch': 0.26}
  4%|▍         | 206/5000 [52:29<17:29:11, 13.13s/it]  4%|▍         | 207/5000 [52:42<17:28:18, 13.12s/it]                                                     {'loss': 266.2411, 'grad_norm': 1640.0, 'learning_rate': 0.0005796, 'epoch': 0.26}
  4%|▍         | 207/5000 [52:42<17:28:18, 13.12s/it]  4%|▍         | 208/5000 [52:56<17:43:02, 13.31s/it]                                                     {'loss': 266.6333, 'grad_norm': 1064.0, 'learning_rate': 0.0005824, 'epoch': 0.26}
  4%|▍         | 208/5000 [52:56<17:43:02, 13.31s/it]  4%|▍         | 209/5000 [53:09<17:39:58, 13.27s/it]                                                     {'loss': 303.0273, 'grad_norm': 4352.0, 'learning_rate': 0.0005852, 'epoch': 0.27}
  4%|▍         | 209/5000 [53:09<17:39:58, 13.27s/it]  4%|▍         | 210/5000 [53:23<17:46:33, 13.36s/it]                                                     {'loss': 296.5331, 'grad_norm': 1696.0, 'learning_rate': 0.000588, 'epoch': 0.27}
  4%|▍         | 210/5000 [53:23<17:46:33, 13.36s/it]  4%|▍         | 211/5000 [53:34<16:55:21, 12.72s/it]                                                     {'loss': 200.1023, 'grad_norm': 708.0, 'learning_rate': 0.0005907999999999999, 'epoch': 0.27}
  4%|▍         | 211/5000 [53:34<16:55:21, 12.72s/it]  4%|▍         | 212/5000 [53:50<18:06:09, 13.61s/it]                                                     {'loss': 250.1305, 'grad_norm': 6720.0, 'learning_rate': 0.0005936, 'epoch': 0.27}
  4%|▍         | 212/5000 [53:50<18:06:09, 13.61s/it]  4%|▍         | 213/5000 [54:03<18:03:43, 13.58s/it]                                                     {'loss': 307.5532, 'grad_norm': 11200.0, 'learning_rate': 0.0005964, 'epoch': 0.27}
  4%|▍         | 213/5000 [54:03<18:03:43, 13.58s/it]  4%|▍         | 214/5000 [54:23<20:22:41, 15.33s/it]                                                     {'loss': 232.8055, 'grad_norm': 3664.0, 'learning_rate': 0.0005991999999999999, 'epoch': 0.27}
  4%|▍         | 214/5000 [54:23<20:22:41, 15.33s/it]  4%|▍         | 215/5000 [54:42<22:00:27, 16.56s/it]                                                     {'loss': 418.4493, 'grad_norm': 5728.0, 'learning_rate': 0.000602, 'epoch': 0.27}
  4%|▍         | 215/5000 [54:42<22:00:27, 16.56s/it]  4%|▍         | 216/5000 [54:57<21:28:27, 16.16s/it]                                                     {'loss': 369.3555, 'grad_norm': 16000.0, 'learning_rate': 0.0006048, 'epoch': 0.27}
  4%|▍         | 216/5000 [54:57<21:28:27, 16.16s/it]  4%|▍         | 217/5000 [55:12<20:44:21, 15.61s/it]                                                     {'loss': 464.3272, 'grad_norm': 3040.0, 'learning_rate': 0.0006076, 'epoch': 0.28}
  4%|▍         | 217/5000 [55:12<20:44:21, 15.61s/it]  4%|▍         | 218/5000 [55:23<19:02:53, 14.34s/it]                                                     {'loss': 400.8837, 'grad_norm': 1720.0, 'learning_rate': 0.0006104, 'epoch': 0.28}
  4%|▍         | 218/5000 [55:23<19:02:53, 14.34s/it]  4%|▍         | 219/5000 [55:38<19:20:18, 14.56s/it]                                                     {'loss': 440.7405, 'grad_norm': 4640.0, 'learning_rate': 0.0006131999999999999, 'epoch': 0.28}
  4%|▍         | 219/5000 [55:38<19:20:18, 14.56s/it]  4%|▍         | 220/5000 [55:52<19:12:12, 14.46s/it]                                                     {'loss': 379.4585, 'grad_norm': 3808.0, 'learning_rate': 0.000616, 'epoch': 0.28}
  4%|▍         | 220/5000 [55:52<19:12:12, 14.46s/it]  4%|▍         | 221/5000 [56:05<18:20:10, 13.81s/it]                                                     {'loss': 477.063, 'grad_norm': 6976.0, 'learning_rate': 0.0006188, 'epoch': 0.28}
  4%|▍         | 221/5000 [56:05<18:20:10, 13.81s/it]  4%|▍         | 222/5000 [56:20<19:07:22, 14.41s/it]                                                     {'loss': 399.3754, 'grad_norm': 3152.0, 'learning_rate': 0.0006216, 'epoch': 0.28}
  4%|▍         | 222/5000 [56:20<19:07:22, 14.41s/it]  4%|▍         | 223/5000 [56:43<22:26:20, 16.91s/it]                                                     {'loss': 351.1951, 'grad_norm': 1152.0, 'learning_rate': 0.0006244, 'epoch': 0.28}
  4%|▍         | 223/5000 [56:43<22:26:20, 16.91s/it]  4%|▍         | 224/5000 [57:05<24:26:21, 18.42s/it]                                                     {'loss': 359.3661, 'grad_norm': 16640.0, 'learning_rate': 0.0006272, 'epoch': 0.28}
  4%|▍         | 224/5000 [57:05<24:26:21, 18.42s/it]  4%|▍         | 225/5000 [57:19<22:47:41, 17.19s/it]                                                     {'loss': 518.7619, 'grad_norm': 2848.0, 'learning_rate': 0.00063, 'epoch': 0.29}
  4%|▍         | 225/5000 [57:19<22:47:41, 17.19s/it]  5%|▍         | 226/5000 [57:36<22:40:40, 17.10s/it]                                                     {'loss': 541.7475, 'grad_norm': 6496.0, 'learning_rate': 0.0006328, 'epoch': 0.29}
  5%|▍         | 226/5000 [57:36<22:40:40, 17.10s/it]  5%|▍         | 227/5000 [57:48<20:29:06, 15.45s/it]                                                     {'loss': 483.0535, 'grad_norm': 1968.0, 'learning_rate': 0.0006356, 'epoch': 0.29}
  5%|▍         | 227/5000 [57:48<20:29:06, 15.45s/it]  5%|▍         | 228/5000 [58:02<20:02:30, 15.12s/it]                                                     {'loss': 519.4169, 'grad_norm': 2096.0, 'learning_rate': 0.0006384, 'epoch': 0.29}
  5%|▍         | 228/5000 [58:02<20:02:30, 15.12s/it]  5%|▍         | 229/5000 [58:20<20:59:41, 15.84s/it]                                                     {'loss': 386.3451, 'grad_norm': 1856.0, 'learning_rate': 0.0006412, 'epoch': 0.29}
  5%|▍         | 229/5000 [58:20<20:59:41, 15.84s/it]  5%|▍         | 230/5000 [58:37<21:25:36, 16.17s/it]                                                     {'loss': 416.5068, 'grad_norm': 3936.0, 'learning_rate': 0.000644, 'epoch': 0.29}
  5%|▍         | 230/5000 [58:37<21:25:36, 16.17s/it]  5%|▍         | 231/5000 [58:50<20:29:30, 15.47s/it]                                                     {'loss': 556.4821, 'grad_norm': 1272.0, 'learning_rate': 0.0006468, 'epoch': 0.29}
  5%|▍         | 231/5000 [58:50<20:29:30, 15.47s/it]  5%|▍         | 232/5000 [59:02<19:06:58, 14.43s/it]                                                     {'loss': 391.4326, 'grad_norm': 27904.0, 'learning_rate': 0.0006496000000000001, 'epoch': 0.29}
  5%|▍         | 232/5000 [59:02<19:06:58, 14.43s/it]  5%|▍         | 233/5000 [59:19<20:05:24, 15.17s/it]                                                     {'loss': 396.4392, 'grad_norm': 1544.0, 'learning_rate': 0.0006524, 'epoch': 0.3}
  5%|▍         | 233/5000 [59:19<20:05:24, 15.17s/it]  5%|▍         | 234/5000 [59:30<18:19:44, 13.84s/it]                                                     {'loss': 442.0155, 'grad_norm': 1056.0, 'learning_rate': 0.0006552, 'epoch': 0.3}
  5%|▍         | 234/5000 [59:30<18:19:44, 13.84s/it]  5%|▍         | 235/5000 [59:50<20:45:32, 15.68s/it]                                                     {'loss': 347.9839, 'grad_norm': 1656.0, 'learning_rate': 0.000658, 'epoch': 0.3}
  5%|▍         | 235/5000 [59:50<20:45:32, 15.68s/it]  5%|▍         | 236/5000 [1:00:06<20:58:09, 15.85s/it]                                                       {'loss': 743.0385, 'grad_norm': 8640.0, 'learning_rate': 0.0006607999999999999, 'epoch': 0.3}
  5%|▍         | 236/5000 [1:00:06<20:58:09, 15.85s/it]  5%|▍         | 237/5000 [1:00:25<21:56:28, 16.58s/it]                                                       {'loss': 353.2093, 'grad_norm': 1080.0, 'learning_rate': 0.0006636, 'epoch': 0.3}
  5%|▍         | 237/5000 [1:00:25<21:56:28, 16.58s/it]  5%|▍         | 238/5000 [1:00:37<20:05:12, 15.19s/it]                                                       {'loss': 349.4969, 'grad_norm': 924.0, 'learning_rate': 0.0006663999999999999, 'epoch': 0.3}
  5%|▍         | 238/5000 [1:00:37<20:05:12, 15.19s/it]  5%|▍         | 239/5000 [1:00:51<19:35:49, 14.82s/it]                                                       {'loss': 410.5454, 'grad_norm': 3088.0, 'learning_rate': 0.0006692, 'epoch': 0.3}
  5%|▍         | 239/5000 [1:00:51<19:35:49, 14.82s/it]  5%|▍         | 240/5000 [1:01:12<22:24:09, 16.94s/it]                                                       {'loss': 299.3331, 'grad_norm': 1440.0, 'learning_rate': 0.000672, 'epoch': 0.3}
  5%|▍         | 240/5000 [1:01:12<22:24:09, 16.94s/it]  5%|▍         | 241/5000 [1:01:30<22:37:02, 17.11s/it]                                                       {'loss': 352.8484, 'grad_norm': 3136.0, 'learning_rate': 0.0006747999999999999, 'epoch': 0.31}
  5%|▍         | 241/5000 [1:01:30<22:37:02, 17.11s/it]  5%|▍         | 242/5000 [1:01:41<20:19:19, 15.38s/it]                                                       {'loss': 416.439, 'grad_norm': 2144.0, 'learning_rate': 0.0006776, 'epoch': 0.31}
  5%|▍         | 242/5000 [1:01:41<20:19:19, 15.38s/it]  5%|▍         | 243/5000 [1:01:55<19:49:11, 15.00s/it]                                                       {'loss': 301.3743, 'grad_norm': 1160.0, 'learning_rate': 0.0006804, 'epoch': 0.31}
  5%|▍         | 243/5000 [1:01:55<19:49:11, 15.00s/it]  5%|▍         | 244/5000 [1:02:11<20:11:12, 15.28s/it]                                                       {'loss': 372.6312, 'grad_norm': 7520.0, 'learning_rate': 0.0006832, 'epoch': 0.31}
  5%|▍         | 244/5000 [1:02:11<20:11:12, 15.28s/it]  5%|▍         | 245/5000 [1:02:36<24:05:14, 18.24s/it]                                                       {'loss': 329.4611, 'grad_norm': 2768.0, 'learning_rate': 0.000686, 'epoch': 0.31}
  5%|▍         | 245/5000 [1:02:36<24:05:14, 18.24s/it]  5%|▍         | 246/5000 [1:02:49<21:39:23, 16.40s/it]                                                       {'loss': 496.0877, 'grad_norm': 1944.0, 'learning_rate': 0.0006887999999999999, 'epoch': 0.31}
  5%|▍         | 246/5000 [1:02:49<21:39:23, 16.40s/it]  5%|▍         | 247/5000 [1:03:02<20:40:41, 15.66s/it]                                                       {'loss': 307.2422, 'grad_norm': 1136.0, 'learning_rate': 0.0006916, 'epoch': 0.31}
  5%|▍         | 247/5000 [1:03:02<20:40:41, 15.66s/it]  5%|▍         | 248/5000 [1:03:17<20:14:02, 15.33s/it]                                                       {'loss': 440.5599, 'grad_norm': 33280.0, 'learning_rate': 0.0006944, 'epoch': 0.31}
  5%|▍         | 248/5000 [1:03:17<20:14:02, 15.33s/it]  5%|▍         | 249/5000 [1:03:27<18:17:42, 13.86s/it]                                                       {'loss': 1193.6361, 'grad_norm': 23296.0, 'learning_rate': 0.0006972, 'epoch': 0.32}
  5%|▍         | 249/5000 [1:03:27<18:17:42, 13.86s/it]  5%|▌         | 250/5000 [1:03:41<18:03:46, 13.69s/it]                                                       {'loss': 342.0272, 'grad_norm': 1792.0, 'learning_rate': 0.0007, 'epoch': 0.32}
  5%|▌         | 250/5000 [1:03:41<18:03:46, 13.69s/it]  5%|▌         | 251/5000 [1:03:56<18:45:39, 14.22s/it]                                                       {'loss': 431.1017, 'grad_norm': 8704.0, 'learning_rate': 0.0006999999234490545, 'epoch': 0.32}
  5%|▌         | 251/5000 [1:03:56<18:45:39, 14.22s/it]  5%|▌         | 252/5000 [1:04:13<19:40:21, 14.92s/it]                                                       {'loss': 433.3514, 'grad_norm': 2624.0, 'learning_rate': 0.0006999996937962515, 'epoch': 0.32}
  5%|▌         | 252/5000 [1:04:13<19:40:21, 14.92s/it]  5%|▌         | 253/5000 [1:04:26<19:07:20, 14.50s/it]                                                       {'loss': 483.2668, 'grad_norm': 2064.0, 'learning_rate': 0.0006999993110416916, 'epoch': 0.32}
  5%|▌         | 253/5000 [1:04:26<19:07:20, 14.50s/it]  5%|▌         | 254/5000 [1:04:41<19:16:09, 14.62s/it]                                                       {'loss': 355.7288, 'grad_norm': 7104.0, 'learning_rate': 0.0006999987751855421, 'epoch': 0.32}
  5%|▌         | 254/5000 [1:04:41<19:16:09, 14.62s/it]  5%|▌         | 255/5000 [1:04:53<18:12:58, 13.82s/it]                                                       {'loss': 439.8206, 'grad_norm': 868.0, 'learning_rate': 0.0006999980862280375, 'epoch': 0.32}
  5%|▌         | 255/5000 [1:04:53<18:12:58, 13.82s/it]  5%|▌         | 256/5000 [1:05:04<17:04:12, 12.95s/it]                                                       {'loss': 510.1486, 'grad_norm': 972.0, 'learning_rate': 0.000699997244169479, 'epoch': 0.33}
  5%|▌         | 256/5000 [1:05:04<17:04:12, 12.95s/it]  5%|▌         | 257/5000 [1:05:15<16:07:19, 12.24s/it]                                                       {'loss': 412.7473, 'grad_norm': 840.0, 'learning_rate': 0.0006999962490102351, 'epoch': 0.33}
  5%|▌         | 257/5000 [1:05:15<16:07:19, 12.24s/it]  5%|▌         | 258/5000 [1:05:29<16:47:09, 12.74s/it]                                                       {'loss': 406.4348, 'grad_norm': 1344.0, 'learning_rate': 0.000699995100750741, 'epoch': 0.33}
  5%|▌         | 258/5000 [1:05:29<16:47:09, 12.74s/it]  5%|▌         | 259/5000 [1:05:41<16:37:29, 12.62s/it]                                                       {'loss': 394.6846, 'grad_norm': 864.0, 'learning_rate': 0.0006999937993914991, 'epoch': 0.33}
  5%|▌         | 259/5000 [1:05:41<16:37:29, 12.62s/it]  5%|▌         | 260/5000 [1:05:54<16:47:06, 12.75s/it]                                                       {'loss': 308.2407, 'grad_norm': 1160.0, 'learning_rate': 0.0006999923449330786, 'epoch': 0.33}
  5%|▌         | 260/5000 [1:05:54<16:47:06, 12.75s/it]  5%|▌         | 261/5000 [1:06:08<17:20:57, 13.18s/it]                                                       {'loss': 473.756, 'grad_norm': 2480.0, 'learning_rate': 0.0006999907373761157, 'epoch': 0.33}
  5%|▌         | 261/5000 [1:06:08<17:20:57, 13.18s/it]  5%|▌         | 262/5000 [1:06:27<19:47:15, 15.03s/it]                                                       {'loss': 398.7651, 'grad_norm': 2688.0, 'learning_rate': 0.0006999889767213138, 'epoch': 0.33}
  5%|▌         | 262/5000 [1:06:27<19:47:15, 15.03s/it]  5%|▌         | 263/5000 [1:06:52<23:38:12, 17.96s/it]                                                       {'loss': 310.8585, 'grad_norm': 792.0, 'learning_rate': 0.0006999870629694428, 'epoch': 0.33}
  5%|▌         | 263/5000 [1:06:52<23:38:12, 17.96s/it]  5%|▌         | 264/5000 [1:07:06<22:08:05, 16.83s/it]                                                       {'loss': 432.9428, 'grad_norm': 1304.0, 'learning_rate': 0.0006999849961213399, 'epoch': 0.34}
  5%|▌         | 264/5000 [1:07:06<22:08:05, 16.83s/it]  5%|▌         | 265/5000 [1:07:21<21:13:32, 16.14s/it]                                                       {'loss': 339.6249, 'grad_norm': 996.0, 'learning_rate': 0.0006999827761779093, 'epoch': 0.34}
  5%|▌         | 265/5000 [1:07:21<21:13:32, 16.14s/it]  5%|▌         | 266/5000 [1:07:34<19:59:54, 15.21s/it]                                                       {'loss': 378.2312, 'grad_norm': 15680.0, 'learning_rate': 0.000699980403140122, 'epoch': 0.34}
  5%|▌         | 266/5000 [1:07:34<19:59:54, 15.21s/it]  5%|▌         | 267/5000 [1:07:59<23:46:20, 18.08s/it]                                                       {'loss': 465.107, 'grad_norm': 29696.0, 'learning_rate': 0.0006999778770090161, 'epoch': 0.34}
  5%|▌         | 267/5000 [1:07:59<23:46:20, 18.08s/it]  5%|▌         | 268/5000 [1:08:13<22:10:03, 16.86s/it]                                                       {'loss': 412.6374, 'grad_norm': 3152.0, 'learning_rate': 0.0006999751977856967, 'epoch': 0.34}
  5%|▌         | 268/5000 [1:08:13<22:10:03, 16.86s/it]  5%|▌         | 269/5000 [1:08:28<21:25:56, 16.31s/it]                                                       {'loss': 410.6865, 'grad_norm': 2320.0, 'learning_rate': 0.0006999723654713357, 'epoch': 0.34}
  5%|▌         | 269/5000 [1:08:28<21:25:56, 16.31s/it]  5%|▌         | 270/5000 [1:08:39<19:17:22, 14.68s/it]                                                       {'loss': 494.1269, 'grad_norm': 1528.0, 'learning_rate': 0.0006999693800671719, 'epoch': 0.34}
  5%|▌         | 270/5000 [1:08:39<19:17:22, 14.68s/it]  5%|▌         | 271/5000 [1:08:52<18:47:40, 14.31s/it]                                                       {'loss': 386.1926, 'grad_norm': 2848.0, 'learning_rate': 0.0006999662415745114, 'epoch': 0.34}
  5%|▌         | 271/5000 [1:08:52<18:47:40, 14.31s/it]  5%|▌         | 272/5000 [1:09:05<18:10:57, 13.84s/it]                                                       {'loss': 749.4236, 'grad_norm': 5568.0, 'learning_rate': 0.000699962949994727, 'epoch': 0.35}
  5%|▌         | 272/5000 [1:09:05<18:10:57, 13.84s/it]  5%|▌         | 273/5000 [1:09:25<20:47:14, 15.83s/it]                                                       {'loss': 313.5733, 'grad_norm': 1040.0, 'learning_rate': 0.0006999595053292587, 'epoch': 0.35}
  5%|▌         | 273/5000 [1:09:25<20:47:14, 15.83s/it]  5%|▌         | 274/5000 [1:09:38<19:37:25, 14.95s/it]                                                       {'loss': 503.3499, 'grad_norm': 3024.0, 'learning_rate': 0.0006999559075796132, 'epoch': 0.35}
  5%|▌         | 274/5000 [1:09:38<19:37:25, 14.95s/it]  6%|▌         | 275/5000 [1:10:02<23:07:34, 17.62s/it]                                                       {'loss': 266.3262, 'grad_norm': 868.0, 'learning_rate': 0.0006999521567473641, 'epoch': 0.35}
  6%|▌         | 275/5000 [1:10:02<23:07:34, 17.62s/it]  6%|▌         | 276/5000 [1:10:18<22:15:20, 16.96s/it]                                                       {'loss': 327.6449, 'grad_norm': 1384.0, 'learning_rate': 0.0006999482528341524, 'epoch': 0.35}
  6%|▌         | 276/5000 [1:10:18<22:15:20, 16.96s/it]  6%|▌         | 277/5000 [1:10:30<20:19:15, 15.49s/it]                                                       {'loss': 463.0037, 'grad_norm': 2080.0, 'learning_rate': 0.0006999441958416858, 'epoch': 0.35}
  6%|▌         | 277/5000 [1:10:30<20:19:15, 15.49s/it]  6%|▌         | 278/5000 [1:10:52<23:12:28, 17.69s/it]                                                       {'loss': 368.2708, 'grad_norm': 2784.0, 'learning_rate': 0.0006999399857717389, 'epoch': 0.35}
  6%|▌         | 278/5000 [1:10:52<23:12:28, 17.69s/it]  6%|▌         | 279/5000 [1:11:04<20:56:01, 15.96s/it]                                                       {'loss': 1028.4727, 'grad_norm': 9152.0, 'learning_rate': 0.0006999356226261532, 'epoch': 0.35}
  6%|▌         | 279/5000 [1:11:04<20:56:01, 15.96s/it]  6%|▌         | 280/5000 [1:11:20<20:37:40, 15.73s/it]                                                       {'loss': 648.6906, 'grad_norm': 14400.0, 'learning_rate': 0.0006999311064068374, 'epoch': 0.36}
  6%|▌         | 280/5000 [1:11:20<20:37:40, 15.73s/it]  6%|▌         | 281/5000 [1:11:34<19:54:37, 15.19s/it]                                                       {'loss': 579.4509, 'grad_norm': 1384.0, 'learning_rate': 0.0006999264371157672, 'epoch': 0.36}
  6%|▌         | 281/5000 [1:11:34<19:54:37, 15.19s/it]  6%|▌         | 282/5000 [1:11:50<20:33:42, 15.69s/it]                                                       {'loss': 416.009, 'grad_norm': 1104.0, 'learning_rate': 0.0006999216147549848, 'epoch': 0.36}
  6%|▌         | 282/5000 [1:11:50<20:33:42, 15.69s/it]  6%|▌         | 283/5000 [1:12:24<27:29:28, 20.98s/it]                                                       {'loss': 483.6164, 'grad_norm': 11968.0, 'learning_rate': 0.0006999166393266, 'epoch': 0.36}
  6%|▌         | 283/5000 [1:12:24<27:29:28, 20.98s/it]  6%|▌         | 284/5000 [1:12:37<24:30:25, 18.71s/it]                                                       {'loss': 517.3188, 'grad_norm': 2688.0, 'learning_rate': 0.0006999115108327888, 'epoch': 0.36}
  6%|▌         | 284/5000 [1:12:37<24:30:25, 18.71s/it]  6%|▌         | 285/5000 [1:12:48<21:29:06, 16.40s/it]                                                       {'loss': 363.1452, 'grad_norm': 1048.0, 'learning_rate': 0.0006999062292757951, 'epoch': 0.36}
  6%|▌         | 285/5000 [1:12:48<21:29:06, 16.40s/it]  6%|▌         | 286/5000 [1:13:15<25:30:07, 19.48s/it]                                                       {'loss': 497.0226, 'grad_norm': 3056.0, 'learning_rate': 0.0006999007946579288, 'epoch': 0.36}
  6%|▌         | 286/5000 [1:13:15<25:30:07, 19.48s/it]  6%|▌         | 287/5000 [1:13:32<24:40:05, 18.84s/it]                                                       {'loss': 442.0481, 'grad_norm': 2560.0, 'learning_rate': 0.0006998952069815674, 'epoch': 0.36}
  6%|▌         | 287/5000 [1:13:32<24:40:05, 18.84s/it]  6%|▌         | 288/5000 [1:13:47<23:10:20, 17.70s/it]                                                       {'loss': 511.2218, 'grad_norm': 4512.0, 'learning_rate': 0.0006998894662491549, 'epoch': 0.37}
  6%|▌         | 288/5000 [1:13:47<23:10:20, 17.70s/it]  6%|▌         | 289/5000 [1:14:03<22:34:09, 17.25s/it]                                                       {'loss': 536.0153, 'grad_norm': 3552.0, 'learning_rate': 0.0006998835724632029, 'epoch': 0.37}
  6%|▌         | 289/5000 [1:14:03<22:34:09, 17.25s/it]  6%|▌         | 290/5000 [1:14:20<22:13:52, 16.99s/it]                                                       {'loss': 472.3818, 'grad_norm': 1096.0, 'learning_rate': 0.0006998775256262892, 'epoch': 0.37}
  6%|▌         | 290/5000 [1:14:20<22:13:52, 16.99s/it]  6%|▌         | 291/5000 [1:14:34<21:05:42, 16.13s/it]                                                       {'loss': 549.5549, 'grad_norm': 2864.0, 'learning_rate': 0.000699871325741059, 'epoch': 0.37}
  6%|▌         | 291/5000 [1:14:34<21:05:42, 16.13s/it]  6%|▌         | 292/5000 [1:14:50<20:53:48, 15.98s/it]                                                       {'loss': 491.8907, 'grad_norm': 2576.0, 'learning_rate': 0.0006998649728102244, 'epoch': 0.37}
  6%|▌         | 292/5000 [1:14:50<20:53:48, 15.98s/it]  6%|▌         | 293/5000 [1:15:06<20:58:32, 16.04s/it]                                                       {'loss': 520.1875, 'grad_norm': 3280.0, 'learning_rate': 0.0006998584668365643, 'epoch': 0.37}
  6%|▌         | 293/5000 [1:15:06<20:58:32, 16.04s/it]  6%|▌         | 294/5000 [1:15:19<19:53:36, 15.22s/it]                                                       {'loss': 530.5985, 'grad_norm': 5216.0, 'learning_rate': 0.0006998518078229248, 'epoch': 0.37}
  6%|▌         | 294/5000 [1:15:19<19:53:36, 15.22s/it]  6%|▌         | 295/5000 [1:15:32<19:09:06, 14.65s/it]                                                       {'loss': 497.7937, 'grad_norm': 1184.0, 'learning_rate': 0.0006998449957722185, 'epoch': 0.37}
  6%|▌         | 295/5000 [1:15:32<19:09:06, 14.65s/it]  6%|▌         | 296/5000 [1:15:49<19:57:32, 15.27s/it]                                                       {'loss': 612.9934, 'grad_norm': 4048.0, 'learning_rate': 0.0006998380306874253, 'epoch': 0.38}
  6%|▌         | 296/5000 [1:15:49<19:57:32, 15.27s/it]  6%|▌         | 297/5000 [1:16:05<20:13:31, 15.48s/it]                                                       {'loss': 521.8468, 'grad_norm': 1360.0, 'learning_rate': 0.0006998309125715922, 'epoch': 0.38}
  6%|▌         | 297/5000 [1:16:05<20:13:31, 15.48s/it]  6%|▌         | 298/5000 [1:16:21<20:25:28, 15.64s/it]                                                       {'loss': 533.3977, 'grad_norm': 69120.0, 'learning_rate': 0.0006998236414278327, 'epoch': 0.38}
  6%|▌         | 298/5000 [1:16:21<20:25:28, 15.64s/it]  6%|▌         | 299/5000 [1:16:47<24:28:05, 18.74s/it]                                                       {'loss': 1323.8481, 'grad_norm': 133693440.0, 'learning_rate': 0.0006998162172593274, 'epoch': 0.38}
  6%|▌         | 299/5000 [1:16:47<24:28:05, 18.74s/it]  6%|▌         | 300/5000 [1:16:59<21:50:19, 16.73s/it]                                                       {'loss': 1118.9883, 'grad_norm': 78848.0, 'learning_rate': 0.0006998086400693242, 'epoch': 0.38}
  6%|▌         | 300/5000 [1:16:59<21:50:19, 16.73s/it]  6%|▌         | 301/5000 [1:17:16<21:59:55, 16.85s/it]                                                       {'loss': 926.2905, 'grad_norm': 10880.0, 'learning_rate': 0.000699800909861137, 'epoch': 0.38}
  6%|▌         | 301/5000 [1:17:16<21:59:55, 16.85s/it]  6%|▌         | 302/5000 [1:17:28<20:06:18, 15.41s/it]                                                       {'loss': 959.8199, 'grad_norm': 1187840.0, 'learning_rate': 0.0006997930266381479, 'epoch': 0.38}
  6%|▌         | 302/5000 [1:17:28<20:06:18, 15.41s/it]  6%|▌         | 303/5000 [1:17:42<19:29:05, 14.93s/it]                                                       {'loss': 930.4352, 'grad_norm': 118784.0, 'learning_rate': 0.0006997849904038051, 'epoch': 0.38}
  6%|▌         | 303/5000 [1:17:42<19:29:05, 14.93s/it]  6%|▌         | 304/5000 [1:18:13<25:54:21, 19.86s/it]                                                       {'loss': 1298.1322, 'grad_norm': 14876672.0, 'learning_rate': 0.0006997768011616237, 'epoch': 0.39}
  6%|▌         | 304/5000 [1:18:13<25:54:21, 19.86s/it]  6%|▌         | 305/5000 [1:18:28<23:55:40, 18.35s/it]                                                       {'loss': 1061.063, 'grad_norm': 282624.0, 'learning_rate': 0.0006997684589151862, 'epoch': 0.39}
  6%|▌         | 305/5000 [1:18:28<23:55:40, 18.35s/it]  6%|▌         | 306/5000 [1:18:41<21:39:23, 16.61s/it]                                                       {'loss': 967.3702, 'grad_norm': 48640.0, 'learning_rate': 0.0006997599636681417, 'epoch': 0.39}
  6%|▌         | 306/5000 [1:18:41<21:39:23, 16.61s/it]  6%|▌         | 307/5000 [1:18:58<21:53:34, 16.79s/it]                                                       {'loss': 1267.0205, 'grad_norm': 151552.0, 'learning_rate': 0.0006997513154242063, 'epoch': 0.39}
  6%|▌         | 307/5000 [1:18:58<21:53:34, 16.79s/it]  6%|▌         | 308/5000 [1:19:24<25:41:21, 19.71s/it]                                                       {'loss': 1137.4818, 'grad_norm': 9408.0, 'learning_rate': 0.0006997425141871628, 'epoch': 0.39}
  6%|▌         | 308/5000 [1:19:24<25:41:21, 19.71s/it]  6%|▌         | 309/5000 [1:19:41<24:30:47, 18.81s/it]                                                       {'loss': 1003.2194, 'grad_norm': 36096.0, 'learning_rate': 0.0006997335599608617, 'epoch': 0.39}
  6%|▌         | 309/5000 [1:19:41<24:30:47, 18.81s/it]  6%|▌         | 310/5000 [1:19:54<22:01:49, 16.91s/it]                                                       {'loss': 860.3427, 'grad_norm': 91648.0, 'learning_rate': 0.0006997244527492195, 'epoch': 0.39}
  6%|▌         | 310/5000 [1:19:54<22:01:49, 16.91s/it]  6%|▌         | 311/5000 [1:20:10<21:44:49, 16.70s/it]                                                       {'loss': 2705.3418, 'grad_norm': 14208.0, 'learning_rate': 0.00069971519255622, 'epoch': 0.39}
  6%|▌         | 311/5000 [1:20:10<21:44:49, 16.70s/it]  6%|▌         | 312/5000 [1:20:35<24:57:45, 19.17s/it]                                                       {'loss': 2350.2656, 'grad_norm': 7296.0, 'learning_rate': 0.0006997057793859141, 'epoch': 0.4}
  6%|▌         | 312/5000 [1:20:35<24:57:45, 19.17s/it]  6%|▋         | 313/5000 [1:20:58<26:33:39, 20.40s/it]                                                       {'loss': 1907.507, 'grad_norm': 7552.0, 'learning_rate': 0.0006996962132424194, 'epoch': 0.4}
  6%|▋         | 313/5000 [1:20:58<26:33:39, 20.40s/it]  6%|▋         | 314/5000 [1:21:15<25:22:15, 19.49s/it]                                                       {'loss': 1962.1188, 'grad_norm': 8096.0, 'learning_rate': 0.0006996864941299203, 'epoch': 0.4}
  6%|▋         | 314/5000 [1:21:15<25:22:15, 19.49s/it]  6%|▋         | 315/5000 [1:21:31<23:40:03, 18.19s/it]                                                       {'loss': 1688.9418, 'grad_norm': 45568.0, 'learning_rate': 0.0006996766220526683, 'epoch': 0.4}
  6%|▋         | 315/5000 [1:21:31<23:40:03, 18.19s/it]  6%|▋         | 316/5000 [1:21:44<21:47:05, 16.74s/it]                                                       {'loss': 1770.1196, 'grad_norm': 7584.0, 'learning_rate': 0.0006996665970149819, 'epoch': 0.4}
  6%|▋         | 316/5000 [1:21:44<21:47:05, 16.74s/it]  6%|▋         | 317/5000 [1:21:55<19:39:14, 15.11s/it]                                                       {'loss': 1660.7626, 'grad_norm': 12864.0, 'learning_rate': 0.0006996564190212464, 'epoch': 0.4}
  6%|▋         | 317/5000 [1:21:55<19:39:14, 15.11s/it]  6%|▋         | 318/5000 [1:22:08<18:43:31, 14.40s/it]                                                       {'loss': 1508.1864, 'grad_norm': 20864.0, 'learning_rate': 0.0006996460880759139, 'epoch': 0.4}
  6%|▋         | 318/5000 [1:22:08<18:43:31, 14.40s/it]  6%|▋         | 319/5000 [1:22:19<17:14:36, 13.26s/it]                                                       {'loss': 1350.4436, 'grad_norm': 70144.0, 'learning_rate': 0.0006996356041835037, 'epoch': 0.41}
  6%|▋         | 319/5000 [1:22:19<17:14:36, 13.26s/it]  6%|▋         | 320/5000 [1:22:43<21:24:50, 16.47s/it]                                                       {'loss': 1362.333, 'grad_norm': 17920.0, 'learning_rate': 0.0006996249673486015, 'epoch': 0.41}
  6%|▋         | 320/5000 [1:22:43<21:24:50, 16.47s/it]  6%|▋         | 321/5000 [1:23:01<22:18:05, 17.16s/it]                                                       {'loss': 1281.2404, 'grad_norm': 36096.0, 'learning_rate': 0.0006996141775758605, 'epoch': 0.41}
  6%|▋         | 321/5000 [1:23:01<22:18:05, 17.16s/it]  6%|▋         | 322/5000 [1:23:20<22:48:20, 17.55s/it]                                                       {'loss': 1471.6321, 'grad_norm': 7232.0, 'learning_rate': 0.0006996032348700003, 'epoch': 0.41}
  6%|▋         | 322/5000 [1:23:20<22:48:20, 17.55s/it]  6%|▋         | 323/5000 [1:23:39<23:29:52, 18.09s/it]                                                       {'loss': 1096.1555, 'grad_norm': 10688.0, 'learning_rate': 0.0006995921392358076, 'epoch': 0.41}
  6%|▋         | 323/5000 [1:23:39<23:29:52, 18.09s/it]  6%|▋         | 324/5000 [1:23:56<22:54:09, 17.63s/it]                                                       {'loss': 1124.1906, 'grad_norm': 69120.0, 'learning_rate': 0.0006995808906781363, 'epoch': 0.41}
  6%|▋         | 324/5000 [1:23:56<22:54:09, 17.63s/it]  6%|▋         | 325/5000 [1:24:15<23:29:34, 18.09s/it]                                                       {'loss': 1040.042, 'grad_norm': 8512.0, 'learning_rate': 0.0006995694892019067, 'epoch': 0.41}
  6%|▋         | 325/5000 [1:24:15<23:29:34, 18.09s/it]  7%|▋         | 326/5000 [1:24:30<22:24:57, 17.27s/it]                                                       {'loss': 1074.9828, 'grad_norm': 4320.0, 'learning_rate': 0.0006995579348121062, 'epoch': 0.41}
  7%|▋         | 326/5000 [1:24:30<22:24:57, 17.27s/it]  7%|▋         | 327/5000 [1:24:45<21:20:43, 16.44s/it]                                                       {'loss': 951.2615, 'grad_norm': 11328.0, 'learning_rate': 0.000699546227513789, 'epoch': 0.42}
  7%|▋         | 327/5000 [1:24:45<21:20:43, 16.44s/it]  7%|▋         | 328/5000 [1:24:59<20:33:45, 15.84s/it]                                                       {'loss': 927.9362, 'grad_norm': 3104.0, 'learning_rate': 0.0006995343673120763, 'epoch': 0.42}
  7%|▋         | 328/5000 [1:24:59<20:33:45, 15.84s/it]  7%|▋         | 329/5000 [1:25:13<19:56:18, 15.37s/it]                                                       {'loss': 779.3914, 'grad_norm': 2256.0, 'learning_rate': 0.0006995223542121563, 'epoch': 0.42}
  7%|▋         | 329/5000 [1:25:13<19:56:18, 15.37s/it]  7%|▋         | 330/5000 [1:25:36<22:45:57, 17.55s/it]                                                       {'loss': 1051.7443, 'grad_norm': 3136.0, 'learning_rate': 0.0006995101882192839, 'epoch': 0.42}
  7%|▋         | 330/5000 [1:25:36<22:45:57, 17.55s/it]  7%|▋         | 331/5000 [1:25:51<21:52:26, 16.87s/it]                                                       {'loss': 935.5762, 'grad_norm': 1456.0, 'learning_rate': 0.0006994978693387807, 'epoch': 0.42}
  7%|▋         | 331/5000 [1:25:51<21:52:26, 16.87s/it]  7%|▋         | 332/5000 [1:26:06<21:07:18, 16.29s/it]                                                       {'loss': 753.3692, 'grad_norm': 105984.0, 'learning_rate': 0.0006994853975760358, 'epoch': 0.42}
  7%|▋         | 332/5000 [1:26:06<21:07:18, 16.29s/it]  7%|▋         | 333/5000 [1:26:21<20:38:35, 15.92s/it]                                                       {'loss': 874.7748, 'grad_norm': 175104.0, 'learning_rate': 0.0006994727729365044, 'epoch': 0.42}
  7%|▋         | 333/5000 [1:26:21<20:38:35, 15.92s/it]  7%|▋         | 334/5000 [1:26:37<20:24:58, 15.75s/it]                                                       {'loss': 964.4221, 'grad_norm': 34560.0, 'learning_rate': 0.0006994599954257091, 'epoch': 0.42}
  7%|▋         | 334/5000 [1:26:37<20:24:58, 15.75s/it]  7%|▋         | 335/5000 [1:26:57<22:15:03, 17.17s/it]                                                       {'loss': 922.581, 'grad_norm': 8768.0, 'learning_rate': 0.0006994470650492393, 'epoch': 0.43}
  7%|▋         | 335/5000 [1:26:57<22:15:03, 17.17s/it]  7%|▋         | 336/5000 [1:27:14<21:56:56, 16.94s/it]                                                       {'loss': 997.7445, 'grad_norm': 133120.0, 'learning_rate': 0.000699433981812751, 'epoch': 0.43}
  7%|▋         | 336/5000 [1:27:14<21:56:56, 16.94s/it]  7%|▋         | 337/5000 [1:27:29<21:23:19, 16.51s/it]                                                       {'loss': 1104.7283, 'grad_norm': 44800.0, 'learning_rate': 0.0006994207457219673, 'epoch': 0.43}
  7%|▋         | 337/5000 [1:27:29<21:23:19, 16.51s/it]  7%|▋         | 338/5000 [1:27:52<23:41:19, 18.29s/it]                                                       {'loss': 1404.647, 'grad_norm': 1736704.0, 'learning_rate': 0.0006994073567826781, 'epoch': 0.43}
  7%|▋         | 338/5000 [1:27:52<23:41:19, 18.29s/it]  7%|▋         | 339/5000 [1:28:07<22:22:46, 17.29s/it]                                                       {'loss': 2052.8804, 'grad_norm': 66060288.0, 'learning_rate': 0.0006993938150007404, 'epoch': 0.43}
  7%|▋         | 339/5000 [1:28:07<22:22:46, 17.29s/it]  7%|▋         | 340/5000 [1:28:30<24:57:49, 19.29s/it]                                                       {'loss': 1856.0778, 'grad_norm': 1089536.0, 'learning_rate': 0.0006993801203820776, 'epoch': 0.43}
  7%|▋         | 340/5000 [1:28:30<24:57:49, 19.29s/it]  7%|▋         | 341/5000 [1:28:45<23:09:31, 17.89s/it]                                                       {'loss': 1869.3486, 'grad_norm': 483328.0, 'learning_rate': 0.0006993662729326802, 'epoch': 0.43}
  7%|▋         | 341/5000 [1:28:45<23:09:31, 17.89s/it]  7%|▋         | 342/5000 [1:29:03<23:19:00, 18.02s/it]                                                       {'loss': 1788.8331, 'grad_norm': 352256.0, 'learning_rate': 0.0006993522726586056, 'epoch': 0.43}
  7%|▋         | 342/5000 [1:29:03<23:19:00, 18.02s/it]  7%|▋         | 343/5000 [1:29:21<22:56:49, 17.74s/it]                                                       {'loss': 2132.647, 'grad_norm': 58195968.0, 'learning_rate': 0.0006993381195659781, 'epoch': 0.44}
  7%|▋         | 343/5000 [1:29:21<22:56:49, 17.74s/it]  7%|▋         | 344/5000 [1:29:38<22:39:26, 17.52s/it]                                                       {'loss': 1726.3062, 'grad_norm': 618496.0, 'learning_rate': 0.0006993238136609886, 'epoch': 0.44}
  7%|▋         | 344/5000 [1:29:38<22:39:26, 17.52s/it]  7%|▋         | 345/5000 [1:29:52<21:23:47, 16.55s/it]                                                       {'loss': 1921.1653, 'grad_norm': 8519680.0, 'learning_rate': 0.0006993093549498951, 'epoch': 0.44}
  7%|▋         | 345/5000 [1:29:52<21:23:47, 16.55s/it]  7%|▋         | 346/5000 [1:30:06<20:24:04, 15.78s/it]                                                       {'loss': 2553.9336, 'grad_norm': 18350080.0, 'learning_rate': 0.0006992947434390222, 'epoch': 0.44}
  7%|▋         | 346/5000 [1:30:06<20:24:04, 15.78s/it]  7%|▋         | 347/5000 [1:30:22<20:23:29, 15.78s/it]                                                       {'loss': 2371.812, 'grad_norm': 39583744.0, 'learning_rate': 0.0006992799791347615, 'epoch': 0.44}
  7%|▋         | 347/5000 [1:30:22<20:23:29, 15.78s/it]  7%|▋         | 348/5000 [1:30:36<19:45:36, 15.29s/it]                                                       {'loss': 2719.6592, 'grad_norm': 5701632.0, 'learning_rate': 0.0006992650620435715, 'epoch': 0.44}
  7%|▋         | 348/5000 [1:30:36<19:45:36, 15.29s/it]  7%|▋         | 349/5000 [1:31:00<23:14:58, 18.00s/it]                                                       {'loss': 2876.6313, 'grad_norm': 82432.0, 'learning_rate': 0.0006992499921719773, 'epoch': 0.44}
  7%|▋         | 349/5000 [1:31:00<23:14:58, 18.00s/it]  7%|▋         | 350/5000 [1:31:16<22:20:58, 17.30s/it]                                                       {'loss': 2318.5364, 'grad_norm': 532480.0, 'learning_rate': 0.0006992347695265711, 'epoch': 0.44}
  7%|▋         | 350/5000 [1:31:16<22:20:58, 17.30s/it]  7%|▋         | 351/5000 [1:31:27<19:55:27, 15.43s/it]                                                       {'loss': 2578.6814, 'grad_norm': 753664.0, 'learning_rate': 0.0006992193941140118, 'epoch': 0.45}
  7%|▋         | 351/5000 [1:31:27<19:55:27, 15.43s/it]  7%|▋         | 352/5000 [1:31:40<19:15:49, 14.92s/it]                                                       {'loss': 2545.3403, 'grad_norm': 61696.0, 'learning_rate': 0.0006992038659410251, 'epoch': 0.45}
  7%|▋         | 352/5000 [1:31:40<19:15:49, 14.92s/it]  7%|▋         | 353/5000 [1:31:55<19:10:08, 14.85s/it]                                                       {'loss': 2332.0918, 'grad_norm': 19456.0, 'learning_rate': 0.0006991881850144034, 'epoch': 0.45}
  7%|▋         | 353/5000 [1:31:55<19:10:08, 14.85s/it]  7%|▋         | 354/5000 [1:32:10<18:57:52, 14.69s/it]                                                       {'loss': 2213.5928, 'grad_norm': 23552.0, 'learning_rate': 0.0006991723513410064, 'epoch': 0.45}
  7%|▋         | 354/5000 [1:32:10<18:57:52, 14.69s/it]  7%|▋         | 355/5000 [1:32:34<22:41:04, 17.58s/it]                                                       {'loss': 2047.5492, 'grad_norm': 235520.0, 'learning_rate': 0.0006991563649277599, 'epoch': 0.45}
  7%|▋         | 355/5000 [1:32:34<22:41:04, 17.58s/it]  7%|▋         | 356/5000 [1:32:49<21:48:30, 16.91s/it]                                                       {'loss': 2490.3179, 'grad_norm': 56064.0, 'learning_rate': 0.0006991402257816573, 'epoch': 0.45}
  7%|▋         | 356/5000 [1:32:49<21:48:30, 16.91s/it]  7%|▋         | 357/5000 [1:33:07<22:10:27, 17.19s/it]                                                       {'loss': 2335.1399, 'grad_norm': 6560.0, 'learning_rate': 0.0006991239339097581, 'epoch': 0.45}
  7%|▋         | 357/5000 [1:33:07<22:10:27, 17.19s/it]  7%|▋         | 358/5000 [1:33:31<24:45:02, 19.19s/it]                                                       {'loss': 2070.2654, 'grad_norm': 128512.0, 'learning_rate': 0.000699107489319189, 'epoch': 0.45}
  7%|▋         | 358/5000 [1:33:31<24:45:02, 19.19s/it]  7%|▋         | 359/5000 [1:33:45<22:55:35, 17.78s/it]                                                       {'loss': 2332.7139, 'grad_norm': 16768.0, 'learning_rate': 0.0006990908920171435, 'epoch': 0.46}
  7%|▋         | 359/5000 [1:33:45<22:55:35, 17.78s/it]  7%|▋         | 360/5000 [1:34:02<22:26:11, 17.41s/it]                                                       {'loss': 1837.5518, 'grad_norm': 32000.0, 'learning_rate': 0.0006990741420108817, 'epoch': 0.46}
  7%|▋         | 360/5000 [1:34:02<22:26:11, 17.41s/it]  7%|▋         | 361/5000 [1:34:27<25:35:11, 19.86s/it]                                                       {'loss': 2698.4578, 'grad_norm': 6688.0, 'learning_rate': 0.0006990572393077309, 'epoch': 0.46}
  7%|▋         | 361/5000 [1:34:27<25:35:11, 19.86s/it]  7%|▋         | 362/5000 [1:34:44<24:25:34, 18.96s/it]                                                       {'loss': 2247.519, 'grad_norm': 5600.0, 'learning_rate': 0.0006990401839150845, 'epoch': 0.46}
  7%|▋         | 362/5000 [1:34:44<24:25:34, 18.96s/it]  7%|▋         | 363/5000 [1:34:58<22:29:59, 17.47s/it]                                                       {'loss': 2249.2998, 'grad_norm': 44288.0, 'learning_rate': 0.0006990229758404034, 'epoch': 0.46}
  7%|▋         | 363/5000 [1:34:58<22:29:59, 17.47s/it]  7%|▋         | 364/5000 [1:35:14<21:45:56, 16.90s/it]                                                       {'loss': 1996.4006, 'grad_norm': 45568.0, 'learning_rate': 0.0006990056150912149, 'epoch': 0.46}
  7%|▋         | 364/5000 [1:35:14<21:45:56, 16.90s/it]  7%|▋         | 365/5000 [1:35:41<25:43:27, 19.98s/it]                                                       {'loss': 1600.7368, 'grad_norm': 9600.0, 'learning_rate': 0.0006989881016751132, 'epoch': 0.46}
  7%|▋         | 365/5000 [1:35:41<25:43:27, 19.98s/it]  7%|▋         | 366/5000 [1:35:57<23:59:44, 18.64s/it]                                                       {'loss': 2477.0354, 'grad_norm': 155648.0, 'learning_rate': 0.0006989704355997592, 'epoch': 0.46}
  7%|▋         | 366/5000 [1:35:57<23:59:44, 18.64s/it]  7%|▋         | 367/5000 [1:36:15<23:51:19, 18.54s/it]                                                       {'loss': 2006.5492, 'grad_norm': 5312.0, 'learning_rate': 0.0006989526168728808, 'epoch': 0.47}
  7%|▋         | 367/5000 [1:36:15<23:51:19, 18.54s/it]  7%|▋         | 368/5000 [1:36:26<21:06:50, 16.41s/it]                                                       {'loss': 2002.5625, 'grad_norm': 9280.0, 'learning_rate': 0.0006989346455022724, 'epoch': 0.47}
  7%|▋         | 368/5000 [1:36:26<21:06:50, 16.41s/it]  7%|▋         | 369/5000 [1:36:49<23:37:18, 18.36s/it]                                                       {'loss': 1925.2856, 'grad_norm': 116736.0, 'learning_rate': 0.0006989165214957952, 'epoch': 0.47}
  7%|▋         | 369/5000 [1:36:49<23:37:18, 18.36s/it]  7%|▋         | 370/5000 [1:37:04<22:03:32, 17.15s/it]                                                       {'loss': 1840.7255, 'grad_norm': 9984.0, 'learning_rate': 0.0006988982448613775, 'epoch': 0.47}
  7%|▋         | 370/5000 [1:37:04<22:03:32, 17.15s/it]  7%|▋         | 371/5000 [1:37:20<21:53:21, 17.02s/it]                                                       {'loss': 1897.5792, 'grad_norm': 84992.0, 'learning_rate': 0.000698879815607014, 'epoch': 0.47}
  7%|▋         | 371/5000 [1:37:20<21:53:21, 17.02s/it]  7%|▋         | 372/5000 [1:37:47<25:45:56, 20.04s/it]                                                       {'loss': 1899.6462, 'grad_norm': 110592.0, 'learning_rate': 0.0006988612337407661, 'epoch': 0.47}
  7%|▋         | 372/5000 [1:37:47<25:45:56, 20.04s/it]  7%|▋         | 373/5000 [1:38:02<23:36:42, 18.37s/it]                                                       {'loss': 1730.0695, 'grad_norm': 40192.0, 'learning_rate': 0.0006988424992707624, 'epoch': 0.47}
  7%|▋         | 373/5000 [1:38:02<23:36:42, 18.37s/it]  7%|▋         | 374/5000 [1:38:21<23:48:03, 18.52s/it]                                                       {'loss': 1811.8593, 'grad_norm': 47104.0, 'learning_rate': 0.0006988236122051979, 'epoch': 0.47}
  7%|▋         | 374/5000 [1:38:21<23:48:03, 18.52s/it]  8%|▊         | 375/5000 [1:38:34<21:54:49, 17.06s/it]                                                       {'loss': 1968.7014, 'grad_norm': 8640.0, 'learning_rate': 0.0006988045725523345, 'epoch': 0.48}
  8%|▊         | 375/5000 [1:38:34<21:54:49, 17.06s/it]  8%|▊         | 376/5000 [1:38:45<19:30:15, 15.18s/it]                                                       {'loss': 2002.045, 'grad_norm': 11968.0, 'learning_rate': 0.0006987853803205006, 'epoch': 0.48}
  8%|▊         | 376/5000 [1:38:45<19:30:15, 15.18s/it]  8%|▊         | 377/5000 [1:39:12<23:58:12, 18.67s/it]                                                       {'loss': 2020.1462, 'grad_norm': 127488.0, 'learning_rate': 0.0006987660355180917, 'epoch': 0.48}
  8%|▊         | 377/5000 [1:39:12<23:58:12, 18.67s/it]  8%|▊         | 378/5000 [1:39:34<25:07:33, 19.57s/it]                                                       {'loss': 2159.2195, 'grad_norm': 19328.0, 'learning_rate': 0.00069874653815357, 'epoch': 0.48}
  8%|▊         | 378/5000 [1:39:34<25:07:33, 19.57s/it]  8%|▊         | 379/5000 [1:39:55<25:56:22, 20.21s/it]                                                       {'loss': 2198.167, 'grad_norm': 11328.0, 'learning_rate': 0.000698726888235464, 'epoch': 0.48}
  8%|▊         | 379/5000 [1:39:55<25:56:22, 20.21s/it]  8%|▊         | 380/5000 [1:40:10<23:52:04, 18.60s/it]                                                       {'loss': 2297.2393, 'grad_norm': 4896.0, 'learning_rate': 0.0006987070857723694, 'epoch': 0.48}
  8%|▊         | 380/5000 [1:40:10<23:52:04, 18.60s/it]  8%|▊         | 381/5000 [1:40:24<22:10:40, 17.29s/it]                                                       {'loss': 1926.5852, 'grad_norm': 598016.0, 'learning_rate': 0.0006986871307729485, 'epoch': 0.48}
  8%|▊         | 381/5000 [1:40:24<22:10:40, 17.29s/it]  8%|▊         | 382/5000 [1:40:37<20:23:28, 15.90s/it]                                                       {'loss': 1470.9746, 'grad_norm': 38656.0, 'learning_rate': 0.0006986670232459303, 'epoch': 0.49}
  8%|▊         | 382/5000 [1:40:37<20:23:28, 15.90s/it]  8%|▊         | 383/5000 [1:40:50<19:09:55, 14.94s/it]                                                       {'loss': 2037.781, 'grad_norm': 6208.0, 'learning_rate': 0.0006986467632001103, 'epoch': 0.49}
  8%|▊         | 383/5000 [1:40:50<19:09:55, 14.94s/it]  8%|▊         | 384/5000 [1:41:03<18:38:30, 14.54s/it]                                                       {'loss': 1675.45, 'grad_norm': 5376.0, 'learning_rate': 0.0006986263506443513, 'epoch': 0.49}
  8%|▊         | 384/5000 [1:41:03<18:38:30, 14.54s/it]  8%|▊         | 385/5000 [1:41:24<21:01:33, 16.40s/it]                                                       {'loss': 1772.1681, 'grad_norm': 42752.0, 'learning_rate': 0.0006986057855875822, 'epoch': 0.49}
  8%|▊         | 385/5000 [1:41:24<21:01:33, 16.40s/it]  8%|▊         | 386/5000 [1:41:38<20:07:14, 15.70s/it]                                                       {'loss': 1796.6107, 'grad_norm': 18432.0, 'learning_rate': 0.0006985850680387988, 'epoch': 0.49}
  8%|▊         | 386/5000 [1:41:38<20:07:14, 15.70s/it]  8%|▊         | 387/5000 [1:41:50<18:29:46, 14.43s/it]                                                       {'loss': 1660.6359, 'grad_norm': 32384.0, 'learning_rate': 0.0006985641980070639, 'epoch': 0.49}
  8%|▊         | 387/5000 [1:41:50<18:29:46, 14.43s/it]  8%|▊         | 388/5000 [1:42:01<17:27:43, 13.63s/it]                                                       {'loss': 1514.6736, 'grad_norm': 28416.0, 'learning_rate': 0.0006985431755015066, 'epoch': 0.49}
  8%|▊         | 388/5000 [1:42:01<17:27:43, 13.63s/it]  8%|▊         | 389/5000 [1:42:23<20:35:10, 16.07s/it]                                                       {'loss': 1917.6772, 'grad_norm': 22656.0, 'learning_rate': 0.0006985220005313228, 'epoch': 0.49}
  8%|▊         | 389/5000 [1:42:23<20:35:10, 16.07s/it]  8%|▊         | 390/5000 [1:42:48<23:46:06, 18.56s/it]                                                       {'loss': 1799.8868, 'grad_norm': 20352.0, 'learning_rate': 0.0006985006731057753, 'epoch': 0.5}
  8%|▊         | 390/5000 [1:42:48<23:46:06, 18.56s/it]  8%|▊         | 391/5000 [1:43:09<24:40:15, 19.27s/it]                                                       {'loss': 1620.7606, 'grad_norm': 9664.0, 'learning_rate': 0.0006984791932341934, 'epoch': 0.5}
  8%|▊         | 391/5000 [1:43:09<24:40:15, 19.27s/it]  8%|▊         | 392/5000 [1:43:30<25:26:20, 19.87s/it]                                                       {'loss': 1352.6583, 'grad_norm': 6112.0, 'learning_rate': 0.000698457560925973, 'epoch': 0.5}
  8%|▊         | 392/5000 [1:43:30<25:26:20, 19.87s/it]  8%|▊         | 393/5000 [1:43:48<24:40:29, 19.28s/it]                                                       {'loss': 1389.6788, 'grad_norm': 5216.0, 'learning_rate': 0.000698435776190577, 'epoch': 0.5}
  8%|▊         | 393/5000 [1:43:48<24:40:29, 19.28s/it]  8%|▊         | 394/5000 [1:44:07<24:38:32, 19.26s/it]                                                       {'loss': 1465.8302, 'grad_norm': 19328.0, 'learning_rate': 0.0006984138390375347, 'epoch': 0.5}
  8%|▊         | 394/5000 [1:44:07<24:38:32, 19.26s/it]  8%|▊         | 395/5000 [1:44:20<22:20:04, 17.46s/it]                                                       {'loss': 1857.0497, 'grad_norm': 26368.0, 'learning_rate': 0.0006983917494764422, 'epoch': 0.5}
  8%|▊         | 395/5000 [1:44:20<22:20:04, 17.46s/it]  8%|▊         | 396/5000 [1:44:31<19:47:11, 15.47s/it]                                                       {'loss': 1649.0425, 'grad_norm': 7616.0, 'learning_rate': 0.0006983695075169621, 'epoch': 0.5}
  8%|▊         | 396/5000 [1:44:31<19:47:11, 15.47s/it]  8%|▊         | 397/5000 [1:44:46<19:40:11, 15.38s/it]                                                       {'loss': 1678.2941, 'grad_norm': 17664.0, 'learning_rate': 0.0006983471131688238, 'epoch': 0.5}
  8%|▊         | 397/5000 [1:44:46<19:40:11, 15.38s/it]  8%|▊         | 398/5000 [1:45:12<23:35:30, 18.46s/it]                                                       {'loss': 1688.0071, 'grad_norm': 17920.0, 'learning_rate': 0.0006983245664418234, 'epoch': 0.51}
  8%|▊         | 398/5000 [1:45:12<23:35:30, 18.46s/it]  8%|▊         | 399/5000 [1:45:32<24:07:05, 18.87s/it]                                                       {'loss': 1768.571, 'grad_norm': 25344.0, 'learning_rate': 0.0006983018673458238, 'epoch': 0.51}
  8%|▊         | 399/5000 [1:45:32<24:07:05, 18.87s/it]  8%|▊         | 400/5000 [1:45:49<23:28:38, 18.37s/it]                                                       {'loss': 1589.7026, 'grad_norm': 94720.0, 'learning_rate': 0.000698279015890754, 'epoch': 0.51}
  8%|▊         | 400/5000 [1:45:49<23:28:38, 18.37s/it]  8%|▊         | 401/5000 [1:46:03<21:49:38, 17.09s/it]                                                       {'loss': 1523.6243, 'grad_norm': 21888.0, 'learning_rate': 0.0006982560120866102, 'epoch': 0.51}
  8%|▊         | 401/5000 [1:46:03<21:49:38, 17.09s/it]  8%|▊         | 402/5000 [1:46:22<22:30:30, 17.62s/it]                                                       {'loss': 1492.4326, 'grad_norm': 17920.0, 'learning_rate': 0.0006982328559434551, 'epoch': 0.51}
  8%|▊         | 402/5000 [1:46:22<22:30:30, 17.62s/it]  8%|▊         | 403/5000 [1:46:35<20:58:04, 16.42s/it]                                                       {'loss': 1764.0876, 'grad_norm': 36096.0, 'learning_rate': 0.0006982095474714178, 'epoch': 0.51}
  8%|▊         | 403/5000 [1:46:35<20:58:04, 16.42s/it]  8%|▊         | 404/5000 [1:46:50<20:09:43, 15.79s/it]                                                       {'loss': 1397.9761, 'grad_norm': 45824.0, 'learning_rate': 0.0006981860866806944, 'epoch': 0.51}
  8%|▊         | 404/5000 [1:46:50<20:09:43, 15.79s/it]  8%|▊         | 405/5000 [1:47:06<20:28:39, 16.04s/it]                                                       {'loss': 1579.4867, 'grad_norm': 19200.0, 'learning_rate': 0.0006981624735815474, 'epoch': 0.51}
  8%|▊         | 405/5000 [1:47:06<20:28:39, 16.04s/it]  8%|▊         | 406/5000 [1:47:28<22:46:09, 17.84s/it]                                                       {'loss': 1214.1893, 'grad_norm': 13504.0, 'learning_rate': 0.0006981387081843057, 'epoch': 0.52}
  8%|▊         | 406/5000 [1:47:28<22:46:09, 17.84s/it]  8%|▊         | 407/5000 [1:47:42<21:17:40, 16.69s/it]                                                       {'loss': 1535.0791, 'grad_norm': 22400.0, 'learning_rate': 0.0006981147904993655, 'epoch': 0.52}
  8%|▊         | 407/5000 [1:47:42<21:17:40, 16.69s/it]  8%|▊         | 408/5000 [1:48:06<24:01:14, 18.83s/it]                                                       {'loss': 1542.7305, 'grad_norm': 20736.0, 'learning_rate': 0.000698090720537189, 'epoch': 0.52}
  8%|▊         | 408/5000 [1:48:06<24:01:14, 18.83s/it]  8%|▊         | 409/5000 [1:48:24<23:44:51, 18.62s/it]                                                       {'loss': 1541.1902, 'grad_norm': 26624.0, 'learning_rate': 0.0006980664983083053, 'epoch': 0.52}
  8%|▊         | 409/5000 [1:48:24<23:44:51, 18.62s/it]  8%|▊         | 410/5000 [1:48:39<22:03:35, 17.30s/it]                                                       {'loss': 1581.1101, 'grad_norm': 27008.0, 'learning_rate': 0.00069804212382331, 'epoch': 0.52}
  8%|▊         | 410/5000 [1:48:39<22:03:35, 17.30s/it]  8%|▊         | 411/5000 [1:49:00<23:28:33, 18.42s/it]                                                       {'loss': 1485.5292, 'grad_norm': 7296.0, 'learning_rate': 0.0006980175970928654, 'epoch': 0.52}
  8%|▊         | 411/5000 [1:49:00<23:28:33, 18.42s/it]  8%|▊         | 412/5000 [1:49:14<21:55:57, 17.21s/it]                                                       {'loss': 1441.6603, 'grad_norm': 8384.0, 'learning_rate': 0.0006979929181277, 'epoch': 0.52}
  8%|▊         | 412/5000 [1:49:14<21:55:57, 17.21s/it]  8%|▊         | 413/5000 [1:49:31<21:50:12, 17.14s/it]                                                       {'loss': 1243.9172, 'grad_norm': 25856.0, 'learning_rate': 0.0006979680869386096, 'epoch': 0.52}
  8%|▊         | 413/5000 [1:49:31<21:50:12, 17.14s/it]  8%|▊         | 414/5000 [1:49:50<22:28:45, 17.65s/it]                                                       {'loss': 1161.6782, 'grad_norm': 13120.0, 'learning_rate': 0.000697943103536456, 'epoch': 0.53}
  8%|▊         | 414/5000 [1:49:50<22:28:45, 17.65s/it]  8%|▊         | 415/5000 [1:50:06<21:46:08, 17.09s/it]                                                       {'loss': 1442.861, 'grad_norm': 4416.0, 'learning_rate': 0.000697917967932168, 'epoch': 0.53}
  8%|▊         | 415/5000 [1:50:06<21:46:08, 17.09s/it]  8%|▊         | 416/5000 [1:50:20<20:45:58, 16.31s/it]                                                       {'loss': 1407.985, 'grad_norm': 26112.0, 'learning_rate': 0.0006978926801367405, 'epoch': 0.53}
  8%|▊         | 416/5000 [1:50:20<20:45:58, 16.31s/it]  8%|▊         | 417/5000 [1:50:45<24:05:55, 18.93s/it]                                                       {'loss': 1067.2172, 'grad_norm': 3696.0, 'learning_rate': 0.0006978672401612353, 'epoch': 0.53}
  8%|▊         | 417/5000 [1:50:45<24:05:55, 18.93s/it]  8%|▊         | 418/5000 [1:50:56<21:08:52, 16.62s/it]                                                       {'loss': 1050.8528, 'grad_norm': 10432.0, 'learning_rate': 0.0006978416480167808, 'epoch': 0.53}
  8%|▊         | 418/5000 [1:50:56<21:08:52, 16.62s/it]  8%|▊         | 419/5000 [1:51:10<20:05:47, 15.79s/it]                                                       {'loss': 1214.4592, 'grad_norm': 15040.0, 'learning_rate': 0.0006978159037145719, 'epoch': 0.53}
  8%|▊         | 419/5000 [1:51:10<20:05:47, 15.79s/it]  8%|▊         | 420/5000 [1:51:25<19:38:42, 15.44s/it]                                                       {'loss': 1192.926, 'grad_norm': 9920.0, 'learning_rate': 0.00069779000726587, 'epoch': 0.53}
  8%|▊         | 420/5000 [1:51:25<19:38:42, 15.44s/it]  8%|▊         | 421/5000 [1:51:41<19:57:03, 15.69s/it]                                                       {'loss': 1006.033, 'grad_norm': 18560.0, 'learning_rate': 0.0006977639586820029, 'epoch': 0.53}
  8%|▊         | 421/5000 [1:51:41<19:57:03, 15.69s/it]  8%|▊         | 422/5000 [1:51:58<20:23:32, 16.04s/it]                                                       {'loss': 1156.3142, 'grad_norm': 3120.0, 'learning_rate': 0.0006977377579743654, 'epoch': 0.54}
  8%|▊         | 422/5000 [1:51:58<20:23:32, 16.04s/it]  8%|▊         | 423/5000 [1:52:13<20:02:12, 15.76s/it]                                                       {'loss': 1204.8142, 'grad_norm': 3792.0, 'learning_rate': 0.0006977114051544185, 'epoch': 0.54}
  8%|▊         | 423/5000 [1:52:13<20:02:12, 15.76s/it]  8%|▊         | 424/5000 [1:52:28<19:50:48, 15.61s/it]                                                       {'loss': 919.5334, 'grad_norm': 2144.0, 'learning_rate': 0.0006976849002336897, 'epoch': 0.54}
  8%|▊         | 424/5000 [1:52:28<19:50:48, 15.61s/it]  8%|▊         | 425/5000 [1:52:46<20:29:39, 16.13s/it]                                                       {'loss': 912.0972, 'grad_norm': 3952.0, 'learning_rate': 0.0006976582432237734, 'epoch': 0.54}
  8%|▊         | 425/5000 [1:52:46<20:29:39, 16.13s/it]  9%|▊         | 426/5000 [1:53:05<21:54:01, 17.24s/it]                                                       {'loss': 902.0771, 'grad_norm': 5888.0, 'learning_rate': 0.0006976314341363301, 'epoch': 0.54}
  9%|▊         | 426/5000 [1:53:05<21:54:01, 17.24s/it]  9%|▊         | 427/5000 [1:53:24<22:24:00, 17.63s/it]                                                       {'loss': 827.683, 'grad_norm': 7520.0, 'learning_rate': 0.0006976044729830868, 'epoch': 0.54}
  9%|▊         | 427/5000 [1:53:24<22:24:00, 17.63s/it]  9%|▊         | 428/5000 [1:53:40<21:39:21, 17.05s/it]                                                       {'loss': 971.9738, 'grad_norm': 9600.0, 'learning_rate': 0.0006975773597758377, 'epoch': 0.54}
  9%|▊         | 428/5000 [1:53:40<21:39:21, 17.05s/it]  9%|▊         | 429/5000 [1:53:57<21:47:36, 17.16s/it]                                                       {'loss': 958.2922, 'grad_norm': 1216.0, 'learning_rate': 0.0006975500945264428, 'epoch': 0.54}
  9%|▊         | 429/5000 [1:53:57<21:47:36, 17.16s/it]  9%|▊         | 430/5000 [1:54:22<24:49:58, 19.56s/it]                                                       {'loss': 919.9536, 'grad_norm': 4832.0, 'learning_rate': 0.0006975226772468286, 'epoch': 0.55}
  9%|▊         | 430/5000 [1:54:22<24:49:58, 19.56s/it]  9%|▊         | 431/5000 [1:54:37<22:51:20, 18.01s/it]                                                       {'loss': 1055.5664, 'grad_norm': 9792.0, 'learning_rate': 0.0006974951079489888, 'epoch': 0.55}
  9%|▊         | 431/5000 [1:54:37<22:51:20, 18.01s/it]  9%|▊         | 432/5000 [1:54:52<21:53:55, 17.26s/it]                                                       {'loss': 882.0188, 'grad_norm': 4704.0, 'learning_rate': 0.0006974673866449829, 'epoch': 0.55}
  9%|▊         | 432/5000 [1:54:52<21:53:55, 17.26s/it]  9%|▊         | 433/5000 [1:55:07<20:50:42, 16.43s/it]                                                       {'loss': 1025.24, 'grad_norm': 2352.0, 'learning_rate': 0.0006974395133469371, 'epoch': 0.55}
  9%|▊         | 433/5000 [1:55:07<20:50:42, 16.43s/it]  9%|▊         | 434/5000 [1:55:26<21:49:31, 17.21s/it]                                                       {'loss': 864.6474, 'grad_norm': 9024.0, 'learning_rate': 0.0006974114880670442, 'epoch': 0.55}
  9%|▊         | 434/5000 [1:55:26<21:49:31, 17.21s/it]  9%|▊         | 435/5000 [1:55:44<22:09:33, 17.48s/it]                                                       {'loss': 904.3644, 'grad_norm': 1832.0, 'learning_rate': 0.0006973833108175636, 'epoch': 0.55}
  9%|▊         | 435/5000 [1:55:44<22:09:33, 17.48s/it]  9%|▊         | 436/5000 [1:55:57<20:25:44, 16.11s/it]                                                       {'loss': 877.6108, 'grad_norm': 896.0, 'learning_rate': 0.0006973549816108207, 'epoch': 0.55}
  9%|▊         | 436/5000 [1:55:57<20:25:44, 16.11s/it]  9%|▊         | 437/5000 [1:56:13<20:29:23, 16.17s/it]                                                       {'loss': 741.4004, 'grad_norm': 948.0, 'learning_rate': 0.0006973265004592078, 'epoch': 0.55}
  9%|▊         | 437/5000 [1:56:13<20:29:23, 16.17s/it]  9%|▉         | 438/5000 [1:56:37<23:20:22, 18.42s/it]                                                       {'loss': 1013.2301, 'grad_norm': 28672.0, 'learning_rate': 0.0006972978673751834, 'epoch': 0.56}
  9%|▉         | 438/5000 [1:56:37<23:20:22, 18.42s/it]  9%|▉         | 439/5000 [1:56:58<24:35:26, 19.41s/it]                                                       {'loss': 2160.125, 'grad_norm': 51968.0, 'learning_rate': 0.0006972690823712727, 'epoch': 0.56}
  9%|▉         | 439/5000 [1:56:58<24:35:26, 19.41s/it]  9%|▉         | 440/5000 [1:57:10<21:33:51, 17.02s/it]                                                       {'loss': 1850.6189, 'grad_norm': 196608.0, 'learning_rate': 0.0006972401454600673, 'epoch': 0.56}
  9%|▉         | 440/5000 [1:57:10<21:33:51, 17.02s/it]  9%|▉         | 441/5000 [1:57:23<20:13:28, 15.97s/it]                                                       {'loss': 1981.6931, 'grad_norm': 83968.0, 'learning_rate': 0.0006972110566542249, 'epoch': 0.56}
  9%|▉         | 441/5000 [1:57:23<20:13:28, 15.97s/it]  9%|▉         | 442/5000 [1:57:42<21:05:33, 16.66s/it]                                                       {'loss': 2007.158, 'grad_norm': 22656.0, 'learning_rate': 0.0006971818159664703, 'epoch': 0.56}
  9%|▉         | 442/5000 [1:57:42<21:05:33, 16.66s/it]  9%|▉         | 443/5000 [1:58:00<21:35:01, 17.05s/it]                                                       {'loss': 2350.824, 'grad_norm': 38144.0, 'learning_rate': 0.000697152423409594, 'epoch': 0.56}
  9%|▉         | 443/5000 [1:58:00<21:35:01, 17.05s/it]  9%|▉         | 444/5000 [1:58:22<23:27:38, 18.54s/it]                                                       {'loss': 2017.8018, 'grad_norm': 18944.0, 'learning_rate': 0.0006971228789964537, 'epoch': 0.56}
  9%|▉         | 444/5000 [1:58:22<23:27:38, 18.54s/it]  9%|▉         | 445/5000 [1:58:40<23:15:11, 18.38s/it]                                                       {'loss': 2010.577, 'grad_norm': 232448.0, 'learning_rate': 0.0006970931827399729, 'epoch': 0.57}
  9%|▉         | 445/5000 [1:58:40<23:15:11, 18.38s/it]  9%|▉         | 446/5000 [1:58:54<21:51:38, 17.28s/it]                                                       {'loss': 1880.2275, 'grad_norm': 21632.0, 'learning_rate': 0.0006970633346531417, 'epoch': 0.57}
  9%|▉         | 446/5000 [1:58:54<21:51:38, 17.28s/it]  9%|▉         | 447/5000 [1:59:11<21:30:43, 17.01s/it]                                                       {'loss': 2048.5842, 'grad_norm': 81920.0, 'learning_rate': 0.0006970333347490168, 'epoch': 0.57}
  9%|▉         | 447/5000 [1:59:11<21:30:43, 17.01s/it]  9%|▉         | 448/5000 [1:59:25<20:19:08, 16.07s/it]                                                       {'loss': 1760.978, 'grad_norm': 147456.0, 'learning_rate': 0.0006970031830407211, 'epoch': 0.57}
  9%|▉         | 448/5000 [1:59:25<20:19:08, 16.07s/it]  9%|▉         | 449/5000 [1:59:36<18:41:12, 14.78s/it]                                                       {'loss': 1994.1272, 'grad_norm': 11712.0, 'learning_rate': 0.000696972879541444, 'epoch': 0.57}
  9%|▉         | 449/5000 [1:59:36<18:41:12, 14.78s/it]  9%|▉         | 450/5000 [1:59:47<17:04:52, 13.51s/it]                                                       {'loss': 1339.3445, 'grad_norm': 11648.0, 'learning_rate': 0.0006969424242644413, 'epoch': 0.57}
  9%|▉         | 450/5000 [1:59:47<17:04:52, 13.51s/it]  9%|▉         | 451/5000 [1:59:58<16:08:05, 12.77s/it]                                                       {'loss': 1311.3174, 'grad_norm': 839680.0, 'learning_rate': 0.0006969118172230352, 'epoch': 0.57}
  9%|▉         | 451/5000 [1:59:58<16:08:05, 12.77s/it]  9%|▉         | 452/5000 [2:00:10<15:42:18, 12.43s/it]                                                       {'loss': 1256.4177, 'grad_norm': 7136.0, 'learning_rate': 0.0006968810584306142, 'epoch': 0.57}
  9%|▉         | 452/5000 [2:00:10<15:42:18, 12.43s/it]  9%|▉         | 453/5000 [2:00:29<18:14:54, 14.45s/it]                                                       {'loss': 1152.9736, 'grad_norm': 14528.0, 'learning_rate': 0.0006968501479006332, 'epoch': 0.58}
  9%|▉         | 453/5000 [2:00:29<18:14:54, 14.45s/it]  9%|▉         | 454/5000 [2:00:41<17:24:49, 13.79s/it]                                                       {'loss': 1143.5858, 'grad_norm': 33024.0, 'learning_rate': 0.0006968190856466136, 'epoch': 0.58}
  9%|▉         | 454/5000 [2:00:41<17:24:49, 13.79s/it]  9%|▉         | 455/5000 [2:00:59<18:50:19, 14.92s/it]                                                       {'loss': 1108.2131, 'grad_norm': 16256.0, 'learning_rate': 0.0006967878716821431, 'epoch': 0.58}
  9%|▉         | 455/5000 [2:00:59<18:50:19, 14.92s/it]  9%|▉         | 456/5000 [2:01:13<18:40:01, 14.79s/it]                                                       {'loss': 1173.0039, 'grad_norm': 4128.0, 'learning_rate': 0.0006967565060208757, 'epoch': 0.58}
  9%|▉         | 456/5000 [2:01:13<18:40:01, 14.79s/it]  9%|▉         | 457/5000 [2:01:30<19:26:50, 15.41s/it]                                                       {'loss': 1181.5195, 'grad_norm': 5216.0, 'learning_rate': 0.0006967249886765317, 'epoch': 0.58}
  9%|▉         | 457/5000 [2:01:30<19:26:50, 15.41s/it]  9%|▉         | 458/5000 [2:01:49<20:45:38, 16.45s/it]                                                       {'loss': 940.2101, 'grad_norm': 18944.0, 'learning_rate': 0.0006966933196628981, 'epoch': 0.58}
  9%|▉         | 458/5000 [2:01:49<20:45:38, 16.45s/it]  9%|▉         | 459/5000 [2:02:05<20:35:17, 16.32s/it]                                                       {'loss': 1230.1132, 'grad_norm': 4544.0, 'learning_rate': 0.0006966614989938279, 'epoch': 0.58}
  9%|▉         | 459/5000 [2:02:05<20:35:17, 16.32s/it]  9%|▉         | 460/5000 [2:02:22<21:00:05, 16.65s/it]                                                       {'loss': 1020.0217, 'grad_norm': 3872.0, 'learning_rate': 0.0006966295266832405, 'epoch': 0.58}
  9%|▉         | 460/5000 [2:02:22<21:00:05, 16.65s/it]  9%|▉         | 461/5000 [2:02:42<22:01:16, 17.47s/it]                                                       {'loss': 890.9955, 'grad_norm': 15488.0, 'learning_rate': 0.0006965974027451216, 'epoch': 0.59}
  9%|▉         | 461/5000 [2:02:42<22:01:16, 17.47s/it]  9%|▉         | 462/5000 [2:02:54<20:11:48, 16.02s/it]                                                       {'loss': 1123.6357, 'grad_norm': 2288.0, 'learning_rate': 0.0006965651271935235, 'epoch': 0.59}
  9%|▉         | 462/5000 [2:02:54<20:11:48, 16.02s/it]  9%|▉         | 463/5000 [2:03:14<21:38:05, 17.17s/it]                                                       {'loss': 1059.4949, 'grad_norm': 8384.0, 'learning_rate': 0.0006965327000425645, 'epoch': 0.59}
  9%|▉         | 463/5000 [2:03:14<21:38:05, 17.17s/it]  9%|▉         | 464/5000 [2:03:30<20:57:55, 16.64s/it]                                                       {'loss': 1128.2603, 'grad_norm': 7008.0, 'learning_rate': 0.0006965001213064293, 'epoch': 0.59}
  9%|▉         | 464/5000 [2:03:30<20:57:55, 16.64s/it]  9%|▉         | 465/5000 [2:03:44<20:01:12, 15.89s/it]                                                       {'loss': 971.3619, 'grad_norm': 4832.0, 'learning_rate': 0.000696467390999369, 'epoch': 0.59}
  9%|▉         | 465/5000 [2:03:44<20:01:12, 15.89s/it]  9%|▉         | 466/5000 [2:03:59<19:46:32, 15.70s/it]                                                       {'loss': 890.5326, 'grad_norm': 4704.0, 'learning_rate': 0.000696434509135701, 'epoch': 0.59}
  9%|▉         | 466/5000 [2:03:59<19:46:32, 15.70s/it]  9%|▉         | 467/5000 [2:04:17<20:36:03, 16.36s/it]                                                       {'loss': 990.1984, 'grad_norm': 4000.0, 'learning_rate': 0.0006964014757298089, 'epoch': 0.59}
  9%|▉         | 467/5000 [2:04:17<20:36:03, 16.36s/it]  9%|▉         | 468/5000 [2:04:35<21:19:07, 16.93s/it]                                                       {'loss': 854.811, 'grad_norm': 2528.0, 'learning_rate': 0.0006963682907961425, 'epoch': 0.59}
  9%|▉         | 468/5000 [2:04:35<21:19:07, 16.93s/it]  9%|▉         | 469/5000 [2:04:49<20:11:06, 16.04s/it]                                                       {'loss': 950.1022, 'grad_norm': 1288.0, 'learning_rate': 0.0006963349543492183, 'epoch': 0.6}
  9%|▉         | 469/5000 [2:04:49<20:11:06, 16.04s/it]  9%|▉         | 470/5000 [2:05:08<21:14:46, 16.88s/it]                                                       {'loss': 877.9464, 'grad_norm': 7776.0, 'learning_rate': 0.0006963014664036186, 'epoch': 0.6}
  9%|▉         | 470/5000 [2:05:08<21:14:46, 16.88s/it]  9%|▉         | 471/5000 [2:05:24<21:01:39, 16.71s/it]                                                       {'loss': 1260.9158, 'grad_norm': 164864.0, 'learning_rate': 0.0006962678269739922, 'epoch': 0.6}
  9%|▉         | 471/5000 [2:05:24<21:01:39, 16.71s/it]  9%|▉         | 472/5000 [2:05:44<22:09:42, 17.62s/it]                                                       {'loss': 2178.2871, 'grad_norm': 179200.0, 'learning_rate': 0.0006962340360750541, 'epoch': 0.6}
  9%|▉         | 472/5000 [2:05:44<22:09:42, 17.62s/it]  9%|▉         | 473/5000 [2:05:59<21:05:02, 16.77s/it]                                                       {'loss': 1978.121, 'grad_norm': 59648.0, 'learning_rate': 0.0006962000937215856, 'epoch': 0.6}
  9%|▉         | 473/5000 [2:05:59<21:05:02, 16.77s/it]  9%|▉         | 474/5000 [2:06:16<21:24:44, 17.03s/it]                                                       {'loss': 1391.6666, 'grad_norm': 56320.0, 'learning_rate': 0.0006961659999284343, 'epoch': 0.6}
  9%|▉         | 474/5000 [2:06:16<21:24:44, 17.03s/it] 10%|▉         | 475/5000 [2:06:30<20:03:28, 15.96s/it]                                                       {'loss': 2223.5347, 'grad_norm': 292864.0, 'learning_rate': 0.0006961317547105139, 'epoch': 0.6}
 10%|▉         | 475/5000 [2:06:30<20:03:28, 15.96s/it] 10%|▉         | 476/5000 [2:06:45<19:34:39, 15.58s/it]                                                       {'loss': 1966.7612, 'grad_norm': 32768.0, 'learning_rate': 0.0006960973580828045, 'epoch': 0.6}
 10%|▉         | 476/5000 [2:06:45<19:34:39, 15.58s/it] 10%|▉         | 477/5000 [2:07:03<20:44:01, 16.50s/it]                                                       {'loss': 1669.1963, 'grad_norm': 111616.0, 'learning_rate': 0.0006960628100603524, 'epoch': 0.61}
 10%|▉         | 477/5000 [2:07:03<20:44:01, 16.50s/it] 10%|▉         | 478/5000 [2:07:16<19:24:41, 15.45s/it]                                                       {'loss': 2013.2764, 'grad_norm': 76800.0, 'learning_rate': 0.0006960281106582699, 'epoch': 0.61}
 10%|▉         | 478/5000 [2:07:16<19:24:41, 15.45s/it] 10%|▉         | 479/5000 [2:07:34<20:20:33, 16.20s/it]                                                       {'loss': 2070.2368, 'grad_norm': 41680896.0, 'learning_rate': 0.0006959932598917358, 'epoch': 0.61}
 10%|▉         | 479/5000 [2:07:34<20:20:33, 16.20s/it] 10%|▉         | 480/5000 [2:07:49<19:57:46, 15.90s/it]                                                       {'loss': 2154.9702, 'grad_norm': 12864.0, 'learning_rate': 0.000695958257775995, 'epoch': 0.61}
 10%|▉         | 480/5000 [2:07:49<19:57:46, 15.90s/it] 10%|▉         | 481/5000 [2:08:06<20:17:58, 16.17s/it]                                                       {'loss': 2280.6572, 'grad_norm': 28544.0, 'learning_rate': 0.0006959231043263586, 'epoch': 0.61}
 10%|▉         | 481/5000 [2:08:06<20:17:58, 16.17s/it] 10%|▉         | 482/5000 [2:08:24<20:53:27, 16.65s/it]                                                       {'loss': 2525.9595, 'grad_norm': 9088.0, 'learning_rate': 0.000695887799558204, 'epoch': 0.61}
 10%|▉         | 482/5000 [2:08:24<20:53:27, 16.65s/it] 10%|▉         | 483/5000 [2:08:50<24:21:40, 19.42s/it]                                                       {'loss': 1691.7831, 'grad_norm': 51712.0, 'learning_rate': 0.0006958523434869746, 'epoch': 0.61}
 10%|▉         | 483/5000 [2:08:50<24:21:40, 19.42s/it] 10%|▉         | 484/5000 [2:09:03<21:55:18, 17.48s/it]                                                       {'loss': 1987.8765, 'grad_norm': 141312.0, 'learning_rate': 0.0006958167361281801, 'epoch': 0.61}
 10%|▉         | 484/5000 [2:09:03<21:55:18, 17.48s/it] 10%|▉         | 485/5000 [2:09:26<24:11:20, 19.29s/it]                                                       {'loss': 2213.0388, 'grad_norm': 7264.0, 'learning_rate': 0.0006957809774973965, 'epoch': 0.62}
 10%|▉         | 485/5000 [2:09:26<24:11:20, 19.29s/it] 10%|▉         | 486/5000 [2:09:55<27:45:43, 22.14s/it]                                                       {'loss': 1666.2067, 'grad_norm': 22016.0, 'learning_rate': 0.0006957450676102656, 'epoch': 0.62}
 10%|▉         | 486/5000 [2:09:55<27:45:43, 22.14s/it] 10%|▉         | 487/5000 [2:10:22<29:29:31, 23.53s/it]                                                       {'loss': 1857.1978, 'grad_norm': 6400.0, 'learning_rate': 0.0006957090064824957, 'epoch': 0.62}
 10%|▉         | 487/5000 [2:10:22<29:29:31, 23.53s/it] 10%|▉         | 488/5000 [2:10:39<27:02:10, 21.57s/it]                                                       {'loss': 1714.2073, 'grad_norm': 47872.0, 'learning_rate': 0.0006956727941298614, 'epoch': 0.62}
 10%|▉         | 488/5000 [2:10:39<27:02:10, 21.57s/it] 10%|▉         | 489/5000 [2:10:56<25:20:07, 20.22s/it]                                                       {'loss': 1846.1228, 'grad_norm': 5280.0, 'learning_rate': 0.0006956364305682029, 'epoch': 0.62}
 10%|▉         | 489/5000 [2:10:56<25:20:07, 20.22s/it] 10%|▉         | 490/5000 [2:11:09<22:35:35, 18.03s/it]                                                       {'loss': 1305.3885, 'grad_norm': 10496.0, 'learning_rate': 0.0006955999158134269, 'epoch': 0.62}
 10%|▉         | 490/5000 [2:11:09<22:35:35, 18.03s/it] 10%|▉         | 491/5000 [2:11:26<22:16:07, 17.78s/it]                                                       {'loss': 1325.6309, 'grad_norm': 7488.0, 'learning_rate': 0.0006955632498815064, 'epoch': 0.62}
 10%|▉         | 491/5000 [2:11:26<22:16:07, 17.78s/it] 10%|▉         | 492/5000 [2:11:55<26:29:53, 21.16s/it]                                                       {'loss': 1470.7539, 'grad_norm': 15872.0, 'learning_rate': 0.0006955264327884802, 'epoch': 0.62}
 10%|▉         | 492/5000 [2:11:55<26:29:53, 21.16s/it] 10%|▉         | 493/5000 [2:12:08<23:21:26, 18.66s/it]                                                       {'loss': 1341.3813, 'grad_norm': 13504.0, 'learning_rate': 0.0006954894645504532, 'epoch': 0.63}
 10%|▉         | 493/5000 [2:12:08<23:21:26, 18.66s/it] 10%|▉         | 494/5000 [2:12:26<23:11:18, 18.53s/it]                                                       {'loss': 1385.1353, 'grad_norm': 12992.0, 'learning_rate': 0.0006954523451835968, 'epoch': 0.63}
 10%|▉         | 494/5000 [2:12:26<23:11:18, 18.53s/it] 10%|▉         | 495/5000 [2:12:50<25:11:26, 20.13s/it]                                                       {'loss': 1438.729, 'grad_norm': 35072.0, 'learning_rate': 0.0006954150747041481, 'epoch': 0.63}
 10%|▉         | 495/5000 [2:12:50<25:11:26, 20.13s/it] 10%|▉         | 496/5000 [2:13:05<23:12:53, 18.56s/it]                                                       {'loss': 1575.2455, 'grad_norm': 12032.0, 'learning_rate': 0.0006953776531284107, 'epoch': 0.63}
 10%|▉         | 496/5000 [2:13:05<23:12:53, 18.56s/it] 10%|▉         | 497/5000 [2:13:30<25:48:06, 20.63s/it]                                                       {'loss': 1604.1218, 'grad_norm': 15104.0, 'learning_rate': 0.0006953400804727538, 'epoch': 0.63}
 10%|▉         | 497/5000 [2:13:30<25:48:06, 20.63s/it] 10%|▉         | 498/5000 [2:13:49<24:57:35, 19.96s/it]                                                       {'loss': 1521.1085, 'grad_norm': 16768.0, 'learning_rate': 0.0006953023567536132, 'epoch': 0.63}
 10%|▉         | 498/5000 [2:13:49<24:57:35, 19.96s/it] 10%|▉         | 499/5000 [2:14:03<22:56:07, 18.34s/it]                                                       {'loss': 1338.3966, 'grad_norm': 5536.0, 'learning_rate': 0.0006952644819874903, 'epoch': 0.63}
 10%|▉         | 499/5000 [2:14:03<22:56:07, 18.34s/it] 10%|█         | 500/5000 [2:14:28<25:21:41, 20.29s/it]                                                       {'loss': 1697.5093, 'grad_norm': 16064.0, 'learning_rate': 0.0006952264561909528, 'epoch': 0.63}
 10%|█         | 500/5000 [2:14:28<25:21:41, 20.29s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:03<02:37,  1.84s/it][A
  3%|▎         | 3/88 [00:05<02:42,  1.92s/it][A
  5%|▍         | 4/88 [00:09<03:32,  2.53s/it][A
  6%|▌         | 5/88 [00:12<04:02,  2.92s/it][A
  7%|▋         | 6/88 [00:16<04:06,  3.01s/it][A
  8%|▊         | 7/88 [00:20<04:51,  3.60s/it][A
  9%|▉         | 8/88 [00:24<04:36,  3.45s/it][A
 10%|█         | 9/88 [00:27<04:21,  3.31s/it][A
 11%|█▏        | 10/88 [00:31<04:36,  3.55s/it][A
 12%|█▎        | 11/88 [00:34<04:21,  3.40s/it][A
 14%|█▎        | 12/88 [00:43<06:31,  5.15s/it][A
 15%|█▍        | 13/88 [00:46<05:45,  4.61s/it][A
 16%|█▌        | 14/88 [00:54<06:52,  5.57s/it][A
 17%|█▋        | 15/88 [00:59<06:23,  5.26s/it][A
 18%|█▊        | 16/88 [01:04<06:16,  5.23s/it][A
 19%|█▉        | 17/88 [01:06<05:14,  4.43s/it][A
 20%|██        | 18/88 [01:10<04:59,  4.28s/it][A
 22%|██▏       | 19/88 [01:14<04:45,  4.14s/it][A
 23%|██▎       | 20/88 [01:23<06:16,  5.54s/it][A
 24%|██▍       | 21/88 [01:26<05:22,  4.82s/it][A
 25%|██▌       | 22/88 [01:29<04:47,  4.35s/it][A
 26%|██▌       | 23/88 [01:32<04:02,  3.73s/it][A
 27%|██▋       | 24/88 [01:35<03:55,  3.68s/it][A
 28%|██▊       | 25/88 [01:39<03:51,  3.67s/it][A
 30%|██▉       | 26/88 [01:44<04:07,  4.00s/it][A
 31%|███       | 27/88 [01:47<03:54,  3.85s/it][A
 32%|███▏      | 28/88 [01:52<04:09,  4.16s/it][A
 33%|███▎      | 29/88 [01:55<03:39,  3.72s/it][A
 34%|███▍      | 30/88 [02:00<03:58,  4.12s/it][A
 35%|███▌      | 31/88 [02:06<04:30,  4.74s/it][A
 36%|███▋      | 32/88 [02:09<04:03,  4.34s/it][A
 38%|███▊      | 33/88 [02:12<03:33,  3.89s/it][A
 39%|███▊      | 34/88 [02:16<03:28,  3.86s/it][A
 40%|███▉      | 35/88 [02:24<04:37,  5.23s/it][A
 41%|████      | 36/88 [02:29<04:20,  5.02s/it][A
 42%|████▏     | 37/88 [02:38<05:24,  6.36s/it][A
 43%|████▎     | 38/88 [02:44<05:00,  6.01s/it][A
 44%|████▍     | 39/88 [02:47<04:13,  5.17s/it][A
 45%|████▌     | 40/88 [02:52<04:03,  5.08s/it][A
 47%|████▋     | 41/88 [02:55<03:40,  4.69s/it][A
 48%|████▊     | 42/88 [02:59<03:19,  4.34s/it][A
 49%|████▉     | 43/88 [03:02<03:00,  4.00s/it][A
 50%|█████     | 44/88 [03:04<02:33,  3.49s/it][A
 51%|█████     | 45/88 [03:10<02:56,  4.11s/it][A
 52%|█████▏    | 46/88 [03:13<02:36,  3.72s/it][A
 53%|█████▎    | 47/88 [03:15<02:18,  3.38s/it][A
 55%|█████▍    | 48/88 [03:20<02:26,  3.67s/it][A
 56%|█████▌    | 49/88 [03:24<02:29,  3.84s/it][A
 57%|█████▋    | 50/88 [03:27<02:20,  3.71s/it][A
 58%|█████▊    | 51/88 [03:34<02:51,  4.62s/it][A
 59%|█████▉    | 52/88 [03:38<02:39,  4.42s/it][A
 60%|██████    | 53/88 [03:42<02:25,  4.15s/it][A
 61%|██████▏   | 54/88 [03:45<02:17,  4.04s/it][A
 62%|██████▎   | 55/88 [03:49<02:13,  4.03s/it][A
 64%|██████▎   | 56/88 [03:57<02:41,  5.04s/it][A
 65%|██████▍   | 57/88 [04:01<02:32,  4.92s/it][A
 66%|██████▌   | 58/88 [04:05<02:11,  4.38s/it][A
 67%|██████▋   | 59/88 [04:13<02:46,  5.75s/it][A
 68%|██████▊   | 60/88 [04:17<02:25,  5.18s/it][A
 69%|██████▉   | 61/88 [04:25<02:39,  5.92s/it][A
 70%|███████   | 62/88 [04:34<02:58,  6.87s/it][A
 72%|███████▏  | 63/88 [04:39<02:34,  6.17s/it][A
 73%|███████▎  | 64/88 [04:47<02:46,  6.95s/it][A
 74%|███████▍  | 65/88 [04:51<02:20,  6.11s/it][A
 75%|███████▌  | 66/88 [04:56<02:05,  5.71s/it][A
 76%|███████▌  | 67/88 [04:58<01:37,  4.66s/it][A
 77%|███████▋  | 68/88 [05:07<01:56,  5.81s/it][A
 78%|███████▊  | 69/88 [05:15<02:04,  6.56s/it][A
 80%|███████▉  | 70/88 [05:21<01:55,  6.40s/it][A
 81%|████████  | 71/88 [05:26<01:38,  5.82s/it][A
 82%|████████▏ | 72/88 [05:29<01:21,  5.08s/it][A
 83%|████████▎ | 73/88 [05:35<01:21,  5.41s/it][A
 84%|████████▍ | 74/88 [05:41<01:16,  5.46s/it][A
 85%|████████▌ | 75/88 [05:45<01:04,  4.97s/it][A
 86%|████████▋ | 76/88 [05:48<00:52,  4.34s/it][A
 88%|████████▊ | 77/88 [05:51<00:45,  4.14s/it][A
 89%|████████▊ | 78/88 [05:55<00:41,  4.12s/it][A
 90%|████████▉ | 79/88 [05:59<00:36,  4.00s/it][A
 91%|█████████ | 80/88 [06:01<00:27,  3.40s/it][A
 92%|█████████▏| 81/88 [06:05<00:24,  3.51s/it][A
 93%|█████████▎| 82/88 [06:09<00:21,  3.58s/it][A
 94%|█████████▍| 83/88 [06:14<00:20,  4.08s/it][A
 95%|█████████▌| 84/88 [06:17<00:14,  3.70s/it][A
 97%|█████████▋| 85/88 [06:20<00:10,  3.66s/it][A
 98%|█████████▊| 86/88 [06:26<00:08,  4.26s/it][A
 99%|█████████▉| 87/88 [06:30<00:04,  4.14s/it][A
100%|██████████| 88/88 [06:34<00:00,  4.17s/it][A                                                       
                                               [A{'eval_loss': 1569.3878173828125, 'eval_runtime': 397.9682, 'eval_samples_per_second': 7.036, 'eval_steps_per_second': 0.221, 'epoch': 0.63}
 10%|█         | 500/5000 [2:21:06<25:21:41, 20.29s/it]
100%|██████████| 88/88 [06:34<00:00,  4.17s/it][A
                                               [A2024-06-12 18:32:36,049 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-12 18:32:48,157 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 10%|█         | 501/5000 [2:21:40<179:49:10, 143.89s/it]                                                         {'loss': 1522.5542, 'grad_norm': 4608.0, 'learning_rate': 0.0006951882793806347, 'epoch': 0.64}
 10%|█         | 501/5000 [2:21:40<179:49:10, 143.89s/it] 10%|█         | 502/5000 [2:21:58<132:17:19, 105.88s/it]                                                         {'loss': 1353.0833, 'grad_norm': 8192.0, 'learning_rate': 0.0006951499515732358, 'epoch': 0.64}
 10%|█         | 502/5000 [2:21:58<132:17:19, 105.88s/it] 10%|█         | 503/5000 [2:22:13<98:20:22, 78.72s/it]                                                         {'loss': 1304.3778, 'grad_norm': 4608.0, 'learning_rate': 0.0006951114727855219, 'epoch': 0.64}
 10%|█         | 503/5000 [2:22:13<98:20:22, 78.72s/it] 10%|█         | 504/5000 [2:22:42<79:45:24, 63.86s/it]                                                       {'loss': 1590.7849, 'grad_norm': 5632.0, 'learning_rate': 0.0006950728430343248, 'epoch': 0.64}
 10%|█         | 504/5000 [2:22:42<79:45:24, 63.86s/it] 10%|█         | 505/5000 [2:23:09<65:50:20, 52.73s/it]                                                       {'loss': 1300.9253, 'grad_norm': 63488.0, 'learning_rate': 0.0006950340623365427, 'epoch': 0.64}
 10%|█         | 505/5000 [2:23:09<65:50:20, 52.73s/it] 10%|█         | 506/5000 [2:23:24<51:53:35, 41.57s/it]                                                       {'loss': 1281.4326, 'grad_norm': 7648.0, 'learning_rate': 0.0006949951307091395, 'epoch': 0.64}
 10%|█         | 506/5000 [2:23:24<51:53:35, 41.57s/it] 10%|█         | 507/5000 [2:23:40<42:17:52, 33.89s/it]                                                       {'loss': 1435.6647, 'grad_norm': 6048.0, 'learning_rate': 0.0006949560481691453, 'epoch': 0.64}
 10%|█         | 507/5000 [2:23:40<42:17:52, 33.89s/it] 10%|█         | 508/5000 [2:23:55<35:13:56, 28.24s/it]                                                       {'loss': 1004.4056, 'grad_norm': 19584.0, 'learning_rate': 0.0006949168147336559, 'epoch': 0.65}
 10%|█         | 508/5000 [2:23:55<35:13:56, 28.24s/it] 10%|█         | 509/5000 [2:24:08<29:21:19, 23.53s/it]                                                       {'loss': 1179.7738, 'grad_norm': 39168.0, 'learning_rate': 0.0006948774304198336, 'epoch': 0.65}
 10%|█         | 509/5000 [2:24:08<29:21:19, 23.53s/it] 10%|█         | 510/5000 [2:24:24<26:23:52, 21.17s/it]                                                       {'loss': 1139.6099, 'grad_norm': 3984.0, 'learning_rate': 0.0006948378952449062, 'epoch': 0.65}
 10%|█         | 510/5000 [2:24:24<26:23:52, 21.17s/it] 10%|█         | 511/5000 [2:24:42<25:21:31, 20.34s/it]                                                       {'loss': 1160.1106, 'grad_norm': 33792.0, 'learning_rate': 0.0006947982092261679, 'epoch': 0.65}
 10%|█         | 511/5000 [2:24:42<25:21:31, 20.34s/it] 10%|█         | 512/5000 [2:24:58<23:53:05, 19.16s/it]                                                       {'loss': 1133.3807, 'grad_norm': 4384.0, 'learning_rate': 0.0006947583723809788, 'epoch': 0.65}
 10%|█         | 512/5000 [2:24:58<23:53:05, 19.16s/it] 10%|█         | 513/5000 [2:25:13<22:07:01, 17.74s/it]                                                       {'loss': 1336.6776, 'grad_norm': 16512.0, 'learning_rate': 0.0006947183847267646, 'epoch': 0.65}
 10%|█         | 513/5000 [2:25:13<22:07:01, 17.74s/it] 10%|█         | 514/5000 [2:25:26<20:30:51, 16.46s/it]                                                       {'loss': 1237.9487, 'grad_norm': 16256.0, 'learning_rate': 0.0006946782462810175, 'epoch': 0.65}
 10%|█         | 514/5000 [2:25:26<20:30:51, 16.46s/it] 10%|█         | 515/5000 [2:25:42<20:22:14, 16.35s/it]                                                       {'loss': 1052.9297, 'grad_norm': 103424.0, 'learning_rate': 0.0006946379570612952, 'epoch': 0.65}
 10%|█         | 515/5000 [2:25:42<20:22:14, 16.35s/it] 10%|█         | 516/5000 [2:26:01<21:19:20, 17.12s/it]                                                       {'loss': 1782.1952, 'grad_norm': 44032.0, 'learning_rate': 0.0006945975170852219, 'epoch': 0.66}
 10%|█         | 516/5000 [2:26:01<21:19:20, 17.12s/it] 10%|█         | 517/5000 [2:26:19<21:33:05, 17.31s/it]                                                       {'loss': 1411.1632, 'grad_norm': 117248.0, 'learning_rate': 0.0006945569263704871, 'epoch': 0.66}
 10%|█         | 517/5000 [2:26:19<21:33:05, 17.31s/it] 10%|█         | 518/5000 [2:26:31<19:37:55, 15.77s/it]                                                       {'loss': 1822.8462, 'grad_norm': 28180480.0, 'learning_rate': 0.0006945161849348468, 'epoch': 0.66}
 10%|█         | 518/5000 [2:26:31<19:37:55, 15.77s/it] 10%|█         | 519/5000 [2:26:48<20:08:52, 16.19s/it]                                                       {'loss': 1384.5463, 'grad_norm': 11328.0, 'learning_rate': 0.0006944752927961225, 'epoch': 0.66}
 10%|█         | 519/5000 [2:26:48<20:08:52, 16.19s/it] 10%|█         | 520/5000 [2:27:07<21:07:46, 16.98s/it]                                                       {'loss': 1269.689, 'grad_norm': 77312.0, 'learning_rate': 0.0006944342499722019, 'epoch': 0.66}
 10%|█         | 520/5000 [2:27:07<21:07:46, 16.98s/it] 10%|█         | 521/5000 [2:27:24<21:01:14, 16.90s/it]                                                       {'loss': 1193.4584, 'grad_norm': 6176.0, 'learning_rate': 0.0006943930564810383, 'epoch': 0.66}
 10%|█         | 521/5000 [2:27:24<21:01:14, 16.90s/it] 10%|█         | 522/5000 [2:27:41<20:55:45, 16.83s/it]                                                       {'loss': 1227.7429, 'grad_norm': 13120.0, 'learning_rate': 0.0006943517123406515, 'epoch': 0.66}
 10%|█         | 522/5000 [2:27:41<20:55:45, 16.83s/it] 10%|█         | 523/5000 [2:27:55<20:05:41, 16.16s/it]                                                       {'loss': 1133.0271, 'grad_norm': 2528.0, 'learning_rate': 0.0006943102175691267, 'epoch': 0.66}
 10%|█         | 523/5000 [2:27:55<20:05:41, 16.16s/it] 10%|█         | 524/5000 [2:28:08<18:55:31, 15.22s/it]                                                       {'loss': 1230.6682, 'grad_norm': 40192.0, 'learning_rate': 0.0006942685721846152, 'epoch': 0.67}
 10%|█         | 524/5000 [2:28:08<18:55:31, 15.22s/it] 10%|█         | 525/5000 [2:28:23<18:49:00, 15.14s/it]                                                       {'loss': 1320.6926, 'grad_norm': 3600.0, 'learning_rate': 0.0006942267762053338, 'epoch': 0.67}
 10%|█         | 525/5000 [2:28:23<18:49:00, 15.14s/it] 11%|█         | 526/5000 [2:28:37<18:24:02, 14.81s/it]                                                       {'loss': 1236.6859, 'grad_norm': 5248.0, 'learning_rate': 0.0006941848296495658, 'epoch': 0.67}
 11%|█         | 526/5000 [2:28:37<18:24:02, 14.81s/it] 11%|█         | 527/5000 [2:28:50<17:35:46, 14.16s/it]                                                       {'loss': 1449.726, 'grad_norm': 98304.0, 'learning_rate': 0.0006941427325356599, 'epoch': 0.67}
 11%|█         | 527/5000 [2:28:50<17:35:46, 14.16s/it] 11%|█         | 528/5000 [2:29:06<18:18:00, 14.73s/it]                                                       {'loss': 916.5867, 'grad_norm': 41216.0, 'learning_rate': 0.0006941004848820308, 'epoch': 0.67}
 11%|█         | 528/5000 [2:29:06<18:18:00, 14.73s/it] 11%|█         | 529/5000 [2:29:26<20:20:14, 16.38s/it]                                                       {'loss': 1212.1711, 'grad_norm': 34048.0, 'learning_rate': 0.0006940580867071591, 'epoch': 0.67}
 11%|█         | 529/5000 [2:29:26<20:20:14, 16.38s/it] 11%|█         | 530/5000 [2:29:39<19:05:10, 15.37s/it]                                                       {'loss': 1495.0931, 'grad_norm': 188416.0, 'learning_rate': 0.0006940155380295913, 'epoch': 0.67}
 11%|█         | 530/5000 [2:29:39<19:05:10, 15.37s/it] 11%|█         | 531/5000 [2:29:53<18:36:18, 14.99s/it]                                                       {'loss': 1287.0151, 'grad_norm': 17664.0, 'learning_rate': 0.0006939728388679395, 'epoch': 0.67}
 11%|█         | 531/5000 [2:29:53<18:36:18, 14.99s/it] 11%|█         | 532/5000 [2:30:11<19:31:29, 15.73s/it]                                                       {'loss': 1732.0986, 'grad_norm': 66048.0, 'learning_rate': 0.0006939299892408817, 'epoch': 0.68}
 11%|█         | 532/5000 [2:30:11<19:31:29, 15.73s/it] 11%|█         | 533/5000 [2:30:32<21:33:02, 17.37s/it]                                                       {'loss': 2055.9746, 'grad_norm': 378880.0, 'learning_rate': 0.000693886989167162, 'epoch': 0.68}
 11%|█         | 533/5000 [2:30:32<21:33:02, 17.37s/it] 11%|█         | 534/5000 [2:30:48<21:08:33, 17.04s/it]                                                       {'loss': 1889.9901, 'grad_norm': 23168.0, 'learning_rate': 0.0006938438386655899, 'epoch': 0.68}
 11%|█         | 534/5000 [2:30:48<21:08:33, 17.04s/it] 11%|█         | 535/5000 [2:31:13<24:10:00, 19.49s/it]                                                       {'loss': 1027.4312, 'grad_norm': 15168.0, 'learning_rate': 0.0006938005377550411, 'epoch': 0.68}
 11%|█         | 535/5000 [2:31:13<24:10:00, 19.49s/it] 11%|█         | 536/5000 [2:31:29<22:37:03, 18.24s/it]                                                       {'loss': 1148.2235, 'grad_norm': 9728.0, 'learning_rate': 0.0006937570864544566, 'epoch': 0.68}
 11%|█         | 536/5000 [2:31:29<22:37:03, 18.24s/it] 11%|█         | 537/5000 [2:31:43<21:10:01, 17.07s/it]                                                       {'loss': 1371.4021, 'grad_norm': 14272.0, 'learning_rate': 0.0006937134847828437, 'epoch': 0.68}
 11%|█         | 537/5000 [2:31:43<21:10:01, 17.07s/it] 11%|█         | 538/5000 [2:31:57<20:00:41, 16.15s/it]                                                       {'loss': 1331.7004, 'grad_norm': 4080.0, 'learning_rate': 0.0006936697327592752, 'epoch': 0.68}
 11%|█         | 538/5000 [2:31:57<20:00:41, 16.15s/it] 11%|█         | 539/5000 [2:32:14<20:11:46, 16.30s/it]                                                       {'loss': 1291.7994, 'grad_norm': 2672.0, 'learning_rate': 0.0006936258304028897, 'epoch': 0.68}
 11%|█         | 539/5000 [2:32:14<20:11:46, 16.30s/it] 11%|█         | 540/5000 [2:32:36<22:19:06, 18.01s/it]                                                       {'loss': 1071.8684, 'grad_norm': 11520.0, 'learning_rate': 0.0006935817777328916, 'epoch': 0.69}
 11%|█         | 540/5000 [2:32:36<22:19:06, 18.01s/it] 11%|█         | 541/5000 [2:32:55<22:37:33, 18.27s/it]                                                       {'loss': 1108.609, 'grad_norm': 3696.0, 'learning_rate': 0.0006935375747685509, 'epoch': 0.69}
 11%|█         | 541/5000 [2:32:55<22:37:33, 18.27s/it] 11%|█         | 542/5000 [2:33:11<21:56:55, 17.72s/it]                                                       {'loss': 1130.5028, 'grad_norm': 5792.0, 'learning_rate': 0.0006934932215292038, 'epoch': 0.69}
 11%|█         | 542/5000 [2:33:11<21:56:55, 17.72s/it] 11%|█         | 543/5000 [2:33:27<21:19:06, 17.22s/it]                                                       {'loss': 1039.0933, 'grad_norm': 2640.0, 'learning_rate': 0.0006934487180342518, 'epoch': 0.69}
 11%|█         | 543/5000 [2:33:27<21:19:06, 17.22s/it] 11%|█         | 544/5000 [2:33:50<23:28:58, 18.97s/it]                                                       {'loss': 1377.9302, 'grad_norm': 222208.0, 'learning_rate': 0.000693404064303162, 'epoch': 0.69}
 11%|█         | 544/5000 [2:33:50<23:28:58, 18.97s/it] 11%|█         | 545/5000 [2:34:04<21:22:29, 17.27s/it]                                                       {'loss': 1363.2712, 'grad_norm': 5472.0, 'learning_rate': 0.0006933592603554675, 'epoch': 0.69}
 11%|█         | 545/5000 [2:34:04<21:22:29, 17.27s/it] 11%|█         | 546/5000 [2:34:27<23:39:24, 19.12s/it]                                                       {'loss': 1354.582, 'grad_norm': 11200.0, 'learning_rate': 0.0006933143062107674, 'epoch': 0.69}
 11%|█         | 546/5000 [2:34:27<23:39:24, 19.12s/it] 11%|█         | 547/5000 [2:34:44<22:54:23, 18.52s/it]                                                       {'loss': 1292.9596, 'grad_norm': 27520.0, 'learning_rate': 0.0006932692018887258, 'epoch': 0.69}
 11%|█         | 547/5000 [2:34:44<22:54:23, 18.52s/it] 11%|█         | 548/5000 [2:34:59<21:34:54, 17.45s/it]                                                       {'loss': 1167.7712, 'grad_norm': 3264.0, 'learning_rate': 0.0006932239474090731, 'epoch': 0.7}
 11%|█         | 548/5000 [2:34:59<21:34:54, 17.45s/it] 11%|█         | 549/5000 [2:35:14<20:41:53, 16.74s/it]                                                       {'loss': 940.3934, 'grad_norm': 6656.0, 'learning_rate': 0.0006931785427916051, 'epoch': 0.7}
 11%|█         | 549/5000 [2:35:14<20:41:53, 16.74s/it] 11%|█         | 550/5000 [2:35:36<22:36:59, 18.30s/it]                                                       {'loss': 1244.0018, 'grad_norm': 26496.0, 'learning_rate': 0.0006931329880561833, 'epoch': 0.7}
 11%|█         | 550/5000 [2:35:36<22:36:59, 18.30s/it] 11%|█         | 551/5000 [2:35:56<23:11:03, 18.76s/it]                                                       {'loss': 1255.814, 'grad_norm': 17563648.0, 'learning_rate': 0.0006930872832227349, 'epoch': 0.7}
 11%|█         | 551/5000 [2:35:56<23:11:03, 18.76s/it] 11%|█         | 552/5000 [2:36:13<22:36:30, 18.30s/it]                                                       {'loss': 1444.2803, 'grad_norm': 499712.0, 'learning_rate': 0.0006930414283112525, 'epoch': 0.7}
 11%|█         | 552/5000 [2:36:13<22:36:30, 18.30s/it] 11%|█         | 553/5000 [2:36:31<22:29:11, 18.20s/it]                                                       {'loss': 2127.5371, 'grad_norm': 18560.0, 'learning_rate': 0.000692995423341795, 'epoch': 0.7}
 11%|█         | 553/5000 [2:36:31<22:29:11, 18.20s/it] 11%|█         | 554/5000 [2:36:46<21:06:28, 17.09s/it]                                                       {'loss': 1341.8005, 'grad_norm': 5376.0, 'learning_rate': 0.0006929492683344864, 'epoch': 0.7}
 11%|█         | 554/5000 [2:36:46<21:06:28, 17.09s/it] 11%|█         | 555/5000 [2:37:00<20:02:00, 16.22s/it]                                                       {'loss': 1390.3756, 'grad_norm': 162816.0, 'learning_rate': 0.0006929029633095164, 'epoch': 0.7}
 11%|█         | 555/5000 [2:37:00<20:02:00, 16.22s/it] 11%|█         | 556/5000 [2:37:18<20:51:30, 16.90s/it]                                                       {'loss': 1420.8225, 'grad_norm': 3408.0, 'learning_rate': 0.0006928565082871404, 'epoch': 0.71}
 11%|█         | 556/5000 [2:37:18<20:51:30, 16.90s/it] 11%|█         | 557/5000 [2:37:46<24:56:43, 20.21s/it]                                                       {'loss': 1328.0632, 'grad_norm': 17920.0, 'learning_rate': 0.0006928099032876793, 'epoch': 0.71}
 11%|█         | 557/5000 [2:37:46<24:56:43, 20.21s/it] 11%|█         | 558/5000 [2:38:15<28:01:11, 22.71s/it]                                                       {'loss': 1385.3713, 'grad_norm': 52480.0, 'learning_rate': 0.00069276314833152, 'epoch': 0.71}
 11%|█         | 558/5000 [2:38:15<28:01:11, 22.71s/it] 11%|█         | 559/5000 [2:38:33<26:17:46, 21.32s/it]                                                       {'loss': 1532.5471, 'grad_norm': 20864.0, 'learning_rate': 0.0006927162434391145, 'epoch': 0.71}
 11%|█         | 559/5000 [2:38:33<26:17:46, 21.32s/it] 11%|█         | 560/5000 [2:38:48<23:52:41, 19.36s/it]                                                       {'loss': 1134.5083, 'grad_norm': 8448.0, 'learning_rate': 0.0006926691886309807, 'epoch': 0.71}
 11%|█         | 560/5000 [2:38:48<23:52:41, 19.36s/it] 11%|█         | 561/5000 [2:39:00<21:18:10, 17.28s/it]                                                       {'loss': 1000.8336, 'grad_norm': 5664.0, 'learning_rate': 0.0006926219839277018, 'epoch': 0.71}
 11%|█         | 561/5000 [2:39:00<21:18:10, 17.28s/it] 11%|█         | 562/5000 [2:39:12<19:22:13, 15.71s/it]                                                       {'loss': 1208.8315, 'grad_norm': 2752.0, 'learning_rate': 0.0006925746293499268, 'epoch': 0.71}
 11%|█         | 562/5000 [2:39:12<19:22:13, 15.71s/it] 11%|█▏        | 563/5000 [2:39:30<20:16:15, 16.45s/it]                                                       {'loss': 1053.3529, 'grad_norm': 5184.0, 'learning_rate': 0.0006925271249183704, 'epoch': 0.71}
 11%|█▏        | 563/5000 [2:39:30<20:16:15, 16.45s/it] 11%|█▏        | 564/5000 [2:39:46<20:07:56, 16.34s/it]                                                       {'loss': 1214.0675, 'grad_norm': 57856.0, 'learning_rate': 0.0006924794706538123, 'epoch': 0.72}
 11%|█▏        | 564/5000 [2:39:46<20:07:56, 16.34s/it] 11%|█▏        | 565/5000 [2:39:59<18:56:22, 15.37s/it]                                                       {'loss': 1086.1229, 'grad_norm': 3696.0, 'learning_rate': 0.0006924316665770984, 'epoch': 0.72}
 11%|█▏        | 565/5000 [2:39:59<18:56:22, 15.37s/it] 11%|█▏        | 566/5000 [2:40:13<18:20:24, 14.89s/it]                                                       {'loss': 1010.3619, 'grad_norm': 3136.0, 'learning_rate': 0.0006923837127091398, 'epoch': 0.72}
 11%|█▏        | 566/5000 [2:40:13<18:20:24, 14.89s/it] 11%|█▏        | 567/5000 [2:40:27<18:03:41, 14.67s/it]                                                       {'loss': 1083.1871, 'grad_norm': 12608.0, 'learning_rate': 0.000692335609070913, 'epoch': 0.72}
 11%|█▏        | 567/5000 [2:40:27<18:03:41, 14.67s/it] 11%|█▏        | 568/5000 [2:40:42<18:12:43, 14.79s/it]                                                       {'loss': 1150.608, 'grad_norm': 1792.0, 'learning_rate': 0.0006922873556834603, 'epoch': 0.72}
 11%|█▏        | 568/5000 [2:40:42<18:12:43, 14.79s/it] 11%|█▏        | 569/5000 [2:41:03<20:17:35, 16.49s/it]                                                       {'loss': 1328.8635, 'grad_norm': 3440.0, 'learning_rate': 0.0006922389525678892, 'epoch': 0.72}
 11%|█▏        | 569/5000 [2:41:03<20:17:35, 16.49s/it] 11%|█▏        | 570/5000 [2:41:18<19:37:42, 15.95s/it]                                                       {'loss': 1378.5582, 'grad_norm': 12928.0, 'learning_rate': 0.0006921903997453731, 'epoch': 0.72}
 11%|█▏        | 570/5000 [2:41:18<19:37:42, 15.95s/it] 11%|█▏        | 571/5000 [2:41:36<20:21:46, 16.55s/it]                                                       {'loss': 1212.9536, 'grad_norm': 51200.0, 'learning_rate': 0.0006921416972371504, 'epoch': 0.73}
 11%|█▏        | 571/5000 [2:41:36<20:21:46, 16.55s/it] 11%|█▏        | 572/5000 [2:41:53<20:44:19, 16.86s/it]                                                       {'loss': 1460.8911, 'grad_norm': 59392.0, 'learning_rate': 0.0006920928450645255, 'epoch': 0.73}
 11%|█▏        | 572/5000 [2:41:53<20:44:19, 16.86s/it] 11%|█▏        | 573/5000 [2:42:05<18:49:39, 15.31s/it]                                                       {'loss': 1112.2041, 'grad_norm': 9152.0, 'learning_rate': 0.0006920438432488677, 'epoch': 0.73}
 11%|█▏        | 573/5000 [2:42:05<18:49:39, 15.31s/it] 11%|█▏        | 574/5000 [2:42:18<17:58:29, 14.62s/it]                                                       {'loss': 1235.4425, 'grad_norm': 2368.0, 'learning_rate': 0.0006919946918116123, 'epoch': 0.73}
 11%|█▏        | 574/5000 [2:42:18<17:58:29, 14.62s/it] 12%|█▏        | 575/5000 [2:42:33<18:08:17, 14.76s/it]                                                       {'loss': 1662.8373, 'grad_norm': 3184.0, 'learning_rate': 0.0006919453907742598, 'epoch': 0.73}
 12%|█▏        | 575/5000 [2:42:33<18:08:17, 14.76s/it] 12%|█▏        | 576/5000 [2:42:58<22:07:20, 18.00s/it]                                                       {'loss': 1465.3511, 'grad_norm': 4896.0, 'learning_rate': 0.0006918959401583759, 'epoch': 0.73}
 12%|█▏        | 576/5000 [2:42:58<22:07:20, 18.00s/it] 12%|█▏        | 577/5000 [2:43:18<22:41:04, 18.46s/it]                                                       {'loss': 1114.6333, 'grad_norm': 46336.0, 'learning_rate': 0.0006918463399855923, 'epoch': 0.73}
 12%|█▏        | 577/5000 [2:43:18<22:41:04, 18.46s/it] 12%|█▏        | 578/5000 [2:43:35<21:58:05, 17.88s/it]                                                       {'loss': 954.4587, 'grad_norm': 1496.0, 'learning_rate': 0.0006917965902776056, 'epoch': 0.73}
 12%|█▏        | 578/5000 [2:43:35<21:58:05, 17.88s/it] 12%|█▏        | 579/5000 [2:43:50<21:13:19, 17.28s/it]                                                       {'loss': 1093.3088, 'grad_norm': 223232.0, 'learning_rate': 0.0006917466910561781, 'epoch': 0.74}
 12%|█▏        | 579/5000 [2:43:50<21:13:19, 17.28s/it] 12%|█▏        | 580/5000 [2:44:07<21:08:23, 17.22s/it]                                                       {'loss': 832.2563, 'grad_norm': 1728.0, 'learning_rate': 0.0006916966423431374, 'epoch': 0.74}
 12%|█▏        | 580/5000 [2:44:07<21:08:23, 17.22s/it] 12%|█▏        | 581/5000 [2:44:19<19:08:55, 15.60s/it]                                                       {'loss': 1112.0505, 'grad_norm': 35840.0, 'learning_rate': 0.0006916464441603765, 'epoch': 0.74}
 12%|█▏        | 581/5000 [2:44:19<19:08:55, 15.60s/it] 12%|█▏        | 582/5000 [2:44:31<17:47:31, 14.50s/it]                                                       {'loss': 1200.4512, 'grad_norm': 4704.0, 'learning_rate': 0.0006915960965298537, 'epoch': 0.74}
 12%|█▏        | 582/5000 [2:44:31<17:47:31, 14.50s/it] 12%|█▏        | 583/5000 [2:44:48<18:39:08, 15.20s/it]                                                       {'loss': 1081.9668, 'grad_norm': 7904.0, 'learning_rate': 0.0006915455994735931, 'epoch': 0.74}
 12%|█▏        | 583/5000 [2:44:48<18:39:08, 15.20s/it] 12%|█▏        | 584/5000 [2:45:09<20:51:43, 17.01s/it]                                                       {'loss': 1719.655, 'grad_norm': 9152.0, 'learning_rate': 0.0006914949530136832, 'epoch': 0.74}
 12%|█▏        | 584/5000 [2:45:09<20:51:43, 17.01s/it] 12%|█▏        | 585/5000 [2:45:27<21:12:12, 17.29s/it]                                                       {'loss': 1248.5867, 'grad_norm': 17792.0, 'learning_rate': 0.0006914441571722791, 'epoch': 0.74}
 12%|█▏        | 585/5000 [2:45:27<21:12:12, 17.29s/it] 12%|█▏        | 586/5000 [2:45:47<22:09:45, 18.08s/it]                                                       {'loss': 1403.7053, 'grad_norm': 5312.0, 'learning_rate': 0.0006913932119716003, 'epoch': 0.74}
 12%|█▏        | 586/5000 [2:45:47<22:09:45, 18.08s/it] 12%|█▏        | 587/5000 [2:46:06<22:25:44, 18.30s/it]                                                       {'loss': 974.3463, 'grad_norm': 1792.0, 'learning_rate': 0.000691342117433932, 'epoch': 0.75}
 12%|█▏        | 587/5000 [2:46:06<22:25:44, 18.30s/it] 12%|█▏        | 588/5000 [2:46:26<23:13:19, 18.95s/it]                                                       {'loss': 1028.9047, 'grad_norm': 3280.0, 'learning_rate': 0.0006912908735816249, 'epoch': 0.75}
 12%|█▏        | 588/5000 [2:46:26<23:13:19, 18.95s/it] 12%|█▏        | 589/5000 [2:46:43<22:11:44, 18.11s/it]                                                       {'loss': 1326.3235, 'grad_norm': 5216.0, 'learning_rate': 0.0006912394804370945, 'epoch': 0.75}
 12%|█▏        | 589/5000 [2:46:43<22:11:44, 18.11s/it] 12%|█▏        | 590/5000 [2:47:01<22:26:51, 18.32s/it]                                                       {'loss': 1119.5393, 'grad_norm': 2352.0, 'learning_rate': 0.0006911879380228221, 'epoch': 0.75}
 12%|█▏        | 590/5000 [2:47:01<22:26:51, 18.32s/it] 12%|█▏        | 591/5000 [2:47:16<20:53:46, 17.06s/it]                                                       {'loss': 901.3251, 'grad_norm': 2672.0, 'learning_rate': 0.000691136246361354, 'epoch': 0.75}
 12%|█▏        | 591/5000 [2:47:16<20:53:46, 17.06s/it] 12%|█▏        | 592/5000 [2:47:28<19:05:52, 15.60s/it]                                                       {'loss': 891.1846, 'grad_norm': 1912.0, 'learning_rate': 0.000691084405475302, 'epoch': 0.75}
 12%|█▏        | 592/5000 [2:47:28<19:05:52, 15.60s/it] 12%|█▏        | 593/5000 [2:47:43<18:49:37, 15.38s/it]                                                       {'loss': 974.2388, 'grad_norm': 4128.0, 'learning_rate': 0.000691032415387343, 'epoch': 0.75}
 12%|█▏        | 593/5000 [2:47:43<18:49:37, 15.38s/it] 12%|█▏        | 594/5000 [2:47:57<18:35:28, 15.19s/it]                                                       {'loss': 1323.5966, 'grad_norm': 4224.0, 'learning_rate': 0.0006909802761202191, 'epoch': 0.75}
 12%|█▏        | 594/5000 [2:47:57<18:35:28, 15.19s/it] 12%|█▏        | 595/5000 [2:48:09<17:23:22, 14.21s/it]                                                       {'loss': 1034.1825, 'grad_norm': 3792.0, 'learning_rate': 0.0006909279876967381, 'epoch': 0.76}
 12%|█▏        | 595/5000 [2:48:09<17:23:22, 14.21s/it] 12%|█▏        | 596/5000 [2:48:29<19:24:53, 15.87s/it]                                                       {'loss': 938.8182, 'grad_norm': 2752.0, 'learning_rate': 0.0006908755501397724, 'epoch': 0.76}
 12%|█▏        | 596/5000 [2:48:29<19:24:53, 15.87s/it] 12%|█▏        | 597/5000 [2:48:40<17:48:27, 14.56s/it]                                                       {'loss': 1037.6377, 'grad_norm': 2560.0, 'learning_rate': 0.0006908229634722602, 'epoch': 0.76}
 12%|█▏        | 597/5000 [2:48:40<17:48:27, 14.56s/it] 12%|█▏        | 598/5000 [2:48:56<18:12:11, 14.89s/it]                                                       {'loss': 1311.8037, 'grad_norm': 5376.0, 'learning_rate': 0.0006907702277172045, 'epoch': 0.76}
 12%|█▏        | 598/5000 [2:48:56<18:12:11, 14.89s/it] 12%|█▏        | 599/5000 [2:49:11<18:18:56, 14.98s/it]                                                       {'loss': 1236.8524, 'grad_norm': 2448.0, 'learning_rate': 0.000690717342897674, 'epoch': 0.76}
 12%|█▏        | 599/5000 [2:49:11<18:18:56, 14.98s/it] 12%|█▏        | 600/5000 [2:49:39<22:57:51, 18.79s/it]                                                       {'loss': 1101.0381, 'grad_norm': 2192.0, 'learning_rate': 0.000690664309036802, 'epoch': 0.76}
 12%|█▏        | 600/5000 [2:49:39<22:57:51, 18.79s/it] 12%|█▏        | 601/5000 [2:50:08<26:37:57, 21.80s/it]                                                       {'loss': 959.7892, 'grad_norm': 1696.0, 'learning_rate': 0.0006906111261577876, 'epoch': 0.76}
 12%|█▏        | 601/5000 [2:50:08<26:37:57, 21.80s/it] 12%|█▏        | 602/5000 [2:50:27<25:42:28, 21.04s/it]                                                       {'loss': 637.5505, 'grad_norm': 2416.0, 'learning_rate': 0.0006905577942838945, 'epoch': 0.76}
 12%|█▏        | 602/5000 [2:50:27<25:42:28, 21.04s/it] 12%|█▏        | 603/5000 [2:50:43<23:50:22, 19.52s/it]                                                       {'loss': 806.7209, 'grad_norm': 98304.0, 'learning_rate': 0.000690504313438452, 'epoch': 0.77}
 12%|█▏        | 603/5000 [2:50:43<23:50:22, 19.52s/it] 12%|█▏        | 604/5000 [2:50:58<22:12:47, 18.19s/it]                                                       {'loss': 880.7332, 'grad_norm': 5824.0, 'learning_rate': 0.0006904506836448546, 'epoch': 0.77}
 12%|█▏        | 604/5000 [2:50:58<22:12:47, 18.19s/it] 12%|█▏        | 605/5000 [2:51:11<20:18:33, 16.64s/it]                                                       {'loss': 1157.5044, 'grad_norm': 105472.0, 'learning_rate': 0.0006903969049265615, 'epoch': 0.77}
 12%|█▏        | 605/5000 [2:51:11<20:18:33, 16.64s/it] 12%|█▏        | 606/5000 [2:51:32<21:46:21, 17.84s/it]                                                       {'loss': 1069.3094, 'grad_norm': 3392.0, 'learning_rate': 0.0006903429773070977, 'epoch': 0.77}
 12%|█▏        | 606/5000 [2:51:32<21:46:21, 17.84s/it] 12%|█▏        | 607/5000 [2:51:47<20:39:38, 16.93s/it]                                                       {'loss': 972.3383, 'grad_norm': 2976.0, 'learning_rate': 0.0006902889008100527, 'epoch': 0.77}
 12%|█▏        | 607/5000 [2:51:47<20:39:38, 16.93s/it] 12%|█▏        | 608/5000 [2:52:06<21:39:29, 17.75s/it]                                                       {'loss': 1035.1326, 'grad_norm': 13952.0, 'learning_rate': 0.0006902346754590814, 'epoch': 0.77}
 12%|█▏        | 608/5000 [2:52:06<21:39:29, 17.75s/it] 12%|█▏        | 609/5000 [2:52:21<20:37:42, 16.91s/it]                                                       {'loss': 1167.2893, 'grad_norm': 4608.0, 'learning_rate': 0.000690180301277904, 'epoch': 0.77}
 12%|█▏        | 609/5000 [2:52:21<20:37:42, 16.91s/it] 12%|█▏        | 610/5000 [2:52:34<19:11:06, 15.73s/it]                                                       {'loss': 984.5515, 'grad_norm': 2704.0, 'learning_rate': 0.0006901257782903053, 'epoch': 0.77}
 12%|█▏        | 610/5000 [2:52:34<19:11:06, 15.73s/it] 12%|█▏        | 611/5000 [2:52:47<18:01:28, 14.78s/it]                                                       {'loss': 1315.8694, 'grad_norm': 3520.0, 'learning_rate': 0.0006900711065201359, 'epoch': 0.78}
 12%|█▏        | 611/5000 [2:52:47<18:01:28, 14.78s/it] 12%|█▏        | 612/5000 [2:53:01<17:44:50, 14.56s/it]                                                       {'loss': 1003.2991, 'grad_norm': 3008.0, 'learning_rate': 0.0006900162859913108, 'epoch': 0.78}
 12%|█▏        | 612/5000 [2:53:01<17:44:50, 14.56s/it] 12%|█▏        | 613/5000 [2:53:16<17:49:46, 14.63s/it]                                                       {'loss': 867.9885, 'grad_norm': 1024.0, 'learning_rate': 0.0006899613167278104, 'epoch': 0.78}
 12%|█▏        | 613/5000 [2:53:16<17:49:46, 14.63s/it] 12%|█▏        | 614/5000 [2:53:30<17:52:54, 14.68s/it]                                                       {'loss': 982.491, 'grad_norm': 4288.0, 'learning_rate': 0.0006899061987536802, 'epoch': 0.78}
 12%|█▏        | 614/5000 [2:53:30<17:52:54, 14.68s/it] 12%|█▏        | 615/5000 [2:53:48<19:06:36, 15.69s/it]                                                       {'loss': 694.8954, 'grad_norm': 3152.0, 'learning_rate': 0.0006898509320930307, 'epoch': 0.78}
 12%|█▏        | 615/5000 [2:53:48<19:06:36, 15.69s/it] 12%|█▏        | 616/5000 [2:54:09<20:56:13, 17.19s/it]                                                       {'loss': 1131.8087, 'grad_norm': 9088.0, 'learning_rate': 0.0006897955167700373, 'epoch': 0.78}
 12%|█▏        | 616/5000 [2:54:09<20:56:13, 17.19s/it] 12%|█▏        | 617/5000 [2:54:34<23:33:03, 19.34s/it]                                                       {'loss': 1311.9291, 'grad_norm': 5824.0, 'learning_rate': 0.0006897399528089407, 'epoch': 0.78}
 12%|█▏        | 617/5000 [2:54:34<23:33:03, 19.34s/it] 12%|█▏        | 618/5000 [2:54:50<22:21:56, 18.37s/it]                                                       {'loss': 1093.8647, 'grad_norm': 3632.0, 'learning_rate': 0.0006896842402340463, 'epoch': 0.78}
 12%|█▏        | 618/5000 [2:54:50<22:21:56, 18.37s/it] 12%|█▏        | 619/5000 [2:55:05<21:06:58, 17.35s/it]                                                       {'loss': 1040.8357, 'grad_norm': 10240.0, 'learning_rate': 0.0006896283790697247, 'epoch': 0.79}
 12%|█▏        | 619/5000 [2:55:05<21:06:58, 17.35s/it] 12%|█▏        | 620/5000 [2:55:21<20:35:54, 16.93s/it]                                                       {'loss': 1003.1296, 'grad_norm': 954368.0, 'learning_rate': 0.0006895723693404116, 'epoch': 0.79}
 12%|█▏        | 620/5000 [2:55:21<20:35:54, 16.93s/it] 12%|█▏        | 621/5000 [2:55:50<25:08:35, 20.67s/it]                                                       {'loss': 1099.3564, 'grad_norm': 238592.0, 'learning_rate': 0.0006895162110706074, 'epoch': 0.79}
 12%|█▏        | 621/5000 [2:55:50<25:08:35, 20.67s/it] 12%|█▏        | 622/5000 [2:56:15<26:32:51, 21.83s/it]                                                       {'loss': 1260.6617, 'grad_norm': 544768.0, 'learning_rate': 0.0006894599042848778, 'epoch': 0.79}
 12%|█▏        | 622/5000 [2:56:15<26:32:51, 21.83s/it] 12%|█▏        | 623/5000 [2:56:30<24:05:12, 19.81s/it]                                                       {'loss': 1143.0261, 'grad_norm': 67584.0, 'learning_rate': 0.0006894034490078531, 'epoch': 0.79}
 12%|█▏        | 623/5000 [2:56:30<24:05:12, 19.81s/it] 12%|█▏        | 624/5000 [2:56:47<23:03:41, 18.97s/it]                                                       {'loss': 1419.4266, 'grad_norm': 48384.0, 'learning_rate': 0.0006893468452642289, 'epoch': 0.79}
 12%|█▏        | 624/5000 [2:56:47<23:03:41, 18.97s/it] 12%|█▎        | 625/5000 [2:57:19<28:02:55, 23.08s/it]                                                       {'loss': 1890.8389, 'grad_norm': 116736.0, 'learning_rate': 0.0006892900930787656, 'epoch': 0.79}
 12%|█▎        | 625/5000 [2:57:19<28:02:55, 23.08s/it] 13%|█▎        | 626/5000 [2:57:44<28:36:37, 23.55s/it]                                                       {'loss': 1161.223, 'grad_norm': 39680.0, 'learning_rate': 0.0006892331924762885, 'epoch': 0.79}
 13%|█▎        | 626/5000 [2:57:44<28:36:37, 23.55s/it] 13%|█▎        | 627/5000 [2:58:00<25:54:17, 21.33s/it]                                                       {'loss': 1470.0537, 'grad_norm': 9664.0, 'learning_rate': 0.0006891761434816879, 'epoch': 0.8}
 13%|█▎        | 627/5000 [2:58:00<25:54:17, 21.33s/it] 13%|█▎        | 628/5000 [2:58:14<23:05:28, 19.01s/it]                                                       {'loss': 1381.4642, 'grad_norm': 6144.0, 'learning_rate': 0.0006891189461199189, 'epoch': 0.8}
 13%|█▎        | 628/5000 [2:58:14<23:05:28, 19.01s/it] 13%|█▎        | 629/5000 [2:58:26<20:42:30, 17.06s/it]                                                       {'loss': 1580.5149, 'grad_norm': 22400.0, 'learning_rate': 0.0006890616004160016, 'epoch': 0.8}
 13%|█▎        | 629/5000 [2:58:26<20:42:30, 17.06s/it] 13%|█▎        | 630/5000 [2:58:44<21:07:38, 17.40s/it]                                                       {'loss': 1265.9478, 'grad_norm': 25600.0, 'learning_rate': 0.0006890041063950208, 'epoch': 0.8}
 13%|█▎        | 630/5000 [2:58:44<21:07:38, 17.40s/it] 13%|█▎        | 631/5000 [2:59:02<21:13:56, 17.50s/it]                                                       {'loss': 1524.9006, 'grad_norm': 14720.0, 'learning_rate': 0.0006889464640821267, 'epoch': 0.8}
 13%|█▎        | 631/5000 [2:59:02<21:13:56, 17.50s/it] 13%|█▎        | 632/5000 [2:59:27<23:55:18, 19.72s/it]                                                       {'loss': 1377.4915, 'grad_norm': 34560.0, 'learning_rate': 0.0006888886735025337, 'epoch': 0.8}
 13%|█▎        | 632/5000 [2:59:27<23:55:18, 19.72s/it] 13%|█▎        | 633/5000 [2:59:41<21:56:04, 18.08s/it]                                                       {'loss': 1384.6837, 'grad_norm': 68096.0, 'learning_rate': 0.0006888307346815215, 'epoch': 0.8}
 13%|█▎        | 633/5000 [2:59:41<21:56:04, 18.08s/it] 13%|█▎        | 634/5000 [3:00:03<23:14:51, 19.17s/it]                                                       {'loss': 1466.5884, 'grad_norm': 31872.0, 'learning_rate': 0.0006887726476444345, 'epoch': 0.81}
 13%|█▎        | 634/5000 [3:00:03<23:14:51, 19.17s/it] 13%|█▎        | 635/5000 [3:00:20<22:24:13, 18.48s/it]                                                       {'loss': 1528.5129, 'grad_norm': 4256.0, 'learning_rate': 0.0006887144124166819, 'epoch': 0.81}
 13%|█▎        | 635/5000 [3:00:20<22:24:13, 18.48s/it] 13%|█▎        | 636/5000 [3:00:33<20:21:07, 16.79s/it]                                                       {'loss': 1403.879, 'grad_norm': 10304.0, 'learning_rate': 0.0006886560290237377, 'epoch': 0.81}
 13%|█▎        | 636/5000 [3:00:33<20:21:07, 16.79s/it] 13%|█▎        | 637/5000 [3:00:57<23:10:25, 19.12s/it]                                                       {'loss': 1314.6877, 'grad_norm': 11968.0, 'learning_rate': 0.0006885974974911409, 'epoch': 0.81}
 13%|█▎        | 637/5000 [3:00:57<23:10:25, 19.12s/it] 13%|█▎        | 638/5000 [3:01:09<20:26:25, 16.87s/it]                                                       {'loss': 1469.8896, 'grad_norm': 3632.0, 'learning_rate': 0.0006885388178444952, 'epoch': 0.81}
 13%|█▎        | 638/5000 [3:01:09<20:26:25, 16.87s/it] 13%|█▎        | 639/5000 [3:01:25<20:03:18, 16.56s/it]                                                       {'loss': 1478.9872, 'grad_norm': 4192.0, 'learning_rate': 0.000688479990109469, 'epoch': 0.81}
 13%|█▎        | 639/5000 [3:01:25<20:03:18, 16.56s/it] 13%|█▎        | 640/5000 [3:01:37<18:28:01, 15.25s/it]                                                       {'loss': 1325.7205, 'grad_norm': 3632.0, 'learning_rate': 0.0006884210143117955, 'epoch': 0.81}
 13%|█▎        | 640/5000 [3:01:37<18:28:01, 15.25s/it] 13%|█▎        | 641/5000 [3:01:54<19:10:53, 15.84s/it]                                                       {'loss': 1641.2837, 'grad_norm': 7744.0, 'learning_rate': 0.0006883618904772728, 'epoch': 0.81}
 13%|█▎        | 641/5000 [3:01:54<19:10:53, 15.84s/it] 13%|█▎        | 642/5000 [3:02:08<18:26:34, 15.24s/it]                                                       {'loss': 1195.7148, 'grad_norm': 4736.0, 'learning_rate': 0.0006883026186317637, 'epoch': 0.82}
 13%|█▎        | 642/5000 [3:02:08<18:26:34, 15.24s/it] 13%|█▎        | 643/5000 [3:02:22<17:58:51, 14.86s/it]                                                       {'loss': 1579.6191, 'grad_norm': 593920.0, 'learning_rate': 0.0006882431988011956, 'epoch': 0.82}
 13%|█▎        | 643/5000 [3:02:22<17:58:51, 14.86s/it] 13%|█▎        | 644/5000 [3:02:34<16:52:32, 13.95s/it]                                                       {'loss': 1413.9741, 'grad_norm': 1680.0, 'learning_rate': 0.0006881836310115609, 'epoch': 0.82}
 13%|█▎        | 644/5000 [3:02:34<16:52:32, 13.95s/it] 13%|█▎        | 645/5000 [3:03:02<21:58:36, 18.17s/it]                                                       {'loss': 1197.0176, 'grad_norm': 2416.0, 'learning_rate': 0.0006881239152889164, 'epoch': 0.82}
 13%|█▎        | 645/5000 [3:03:02<21:58:36, 18.17s/it] 13%|█▎        | 646/5000 [3:03:28<25:04:20, 20.73s/it]                                                       {'loss': 1213.3665, 'grad_norm': 14464.0, 'learning_rate': 0.000688064051659384, 'epoch': 0.82}
 13%|█▎        | 646/5000 [3:03:28<25:04:20, 20.73s/it] 13%|█▎        | 647/5000 [3:03:52<26:09:07, 21.63s/it]                                                       {'loss': 1393.2473, 'grad_norm': 23040.0, 'learning_rate': 0.0006880040401491499, 'epoch': 0.82}
 13%|█▎        | 647/5000 [3:03:52<26:09:07, 21.63s/it] 13%|█▎        | 648/5000 [3:04:08<24:03:45, 19.90s/it]                                                       {'loss': 1170.0017, 'grad_norm': 8096.0, 'learning_rate': 0.0006879438807844653, 'epoch': 0.82}
 13%|█▎        | 648/5000 [3:04:08<24:03:45, 19.90s/it] 13%|█▎        | 649/5000 [3:04:28<24:02:51, 19.90s/it]                                                       {'loss': 1523.9739, 'grad_norm': 6912.0, 'learning_rate': 0.0006878835735916459, 'epoch': 0.82}
 13%|█▎        | 649/5000 [3:04:28<24:02:51, 19.90s/it] 13%|█▎        | 650/5000 [3:04:45<23:08:28, 19.15s/it]                                                       {'loss': 1386.5878, 'grad_norm': 33280.0, 'learning_rate': 0.000687823118597072, 'epoch': 0.83}
 13%|█▎        | 650/5000 [3:04:45<23:08:28, 19.15s/it] 13%|█▎        | 651/5000 [3:05:08<24:34:46, 20.35s/it]                                                       {'loss': 962.2417, 'grad_norm': 2128.0, 'learning_rate': 0.0006877625158271889, 'epoch': 0.83}
 13%|█▎        | 651/5000 [3:05:08<24:34:46, 20.35s/it] 13%|█▎        | 652/5000 [3:05:20<21:23:56, 17.72s/it]                                                       {'loss': 1150.8199, 'grad_norm': 916.0, 'learning_rate': 0.0006877017653085061, 'epoch': 0.83}
 13%|█▎        | 652/5000 [3:05:20<21:23:56, 17.72s/it] 13%|█▎        | 653/5000 [3:05:36<20:36:38, 17.07s/it]                                                       {'loss': 1289.7412, 'grad_norm': 5600.0, 'learning_rate': 0.0006876408670675982, 'epoch': 0.83}
 13%|█▎        | 653/5000 [3:05:36<20:36:38, 17.07s/it] 13%|█▎        | 654/5000 [3:06:00<23:17:49, 19.30s/it]                                                       {'loss': 1161.907, 'grad_norm': 3056.0, 'learning_rate': 0.0006875798211311038, 'epoch': 0.83}
 13%|█▎        | 654/5000 [3:06:00<23:17:49, 19.30s/it] 13%|█▎        | 655/5000 [3:06:18<22:48:28, 18.90s/it]                                                       {'loss': 1232.8569, 'grad_norm': 2944.0, 'learning_rate': 0.0006875186275257267, 'epoch': 0.83}
 13%|█▎        | 655/5000 [3:06:18<22:48:28, 18.90s/it] 13%|█▎        | 656/5000 [3:06:42<24:36:19, 20.39s/it]                                                       {'loss': 947.5463, 'grad_norm': 5984.0, 'learning_rate': 0.000687457286278235, 'epoch': 0.83}
 13%|█▎        | 656/5000 [3:06:42<24:36:19, 20.39s/it] 13%|█▎        | 657/5000 [3:07:09<26:57:42, 22.35s/it]                                                       {'loss': 902.8881, 'grad_norm': 2656.0, 'learning_rate': 0.0006873957974154615, 'epoch': 0.83}
 13%|█▎        | 657/5000 [3:07:09<26:57:42, 22.35s/it] 13%|█▎        | 658/5000 [3:07:35<28:27:36, 23.60s/it]                                                       {'loss': 1091.8582, 'grad_norm': 4864.0, 'learning_rate': 0.0006873341609643036, 'epoch': 0.84}
 13%|█▎        | 658/5000 [3:07:35<28:27:36, 23.60s/it] 13%|█▎        | 659/5000 [3:07:51<25:24:28, 21.07s/it]                                                       {'loss': 855.0507, 'grad_norm': 4608.0, 'learning_rate': 0.0006872723769517229, 'epoch': 0.84}
 13%|█▎        | 659/5000 [3:07:51<25:24:28, 21.07s/it] 13%|█▎        | 660/5000 [3:08:07<23:53:21, 19.82s/it]                                                       {'loss': 1196.0525, 'grad_norm': 133120.0, 'learning_rate': 0.000687210445404746, 'epoch': 0.84}
 13%|█▎        | 660/5000 [3:08:07<23:53:21, 19.82s/it] 13%|█▎        | 661/5000 [3:08:43<29:37:23, 24.58s/it]                                                       {'loss': 1411.272, 'grad_norm': 36352.0, 'learning_rate': 0.0006871483663504639, 'epoch': 0.84}
 13%|█▎        | 661/5000 [3:08:43<29:37:23, 24.58s/it] 13%|█▎        | 662/5000 [3:08:58<26:04:13, 21.64s/it]                                                       {'loss': 1269.1211, 'grad_norm': 6816.0, 'learning_rate': 0.0006870861398160321, 'epoch': 0.84}
 13%|█▎        | 662/5000 [3:08:58<26:04:13, 21.64s/it] 13%|█▎        | 663/5000 [3:09:24<27:37:46, 22.93s/it]                                                       {'loss': 1783.6414, 'grad_norm': 15872.0, 'learning_rate': 0.0006870237658286704, 'epoch': 0.84}
 13%|█▎        | 663/5000 [3:09:24<27:37:46, 22.93s/it] 13%|█▎        | 664/5000 [3:09:46<27:18:11, 22.67s/it]                                                       {'loss': 1700.1339, 'grad_norm': 4512.0, 'learning_rate': 0.0006869612444156634, 'epoch': 0.84}
 13%|█▎        | 664/5000 [3:09:46<27:18:11, 22.67s/it] 13%|█▎        | 665/5000 [3:10:03<25:07:42, 20.87s/it]                                                       {'loss': 977.3867, 'grad_norm': 6976.0, 'learning_rate': 0.0006868985756043602, 'epoch': 0.84}
 13%|█▎        | 665/5000 [3:10:03<25:07:42, 20.87s/it] 13%|█▎        | 666/5000 [3:10:16<22:34:55, 18.76s/it]                                                       {'loss': 1293.6605, 'grad_norm': 28288.0, 'learning_rate': 0.0006868357594221742, 'epoch': 0.85}
 13%|█▎        | 666/5000 [3:10:16<22:34:55, 18.76s/it] 13%|█▎        | 667/5000 [3:10:34<22:12:28, 18.45s/it]                                                       {'loss': 988.7723, 'grad_norm': 48640.0, 'learning_rate': 0.0006867727958965833, 'epoch': 0.85}
 13%|█▎        | 667/5000 [3:10:34<22:12:28, 18.45s/it] 13%|█▎        | 668/5000 [3:10:48<20:40:55, 17.19s/it]                                                       {'loss': 1333.4668, 'grad_norm': 11904.0, 'learning_rate': 0.00068670968505513, 'epoch': 0.85}
 13%|█▎        | 668/5000 [3:10:48<20:40:55, 17.19s/it] 13%|█▎        | 669/5000 [3:11:05<20:22:23, 16.93s/it]                                                       {'loss': 1481.5354, 'grad_norm': 30208.0, 'learning_rate': 0.0006866464269254211, 'epoch': 0.85}
 13%|█▎        | 669/5000 [3:11:05<20:22:23, 16.93s/it] 13%|█▎        | 670/5000 [3:11:17<18:36:05, 15.47s/it]                                                       {'loss': 1370.365, 'grad_norm': 5344.0, 'learning_rate': 0.0006865830215351276, 'epoch': 0.85}
 13%|█▎        | 670/5000 [3:11:17<18:36:05, 15.47s/it] 13%|█▎        | 671/5000 [3:11:42<22:15:09, 18.51s/it]                                                       {'loss': 1548.1123, 'grad_norm': 55040.0, 'learning_rate': 0.0006865194689119855, 'epoch': 0.85}
 13%|█▎        | 671/5000 [3:11:42<22:15:09, 18.51s/it] 13%|█▎        | 672/5000 [3:12:02<22:42:13, 18.88s/it]                                                       {'loss': 1411.4392, 'grad_norm': 38912.0, 'learning_rate': 0.0006864557690837947, 'epoch': 0.85}
 13%|█▎        | 672/5000 [3:12:02<22:42:13, 18.88s/it] 13%|█▎        | 673/5000 [3:12:17<21:05:35, 17.55s/it]                                                       {'loss': 1306.1962, 'grad_norm': 7584.0, 'learning_rate': 0.0006863919220784196, 'epoch': 0.85}
 13%|█▎        | 673/5000 [3:12:17<21:05:35, 17.55s/it] 13%|█▎        | 674/5000 [3:12:36<21:44:39, 18.10s/it]                                                       {'loss': 1124.3541, 'grad_norm': 12288.0, 'learning_rate': 0.0006863279279237894, 'epoch': 0.86}
 13%|█▎        | 674/5000 [3:12:36<21:44:39, 18.10s/it] 14%|█▎        | 675/5000 [3:12:53<21:19:28, 17.75s/it]                                                       {'loss': 1275.8306, 'grad_norm': 4128.0, 'learning_rate': 0.0006862637866478969, 'epoch': 0.86}
 14%|█▎        | 675/5000 [3:12:53<21:19:28, 17.75s/it] 14%|█▎        | 676/5000 [3:13:05<19:18:25, 16.07s/it]                                                       {'loss': 1227.1533, 'grad_norm': 28928.0, 'learning_rate': 0.0006861994982787998, 'epoch': 0.86}
 14%|█▎        | 676/5000 [3:13:05<19:18:25, 16.07s/it] 14%|█▎        | 677/5000 [3:13:17<17:58:03, 14.96s/it]                                                       {'loss': 1475.7007, 'grad_norm': 14592.0, 'learning_rate': 0.0006861350628446202, 'epoch': 0.86}
 14%|█▎        | 677/5000 [3:13:17<17:58:03, 14.96s/it] 14%|█▎        | 678/5000 [3:13:35<18:46:46, 15.64s/it]                                                       {'loss': 1841.2206, 'grad_norm': 3888.0, 'learning_rate': 0.0006860704803735441, 'epoch': 0.86}
 14%|█▎        | 678/5000 [3:13:35<18:46:46, 15.64s/it] 14%|█▎        | 679/5000 [3:13:51<18:55:28, 15.77s/it]                                                       {'loss': 946.1116, 'grad_norm': 708608.0, 'learning_rate': 0.0006860057508938223, 'epoch': 0.86}
 14%|█▎        | 679/5000 [3:13:51<18:55:28, 15.77s/it] 14%|█▎        | 680/5000 [3:14:03<17:39:02, 14.71s/it]                                                       {'loss': 1422.4401, 'grad_norm': 3328.0, 'learning_rate': 0.0006859408744337694, 'epoch': 0.86}
 14%|█▎        | 680/5000 [3:14:03<17:39:02, 14.71s/it] 14%|█▎        | 681/5000 [3:14:23<19:24:33, 16.18s/it]                                                       {'loss': 952.2675, 'grad_norm': 5312.0, 'learning_rate': 0.000685875851021765, 'epoch': 0.86}
 14%|█▎        | 681/5000 [3:14:23<19:24:33, 16.18s/it] 14%|█▎        | 682/5000 [3:14:42<20:38:41, 17.21s/it]                                                       {'loss': 1713.2548, 'grad_norm': 107520.0, 'learning_rate': 0.000685810680686252, 'epoch': 0.87}
 14%|█▎        | 682/5000 [3:14:42<20:38:41, 17.21s/it] 14%|█▎        | 683/5000 [3:14:56<19:15:42, 16.06s/it]                                                       {'loss': 997.9087, 'grad_norm': 33280.0, 'learning_rate': 0.0006857453634557386, 'epoch': 0.87}
 14%|█▎        | 683/5000 [3:14:56<19:15:42, 16.06s/it] 14%|█▎        | 684/5000 [3:15:08<17:51:42, 14.90s/it]                                                       {'loss': 1181.8733, 'grad_norm': 17152.0, 'learning_rate': 0.0006856798993587964, 'epoch': 0.87}
 14%|█▎        | 684/5000 [3:15:08<17:51:42, 14.90s/it] 14%|█▎        | 685/5000 [3:15:28<19:36:59, 16.37s/it]                                                       {'loss': 1610.9961, 'grad_norm': 14784.0, 'learning_rate': 0.0006856142884240618, 'epoch': 0.87}
 14%|█▎        | 685/5000 [3:15:28<19:36:59, 16.37s/it] 14%|█▎        | 686/5000 [3:15:42<18:50:23, 15.72s/it]                                                       {'loss': 1219.8245, 'grad_norm': 96256.0, 'learning_rate': 0.0006855485306802353, 'epoch': 0.87}
 14%|█▎        | 686/5000 [3:15:42<18:50:23, 15.72s/it] 14%|█▎        | 687/5000 [3:15:54<17:39:22, 14.74s/it]                                                       {'loss': 1175.9062, 'grad_norm': 63232.0, 'learning_rate': 0.0006854826261560816, 'epoch': 0.87}
 14%|█▎        | 687/5000 [3:15:54<17:39:22, 14.74s/it] 14%|█▍        | 688/5000 [3:16:16<20:07:12, 16.80s/it]                                                       {'loss': 1151.0913, 'grad_norm': 12544.0, 'learning_rate': 0.0006854165748804293, 'epoch': 0.87}
 14%|█▍        | 688/5000 [3:16:16<20:07:12, 16.80s/it] 14%|█▍        | 689/5000 [3:16:27<18:16:12, 15.26s/it]                                                       {'loss': 1413.0789, 'grad_norm': 2320.0, 'learning_rate': 0.0006853503768821718, 'epoch': 0.87}
 14%|█▍        | 689/5000 [3:16:27<18:16:12, 15.26s/it] 14%|█▍        | 690/5000 [3:16:44<18:42:43, 15.63s/it]                                                       {'loss': 1445.4021, 'grad_norm': 17664.0, 'learning_rate': 0.0006852840321902662, 'epoch': 0.88}
 14%|█▍        | 690/5000 [3:16:44<18:42:43, 15.63s/it] 14%|█▍        | 691/5000 [3:17:02<19:32:41, 16.33s/it]                                                       {'loss': 1231.2131, 'grad_norm': 22144.0, 'learning_rate': 0.0006852175408337339, 'epoch': 0.88}
 14%|█▍        | 691/5000 [3:17:02<19:32:41, 16.33s/it] 14%|█▍        | 692/5000 [3:17:17<18:59:41, 15.87s/it]                                                       {'loss': 1445.1526, 'grad_norm': 6240.0, 'learning_rate': 0.0006851509028416605, 'epoch': 0.88}
 14%|█▍        | 692/5000 [3:17:17<18:59:41, 15.87s/it] 14%|█▍        | 693/5000 [3:17:31<18:33:47, 15.52s/it]                                                       {'loss': 1197.1948, 'grad_norm': 16512.0, 'learning_rate': 0.0006850841182431958, 'epoch': 0.88}
 14%|█▍        | 693/5000 [3:17:31<18:33:47, 15.52s/it] 14%|█▍        | 694/5000 [3:17:44<17:32:01, 14.66s/it]                                                       {'loss': 1119.2183, 'grad_norm': 3760.0, 'learning_rate': 0.0006850171870675535, 'epoch': 0.88}
 14%|█▍        | 694/5000 [3:17:44<17:32:01, 14.66s/it] 14%|█▍        | 695/5000 [3:17:56<16:35:34, 13.88s/it]                                                       {'loss': 1143.0674, 'grad_norm': 1656.0, 'learning_rate': 0.0006849501093440116, 'epoch': 0.88}
 14%|█▍        | 695/5000 [3:17:56<16:35:34, 13.88s/it] 14%|█▍        | 696/5000 [3:18:16<18:37:58, 15.59s/it]                                                       {'loss': 910.8603, 'grad_norm': 1360.0, 'learning_rate': 0.0006848828851019124, 'epoch': 0.88}
 14%|█▍        | 696/5000 [3:18:16<18:37:58, 15.59s/it] 14%|█▍        | 697/5000 [3:18:35<19:58:31, 16.71s/it]                                                       {'loss': 1300.8455, 'grad_norm': 36096.0, 'learning_rate': 0.0006848155143706619, 'epoch': 0.89}
 14%|█▍        | 697/5000 [3:18:35<19:58:31, 16.71s/it] 14%|█▍        | 698/5000 [3:18:56<21:36:59, 18.09s/it]                                                       {'loss': 919.1113, 'grad_norm': 12480.0, 'learning_rate': 0.0006847479971797303, 'epoch': 0.89}
 14%|█▍        | 698/5000 [3:18:56<21:36:59, 18.09s/it] 14%|█▍        | 699/5000 [3:19:22<24:19:21, 20.36s/it]                                                       {'loss': 1043.7336, 'grad_norm': 10880.0, 'learning_rate': 0.0006846803335586518, 'epoch': 0.89}
 14%|█▍        | 699/5000 [3:19:22<24:19:21, 20.36s/it] 14%|█▍        | 700/5000 [3:19:36<22:10:51, 18.57s/it]                                                       {'loss': 1575.6967, 'grad_norm': 13952.0, 'learning_rate': 0.0006846125235370252, 'epoch': 0.89}
 14%|█▍        | 700/5000 [3:19:36<22:10:51, 18.57s/it] 14%|█▍        | 701/5000 [3:20:01<24:12:54, 20.28s/it]                                                       {'loss': 1458.2427, 'grad_norm': 21888.0, 'learning_rate': 0.0006845445671445125, 'epoch': 0.89}
 14%|█▍        | 701/5000 [3:20:01<24:12:54, 20.28s/it] 14%|█▍        | 702/5000 [3:20:24<25:26:08, 21.30s/it]                                                       {'loss': 1397.2095, 'grad_norm': 14976.0, 'learning_rate': 0.0006844764644108402, 'epoch': 0.89}
 14%|█▍        | 702/5000 [3:20:24<25:26:08, 21.30s/it] 14%|█▍        | 703/5000 [3:20:37<22:09:45, 18.57s/it]                                                       {'loss': 1576.6587, 'grad_norm': 6720.0, 'learning_rate': 0.000684408215365799, 'epoch': 0.89}
 14%|█▍        | 703/5000 [3:20:37<22:09:45, 18.57s/it] 14%|█▍        | 704/5000 [3:20:53<21:19:43, 17.87s/it]                                                       {'loss': 982.7759, 'grad_norm': 13312.0, 'learning_rate': 0.0006843398200392431, 'epoch': 0.89}
 14%|█▍        | 704/5000 [3:20:53<21:19:43, 17.87s/it] 14%|█▍        | 705/5000 [3:21:10<21:05:06, 17.67s/it]                                                       {'loss': 1047.845, 'grad_norm': 4512.0, 'learning_rate': 0.000684271278461091, 'epoch': 0.9}
 14%|█▍        | 705/5000 [3:21:10<21:05:06, 17.67s/it] 14%|█▍        | 706/5000 [3:21:25<20:07:02, 16.87s/it]                                                       {'loss': 1030.3092, 'grad_norm': 88576.0, 'learning_rate': 0.000684202590661325, 'epoch': 0.9}
 14%|█▍        | 706/5000 [3:21:25<20:07:02, 16.87s/it] 14%|█▍        | 707/5000 [3:21:41<19:39:56, 16.49s/it]                                                       {'loss': 1201.979, 'grad_norm': 16768.0, 'learning_rate': 0.0006841337566699917, 'epoch': 0.9}
 14%|█▍        | 707/5000 [3:21:41<19:39:56, 16.49s/it] 14%|█▍        | 708/5000 [3:21:56<19:23:33, 16.27s/it]                                                       {'loss': 1079.5098, 'grad_norm': 13760.0, 'learning_rate': 0.0006840647765172013, 'epoch': 0.9}
 14%|█▍        | 708/5000 [3:21:56<19:23:33, 16.27s/it] 14%|█▍        | 709/5000 [3:22:09<18:16:23, 15.33s/it]                                                       {'loss': 1256.1418, 'grad_norm': 8064.0, 'learning_rate': 0.0006839956502331282, 'epoch': 0.9}
 14%|█▍        | 709/5000 [3:22:09<18:16:23, 15.33s/it] 14%|█▍        | 710/5000 [3:22:27<18:56:34, 15.90s/it]                                                       {'loss': 914.308, 'grad_norm': 8768.0, 'learning_rate': 0.0006839263778480104, 'epoch': 0.9}
 14%|█▍        | 710/5000 [3:22:27<18:56:34, 15.90s/it] 14%|█▍        | 711/5000 [3:22:43<18:59:26, 15.94s/it]                                                       {'loss': 1191.9241, 'grad_norm': 7648.0, 'learning_rate': 0.0006838569593921501, 'epoch': 0.9}
 14%|█▍        | 711/5000 [3:22:43<18:59:26, 15.94s/it] 14%|█▍        | 712/5000 [3:22:58<18:39:49, 15.67s/it]                                                       {'loss': 1114.6387, 'grad_norm': 741376.0, 'learning_rate': 0.0006837873948959132, 'epoch': 0.9}
 14%|█▍        | 712/5000 [3:22:58<18:39:49, 15.67s/it] 14%|█▍        | 713/5000 [3:23:14<18:44:19, 15.74s/it]                                                       {'loss': 837.5178, 'grad_norm': 37888.0, 'learning_rate': 0.0006837176843897298, 'epoch': 0.91}
 14%|█▍        | 713/5000 [3:23:14<18:44:19, 15.74s/it] 14%|█▍        | 714/5000 [3:23:32<19:33:26, 16.43s/it]                                                       {'loss': 1042.3048, 'grad_norm': 70656.0, 'learning_rate': 0.0006836478279040933, 'epoch': 0.91}
 14%|█▍        | 714/5000 [3:23:32<19:33:26, 16.43s/it] 14%|█▍        | 715/5000 [3:23:49<19:49:35, 16.66s/it]                                                       {'loss': 996.3904, 'grad_norm': 127488.0, 'learning_rate': 0.0006835778254695615, 'epoch': 0.91}
 14%|█▍        | 715/5000 [3:23:49<19:49:35, 16.66s/it] 14%|█▍        | 716/5000 [3:24:16<23:29:07, 19.74s/it]                                                       {'loss': 1065.6558, 'grad_norm': 288768.0, 'learning_rate': 0.0006835076771167559, 'epoch': 0.91}
 14%|█▍        | 716/5000 [3:24:16<23:29:07, 19.74s/it] 14%|█▍        | 717/5000 [3:24:31<21:40:48, 18.22s/it]                                                       {'loss': 1106.5737, 'grad_norm': 10304.0, 'learning_rate': 0.0006834373828763615, 'epoch': 0.91}
 14%|█▍        | 717/5000 [3:24:31<21:40:48, 18.22s/it] 14%|█▍        | 718/5000 [3:24:42<19:14:09, 16.17s/it]                                                       {'loss': 1138.8696, 'grad_norm': 8960.0, 'learning_rate': 0.0006833669427791278, 'epoch': 0.91}
 14%|█▍        | 718/5000 [3:24:42<19:14:09, 16.17s/it] 14%|█▍        | 719/5000 [3:24:54<17:52:39, 15.03s/it]                                                       {'loss': 1334.7349, 'grad_norm': 11796480.0, 'learning_rate': 0.0006832963568558675, 'epoch': 0.91}
 14%|█▍        | 719/5000 [3:24:54<17:52:39, 15.03s/it] 14%|█▍        | 720/5000 [3:25:10<18:06:01, 15.22s/it]                                                       {'loss': 1021.2406, 'grad_norm': 6176.0, 'learning_rate': 0.0006832256251374571, 'epoch': 0.91}
 14%|█▍        | 720/5000 [3:25:10<18:06:01, 15.22s/it] 14%|█▍        | 721/5000 [3:25:25<17:54:09, 15.06s/it]                                                       {'loss': 1133.7117, 'grad_norm': 87040.0, 'learning_rate': 0.0006831547476548373, 'epoch': 0.92}
 14%|█▍        | 721/5000 [3:25:25<17:54:09, 15.06s/it] 14%|█▍        | 722/5000 [3:25:41<18:19:13, 15.42s/it]                                                       {'loss': 1260.6719, 'grad_norm': 16064.0, 'learning_rate': 0.0006830837244390123, 'epoch': 0.92}
 14%|█▍        | 722/5000 [3:25:41<18:19:13, 15.42s/it] 14%|█▍        | 723/5000 [3:25:54<17:19:20, 14.58s/it]                                                       {'loss': 1317.8477, 'grad_norm': 522240.0, 'learning_rate': 0.0006830125555210498, 'epoch': 0.92}
 14%|█▍        | 723/5000 [3:25:54<17:19:20, 14.58s/it] 14%|█▍        | 724/5000 [3:26:10<18:00:55, 15.17s/it]                                                       {'loss': 1213.1073, 'grad_norm': 37376.0, 'learning_rate': 0.0006829412409320819, 'epoch': 0.92}
 14%|█▍        | 724/5000 [3:26:10<18:00:55, 15.17s/it] 14%|█▍        | 725/5000 [3:26:28<18:57:30, 15.96s/it]                                                       {'loss': 1107.1062, 'grad_norm': 3620864.0, 'learning_rate': 0.0006828697807033038, 'epoch': 0.92}
 14%|█▍        | 725/5000 [3:26:28<18:57:30, 15.96s/it] 15%|█▍        | 726/5000 [3:26:41<17:56:38, 15.11s/it]                                                       {'loss': 1397.9003, 'grad_norm': 1269760.0, 'learning_rate': 0.0006827981748659745, 'epoch': 0.92}
 15%|█▍        | 726/5000 [3:26:41<17:56:38, 15.11s/it] 15%|█▍        | 727/5000 [3:26:53<16:59:22, 14.31s/it]                                                       {'loss': 1679.5825, 'grad_norm': 1679360.0, 'learning_rate': 0.0006827264234514171, 'epoch': 0.92}
 15%|█▍        | 727/5000 [3:26:53<16:59:22, 14.31s/it] 15%|█▍        | 728/5000 [3:27:16<19:59:14, 16.84s/it]                                                       {'loss': 1863.9856, 'grad_norm': 135168.0, 'learning_rate': 0.0006826545264910181, 'epoch': 0.92}
 15%|█▍        | 728/5000 [3:27:16<19:59:14, 16.84s/it] 15%|█▍        | 729/5000 [3:27:42<23:17:31, 19.63s/it]                                                       {'loss': 1792.5874, 'grad_norm': 183296.0, 'learning_rate': 0.0006825824840162273, 'epoch': 0.93}
 15%|█▍        | 729/5000 [3:27:42<23:17:31, 19.63s/it] 15%|█▍        | 730/5000 [3:27:57<21:30:03, 18.13s/it]                                                       {'loss': 1826.6205, 'grad_norm': 19328.0, 'learning_rate': 0.0006825102960585591, 'epoch': 0.93}
 15%|█▍        | 730/5000 [3:27:57<21:30:03, 18.13s/it] 15%|█▍        | 731/5000 [3:28:12<20:18:25, 17.12s/it]                                                       {'loss': 1827.1592, 'grad_norm': 9088.0, 'learning_rate': 0.0006824379626495904, 'epoch': 0.93}
 15%|█▍        | 731/5000 [3:28:12<20:18:25, 17.12s/it] 15%|█▍        | 732/5000 [3:28:29<20:14:49, 17.08s/it]                                                       {'loss': 1582.0101, 'grad_norm': 20480.0, 'learning_rate': 0.0006823654838209627, 'epoch': 0.93}
 15%|█▍        | 732/5000 [3:28:29<20:14:49, 17.08s/it] 15%|█▍        | 733/5000 [3:28:41<18:42:41, 15.79s/it]                                                       {'loss': 1548.9745, 'grad_norm': 8896.0, 'learning_rate': 0.0006822928596043804, 'epoch': 0.93}
 15%|█▍        | 733/5000 [3:28:41<18:42:41, 15.79s/it] 15%|█▍        | 734/5000 [3:29:06<21:47:06, 18.38s/it]                                                       {'loss': 1224.2896, 'grad_norm': 7840.0, 'learning_rate': 0.0006822200900316121, 'epoch': 0.93}
 15%|█▍        | 734/5000 [3:29:06<21:47:06, 18.38s/it] 15%|█▍        | 735/5000 [3:29:20<20:22:26, 17.20s/it]                                                       {'loss': 1161.8829, 'grad_norm': 11200.0, 'learning_rate': 0.0006821471751344895, 'epoch': 0.93}
 15%|█▍        | 735/5000 [3:29:20<20:22:26, 17.20s/it] 15%|█▍        | 736/5000 [3:29:36<19:40:58, 16.62s/it]                                                       {'loss': 1198.948, 'grad_norm': 1928.0, 'learning_rate': 0.000682074114944908, 'epoch': 0.93}
 15%|█▍        | 736/5000 [3:29:36<19:40:58, 16.62s/it] 15%|█▍        | 737/5000 [3:29:56<21:00:40, 17.74s/it]                                                       {'loss': 1014.0638, 'grad_norm': 4032.0, 'learning_rate': 0.0006820009094948268, 'epoch': 0.94}
 15%|█▍        | 737/5000 [3:29:56<21:00:40, 17.74s/it] 15%|█▍        | 738/5000 [3:30:09<19:29:46, 16.47s/it]                                                       {'loss': 987.3152, 'grad_norm': 3088.0, 'learning_rate': 0.0006819275588162682, 'epoch': 0.94}
 15%|█▍        | 738/5000 [3:30:09<19:29:46, 16.47s/it] 15%|█▍        | 739/5000 [3:30:25<19:08:44, 16.18s/it]                                                       {'loss': 1000.1812, 'grad_norm': 5568.0, 'learning_rate': 0.0006818540629413186, 'epoch': 0.94}
 15%|█▍        | 739/5000 [3:30:25<19:08:44, 16.18s/it] 15%|█▍        | 740/5000 [3:30:45<20:31:34, 17.35s/it]                                                       {'loss': 918.6331, 'grad_norm': 26624.0, 'learning_rate': 0.0006817804219021274, 'epoch': 0.94}
 15%|█▍        | 740/5000 [3:30:45<20:31:34, 17.35s/it] 15%|█▍        | 741/5000 [3:30:57<18:35:30, 15.72s/it]                                                       {'loss': 1126.9736, 'grad_norm': 3168.0, 'learning_rate': 0.0006817066357309075, 'epoch': 0.94}
 15%|█▍        | 741/5000 [3:30:57<18:35:30, 15.72s/it] 15%|█▍        | 742/5000 [3:31:10<17:42:52, 14.98s/it]                                                       {'loss': 1295.8936, 'grad_norm': 49152.0, 'learning_rate': 0.000681632704459936, 'epoch': 0.94}
 15%|█▍        | 742/5000 [3:31:10<17:42:52, 14.98s/it] 15%|█▍        | 743/5000 [3:31:23<16:56:08, 14.32s/it]                                                       {'loss': 1254.9479, 'grad_norm': 68096.0, 'learning_rate': 0.0006815586281215524, 'epoch': 0.94}
 15%|█▍        | 743/5000 [3:31:23<16:56:08, 14.32s/it] 15%|█▍        | 744/5000 [3:32:00<25:01:17, 21.16s/it]                                                       {'loss': 1534.207, 'grad_norm': 20736.0, 'learning_rate': 0.0006814844067481605, 'epoch': 0.94}
 15%|█▍        | 744/5000 [3:32:00<25:01:17, 21.16s/it] 15%|█▍        | 745/5000 [3:32:24<26:05:56, 22.08s/it]                                                       {'loss': 1565.8557, 'grad_norm': 118784.0, 'learning_rate': 0.0006814100403722272, 'epoch': 0.95}
 15%|█▍        | 745/5000 [3:32:24<26:05:56, 22.08s/it] 15%|█▍        | 746/5000 [3:32:37<22:48:35, 19.30s/it]                                                       {'loss': 1934.4554, 'grad_norm': 41984.0, 'learning_rate': 0.0006813355290262829, 'epoch': 0.95}
 15%|█▍        | 746/5000 [3:32:37<22:48:35, 19.30s/it] 15%|█▍        | 747/5000 [3:33:03<25:03:53, 21.22s/it]                                                       {'loss': 1618.4825, 'grad_norm': 147456.0, 'learning_rate': 0.0006812608727429213, 'epoch': 0.95}
 15%|█▍        | 747/5000 [3:33:03<25:03:53, 21.22s/it] 15%|█▍        | 748/5000 [3:33:18<22:54:21, 19.39s/it]                                                       {'loss': 1511.3086, 'grad_norm': 14656.0, 'learning_rate': 0.0006811860715547997, 'epoch': 0.95}
 15%|█▍        | 748/5000 [3:33:18<22:54:21, 19.39s/it] 15%|█▍        | 749/5000 [3:33:43<24:51:35, 21.05s/it]                                                       {'loss': 1388.3262, 'grad_norm': 7552.0, 'learning_rate': 0.0006811111254946387, 'epoch': 0.95}
 15%|█▍        | 749/5000 [3:33:43<24:51:35, 21.05s/it] 15%|█▌        | 750/5000 [3:34:00<23:16:46, 19.72s/it]                                                       {'loss': 1391.0719, 'grad_norm': 5664.0, 'learning_rate': 0.0006810360345952221, 'epoch': 0.95}
 15%|█▌        | 750/5000 [3:34:00<23:16:46, 19.72s/it] 15%|█▌        | 751/5000 [3:34:15<21:49:48, 18.50s/it]                                                       {'loss': 1342.2241, 'grad_norm': 40192.0, 'learning_rate': 0.0006809607988893974, 'epoch': 0.95}
 15%|█▌        | 751/5000 [3:34:15<21:49:48, 18.50s/it] 15%|█▌        | 752/5000 [3:34:37<22:55:17, 19.43s/it]                                                       {'loss': 1334.5791, 'grad_norm': 4576.0, 'learning_rate': 0.000680885418410075, 'epoch': 0.95}
 15%|█▌        | 752/5000 [3:34:37<22:55:17, 19.43s/it] 15%|█▌        | 753/5000 [3:34:55<22:33:01, 19.11s/it]                                                       {'loss': 1189.7198, 'grad_norm': 2928.0, 'learning_rate': 0.0006808098931902292, 'epoch': 0.96}
 15%|█▌        | 753/5000 [3:34:55<22:33:01, 19.11s/it] 15%|█▌        | 754/5000 [3:35:10<21:10:03, 17.95s/it]                                                       {'loss': 1224.1064, 'grad_norm': 53504.0, 'learning_rate': 0.0006807342232628971, 'epoch': 0.96}
 15%|█▌        | 754/5000 [3:35:10<21:10:03, 17.95s/it] 15%|█▌        | 755/5000 [3:35:24<19:34:28, 16.60s/it]                                                       {'loss': 1375.4766, 'grad_norm': 33280.0, 'learning_rate': 0.0006806584086611793, 'epoch': 0.96}
 15%|█▌        | 755/5000 [3:35:24<19:34:28, 16.60s/it] 15%|█▌        | 756/5000 [3:35:40<19:18:30, 16.38s/it]                                                       {'loss': 1146.8921, 'grad_norm': 18304.0, 'learning_rate': 0.0006805824494182398, 'epoch': 0.96}
 15%|█▌        | 756/5000 [3:35:40<19:18:30, 16.38s/it] 15%|█▌        | 757/5000 [3:35:55<18:50:36, 15.99s/it]                                                       {'loss': 1143.2341, 'grad_norm': 3104.0, 'learning_rate': 0.0006805063455673057, 'epoch': 0.96}
 15%|█▌        | 757/5000 [3:35:55<18:50:36, 15.99s/it] 15%|█▌        | 758/5000 [3:36:15<20:24:23, 17.32s/it]                                                       {'loss': 1320.829, 'grad_norm': 3184.0, 'learning_rate': 0.0006804300971416673, 'epoch': 0.96}
 15%|█▌        | 758/5000 [3:36:15<20:24:23, 17.32s/it] 15%|█▌        | 759/5000 [3:36:34<20:48:50, 17.67s/it]                                                       {'loss': 1007.718, 'grad_norm': 1416.0, 'learning_rate': 0.0006803537041746784, 'epoch': 0.96}
 15%|█▌        | 759/5000 [3:36:34<20:48:50, 17.67s/it] 15%|█▌        | 760/5000 [3:36:47<19:16:42, 16.37s/it]                                                       {'loss': 891.5491, 'grad_norm': 10816.0, 'learning_rate': 0.0006802771666997557, 'epoch': 0.97}
 15%|█▌        | 760/5000 [3:36:47<19:16:42, 16.37s/it] 15%|█▌        | 761/5000 [3:37:04<19:26:00, 16.50s/it]                                                       {'loss': 1156.2803, 'grad_norm': 7072.0, 'learning_rate': 0.0006802004847503794, 'epoch': 0.97}
 15%|█▌        | 761/5000 [3:37:04<19:26:00, 16.50s/it] 15%|█▌        | 762/5000 [3:37:17<18:20:01, 15.57s/it]                                                       {'loss': 1366.7693, 'grad_norm': 77312.0, 'learning_rate': 0.0006801236583600929, 'epoch': 0.97}
 15%|█▌        | 762/5000 [3:37:17<18:20:01, 15.57s/it] 15%|█▌        | 763/5000 [3:37:33<18:21:24, 15.60s/it]                                                       {'loss': 1151.0704, 'grad_norm': 29952.0, 'learning_rate': 0.0006800466875625026, 'epoch': 0.97}
 15%|█▌        | 763/5000 [3:37:33<18:21:24, 15.60s/it] 15%|█▌        | 764/5000 [3:37:48<18:02:58, 15.34s/it]                                                       {'loss': 1538.3671, 'grad_norm': 6784.0, 'learning_rate': 0.0006799695723912779, 'epoch': 0.97}
 15%|█▌        | 764/5000 [3:37:48<18:02:58, 15.34s/it] 15%|█▌        | 765/5000 [3:38:02<17:37:04, 14.98s/it]                                                       {'loss': 1394.0396, 'grad_norm': 13376.0, 'learning_rate': 0.000679892312880152, 'epoch': 0.97}
 15%|█▌        | 765/5000 [3:38:02<17:37:04, 14.98s/it] 15%|█▌        | 766/5000 [3:38:15<17:01:04, 14.47s/it]                                                       {'loss': 1195.3818, 'grad_norm': 7840.0, 'learning_rate': 0.0006798149090629207, 'epoch': 0.97}
 15%|█▌        | 766/5000 [3:38:15<17:01:04, 14.47s/it] 15%|█▌        | 767/5000 [3:38:34<18:40:04, 15.88s/it]                                                       {'loss': 1275.2291, 'grad_norm': 16320.0, 'learning_rate': 0.0006797373609734429, 'epoch': 0.97}
 15%|█▌        | 767/5000 [3:38:34<18:40:04, 15.88s/it] 15%|█▌        | 768/5000 [3:38:57<21:16:01, 18.09s/it]                                                       {'loss': 1038.2324, 'grad_norm': 15808.0, 'learning_rate': 0.0006796596686456408, 'epoch': 0.98}
 15%|█▌        | 768/5000 [3:38:57<21:16:01, 18.09s/it] 15%|█▌        | 769/5000 [3:39:23<23:48:53, 20.26s/it]                                                       {'loss': 1013.1848, 'grad_norm': 12480.0, 'learning_rate': 0.0006795818321134998, 'epoch': 0.98}
 15%|█▌        | 769/5000 [3:39:23<23:48:53, 20.26s/it] 15%|█▌        | 770/5000 [3:39:40<22:35:00, 19.22s/it]                                                       {'loss': 2206.5625, 'grad_norm': 276480.0, 'learning_rate': 0.0006795038514110682, 'epoch': 0.98}
 15%|█▌        | 770/5000 [3:39:40<22:35:00, 19.22s/it] 15%|█▌        | 771/5000 [3:39:56<21:33:58, 18.36s/it]                                                       {'loss': 1803.2455, 'grad_norm': 202752.0, 'learning_rate': 0.0006794257265724574, 'epoch': 0.98}
 15%|█▌        | 771/5000 [3:39:56<21:33:58, 18.36s/it] 15%|█▌        | 772/5000 [3:40:12<20:42:27, 17.63s/it]                                                       {'loss': 1136.2354, 'grad_norm': 6208.0, 'learning_rate': 0.0006793474576318418, 'epoch': 0.98}
 15%|█▌        | 772/5000 [3:40:12<20:42:27, 17.63s/it] 15%|█▌        | 773/5000 [3:40:24<18:45:18, 15.97s/it]                                                       {'loss': 1434.5918, 'grad_norm': 5088.0, 'learning_rate': 0.0006792690446234588, 'epoch': 0.98}
 15%|█▌        | 773/5000 [3:40:24<18:45:18, 15.97s/it] 15%|█▌        | 774/5000 [3:40:47<21:17:00, 18.13s/it]                                                       {'loss': 1074.0564, 'grad_norm': 2998272.0, 'learning_rate': 0.0006791904875816093, 'epoch': 0.98}
 15%|█▌        | 774/5000 [3:40:47<21:17:00, 18.13s/it] 16%|█▌        | 775/5000 [3:41:01<19:39:13, 16.75s/it]                                                       {'loss': 1238.1213, 'grad_norm': 6464.0, 'learning_rate': 0.0006791117865406564, 'epoch': 0.98}
 16%|█▌        | 775/5000 [3:41:01<19:39:13, 16.75s/it] 16%|█▌        | 776/5000 [3:41:28<23:32:36, 20.07s/it]                                                       {'loss': 1111.8247, 'grad_norm': 88576.0, 'learning_rate': 0.0006790329415350268, 'epoch': 0.99}
 16%|█▌        | 776/5000 [3:41:28<23:32:36, 20.07s/it] 16%|█▌        | 777/5000 [3:41:50<24:11:42, 20.63s/it]                                                       {'loss': 1260.95, 'grad_norm': 33024.0, 'learning_rate': 0.0006789539525992101, 'epoch': 0.99}
 16%|█▌        | 777/5000 [3:41:50<24:11:42, 20.63s/it] 16%|█▌        | 778/5000 [3:42:05<22:13:33, 18.95s/it]                                                       {'loss': 1258.5776, 'grad_norm': 2592.0, 'learning_rate': 0.0006788748197677586, 'epoch': 0.99}
 16%|█▌        | 778/5000 [3:42:05<22:13:33, 18.95s/it] 16%|█▌        | 779/5000 [3:42:18<20:00:50, 17.07s/it]                                                       {'loss': 1490.5737, 'grad_norm': 25472.0, 'learning_rate': 0.0006787955430752876, 'epoch': 0.99}
 16%|█▌        | 779/5000 [3:42:18<20:00:50, 17.07s/it] 16%|█▌        | 780/5000 [3:42:32<18:49:36, 16.06s/it]                                                       {'loss': 1365.1302, 'grad_norm': 7008.0, 'learning_rate': 0.0006787161225564754, 'epoch': 0.99}
 16%|█▌        | 780/5000 [3:42:32<18:49:36, 16.06s/it] 16%|█▌        | 781/5000 [3:42:44<17:34:17, 14.99s/it]                                                       {'loss': 1416.4819, 'grad_norm': 2496.0, 'learning_rate': 0.0006786365582460636, 'epoch': 0.99}
 16%|█▌        | 781/5000 [3:42:44<17:34:17, 14.99s/it][2024-06-12 19:54:25,007] torch.distributed.elastic.agent.server.api: [WARNING] Received 1 death signal, shutting down workers
[2024-06-12 19:54:25,009] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1119658 closing signal SIGHUP
[2024-06-12 19:54:25,009] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1119659 closing signal SIGHUP
[2024-06-12 19:54:25,009] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1119660 closing signal SIGHUP
[2024-06-12 19:54:25,009] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1119661 closing signal SIGHUP
Traceback (most recent call last):
  File "/data0/xjw/anaconda3/envs/llm311/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1010, in launch_command
    multi_gpu_launcher(args)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/accelerate/commands/launch.py", line 672, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py", line 877, in _invoke_run
    time.sleep(monitor_interval)
  File "/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1119623 got signal: 1
OrderedDict([('hf_auth', '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')])
OrderedDict([('hf_auth', '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')])
OrderedDict([('hf_auth', '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')])
OrderedDict([('hf_auth', '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')])
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
load datasets from ['/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/formal_fallacies_syllogisms_negation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/language_identification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/play_dialog_same_or_different', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/paragraph_segmentation', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/epistemic_reasoning', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/logical_deduction', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_de', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/strategyqa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/tracking_shuffled_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_es', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/goal_step_wikihow', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/abstract_narrative_understanding', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cnn_dailymail', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/news_commentary_it', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/winowhy', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/reasoning_about_colored_objects', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/gsm8k', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/vitaminc_fact_verification', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/object_counting', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/unit_conversion', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/disfl_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/topical_chat', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/contextual_parametric_knowledge_conflicts', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/question_selection', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/cs_algorithms', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/linguistics_puzzles', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/elementary_math_qa', '/data0/ljy/workspace/BIG-bench/fuze_28_balance_no_sys/alpaca']
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2840
})> 0
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2840
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 2000
})> 1
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 652
})> 2
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 652
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1800
})> 3
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1800
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 4
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 300
})> 5
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 300
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 6
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 457
})> 7
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 457
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 8
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 750
})> 8
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 750
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 9
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 20000
})> 9
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 20000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1410
})> 10
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1410
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 600
})> 11
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 12
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 5000
})> 12
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 5000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 13
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 8001
})> 13
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 8001
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 14
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 572
})> 14
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 572
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 15
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 15
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 16
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1319
})> 16
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1319
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})>
 21<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})>
 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 10933
})> 17
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 10933
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 200
})> 18
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 200
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4785
})> 19
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4785
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1600
})> 20
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1600
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 4459
})> 21
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 4459
})
Map (num_proc=4):  17%|█▋        | 168/1000 [00:00<00:02, 317.51 examples/s]Map (num_proc=4):  19%|█▉        | 190/1000 [00:00<00:02, 357.50 examples/s]Map (num_proc=4):  70%|███████   | 701/1000 [00:00<00:00, 1247.35 examples/s]Map (num_proc=4):  71%|███████   | 709/1000 [00:00<00:00, 1180.82 examples/s]Map (num_proc=4):  96%|█████████▌| 961/1000 [00:00<00:00, 1496.98 examples/s]<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):  95%|█████████▍| 946/1000 [00:00<00:00, 1266.33 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1074.58 examples/s]
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4): 100%|██████████| 1000/1000 [00:01<00:00, 990.31 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 22
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4):  21%|██▏       | 213/1000 [00:00<00:01, 451.68 examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 3505
})> 22
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 3505
})
Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 953.46 examples/s]Map (num_proc=4):  97%|█████████▋| 971/1000 [00:00<00:00, 1650.85 examples/s]Map (num_proc=4):  20%|█▉        | 198/1000 [00:00<00:02, 379.90 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1120.89 examples/s]
Map (num_proc=4):  21%|██        | 210/1000 [00:00<00:01, 442.08 examples/s]Map (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 899.58 examples/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):  44%|████▍     | 439/1000 [00:00<00:00, 863.45 examples/s]Map (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 1242.76 examples/s]Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 1375.16 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1037.21 examples/s]
Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1135.90 examples/s]
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 316
})> 23
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 316
})
dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Map (num_proc=4):  22%|██▏       | 215/1000 [00:00<00:01, 434.98 examples/s]Map (num_proc=4):  70%|██████▉   | 697/1000 [00:00<00:00, 1304.19 examples/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Map (num_proc=4):  94%|█████████▍| 940/1000 [00:00<00:00, 1432.61 examples/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.08it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.81it/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1096.28 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.07it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.24it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.11it/s]
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 24
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 264
})> 24
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 264
})
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  7.25it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.33it/s]
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 400
})> 25
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 400
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 7629
})> 26
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 7629
})
model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4):  20%|██        | 205/1000 [00:00<00:02, 371.53 examples/s]Map (num_proc=4):  70%|███████   | 704/1000 [00:00<00:00, 1117.38 examples/s]Map (num_proc=4):  93%|█████████▎| 929/1000 [00:00<00:00, 1240.27 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:01<00:00, 980.13 examples/s]
model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
<bound method Dataset.map of Dataset({
    features: ['prompt', 'response'],
    num_rows: 1000
})> 27
Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=4):  20%|██        | 201/1000 [00:00<00:02, 392.66 examples/s]Map (num_proc=4):  45%|████▍     | 447/1000 [00:00<00:00, 816.65 examples/s]Map (num_proc=4):  96%|█████████▌| 958/1000 [00:00<00:00, 1649.02 examples/s]Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1101.98 examples/s]
Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
})
dataset is loaded, train: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 25200
}) 
test: Dataset({
    features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2800
}) DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 25200
    })
    test: Dataset({
        features: ['prompt', 'response', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2800
    })
})
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.86it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.19it/s]
Load adapter, lora1
Load adapter, lora1
model loaded LlamaMeteorForCausalLM(
  (model): LlamaMeteorModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaMeteorDecoderLayer(
        (self_attn): LlamaMeteorFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaMeteorRotaryEmbedding()
        )
        (mlp): LlamaMeteorMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaMeteorRMSNorm()
        (post_attention_layernorm): LlamaMeteorRMSNorm()
      )
    )
    (norm): LlamaMeteorRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
load adapters from {'lora1': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/formal_fallacies_syllogisms_negation_no_sys', 'lora2': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/language_identification_no_sys', 'lora3': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/play_dialog_same_or_different_no_sys', 'lora4': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/paragraph_segmentation_no_sys', 'lora5': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/epistemic_reasoning_no_sys', 'lora6': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/logical_deduction_no_sys', 'lora7': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_de_no_sys', 'lora8': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/strategyqa_no_sys', 'lora9': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/tracking_shuffled_objects_no_sys', 'lora10': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_es_no_sys', 'lora11': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/goal_step_wikihow_no_sys', 'lora12': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/abstract_narrative_understanding_no_sys', 'lora13': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cnn_dailymail_no_sys', 'lora14': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/news_commentary_it_no_sys', 'lora15': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/winowhy_no_sys', 'lora16': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/reasoning_about_colored_objects_no_sys', 'lora17': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/gsm8k_no_sys', 'lora18': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/vitaminc_fact_verification_no_sys', 'lora19': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/object_counting_no_sys', 'lora20': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/unit_conversion_no_sys', 'lora21': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/disfl_qa_no_sys', 'lora22': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/topical_chat_no_sys', 'lora23': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/contextual_parametric_knowledge_conflicts_no_sys', 'lora24': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/question_selection_no_sys', 'lora25': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/cs_algorithms_no_sys', 'lora26': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/linguistics_puzzles_no_sys', 'lora27': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/elementary_math_qa_no_sys', 'lora28': '/data0/ljy/workspace/LLaMA-Factory/ckpt/llama3_8b_fuze27_no_sys/alpaca_no_sys'}
Load adapter, lora2
Load adapter, lora2
Load adapter, lora3
Load adapter, lora3
Load adapter, lora1
Load adapter, lora2
Load adapter, lora4
Load adapter, lora4
Load adapter, lora3
Load adapter, lora5
Load adapter, lora4
Load adapter, lora5
Load adapter, lora5
Load adapter, lora6
Load adapter, lora6
Load adapter, lora6
Load adapter, lora7
Load adapter, lora7
Load adapter, lora7
Load adapter, lora8
Load adapter, lora8
Load adapter, lora8
Load adapter, lora1
Load adapter, lora9
Load adapter, lora9
Load adapter, lora9
Load adapter, lora2
Load adapter, lora3
Load adapter, lora10
Load adapter, lora10
Load adapter, lora4
Load adapter, lora10
Load adapter, lora5
Load adapter, lora11
Load adapter, lora11
Load adapter, lora6
Load adapter, lora11
Load adapter, lora7
Load adapter, lora12
Load adapter, lora12
Load adapter, lora12
Load adapter, lora8
Load adapter, lora13
Load adapter, lora13
Load adapter, lora9
Load adapter, lora13
Load adapter, lora10
Load adapter, lora14
Load adapter, lora14
Load adapter, lora14
Load adapter, lora11
Load adapter, lora15
Load adapter, lora15
Load adapter, lora15
Load adapter, lora12
Load adapter, lora16
Load adapter, lora16
Load adapter, lora13
Load adapter, lora16
Load adapter, lora14
Load adapter, lora17
Load adapter, lora17
Load adapter, lora17
Load adapter, lora15
Load adapter, lora18
Load adapter, lora18
Load adapter, lora18
Load adapter, lora16
Load adapter, lora19
Load adapter, lora19
Load adapter, lora19
Load adapter, lora17
Load adapter, lora20
Load adapter, lora20
Load adapter, lora20
Load adapter, lora18
Load adapter, lora21
Load adapter, lora21
Load adapter, lora21
Load adapter, lora19
Load adapter, lora22
Load adapter, lora22
Load adapter, lora22
Load adapter, lora20
Load adapter, lora23
Load adapter, lora23
Load adapter, lora23
Load adapter, lora21
Load adapter, lora24
Load adapter, lora24
Load adapter, lora22
Load adapter, lora24
Load adapter, lora23
Load adapter, lora25
Load adapter, lora25
Load adapter, lora25
Load adapter, lora24
Load adapter, lora26
Load adapter, lora26
Load adapter, lora26
Load adapter, lora25
Load adapter, lora27
Load adapter, lora27
Load adapter, lora27
Load adapter, lora26
Load adapter, lora28
Load adapter, lora28
Load adapter, lora28
Load adapter, lora27
adapter model loaded PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=7e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun13_09-33-36_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
max_steps is given, it will override any value given in num_train_epochs
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Load adapter, lora28
adapter model loaded PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=7e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun13_09-33-36_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
max_steps is given, it will override any value given in num_train_epochs
adapter model loaded PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=7e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun13_09-33-36_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
max_steps is given, it will override any value given in num_train_epochs
adapter model loaded PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaMeteorForCausalLM(
      (model): LlamaMeteorModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaMeteorDecoderLayer(
            (self_attn): LlamaMeteorFlashAttention2(
              (q_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=1024, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaMeteorRotaryEmbedding()
            )
            (mlp): LlamaMeteorMLP(
              (gate_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.MoELinear(
                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
                (moe_gate): Linear(in_features=4096, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=4096, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=14336, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.MoELinear(
                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
                (moe_gate): Linear(in_features=14336, out_features=28, bias=False)
                (lora_dropout): ModuleDict(
                  (lora1): Dropout(p=0.1, inplace=False)
                  (lora2): Dropout(p=0.1, inplace=False)
                  (lora3): Dropout(p=0.1, inplace=False)
                  (lora4): Dropout(p=0.1, inplace=False)
                  (lora5): Dropout(p=0.1, inplace=False)
                  (lora6): Dropout(p=0.1, inplace=False)
                  (lora7): Dropout(p=0.1, inplace=False)
                  (lora8): Dropout(p=0.1, inplace=False)
                  (lora9): Dropout(p=0.1, inplace=False)
                  (lora10): Dropout(p=0.1, inplace=False)
                  (lora11): Dropout(p=0.1, inplace=False)
                  (lora12): Dropout(p=0.1, inplace=False)
                  (lora13): Dropout(p=0.1, inplace=False)
                  (lora14): Dropout(p=0.1, inplace=False)
                  (lora15): Dropout(p=0.1, inplace=False)
                  (lora16): Dropout(p=0.1, inplace=False)
                  (lora17): Dropout(p=0.1, inplace=False)
                  (lora18): Dropout(p=0.1, inplace=False)
                  (lora19): Dropout(p=0.1, inplace=False)
                  (lora20): Dropout(p=0.1, inplace=False)
                  (lora21): Dropout(p=0.1, inplace=False)
                  (lora22): Dropout(p=0.1, inplace=False)
                  (lora23): Dropout(p=0.1, inplace=False)
                  (lora24): Dropout(p=0.1, inplace=False)
                  (lora25): Dropout(p=0.1, inplace=False)
                  (lora26): Dropout(p=0.1, inplace=False)
                  (lora27): Dropout(p=0.1, inplace=False)
                  (lora28): Dropout(p=0.1, inplace=False)
                )
                (lora_A): Linear(in_features=14336, out_features=224, bias=False)
                (lora_B): Linear(in_features=224, out_features=4096, bias=False)
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaMeteorRMSNorm()
            (post_attention_layernorm): LlamaMeteorRMSNorm()
          )
        )
        (norm): LlamaMeteorRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
Training 622.1M parameters over 8.65B in total: 7.19%
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=7e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras/runs/Jun13_09-33-36_amax,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.3,
max_steps=5000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.PAGED_ADAMW,
optim_args=None,
optim_target_modules=None,
output_dir=/data2/xjw/llama-meteor-data/train_gate_and_loras,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=/data2/xjw/llama-meteor-data/train_gate_and_loras,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
max_steps is given, it will override any value given in num_train_epochs
wandb: Currently logged in as: jingwei-xu-nju (nju-ics). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /data2/xjw/llm_development/LLaMA-METEOR/wandb/run-20240613_093529-6bkieii5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-capybara-386
wandb: ⭐️ View project at https://wandb.ai/nju-ics/huggingface
wandb: 🚀 View run at https://wandb.ai/nju-ics/huggingface/runs/6bkieii5
  0%|          | 0/5000 [00:00<?, ?it/s]/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/5000 [00:30<43:00:34, 30.97s/it]                                                   {'loss': 2064.9663, 'grad_norm': 173.0, 'learning_rate': 2.7999999999999997e-07, 'epoch': 0.0}
  0%|          | 1/5000 [00:30<43:00:34, 30.97s/it]  0%|          | 2/5000 [00:50<33:40:54, 24.26s/it]                                                   {'loss': 2072.6665, 'grad_norm': 188.0, 'learning_rate': 5.599999999999999e-07, 'epoch': 0.0}
  0%|          | 2/5000 [00:50<33:40:54, 24.26s/it]  0%|          | 3/5000 [01:06<28:31:36, 20.55s/it]                                                   {'loss': 2059.4282, 'grad_norm': 179.0, 'learning_rate': 8.399999999999999e-07, 'epoch': 0.0}
  0%|          | 3/5000 [01:06<28:31:36, 20.55s/it]  0%|          | 4/5000 [01:30<30:31:57, 22.00s/it]                                                   {'loss': 2065.4731, 'grad_norm': 168.0, 'learning_rate': 1.1199999999999999e-06, 'epoch': 0.01}
  0%|          | 4/5000 [01:31<30:31:57, 22.00s/it]  0%|          | 5/5000 [01:45<26:59:40, 19.46s/it]                                                   {'loss': 2063.2661, 'grad_norm': 192.0, 'learning_rate': 1.4e-06, 'epoch': 0.01}
  0%|          | 5/5000 [01:45<26:59:40, 19.46s/it]  0%|          | 6/5000 [02:01<25:06:41, 18.10s/it]                                                   {'loss': 2065.7944, 'grad_norm': 171.0, 'learning_rate': 1.6799999999999998e-06, 'epoch': 0.01}
  0%|          | 6/5000 [02:01<25:06:41, 18.10s/it]  0%|          | 7/5000 [02:17<24:12:13, 17.45s/it]                                                   {'loss': 2070.751, 'grad_norm': 186.0, 'learning_rate': 1.96e-06, 'epoch': 0.01}
  0%|          | 7/5000 [02:17<24:12:13, 17.45s/it]  0%|          | 8/5000 [02:31<22:45:16, 16.41s/it]                                                   {'loss': 2058.353, 'grad_norm': 187.0, 'learning_rate': 2.2399999999999997e-06, 'epoch': 0.01}
  0%|          | 8/5000 [02:31<22:45:16, 16.41s/it]  0%|          | 9/5000 [02:51<24:27:41, 17.64s/it]                                                   {'loss': 2082.4341, 'grad_norm': 170.0, 'learning_rate': 2.5199999999999996e-06, 'epoch': 0.01}
  0%|          | 9/5000 [02:52<24:27:41, 17.64s/it]  0%|          | 10/5000 [03:07<23:42:04, 17.10s/it]                                                    {'loss': 2061.1602, 'grad_norm': 200.0, 'learning_rate': 2.8e-06, 'epoch': 0.01}
  0%|          | 10/5000 [03:07<23:42:04, 17.10s/it]  0%|          | 11/5000 [03:33<27:17:45, 19.70s/it]                                                    {'loss': 2063.8516, 'grad_norm': 182.0, 'learning_rate': 3.0799999999999997e-06, 'epoch': 0.01}
  0%|          | 11/5000 [03:33<27:17:45, 19.70s/it]  0%|          | 12/5000 [03:49<25:52:38, 18.68s/it]                                                    {'loss': 2060.0889, 'grad_norm': 180.0, 'learning_rate': 3.3599999999999996e-06, 'epoch': 0.02}
  0%|          | 12/5000 [03:49<25:52:38, 18.68s/it]  0%|          | 13/5000 [04:09<26:17:39, 18.98s/it]                                                    {'loss': 2060.2273, 'grad_norm': 186.0, 'learning_rate': 3.6399999999999995e-06, 'epoch': 0.02}
  0%|          | 13/5000 [04:09<26:17:39, 18.98s/it]  0%|          | 14/5000 [04:27<26:06:16, 18.85s/it]                                                    {'loss': 2061.4919, 'grad_norm': 174.0, 'learning_rate': 3.92e-06, 'epoch': 0.02}
  0%|          | 14/5000 [04:28<26:06:16, 18.85s/it]  0%|          | 15/5000 [04:45<25:39:15, 18.53s/it]                                                    {'loss': 2061.8552, 'grad_norm': 155.0, 'learning_rate': 4.2e-06, 'epoch': 0.02}
  0%|          | 15/5000 [04:45<25:39:15, 18.53s/it]  0%|          | 16/5000 [05:02<24:52:27, 17.97s/it]                                                    {'loss': 2055.9922, 'grad_norm': 208.0, 'learning_rate': 4.4799999999999995e-06, 'epoch': 0.02}
  0%|          | 16/5000 [05:02<24:52:27, 17.97s/it]  0%|          | 17/5000 [05:30<29:13:43, 21.12s/it]                                                    {'loss': 2082.6997, 'grad_norm': 175.0, 'learning_rate': 4.76e-06, 'epoch': 0.02}
  0%|          | 17/5000 [05:30<29:13:43, 21.12s/it]  0%|          | 18/5000 [05:47<27:24:18, 19.80s/it]                                                    {'loss': 2060.2251, 'grad_norm': 183.0, 'learning_rate': 5.039999999999999e-06, 'epoch': 0.02}
  0%|          | 18/5000 [05:47<27:24:18, 19.80s/it]  0%|          | 19/5000 [06:02<25:25:09, 18.37s/it]                                                    {'loss': 2046.8015, 'grad_norm': 187.0, 'learning_rate': 5.319999999999999e-06, 'epoch': 0.02}
  0%|          | 19/5000 [06:02<25:25:09, 18.37s/it]  0%|          | 20/5000 [06:18<24:18:00, 17.57s/it]                                                    {'loss': 2051.9443, 'grad_norm': 201.0, 'learning_rate': 5.6e-06, 'epoch': 0.03}
  0%|          | 20/5000 [06:18<24:18:00, 17.57s/it]  0%|          | 21/5000 [06:38<25:19:00, 18.30s/it]                                                    {'loss': 2066.4058, 'grad_norm': 262.0, 'learning_rate': 5.88e-06, 'epoch': 0.03}
  0%|          | 21/5000 [06:38<25:19:00, 18.30s/it]  0%|          | 22/5000 [07:09<30:36:55, 22.14s/it]                                                    {'loss': 2050.7129, 'grad_norm': 185.0, 'learning_rate': 6.1599999999999995e-06, 'epoch': 0.03}
  0%|          | 22/5000 [07:09<30:36:55, 22.14s/it]  0%|          | 23/5000 [07:25<27:59:45, 20.25s/it]                                                    {'loss': 2069.1499, 'grad_norm': 193.0, 'learning_rate': 6.439999999999999e-06, 'epoch': 0.03}
  0%|          | 23/5000 [07:25<27:59:45, 20.25s/it]  0%|          | 24/5000 [07:41<26:29:00, 19.16s/it]                                                    {'loss': 2058.7727, 'grad_norm': 178.0, 'learning_rate': 6.719999999999999e-06, 'epoch': 0.03}
  0%|          | 24/5000 [07:41<26:29:00, 19.16s/it]  0%|          | 25/5000 [07:55<24:06:22, 17.44s/it]                                                    {'loss': 2015.9058, 'grad_norm': 216.0, 'learning_rate': 7e-06, 'epoch': 0.03}
  0%|          | 25/5000 [07:55<24:06:22, 17.44s/it]  1%|          | 26/5000 [08:12<23:49:06, 17.24s/it]                                                    {'loss': 2053.0537, 'grad_norm': 218.0, 'learning_rate': 7.279999999999999e-06, 'epoch': 0.03}
  1%|          | 26/5000 [08:12<23:49:06, 17.24s/it]  1%|          | 27/5000 [08:43<29:46:37, 21.56s/it]                                                    {'loss': 2044.4222, 'grad_norm': 213.0, 'learning_rate': 7.56e-06, 'epoch': 0.03}
  1%|          | 27/5000 [08:43<29:46:37, 21.56s/it]  1%|          | 28/5000 [08:56<26:18:08, 19.04s/it]                                                    {'loss': 2030.5289, 'grad_norm': 197.0, 'learning_rate': 7.84e-06, 'epoch': 0.04}
  1%|          | 28/5000 [08:56<26:18:08, 19.04s/it]  1%|          | 29/5000 [09:23<29:32:26, 21.39s/it]                                                    {'loss': 2043.7222, 'grad_norm': 192.0, 'learning_rate': 8.12e-06, 'epoch': 0.04}
  1%|          | 29/5000 [09:23<29:32:26, 21.39s/it]  1%|          | 30/5000 [09:39<27:13:58, 19.73s/it]                                                    {'loss': 2039.5338, 'grad_norm': 196.0, 'learning_rate': 8.4e-06, 'epoch': 0.04}
  1%|          | 30/5000 [09:39<27:13:58, 19.73s/it]  1%|          | 31/5000 [10:01<28:18:09, 20.51s/it]                                                    {'loss': 2045.8578, 'grad_norm': 190.0, 'learning_rate': 8.68e-06, 'epoch': 0.04}
  1%|          | 31/5000 [10:01<28:18:09, 20.51s/it]  1%|          | 32/5000 [10:19<27:00:31, 19.57s/it]                                                    {'loss': 2019.2708, 'grad_norm': 212.0, 'learning_rate': 8.959999999999999e-06, 'epoch': 0.04}
  1%|          | 32/5000 [10:19<27:00:31, 19.57s/it]  1%|          | 33/5000 [10:42<28:24:31, 20.59s/it]                                                    {'loss': 2029.7705, 'grad_norm': 203.0, 'learning_rate': 9.24e-06, 'epoch': 0.04}
  1%|          | 33/5000 [10:42<28:24:31, 20.59s/it]  1%|          | 34/5000 [10:55<25:30:54, 18.50s/it]                                                    {'loss': 1991.5162, 'grad_norm': 282.0, 'learning_rate': 9.52e-06, 'epoch': 0.04}
  1%|          | 34/5000 [10:55<25:30:54, 18.50s/it]  1%|          | 35/5000 [11:15<26:03:37, 18.90s/it]                                                    {'loss': 2038.9978, 'grad_norm': 196.0, 'learning_rate': 9.8e-06, 'epoch': 0.04}
  1%|          | 35/5000 [11:15<26:03:37, 18.90s/it]  1%|          | 36/5000 [11:34<25:52:11, 18.76s/it]                                                    {'loss': 2034.8767, 'grad_norm': 208.0, 'learning_rate': 1.0079999999999998e-05, 'epoch': 0.05}
  1%|          | 36/5000 [11:34<25:52:11, 18.76s/it]  1%|          | 37/5000 [11:52<25:48:19, 18.72s/it]                                                    {'loss': 2023.6714, 'grad_norm': 227.0, 'learning_rate': 1.0359999999999999e-05, 'epoch': 0.05}
  1%|          | 37/5000 [11:52<25:48:19, 18.72s/it]  1%|          | 38/5000 [12:08<24:30:57, 17.79s/it]                                                    {'loss': 2013.2288, 'grad_norm': 221.0, 'learning_rate': 1.0639999999999998e-05, 'epoch': 0.05}
  1%|          | 38/5000 [12:08<24:30:57, 17.79s/it]  1%|          | 39/5000 [12:25<24:16:26, 17.61s/it]                                                    {'loss': 2024.2444, 'grad_norm': 209.0, 'learning_rate': 1.0919999999999999e-05, 'epoch': 0.05}
  1%|          | 39/5000 [12:25<24:16:26, 17.61s/it]  1%|          | 40/5000 [12:53<28:25:40, 20.63s/it]                                                    {'loss': 2020.4489, 'grad_norm': 253.0, 'learning_rate': 1.12e-05, 'epoch': 0.05}
  1%|          | 40/5000 [12:53<28:25:40, 20.63s/it]  1%|          | 41/5000 [13:08<26:17:05, 19.08s/it]                                                    {'loss': 2011.0104, 'grad_norm': 241.0, 'learning_rate': 1.148e-05, 'epoch': 0.05}
  1%|          | 41/5000 [13:08<26:17:05, 19.08s/it]  1%|          | 42/5000 [13:26<25:45:44, 18.71s/it]                                                    {'loss': 2006.8354, 'grad_norm': 296.0, 'learning_rate': 1.176e-05, 'epoch': 0.05}
  1%|          | 42/5000 [13:26<25:45:44, 18.71s/it]  1%|          | 43/5000 [13:52<28:35:15, 20.76s/it]                                                    {'loss': 2013.438, 'grad_norm': 239.0, 'learning_rate': 1.2039999999999998e-05, 'epoch': 0.05}
  1%|          | 43/5000 [13:52<28:35:15, 20.76s/it]  1%|          | 44/5000 [14:06<26:06:40, 18.97s/it]                                                    {'loss': 1975.0762, 'grad_norm': 334.0, 'learning_rate': 1.2319999999999999e-05, 'epoch': 0.06}
  1%|          | 44/5000 [14:06<26:06:40, 18.97s/it]  1%|          | 45/5000 [14:24<25:32:19, 18.55s/it]                                                    {'loss': 1981.9215, 'grad_norm': 310.0, 'learning_rate': 1.2599999999999998e-05, 'epoch': 0.06}
  1%|          | 45/5000 [14:24<25:32:19, 18.55s/it]  1%|          | 46/5000 [14:40<24:36:48, 17.89s/it]                                                    {'loss': 1955.7921, 'grad_norm': 382.0, 'learning_rate': 1.2879999999999999e-05, 'epoch': 0.06}
  1%|          | 46/5000 [14:40<24:36:48, 17.89s/it]  1%|          | 47/5000 [14:56<23:30:13, 17.08s/it]                                                    {'loss': 1948.8497, 'grad_norm': 376.0, 'learning_rate': 1.316e-05, 'epoch': 0.06}
  1%|          | 47/5000 [14:56<23:30:13, 17.08s/it]  1%|          | 48/5000 [15:10<22:11:14, 16.13s/it]                                                    {'loss': 1960.7919, 'grad_norm': 502.0, 'learning_rate': 1.3439999999999998e-05, 'epoch': 0.06}
  1%|          | 48/5000 [15:10<22:11:14, 16.13s/it]  1%|          | 49/5000 [15:26<22:09:27, 16.11s/it]                                                    {'loss': 1965.4037, 'grad_norm': 424.0, 'learning_rate': 1.3719999999999999e-05, 'epoch': 0.06}
  1%|          | 49/5000 [15:26<22:09:27, 16.11s/it]  1%|          | 50/5000 [15:42<22:26:47, 16.32s/it]                                                    {'loss': 1961.1692, 'grad_norm': 440.0, 'learning_rate': 1.4e-05, 'epoch': 0.06}
  1%|          | 50/5000 [15:42<22:26:47, 16.32s/it]  1%|          | 51/5000 [16:02<23:38:51, 17.20s/it]                                                    {'loss': 1968.5286, 'grad_norm': 600.0, 'learning_rate': 1.4279999999999997e-05, 'epoch': 0.06}
  1%|          | 51/5000 [16:02<23:38:51, 17.20s/it]  1%|          | 52/5000 [16:29<27:52:37, 20.28s/it]                                                    {'loss': 1964.1243, 'grad_norm': 524.0, 'learning_rate': 1.4559999999999998e-05, 'epoch': 0.07}
  1%|          | 52/5000 [16:29<27:52:37, 20.28s/it]  1%|          | 53/5000 [16:49<27:37:54, 20.11s/it]                                                    {'loss': 1946.9915, 'grad_norm': 592.0, 'learning_rate': 1.4839999999999999e-05, 'epoch': 0.07}
  1%|          | 53/5000 [16:49<27:37:54, 20.11s/it]  1%|          | 54/5000 [17:07<26:44:11, 19.46s/it]                                                    {'loss': 1925.3848, 'grad_norm': 600.0, 'learning_rate': 1.512e-05, 'epoch': 0.07}
  1%|          | 54/5000 [17:07<26:44:11, 19.46s/it]  1%|          | 55/5000 [17:21<24:43:57, 18.01s/it]                                                    {'loss': 1895.2622, 'grad_norm': 704.0, 'learning_rate': 1.5399999999999998e-05, 'epoch': 0.07}
  1%|          | 55/5000 [17:21<24:43:57, 18.01s/it]  1%|          | 56/5000 [17:36<23:28:38, 17.10s/it]                                                    {'loss': 1872.8528, 'grad_norm': 780.0, 'learning_rate': 1.568e-05, 'epoch': 0.07}
  1%|          | 56/5000 [17:36<23:28:38, 17.10s/it]  1%|          | 57/5000 [17:58<25:15:49, 18.40s/it]                                                    {'loss': 1876.5531, 'grad_norm': 740.0, 'learning_rate': 1.596e-05, 'epoch': 0.07}
  1%|          | 57/5000 [17:58<25:15:49, 18.40s/it]  1%|          | 58/5000 [18:17<25:30:45, 18.58s/it]                                                    {'loss': 1834.4709, 'grad_norm': 804.0, 'learning_rate': 1.624e-05, 'epoch': 0.07}
  1%|          | 58/5000 [18:17<25:30:45, 18.58s/it]  1%|          | 59/5000 [18:43<28:34:48, 20.82s/it]                                                    {'loss': 1875.4165, 'grad_norm': 828.0, 'learning_rate': 1.6519999999999998e-05, 'epoch': 0.07}
  1%|          | 59/5000 [18:43<28:34:48, 20.82s/it]  1%|          | 60/5000 [19:00<26:57:45, 19.65s/it]                                                    {'loss': 1846.1453, 'grad_norm': 828.0, 'learning_rate': 1.68e-05, 'epoch': 0.08}
  1%|          | 60/5000 [19:00<26:57:45, 19.65s/it]  1%|          | 61/5000 [19:13<24:07:46, 17.59s/it]                                                    {'loss': 1768.2996, 'grad_norm': 1056.0, 'learning_rate': 1.708e-05, 'epoch': 0.08}
  1%|          | 61/5000 [19:13<24:07:46, 17.59s/it]  1%|          | 62/5000 [19:29<23:46:58, 17.34s/it]                                                    {'loss': 1799.207, 'grad_norm': 868.0, 'learning_rate': 1.736e-05, 'epoch': 0.08}
  1%|          | 62/5000 [19:29<23:46:58, 17.34s/it]  1%|▏         | 63/5000 [19:55<27:24:37, 19.99s/it]                                                    {'loss': 1791.7529, 'grad_norm': 920.0, 'learning_rate': 1.7639999999999997e-05, 'epoch': 0.08}
  1%|▏         | 63/5000 [19:55<27:24:37, 19.99s/it]  1%|▏         | 64/5000 [20:21<29:34:32, 21.57s/it]                                                    {'loss': 1759.3813, 'grad_norm': 1192.0, 'learning_rate': 1.7919999999999998e-05, 'epoch': 0.08}
  1%|▏         | 64/5000 [20:21<29:34:32, 21.57s/it]  1%|▏         | 65/5000 [20:36<26:51:08, 19.59s/it]                                                    {'loss': 1706.6062, 'grad_norm': 1120.0, 'learning_rate': 1.82e-05, 'epoch': 0.08}
  1%|▏         | 65/5000 [20:36<26:51:08, 19.59s/it]  1%|▏         | 66/5000 [20:52<25:18:13, 18.46s/it]                                                    {'loss': 1749.0168, 'grad_norm': 1360.0, 'learning_rate': 1.848e-05, 'epoch': 0.08}
  1%|▏         | 66/5000 [20:52<25:18:13, 18.46s/it]  1%|▏         | 67/5000 [21:07<24:12:03, 17.66s/it]                                                    {'loss': 1696.9597, 'grad_norm': 1064.0, 'learning_rate': 1.876e-05, 'epoch': 0.09}
  1%|▏         | 67/5000 [21:07<24:12:03, 17.66s/it]  1%|▏         | 68/5000 [21:26<24:29:45, 17.88s/it]                                                    {'loss': 1689.5906, 'grad_norm': 1056.0, 'learning_rate': 1.904e-05, 'epoch': 0.09}
  1%|▏         | 68/5000 [21:26<24:29:45, 17.88s/it]  1%|▏         | 69/5000 [21:42<23:57:10, 17.49s/it]                                                    {'loss': 1669.2163, 'grad_norm': 1256.0, 'learning_rate': 1.932e-05, 'epoch': 0.09}
  1%|▏         | 69/5000 [21:42<23:57:10, 17.49s/it]  1%|▏         | 70/5000 [21:55<21:48:12, 15.92s/it]                                                    {'loss': 1566.0905, 'grad_norm': 1368.0, 'learning_rate': 1.96e-05, 'epoch': 0.09}
  1%|▏         | 70/5000 [21:55<21:48:12, 15.92s/it]  1%|▏         | 71/5000 [22:19<25:20:04, 18.50s/it]                                                    {'loss': 1607.7001, 'grad_norm': 1312.0, 'learning_rate': 1.9879999999999996e-05, 'epoch': 0.09}
  1%|▏         | 71/5000 [22:19<25:20:04, 18.50s/it]  1%|▏         | 72/5000 [22:33<23:31:32, 17.19s/it]                                                    {'loss': 1533.1296, 'grad_norm': 1264.0, 'learning_rate': 2.0159999999999997e-05, 'epoch': 0.09}
  1%|▏         | 72/5000 [22:33<23:31:32, 17.19s/it]  1%|▏         | 73/5000 [22:55<25:19:41, 18.51s/it]                                                    {'loss': 1514.0256, 'grad_norm': 1576.0, 'learning_rate': 2.0439999999999997e-05, 'epoch': 0.09}
  1%|▏         | 73/5000 [22:55<25:19:41, 18.51s/it]  1%|▏         | 74/5000 [23:11<24:29:27, 17.90s/it]                                                    {'loss': 1481.1228, 'grad_norm': 1264.0, 'learning_rate': 2.0719999999999998e-05, 'epoch': 0.09}
  1%|▏         | 74/5000 [23:11<24:29:27, 17.90s/it]  2%|▏         | 75/5000 [23:27<23:31:27, 17.20s/it]                                                    {'loss': 1443.3793, 'grad_norm': 1304.0, 'learning_rate': 2.1e-05, 'epoch': 0.1}
  2%|▏         | 75/5000 [23:27<23:31:27, 17.20s/it]  2%|▏         | 76/5000 [23:42<22:43:26, 16.61s/it]                                                    {'loss': 1402.1924, 'grad_norm': 1200.0, 'learning_rate': 2.1279999999999996e-05, 'epoch': 0.1}
  2%|▏         | 76/5000 [23:42<22:43:26, 16.61s/it]  2%|▏         | 77/5000 [23:59<22:58:36, 16.80s/it]                                                    {'loss': 1370.0994, 'grad_norm': 1064.0, 'learning_rate': 2.1559999999999997e-05, 'epoch': 0.1}
  2%|▏         | 77/5000 [23:59<22:58:36, 16.80s/it]  2%|▏         | 78/5000 [24:11<21:03:21, 15.40s/it]                                                    {'loss': 1328.145, 'grad_norm': 1192.0, 'learning_rate': 2.1839999999999998e-05, 'epoch': 0.1}
  2%|▏         | 78/5000 [24:11<21:03:21, 15.40s/it]  2%|▏         | 79/5000 [24:35<24:30:57, 17.93s/it]                                                    {'loss': 1340.5171, 'grad_norm': 1384.0, 'learning_rate': 2.2119999999999998e-05, 'epoch': 0.1}
  2%|▏         | 79/5000 [24:35<24:30:57, 17.93s/it]  2%|▏         | 80/5000 [24:50<23:02:02, 16.85s/it]                                                    {'loss': 1276.0386, 'grad_norm': 1224.0, 'learning_rate': 2.24e-05, 'epoch': 0.1}
  2%|▏         | 80/5000 [24:50<23:02:02, 16.85s/it]  2%|▏         | 81/5000 [25:07<23:20:08, 17.08s/it]                                                    {'loss': 1246.1432, 'grad_norm': 1152.0, 'learning_rate': 2.268e-05, 'epoch': 0.1}
  2%|▏         | 81/5000 [25:07<23:20:08, 17.08s/it]  2%|▏         | 82/5000 [25:20<21:40:29, 15.87s/it]                                                    {'loss': 1188.3263, 'grad_norm': 1152.0, 'learning_rate': 2.296e-05, 'epoch': 0.1}
  2%|▏         | 82/5000 [25:20<21:40:29, 15.87s/it]  2%|▏         | 83/5000 [25:37<22:05:11, 16.17s/it]                                                    {'loss': 1169.2104, 'grad_norm': 1112.0, 'learning_rate': 2.3239999999999998e-05, 'epoch': 0.11}
  2%|▏         | 83/5000 [25:37<22:05:11, 16.17s/it]  2%|▏         | 84/5000 [25:50<20:52:49, 15.29s/it]                                                    {'loss': 1147.7756, 'grad_norm': 1144.0, 'learning_rate': 2.352e-05, 'epoch': 0.11}
  2%|▏         | 84/5000 [25:50<20:52:49, 15.29s/it]  2%|▏         | 85/5000 [26:03<19:54:11, 14.58s/it]                                                    {'loss': 1100.2371, 'grad_norm': 1032.0, 'learning_rate': 2.38e-05, 'epoch': 0.11}
  2%|▏         | 85/5000 [26:03<19:54:11, 14.58s/it]  2%|▏         | 86/5000 [26:28<24:00:31, 17.59s/it]                                                    {'loss': 1093.4683, 'grad_norm': 1096.0, 'learning_rate': 2.4079999999999996e-05, 'epoch': 0.11}
  2%|▏         | 86/5000 [26:28<24:00:31, 17.59s/it]  2%|▏         | 87/5000 [26:44<23:33:58, 17.27s/it]                                                    {'loss': 1076.4242, 'grad_norm': 1024.0, 'learning_rate': 2.4359999999999997e-05, 'epoch': 0.11}
  2%|▏         | 87/5000 [26:44<23:33:58, 17.27s/it]  2%|▏         | 88/5000 [26:58<22:13:51, 16.29s/it]                                                    {'loss': 998.8192, 'grad_norm': 1200.0, 'learning_rate': 2.4639999999999998e-05, 'epoch': 0.11}
  2%|▏         | 88/5000 [26:58<22:13:51, 16.29s/it]  2%|▏         | 89/5000 [27:23<25:42:34, 18.85s/it]                                                    {'loss': 1029.8123, 'grad_norm': 1512.0, 'learning_rate': 2.4919999999999995e-05, 'epoch': 0.11}
  2%|▏         | 89/5000 [27:23<25:42:34, 18.85s/it]  2%|▏         | 90/5000 [27:36<23:03:24, 16.91s/it]                                                    {'loss': 978.9951, 'grad_norm': 1192.0, 'learning_rate': 2.5199999999999996e-05, 'epoch': 0.11}
  2%|▏         | 90/5000 [27:36<23:03:24, 16.91s/it]  2%|▏         | 91/5000 [27:50<21:52:21, 16.04s/it]                                                    {'loss': 941.7469, 'grad_norm': 1304.0, 'learning_rate': 2.5479999999999997e-05, 'epoch': 0.12}
  2%|▏         | 91/5000 [27:50<21:52:21, 16.04s/it]  2%|▏         | 92/5000 [28:05<21:26:37, 15.73s/it]                                                    {'loss': 937.0752, 'grad_norm': 1208.0, 'learning_rate': 2.5759999999999997e-05, 'epoch': 0.12}
  2%|▏         | 92/5000 [28:05<21:26:37, 15.73s/it]  2%|▏         | 93/5000 [28:16<19:39:17, 14.42s/it]                                                    {'loss': 943.1902, 'grad_norm': 1004.0, 'learning_rate': 2.6039999999999998e-05, 'epoch': 0.12}
  2%|▏         | 93/5000 [28:16<19:39:17, 14.42s/it]  2%|▏         | 94/5000 [28:33<20:51:53, 15.31s/it]                                                    {'loss': 901.1053, 'grad_norm': 944.0, 'learning_rate': 2.632e-05, 'epoch': 0.12}
  2%|▏         | 94/5000 [28:33<20:51:53, 15.31s/it]  2%|▏         | 95/5000 [28:55<23:34:27, 17.30s/it]                                                    {'loss': 865.6393, 'grad_norm': 1328.0, 'learning_rate': 2.66e-05, 'epoch': 0.12}
  2%|▏         | 95/5000 [28:55<23:34:27, 17.30s/it]  2%|▏         | 96/5000 [29:10<22:31:41, 16.54s/it]                                                    {'loss': 836.8407, 'grad_norm': 988.0, 'learning_rate': 2.6879999999999997e-05, 'epoch': 0.12}
  2%|▏         | 96/5000 [29:10<22:31:41, 16.54s/it]  2%|▏         | 97/5000 [29:24<21:21:17, 15.68s/it]                                                    {'loss': 810.5738, 'grad_norm': 2608.0, 'learning_rate': 2.7159999999999997e-05, 'epoch': 0.12}
  2%|▏         | 97/5000 [29:24<21:21:17, 15.68s/it]  2%|▏         | 98/5000 [29:36<20:07:51, 14.78s/it]                                                    {'loss': 868.6664, 'grad_norm': 1144.0, 'learning_rate': 2.7439999999999998e-05, 'epoch': 0.12}
  2%|▏         | 98/5000 [29:36<20:07:51, 14.78s/it]  2%|▏         | 99/5000 [30:02<24:42:55, 18.15s/it]                                                    {'loss': 783.507, 'grad_norm': 1064.0, 'learning_rate': 2.772e-05, 'epoch': 0.13}
  2%|▏         | 99/5000 [30:02<24:42:55, 18.15s/it]  2%|▏         | 100/5000 [30:15<22:32:55, 16.57s/it]                                                     {'loss': 793.8832, 'grad_norm': 1216.0, 'learning_rate': 2.8e-05, 'epoch': 0.13}
  2%|▏         | 100/5000 [30:15<22:32:55, 16.57s/it]  2%|▏         | 101/5000 [30:31<22:19:57, 16.41s/it]                                                     {'loss': 751.1975, 'grad_norm': 1256.0, 'learning_rate': 2.828e-05, 'epoch': 0.13}
  2%|▏         | 101/5000 [30:31<22:19:57, 16.41s/it]  2%|▏         | 102/5000 [30:44<20:44:21, 15.24s/it]                                                     {'loss': 755.6198, 'grad_norm': 1160.0, 'learning_rate': 2.8559999999999994e-05, 'epoch': 0.13}
  2%|▏         | 102/5000 [30:44<20:44:21, 15.24s/it]  2%|▏         | 103/5000 [31:08<24:19:46, 17.89s/it]                                                     {'loss': 716.3357, 'grad_norm': 1232.0, 'learning_rate': 2.8839999999999995e-05, 'epoch': 0.13}
  2%|▏         | 103/5000 [31:08<24:19:46, 17.89s/it]  2%|▏         | 104/5000 [31:24<23:37:10, 17.37s/it]                                                     {'loss': 725.0579, 'grad_norm': 1096.0, 'learning_rate': 2.9119999999999996e-05, 'epoch': 0.13}
  2%|▏         | 104/5000 [31:24<23:37:10, 17.37s/it]  2%|▏         | 105/5000 [31:37<21:38:53, 15.92s/it]                                                     {'loss': 702.0176, 'grad_norm': 1520.0, 'learning_rate': 2.9399999999999996e-05, 'epoch': 0.13}
  2%|▏         | 105/5000 [31:37<21:38:53, 15.92s/it]  2%|▏         | 106/5000 [31:49<20:08:05, 14.81s/it]                                                     {'loss': 709.8951, 'grad_norm': 1368.0, 'learning_rate': 2.9679999999999997e-05, 'epoch': 0.13}
  2%|▏         | 106/5000 [31:49<20:08:05, 14.81s/it]  2%|▏         | 107/5000 [32:01<19:02:34, 14.01s/it]                                                     {'loss': 696.1864, 'grad_norm': 1368.0, 'learning_rate': 2.9959999999999998e-05, 'epoch': 0.14}
  2%|▏         | 107/5000 [32:01<19:02:34, 14.01s/it]  2%|▏         | 108/5000 [32:17<19:57:54, 14.69s/it]                                                     {'loss': 658.7477, 'grad_norm': 1224.0, 'learning_rate': 3.024e-05, 'epoch': 0.14}
  2%|▏         | 108/5000 [32:17<19:57:54, 14.69s/it]  2%|▏         | 109/5000 [32:28<18:28:19, 13.60s/it]                                                     {'loss': 680.7177, 'grad_norm': 1072.0, 'learning_rate': 3.052e-05, 'epoch': 0.14}
  2%|▏         | 109/5000 [32:28<18:28:19, 13.60s/it]  2%|▏         | 110/5000 [32:46<19:59:20, 14.72s/it]                                                     {'loss': 625.742, 'grad_norm': 2528.0, 'learning_rate': 3.0799999999999996e-05, 'epoch': 0.14}
  2%|▏         | 110/5000 [32:46<19:59:20, 14.72s/it]  2%|▏         | 111/5000 [32:57<18:26:04, 13.57s/it]                                                     {'loss': 645.5282, 'grad_norm': 1232.0, 'learning_rate': 3.108e-05, 'epoch': 0.14}
  2%|▏         | 111/5000 [32:57<18:26:04, 13.57s/it]  2%|▏         | 112/5000 [33:10<18:10:28, 13.39s/it]                                                     {'loss': 649.9614, 'grad_norm': 1472.0, 'learning_rate': 3.136e-05, 'epoch': 0.14}
  2%|▏         | 112/5000 [33:10<18:10:28, 13.39s/it]  2%|▏         | 113/5000 [33:20<17:01:57, 12.55s/it]                                                     {'loss': 663.0889, 'grad_norm': 1232.0, 'learning_rate': 3.1639999999999995e-05, 'epoch': 0.14}
  2%|▏         | 113/5000 [33:20<17:01:57, 12.55s/it]  2%|▏         | 114/5000 [33:45<22:13:27, 16.37s/it]                                                     {'loss': 614.3536, 'grad_norm': 1624.0, 'learning_rate': 3.192e-05, 'epoch': 0.14}
  2%|▏         | 114/5000 [33:45<22:13:27, 16.37s/it]  2%|▏         | 115/5000 [34:08<24:37:22, 18.15s/it]                                                     {'loss': 592.3597, 'grad_norm': 1240.0, 'learning_rate': 3.22e-05, 'epoch': 0.15}
  2%|▏         | 115/5000 [34:08<24:37:22, 18.15s/it]  2%|▏         | 116/5000 [34:21<22:29:47, 16.58s/it]                                                     {'loss': 577.2347, 'grad_norm': 1056.0, 'learning_rate': 3.248e-05, 'epoch': 0.15}
  2%|▏         | 116/5000 [34:21<22:29:47, 16.58s/it]  2%|▏         | 117/5000 [34:43<24:49:33, 18.30s/it]                                                     {'loss': 576.7449, 'grad_norm': 1160.0, 'learning_rate': 3.276e-05, 'epoch': 0.15}
  2%|▏         | 117/5000 [34:43<24:49:33, 18.30s/it]  2%|▏         | 118/5000 [34:54<21:42:37, 16.01s/it]                                                     {'loss': 632.8169, 'grad_norm': 1160.0, 'learning_rate': 3.3039999999999995e-05, 'epoch': 0.15}
  2%|▏         | 118/5000 [34:54<21:42:37, 16.01s/it]  2%|▏         | 119/5000 [35:08<21:07:18, 15.58s/it]                                                     {'loss': 561.6897, 'grad_norm': 1464.0, 'learning_rate': 3.331999999999999e-05, 'epoch': 0.15}
  2%|▏         | 119/5000 [35:08<21:07:18, 15.58s/it]  2%|▏         | 120/5000 [35:30<23:44:30, 17.51s/it]                                                     {'loss': 587.3833, 'grad_norm': 1432.0, 'learning_rate': 3.36e-05, 'epoch': 0.15}
  2%|▏         | 120/5000 [35:30<23:44:30, 17.51s/it]  2%|▏         | 121/5000 [35:44<22:14:25, 16.41s/it]                                                     {'loss': 586.5173, 'grad_norm': 1664.0, 'learning_rate': 3.3879999999999994e-05, 'epoch': 0.15}
  2%|▏         | 121/5000 [35:44<22:14:25, 16.41s/it]  2%|▏         | 122/5000 [36:09<25:39:41, 18.94s/it]                                                     {'loss': 553.1946, 'grad_norm': 1288.0, 'learning_rate': 3.416e-05, 'epoch': 0.15}
  2%|▏         | 122/5000 [36:09<25:39:41, 18.94s/it]  2%|▏         | 123/5000 [36:31<26:54:42, 19.87s/it]                                                     {'loss': 559.6143, 'grad_norm': 1080.0, 'learning_rate': 3.4439999999999996e-05, 'epoch': 0.16}
  2%|▏         | 123/5000 [36:31<26:54:42, 19.87s/it]  2%|▏         | 124/5000 [36:44<24:10:02, 17.84s/it]                                                     {'loss': 553.2474, 'grad_norm': 1256.0, 'learning_rate': 3.472e-05, 'epoch': 0.16}
  2%|▏         | 124/5000 [36:44<24:10:02, 17.84s/it]  2%|▎         | 125/5000 [36:55<21:20:56, 15.77s/it]                                                     {'loss': 556.9438, 'grad_norm': 1032.0, 'learning_rate': 3.5e-05, 'epoch': 0.16}
  2%|▎         | 125/5000 [36:55<21:20:56, 15.77s/it]  3%|▎         | 126/5000 [37:09<20:29:58, 15.14s/it]                                                     {'loss': 507.3107, 'grad_norm': 1368.0, 'learning_rate': 3.5279999999999994e-05, 'epoch': 0.16}
  3%|▎         | 126/5000 [37:09<20:29:58, 15.14s/it]  3%|▎         | 127/5000 [37:26<21:20:58, 15.77s/it]                                                     {'loss': 510.1877, 'grad_norm': 1168.0, 'learning_rate': 3.556e-05, 'epoch': 0.16}
  3%|▎         | 127/5000 [37:26<21:20:58, 15.77s/it]  3%|▎         | 128/5000 [37:39<20:13:25, 14.94s/it]                                                     {'loss': 512.8249, 'grad_norm': 1784.0, 'learning_rate': 3.5839999999999996e-05, 'epoch': 0.16}
  3%|▎         | 128/5000 [37:39<20:13:25, 14.94s/it]  3%|▎         | 129/5000 [37:51<18:56:48, 14.00s/it]                                                     {'loss': 504.7279, 'grad_norm': 1384.0, 'learning_rate': 3.612e-05, 'epoch': 0.16}
  3%|▎         | 129/5000 [37:51<18:56:48, 14.00s/it]  3%|▎         | 130/5000 [38:05<18:55:39, 13.99s/it]                                                     {'loss': 514.8852, 'grad_norm': 1240.0, 'learning_rate': 3.64e-05, 'epoch': 0.17}
  3%|▎         | 130/5000 [38:05<18:55:39, 13.99s/it]  3%|▎         | 131/5000 [38:19<19:11:42, 14.19s/it]                                                     {'loss': 477.5776, 'grad_norm': 1544.0, 'learning_rate': 3.668e-05, 'epoch': 0.17}
  3%|▎         | 131/5000 [38:19<19:11:42, 14.19s/it]  3%|▎         | 132/5000 [38:33<18:54:02, 13.98s/it]                                                     {'loss': 476.3107, 'grad_norm': 1424.0, 'learning_rate': 3.696e-05, 'epoch': 0.17}
  3%|▎         | 132/5000 [38:33<18:54:02, 13.98s/it]  3%|▎         | 133/5000 [38:59<23:50:38, 17.64s/it]                                                     {'loss': 422.3112, 'grad_norm': 1440.0, 'learning_rate': 3.7239999999999996e-05, 'epoch': 0.17}
  3%|▎         | 133/5000 [38:59<23:50:38, 17.64s/it]  3%|▎         | 134/5000 [39:21<25:44:50, 19.05s/it]                                                     {'loss': 474.4868, 'grad_norm': 1616.0, 'learning_rate': 3.752e-05, 'epoch': 0.17}
  3%|▎         | 134/5000 [39:21<25:44:50, 19.05s/it]  3%|▎         | 135/5000 [39:34<23:12:58, 17.18s/it]                                                     {'loss': 506.329, 'grad_norm': 1816.0, 'learning_rate': 3.78e-05, 'epoch': 0.17}
  3%|▎         | 135/5000 [39:34<23:12:58, 17.18s/it]  3%|▎         | 136/5000 [39:46<20:53:31, 15.46s/it]                                                     {'loss': 509.3468, 'grad_norm': 1720.0, 'learning_rate': 3.808e-05, 'epoch': 0.17}
  3%|▎         | 136/5000 [39:46<20:53:31, 15.46s/it]  3%|▎         | 137/5000 [40:08<23:38:32, 17.50s/it]                                                     {'loss': 448.9369, 'grad_norm': 1744.0, 'learning_rate': 3.836e-05, 'epoch': 0.17}
  3%|▎         | 137/5000 [40:08<23:38:32, 17.50s/it]  3%|▎         | 138/5000 [40:19<21:09:42, 15.67s/it]                                                     {'loss': 471.4856, 'grad_norm': 1584.0, 'learning_rate': 3.864e-05, 'epoch': 0.18}
  3%|▎         | 138/5000 [40:19<21:09:42, 15.67s/it]  3%|▎         | 139/5000 [40:33<20:30:16, 15.19s/it]                                                     {'loss': 454.7537, 'grad_norm': 1520.0, 'learning_rate': 3.892e-05, 'epoch': 0.18}
  3%|▎         | 139/5000 [40:33<20:30:16, 15.19s/it]  3%|▎         | 140/5000 [40:48<20:14:03, 14.99s/it]                                                     {'loss': 437.8446, 'grad_norm': 1384.0, 'learning_rate': 3.92e-05, 'epoch': 0.18}
  3%|▎         | 140/5000 [40:48<20:14:03, 14.99s/it]  3%|▎         | 141/5000 [41:12<24:06:21, 17.86s/it]                                                     {'loss': 416.3396, 'grad_norm': 2016.0, 'learning_rate': 3.9479999999999995e-05, 'epoch': 0.18}
  3%|▎         | 141/5000 [41:12<24:06:21, 17.86s/it]  3%|▎         | 142/5000 [41:25<22:04:54, 16.36s/it]                                                     {'loss': 426.5018, 'grad_norm': 1976.0, 'learning_rate': 3.975999999999999e-05, 'epoch': 0.18}
  3%|▎         | 142/5000 [41:25<22:04:54, 16.36s/it]  3%|▎         | 143/5000 [41:36<19:46:53, 14.66s/it]                                                     {'loss': 433.2274, 'grad_norm': 2736.0, 'learning_rate': 4.0039999999999996e-05, 'epoch': 0.18}
  3%|▎         | 143/5000 [41:36<19:46:53, 14.66s/it]  3%|▎         | 144/5000 [41:54<21:09:19, 15.68s/it]                                                     {'loss': 416.2198, 'grad_norm': 2752.0, 'learning_rate': 4.0319999999999993e-05, 'epoch': 0.18}
  3%|▎         | 144/5000 [41:54<21:09:19, 15.68s/it]  3%|▎         | 145/5000 [42:17<24:10:13, 17.92s/it]                                                     {'loss': 384.2115, 'grad_norm': 1272.0, 'learning_rate': 4.059999999999999e-05, 'epoch': 0.18}
  3%|▎         | 145/5000 [42:17<24:10:13, 17.92s/it]  3%|▎         | 146/5000 [42:37<24:45:37, 18.36s/it]                                                     {'loss': 395.0938, 'grad_norm': 3488.0, 'learning_rate': 4.0879999999999995e-05, 'epoch': 0.19}
  3%|▎         | 146/5000 [42:37<24:45:37, 18.36s/it]  3%|▎         | 147/5000 [42:47<21:33:04, 15.99s/it]                                                     {'loss': 446.0762, 'grad_norm': 2688.0, 'learning_rate': 4.115999999999999e-05, 'epoch': 0.19}
  3%|▎         | 147/5000 [42:47<21:33:04, 15.99s/it]  3%|▎         | 148/5000 [43:01<20:31:42, 15.23s/it]                                                     {'loss': 398.3671, 'grad_norm': 2240.0, 'learning_rate': 4.1439999999999996e-05, 'epoch': 0.19}
  3%|▎         | 148/5000 [43:01<20:31:42, 15.23s/it]  3%|▎         | 149/5000 [43:14<19:37:51, 14.57s/it]                                                     {'loss': 419.0346, 'grad_norm': 1624.0, 'learning_rate': 4.1719999999999994e-05, 'epoch': 0.19}
  3%|▎         | 149/5000 [43:14<19:37:51, 14.57s/it]  3%|▎         | 150/5000 [43:29<19:52:57, 14.76s/it]                                                     {'loss': 382.0664, 'grad_norm': 1616.0, 'learning_rate': 4.2e-05, 'epoch': 0.19}
  3%|▎         | 150/5000 [43:29<19:52:57, 14.76s/it]  3%|▎         | 151/5000 [43:44<20:04:42, 14.91s/it]                                                     {'loss': 353.0563, 'grad_norm': 1664.0, 'learning_rate': 4.2279999999999995e-05, 'epoch': 0.19}
  3%|▎         | 151/5000 [43:44<20:04:42, 14.91s/it]  3%|▎         | 152/5000 [44:02<21:30:01, 15.97s/it]                                                     {'loss': 319.2178, 'grad_norm': 1832.0, 'learning_rate': 4.255999999999999e-05, 'epoch': 0.19}
  3%|▎         | 152/5000 [44:02<21:30:01, 15.97s/it]  3%|▎         | 153/5000 [44:19<21:43:53, 16.14s/it]                                                     {'loss': 327.8548, 'grad_norm': 1664.0, 'learning_rate': 4.2839999999999996e-05, 'epoch': 0.19}
  3%|▎         | 153/5000 [44:19<21:43:53, 16.14s/it]  3%|▎         | 154/5000 [44:40<23:43:28, 17.62s/it]                                                     {'loss': 315.2702, 'grad_norm': 1848.0, 'learning_rate': 4.3119999999999994e-05, 'epoch': 0.2}
  3%|▎         | 154/5000 [44:40<23:43:28, 17.62s/it]  3%|▎         | 155/5000 [44:54<22:10:46, 16.48s/it]                                                     {'loss': 362.2711, 'grad_norm': 1592.0, 'learning_rate': 4.34e-05, 'epoch': 0.2}
  3%|▎         | 155/5000 [44:54<22:10:46, 16.48s/it]  3%|▎         | 156/5000 [45:18<25:21:58, 18.85s/it]                                                     {'loss': 346.5941, 'grad_norm': 1376.0, 'learning_rate': 4.3679999999999995e-05, 'epoch': 0.2}
  3%|▎         | 156/5000 [45:18<25:21:58, 18.85s/it]  3%|▎         | 157/5000 [45:35<24:24:33, 18.14s/it]                                                     {'loss': 346.1389, 'grad_norm': 1208.0, 'learning_rate': 4.396e-05, 'epoch': 0.2}
  3%|▎         | 157/5000 [45:35<24:24:33, 18.14s/it]  3%|▎         | 158/5000 [45:47<21:53:31, 16.28s/it]                                                     {'loss': 398.6539, 'grad_norm': 1992.0, 'learning_rate': 4.4239999999999997e-05, 'epoch': 0.2}
  3%|▎         | 158/5000 [45:47<21:53:31, 16.28s/it]  3%|▎         | 159/5000 [46:01<21:07:28, 15.71s/it]                                                     {'loss': 368.0522, 'grad_norm': 1520.0, 'learning_rate': 4.4519999999999994e-05, 'epoch': 0.2}
  3%|▎         | 159/5000 [46:01<21:07:28, 15.71s/it]  3%|▎         | 160/5000 [46:18<21:35:36, 16.06s/it]                                                     {'loss': 357.0147, 'grad_norm': 1536.0, 'learning_rate': 4.48e-05, 'epoch': 0.2}
  3%|▎         | 160/5000 [46:18<21:35:36, 16.06s/it]  3%|▎         | 161/5000 [46:33<21:08:36, 15.73s/it]                                                     {'loss': 356.7666, 'grad_norm': 1408.0, 'learning_rate': 4.5079999999999995e-05, 'epoch': 0.2}
  3%|▎         | 161/5000 [46:33<21:08:36, 15.73s/it]  3%|▎         | 162/5000 [46:48<20:57:54, 15.60s/it]                                                     {'loss': 320.8968, 'grad_norm': 1584.0, 'learning_rate': 4.536e-05, 'epoch': 0.21}
  3%|▎         | 162/5000 [46:48<20:57:54, 15.60s/it]  3%|▎         | 163/5000 [47:05<21:23:39, 15.92s/it]                                                     {'loss': 308.4234, 'grad_norm': 1464.0, 'learning_rate': 4.564e-05, 'epoch': 0.21}
  3%|▎         | 163/5000 [47:05<21:23:39, 15.92s/it]  3%|▎         | 164/5000 [47:19<20:37:38, 15.36s/it]                                                     {'loss': 353.8794, 'grad_norm': 1712.0, 'learning_rate': 4.592e-05, 'epoch': 0.21}
  3%|▎         | 164/5000 [47:19<20:37:38, 15.36s/it]  3%|▎         | 165/5000 [47:30<19:02:31, 14.18s/it]                                                     {'loss': 339.5983, 'grad_norm': 1408.0, 'learning_rate': 4.62e-05, 'epoch': 0.21}
  3%|▎         | 165/5000 [47:30<19:02:31, 14.18s/it]  3%|▎         | 166/5000 [47:45<19:03:12, 14.19s/it]                                                     {'loss': 347.7383, 'grad_norm': 1272.0, 'learning_rate': 4.6479999999999995e-05, 'epoch': 0.21}
  3%|▎         | 166/5000 [47:45<19:03:12, 14.19s/it]  3%|▎         | 167/5000 [47:57<18:30:28, 13.79s/it]                                                     {'loss': 321.9062, 'grad_norm': 2096.0, 'learning_rate': 4.676e-05, 'epoch': 0.21}
  3%|▎         | 167/5000 [47:57<18:30:28, 13.79s/it]  3%|▎         | 168/5000 [48:15<19:55:54, 14.85s/it]                                                     {'loss': 296.0255, 'grad_norm': 1472.0, 'learning_rate': 4.704e-05, 'epoch': 0.21}
  3%|▎         | 168/5000 [48:15<19:55:54, 14.85s/it]  3%|▎         | 169/5000 [48:32<20:57:04, 15.61s/it]                                                     {'loss': 307.5474, 'grad_norm': 1240.0, 'learning_rate': 4.732e-05, 'epoch': 0.21}
  3%|▎         | 169/5000 [48:32<20:57:04, 15.61s/it]  3%|▎         | 170/5000 [48:48<21:05:47, 15.72s/it]                                                     {'loss': 296.8107, 'grad_norm': 1696.0, 'learning_rate': 4.76e-05, 'epoch': 0.22}
  3%|▎         | 170/5000 [48:48<21:05:47, 15.72s/it]  3%|▎         | 171/5000 [48:59<19:04:47, 14.22s/it]                                                     {'loss': 337.3852, 'grad_norm': 1544.0, 'learning_rate': 4.788e-05, 'epoch': 0.22}
  3%|▎         | 171/5000 [48:59<19:04:47, 14.22s/it]  3%|▎         | 172/5000 [49:13<19:12:07, 14.32s/it]                                                     {'loss': 298.8354, 'grad_norm': 1552.0, 'learning_rate': 4.815999999999999e-05, 'epoch': 0.22}
  3%|▎         | 172/5000 [49:13<19:12:07, 14.32s/it]  3%|▎         | 173/5000 [49:24<17:48:11, 13.28s/it]                                                     {'loss': 319.7263, 'grad_norm': 1736.0, 'learning_rate': 4.843999999999999e-05, 'epoch': 0.22}
  3%|▎         | 173/5000 [49:24<17:48:11, 13.28s/it]  3%|▎         | 174/5000 [49:36<17:11:01, 12.82s/it]                                                     {'loss': 321.4032, 'grad_norm': 1864.0, 'learning_rate': 4.8719999999999994e-05, 'epoch': 0.22}
  3%|▎         | 174/5000 [49:36<17:11:01, 12.82s/it]  4%|▎         | 175/5000 [49:52<18:34:31, 13.86s/it]                                                     {'loss': 274.9662, 'grad_norm': 1808.0, 'learning_rate': 4.899999999999999e-05, 'epoch': 0.22}
  4%|▎         | 175/5000 [49:52<18:34:31, 13.86s/it]  4%|▎         | 176/5000 [50:08<19:12:31, 14.33s/it]                                                     {'loss': 288.8738, 'grad_norm': 1504.0, 'learning_rate': 4.9279999999999996e-05, 'epoch': 0.22}
  4%|▎         | 176/5000 [50:08<19:12:31, 14.33s/it]  4%|▎         | 177/5000 [50:21<18:36:14, 13.89s/it]                                                     {'loss': 279.4634, 'grad_norm': 1384.0, 'learning_rate': 4.955999999999999e-05, 'epoch': 0.22}
  4%|▎         | 177/5000 [50:21<18:36:14, 13.89s/it]  4%|▎         | 178/5000 [50:32<17:46:44, 13.27s/it]                                                     {'loss': 382.5129, 'grad_norm': 5600.0, 'learning_rate': 4.983999999999999e-05, 'epoch': 0.23}
  4%|▎         | 178/5000 [50:32<17:46:44, 13.27s/it]  4%|▎         | 179/5000 [50:45<17:38:27, 13.17s/it]                                                     {'loss': 304.0134, 'grad_norm': 1296.0, 'learning_rate': 5.0119999999999994e-05, 'epoch': 0.23}
  4%|▎         | 179/5000 [50:45<17:38:27, 13.17s/it]  4%|▎         | 180/5000 [50:58<17:21:25, 12.96s/it]                                                     {'loss': 297.733, 'grad_norm': 2480.0, 'learning_rate': 5.039999999999999e-05, 'epoch': 0.23}
  4%|▎         | 180/5000 [50:58<17:21:25, 12.96s/it]  4%|▎         | 181/5000 [51:20<21:15:03, 15.88s/it]                                                     {'loss': 274.3181, 'grad_norm': 3088.0, 'learning_rate': 5.0679999999999996e-05, 'epoch': 0.23}
  4%|▎         | 181/5000 [51:20<21:15:03, 15.88s/it]  4%|▎         | 182/5000 [51:33<20:01:50, 14.97s/it]                                                     {'loss': 283.0466, 'grad_norm': 1064.0, 'learning_rate': 5.095999999999999e-05, 'epoch': 0.23}
  4%|▎         | 182/5000 [51:33<20:01:50, 14.97s/it]  4%|▎         | 183/5000 [51:55<22:53:48, 17.11s/it]                                                     {'loss': 275.9294, 'grad_norm': 1296.0, 'learning_rate': 5.124e-05, 'epoch': 0.23}
  4%|▎         | 183/5000 [51:55<22:53:48, 17.11s/it]  4%|▎         | 184/5000 [52:08<21:15:12, 15.89s/it]                                                     {'loss': 300.4029, 'grad_norm': 1176.0, 'learning_rate': 5.1519999999999995e-05, 'epoch': 0.23}
  4%|▎         | 184/5000 [52:08<21:15:12, 15.89s/it]  4%|▎         | 185/5000 [52:22<20:24:25, 15.26s/it]                                                     {'loss': 270.8998, 'grad_norm': 1456.0, 'learning_rate': 5.179999999999999e-05, 'epoch': 0.23}
  4%|▎         | 185/5000 [52:22<20:24:25, 15.26s/it]  4%|▎         | 186/5000 [52:36<19:54:36, 14.89s/it]                                                     {'loss': 274.5401, 'grad_norm': 1288.0, 'learning_rate': 5.2079999999999996e-05, 'epoch': 0.24}
  4%|▎         | 186/5000 [52:36<19:54:36, 14.89s/it]  4%|▎         | 187/5000 [52:59<22:51:58, 17.10s/it]                                                     {'loss': 244.1648, 'grad_norm': 1144.0, 'learning_rate': 5.235999999999999e-05, 'epoch': 0.24}
  4%|▎         | 187/5000 [52:59<22:51:58, 17.10s/it]  4%|▍         | 188/5000 [53:16<22:56:33, 17.16s/it]                                                     {'loss': 256.3302, 'grad_norm': 1192.0, 'learning_rate': 5.264e-05, 'epoch': 0.24}
  4%|▍         | 188/5000 [53:16<22:56:33, 17.16s/it]  4%|▍         | 189/5000 [53:30<21:37:39, 16.18s/it]                                                     {'loss': 267.8108, 'grad_norm': 1496.0, 'learning_rate': 5.2919999999999995e-05, 'epoch': 0.24}
  4%|▍         | 189/5000 [53:30<21:37:39, 16.18s/it]  4%|▍         | 190/5000 [53:41<19:43:00, 14.76s/it]                                                     {'loss': 259.964, 'grad_norm': 1144.0, 'learning_rate': 5.32e-05, 'epoch': 0.24}
  4%|▍         | 190/5000 [53:41<19:43:00, 14.76s/it]  4%|▍         | 191/5000 [53:55<19:16:15, 14.43s/it]                                                     {'loss': 279.0881, 'grad_norm': 1240.0, 'learning_rate': 5.3479999999999996e-05, 'epoch': 0.24}
  4%|▍         | 191/5000 [53:55<19:16:15, 14.43s/it]  4%|▍         | 192/5000 [54:08<18:55:29, 14.17s/it]                                                     {'loss': 260.0403, 'grad_norm': 1736.0, 'learning_rate': 5.3759999999999994e-05, 'epoch': 0.24}
  4%|▍         | 192/5000 [54:08<18:55:29, 14.17s/it]  4%|▍         | 193/5000 [54:27<20:34:45, 15.41s/it]                                                     {'loss': 255.789, 'grad_norm': 1248.0, 'learning_rate': 5.404e-05, 'epoch': 0.25}
  4%|▍         | 193/5000 [54:27<20:34:45, 15.41s/it]  4%|▍         | 194/5000 [54:41<20:18:32, 15.21s/it]                                                     {'loss': 264.8228, 'grad_norm': 1216.0, 'learning_rate': 5.4319999999999995e-05, 'epoch': 0.25}
  4%|▍         | 194/5000 [54:41<20:18:32, 15.21s/it]  4%|▍         | 195/5000 [55:03<22:59:50, 17.23s/it]                                                     {'loss': 250.5598, 'grad_norm': 988.0, 'learning_rate': 5.46e-05, 'epoch': 0.25}
  4%|▍         | 195/5000 [55:03<22:59:50, 17.23s/it]  4%|▍         | 196/5000 [55:19<22:11:59, 16.64s/it]                                                     {'loss': 213.9353, 'grad_norm': 1304.0, 'learning_rate': 5.4879999999999996e-05, 'epoch': 0.25}
  4%|▍         | 196/5000 [55:19<22:11:59, 16.64s/it]  4%|▍         | 197/5000 [55:34<21:32:47, 16.15s/it]                                                     {'loss': 269.9767, 'grad_norm': 1768.0, 'learning_rate': 5.516e-05, 'epoch': 0.25}
  4%|▍         | 197/5000 [55:34<21:32:47, 16.15s/it]  4%|▍         | 198/5000 [55:46<20:01:28, 15.01s/it]                                                     {'loss': 248.4091, 'grad_norm': 1752.0, 'learning_rate': 5.544e-05, 'epoch': 0.25}
  4%|▍         | 198/5000 [55:46<20:01:28, 15.01s/it]  4%|▍         | 199/5000 [56:00<19:26:13, 14.57s/it]                                                     {'loss': 232.1787, 'grad_norm': 1192.0, 'learning_rate': 5.5719999999999995e-05, 'epoch': 0.25}
  4%|▍         | 199/5000 [56:00<19:26:13, 14.57s/it]  4%|▍         | 200/5000 [56:13<18:51:01, 14.14s/it]                                                     {'loss': 247.8618, 'grad_norm': 1544.0, 'learning_rate': 5.6e-05, 'epoch': 0.25}
  4%|▍         | 200/5000 [56:13<18:51:01, 14.14s/it]  4%|▍         | 201/5000 [56:26<18:27:55, 13.85s/it]                                                     {'loss': 231.9415, 'grad_norm': 1160.0, 'learning_rate': 5.6279999999999996e-05, 'epoch': 0.26}
  4%|▍         | 201/5000 [56:26<18:27:55, 13.85s/it]  4%|▍         | 202/5000 [56:49<22:14:39, 16.69s/it]                                                     {'loss': 224.6574, 'grad_norm': 1208.0, 'learning_rate': 5.656e-05, 'epoch': 0.26}
  4%|▍         | 202/5000 [56:49<22:14:39, 16.69s/it]  4%|▍         | 203/5000 [57:01<20:07:08, 15.10s/it]                                                     {'loss': 267.9529, 'grad_norm': 1592.0, 'learning_rate': 5.684e-05, 'epoch': 0.26}
  4%|▍         | 203/5000 [57:01<20:07:08, 15.10s/it]  4%|▍         | 204/5000 [57:13<19:04:34, 14.32s/it]                                                     {'loss': 265.9461, 'grad_norm': 2000.0, 'learning_rate': 5.711999999999999e-05, 'epoch': 0.26}
  4%|▍         | 204/5000 [57:13<19:04:34, 14.32s/it]  4%|▍         | 205/5000 [57:35<22:04:42, 16.58s/it]                                                     {'loss': 255.7723, 'grad_norm': 1128.0, 'learning_rate': 5.739999999999999e-05, 'epoch': 0.26}
  4%|▍         | 205/5000 [57:35<22:04:42, 16.58s/it]  4%|▍         | 206/5000 [57:49<21:06:43, 15.85s/it]                                                     {'loss': 255.1676, 'grad_norm': 1488.0, 'learning_rate': 5.767999999999999e-05, 'epoch': 0.26}
  4%|▍         | 206/5000 [57:49<21:06:43, 15.85s/it]  4%|▍         | 207/5000 [58:22<28:02:00, 21.06s/it]                                                     {'loss': 214.3749, 'grad_norm': 1472.0, 'learning_rate': 5.7959999999999994e-05, 'epoch': 0.26}
  4%|▍         | 207/5000 [58:22<28:02:00, 21.06s/it]  4%|▍         | 208/5000 [58:36<25:00:23, 18.79s/it]                                                     {'loss': 252.2939, 'grad_norm': 1184.0, 'learning_rate': 5.823999999999999e-05, 'epoch': 0.26}
  4%|▍         | 208/5000 [58:36<25:00:23, 18.79s/it]  4%|▍         | 209/5000 [58:48<22:18:44, 16.77s/it]                                                     {'loss': 242.2139, 'grad_norm': 1952.0, 'learning_rate': 5.8519999999999995e-05, 'epoch': 0.27}
  4%|▍         | 209/5000 [58:48<22:18:44, 16.77s/it]  4%|▍         | 210/5000 [58:59<19:55:47, 14.98s/it]                                                     {'loss': 251.2558, 'grad_norm': 1088.0, 'learning_rate': 5.879999999999999e-05, 'epoch': 0.27}
  4%|▍         | 210/5000 [58:59<19:55:47, 14.98s/it]  4%|▍         | 211/5000 [59:23<23:49:01, 17.90s/it]                                                     {'loss': 202.8586, 'grad_norm': 1264.0, 'learning_rate': 5.907999999999999e-05, 'epoch': 0.27}
  4%|▍         | 211/5000 [59:23<23:49:01, 17.90s/it]  4%|▍         | 212/5000 [59:41<23:40:11, 17.80s/it]                                                     {'loss': 225.2074, 'grad_norm': 2720.0, 'learning_rate': 5.9359999999999994e-05, 'epoch': 0.27}
  4%|▍         | 212/5000 [59:41<23:40:11, 17.80s/it]  4%|▍         | 213/5000 [59:54<21:51:49, 16.44s/it]                                                     {'loss': 254.2299, 'grad_norm': 1912.0, 'learning_rate': 5.963999999999999e-05, 'epoch': 0.27}
  4%|▍         | 213/5000 [59:54<21:51:49, 16.44s/it]  4%|▍         | 214/5000 [1:00:08<20:50:19, 15.67s/it]                                                       {'loss': 234.5891, 'grad_norm': 1704.0, 'learning_rate': 5.9919999999999996e-05, 'epoch': 0.27}
  4%|▍         | 214/5000 [1:00:08<20:50:19, 15.67s/it]  4%|▍         | 215/5000 [1:00:20<19:24:45, 14.61s/it]                                                       {'loss': 248.6528, 'grad_norm': 1184.0, 'learning_rate': 6.019999999999999e-05, 'epoch': 0.27}
  4%|▍         | 215/5000 [1:00:20<19:24:45, 14.61s/it]  4%|▍         | 216/5000 [1:00:31<17:53:23, 13.46s/it]                                                       {'loss': 248.7675, 'grad_norm': 1560.0, 'learning_rate': 6.048e-05, 'epoch': 0.27}
  4%|▍         | 216/5000 [1:00:31<17:53:23, 13.46s/it]  4%|▍         | 217/5000 [1:00:55<22:15:05, 16.75s/it]                                                       {'loss': 219.6535, 'grad_norm': 1680.0, 'learning_rate': 6.0759999999999994e-05, 'epoch': 0.28}
  4%|▍         | 217/5000 [1:00:55<22:15:05, 16.75s/it]  4%|▍         | 218/5000 [1:01:09<21:00:15, 15.81s/it]                                                       {'loss': 228.175, 'grad_norm': 1264.0, 'learning_rate': 6.104e-05, 'epoch': 0.28}
  4%|▍         | 218/5000 [1:01:09<21:00:15, 15.81s/it]  4%|▍         | 219/5000 [1:01:26<21:33:02, 16.23s/it]                                                       {'loss': 245.4761, 'grad_norm': 1296.0, 'learning_rate': 6.131999999999999e-05, 'epoch': 0.28}
  4%|▍         | 219/5000 [1:01:26<21:33:02, 16.23s/it]  4%|▍         | 220/5000 [1:01:39<20:01:59, 15.09s/it]                                                       {'loss': 229.8573, 'grad_norm': 1240.0, 'learning_rate': 6.159999999999999e-05, 'epoch': 0.28}
  4%|▍         | 220/5000 [1:01:39<20:01:59, 15.09s/it]  4%|▍         | 221/5000 [1:01:52<19:27:02, 14.65s/it]                                                       {'loss': 216.8455, 'grad_norm': 1032.0, 'learning_rate': 6.188e-05, 'epoch': 0.28}
  4%|▍         | 221/5000 [1:01:52<19:27:02, 14.65s/it]  4%|▍         | 222/5000 [1:02:04<18:15:31, 13.76s/it]                                                       {'loss': 233.7455, 'grad_norm': 984.0, 'learning_rate': 6.216e-05, 'epoch': 0.28}
  4%|▍         | 222/5000 [1:02:04<18:15:31, 13.76s/it]  4%|▍         | 223/5000 [1:02:19<18:57:36, 14.29s/it]                                                       {'loss': 210.5659, 'grad_norm': 1448.0, 'learning_rate': 6.243999999999999e-05, 'epoch': 0.28}
  4%|▍         | 223/5000 [1:02:19<18:57:36, 14.29s/it]  4%|▍         | 224/5000 [1:02:33<18:27:43, 13.92s/it]                                                       {'loss': 224.1729, 'grad_norm': 1256.0, 'learning_rate': 6.272e-05, 'epoch': 0.28}
  4%|▍         | 224/5000 [1:02:33<18:27:43, 13.92s/it]  4%|▍         | 225/5000 [1:02:46<18:19:18, 13.81s/it]                                                       {'loss': 222.9987, 'grad_norm': 1344.0, 'learning_rate': 6.3e-05, 'epoch': 0.29}
  4%|▍         | 225/5000 [1:02:46<18:19:18, 13.81s/it]  5%|▍         | 226/5000 [1:03:03<19:22:24, 14.61s/it]                                                       {'loss': 213.9814, 'grad_norm': 1344.0, 'learning_rate': 6.327999999999999e-05, 'epoch': 0.29}
  5%|▍         | 226/5000 [1:03:03<19:22:24, 14.61s/it]  5%|▍         | 227/5000 [1:03:23<21:32:13, 16.24s/it]                                                       {'loss': 191.4521, 'grad_norm': 1384.0, 'learning_rate': 6.356e-05, 'epoch': 0.29}
  5%|▍         | 227/5000 [1:03:23<21:32:13, 16.24s/it]  5%|▍         | 228/5000 [1:03:38<21:00:24, 15.85s/it]                                                       {'loss': 221.912, 'grad_norm': 1512.0, 'learning_rate': 6.384e-05, 'epoch': 0.29}
  5%|▍         | 228/5000 [1:03:38<21:00:24, 15.85s/it]  5%|▍         | 229/5000 [1:03:54<21:08:31, 15.95s/it]                                                       {'loss': 220.8096, 'grad_norm': 1304.0, 'learning_rate': 6.412e-05, 'epoch': 0.29}
  5%|▍         | 229/5000 [1:03:54<21:08:31, 15.95s/it]  5%|▍         | 230/5000 [1:04:07<20:10:18, 15.22s/it]                                                       {'loss': 215.0849, 'grad_norm': 1296.0, 'learning_rate': 6.44e-05, 'epoch': 0.29}
  5%|▍         | 230/5000 [1:04:07<20:10:18, 15.22s/it]  5%|▍         | 231/5000 [1:04:22<20:02:07, 15.12s/it]                                                       {'loss': 242.7614, 'grad_norm': 1408.0, 'learning_rate': 6.468e-05, 'epoch': 0.29}
  5%|▍         | 231/5000 [1:04:22<20:02:07, 15.12s/it]  5%|▍         | 232/5000 [1:04:36<19:39:54, 14.85s/it]                                                       {'loss': 215.5135, 'grad_norm': 1232.0, 'learning_rate': 6.496e-05, 'epoch': 0.29}
  5%|▍         | 232/5000 [1:04:36<19:39:54, 14.85s/it]  5%|▍         | 233/5000 [1:04:57<22:06:32, 16.70s/it]                                                       {'loss': 209.9757, 'grad_norm': 2256.0, 'learning_rate': 6.523999999999999e-05, 'epoch': 0.3}
  5%|▍         | 233/5000 [1:04:57<22:06:32, 16.70s/it]  5%|▍         | 234/5000 [1:05:13<21:45:08, 16.43s/it]                                                       {'loss': 221.3182, 'grad_norm': 1400.0, 'learning_rate': 6.552e-05, 'epoch': 0.3}
  5%|▍         | 234/5000 [1:05:13<21:45:08, 16.43s/it]  5%|▍         | 235/5000 [1:05:27<20:51:04, 15.75s/it]                                                       {'loss': 191.2682, 'grad_norm': 1104.0, 'learning_rate': 6.579999999999999e-05, 'epoch': 0.3}
  5%|▍         | 235/5000 [1:05:27<20:51:04, 15.75s/it]  5%|▍         | 236/5000 [1:05:42<20:24:52, 15.43s/it]                                                       {'loss': 236.8587, 'grad_norm': 1440.0, 'learning_rate': 6.607999999999999e-05, 'epoch': 0.3}
  5%|▍         | 236/5000 [1:05:42<20:24:52, 15.43s/it]  5%|▍         | 237/5000 [1:05:57<20:02:50, 15.15s/it]                                                       {'loss': 217.8032, 'grad_norm': 1184.0, 'learning_rate': 6.636e-05, 'epoch': 0.3}
  5%|▍         | 237/5000 [1:05:57<20:02:50, 15.15s/it]  5%|▍         | 238/5000 [1:06:08<18:28:29, 13.97s/it]                                                       {'loss': 230.7921, 'grad_norm': 1016.0, 'learning_rate': 6.663999999999999e-05, 'epoch': 0.3}
  5%|▍         | 238/5000 [1:06:08<18:28:29, 13.97s/it]  5%|▍         | 239/5000 [1:06:36<24:07:38, 18.24s/it]                                                       {'loss': 167.9778, 'grad_norm': 888.0, 'learning_rate': 6.691999999999999e-05, 'epoch': 0.3}
  5%|▍         | 239/5000 [1:06:36<24:07:38, 18.24s/it]  5%|▍         | 240/5000 [1:06:52<23:06:04, 17.47s/it]                                                       {'loss': 201.6793, 'grad_norm': 1224.0, 'learning_rate': 6.72e-05, 'epoch': 0.3}
  5%|▍         | 240/5000 [1:06:52<23:06:04, 17.47s/it]  5%|▍         | 241/5000 [1:07:05<21:33:22, 16.31s/it]                                                       {'loss': 177.1256, 'grad_norm': 896.0, 'learning_rate': 6.748e-05, 'epoch': 0.31}
  5%|▍         | 241/5000 [1:07:05<21:33:22, 16.31s/it]  5%|▍         | 242/5000 [1:07:18<20:15:38, 15.33s/it]                                                       {'loss': 225.3124, 'grad_norm': 1616.0, 'learning_rate': 6.775999999999999e-05, 'epoch': 0.31}
  5%|▍         | 242/5000 [1:07:18<20:15:38, 15.33s/it]  5%|▍         | 243/5000 [1:07:41<23:01:39, 17.43s/it]                                                       {'loss': 184.4118, 'grad_norm': 1176.0, 'learning_rate': 6.803999999999999e-05, 'epoch': 0.31}
  5%|▍         | 243/5000 [1:07:41<23:01:39, 17.43s/it]  5%|▍         | 244/5000 [1:07:58<22:54:20, 17.34s/it]                                                       {'loss': 196.9501, 'grad_norm': 1004.0, 'learning_rate': 6.832e-05, 'epoch': 0.31}
  5%|▍         | 244/5000 [1:07:58<22:54:20, 17.34s/it]  5%|▍         | 245/5000 [1:08:24<26:19:57, 19.94s/it]                                                       {'loss': 173.4827, 'grad_norm': 1264.0, 'learning_rate': 6.859999999999999e-05, 'epoch': 0.31}
  5%|▍         | 245/5000 [1:08:24<26:19:57, 19.94s/it]  5%|▍         | 246/5000 [1:08:35<22:59:15, 17.41s/it]                                                       {'loss': 213.1059, 'grad_norm': 1112.0, 'learning_rate': 6.887999999999999e-05, 'epoch': 0.31}
  5%|▍         | 246/5000 [1:08:35<22:59:15, 17.41s/it]  5%|▍         | 247/5000 [1:08:54<23:39:27, 17.92s/it]                                                       {'loss': 216.7493, 'grad_norm': 928.0, 'learning_rate': 6.916e-05, 'epoch': 0.31}
  5%|▍         | 247/5000 [1:08:54<23:39:27, 17.92s/it]  5%|▍         | 248/5000 [1:09:11<23:00:47, 17.43s/it]                                                       {'loss': 206.4283, 'grad_norm': 4128.0, 'learning_rate': 6.944e-05, 'epoch': 0.31}
  5%|▍         | 248/5000 [1:09:11<23:00:47, 17.43s/it]  5%|▍         | 249/5000 [1:09:23<21:11:24, 16.06s/it]                                                       {'loss': 203.0062, 'grad_norm': 1192.0, 'learning_rate': 6.971999999999999e-05, 'epoch': 0.32}
  5%|▍         | 249/5000 [1:09:23<21:11:24, 16.06s/it]  5%|▌         | 250/5000 [1:09:35<19:31:07, 14.79s/it]                                                       {'loss': 218.6264, 'grad_norm': 1536.0, 'learning_rate': 7e-05, 'epoch': 0.32}
  5%|▌         | 250/5000 [1:09:35<19:31:07, 14.79s/it]  5%|▌         | 251/5000 [1:09:51<20:02:02, 15.19s/it]                                                       {'loss': 202.8927, 'grad_norm': 1704.0, 'learning_rate': 6.999999234490545e-05, 'epoch': 0.32}
  5%|▌         | 251/5000 [1:09:51<20:02:02, 15.19s/it]  5%|▌         | 252/5000 [1:10:06<19:43:02, 14.95s/it]                                                       {'loss': 196.1525, 'grad_norm': 1888.0, 'learning_rate': 6.999996937962515e-05, 'epoch': 0.32}
  5%|▌         | 252/5000 [1:10:06<19:43:02, 14.95s/it]  5%|▌         | 253/5000 [1:10:18<18:33:28, 14.07s/it]                                                       {'loss': 192.1305, 'grad_norm': 1288.0, 'learning_rate': 6.999993110416915e-05, 'epoch': 0.32}
  5%|▌         | 253/5000 [1:10:18<18:33:28, 14.07s/it]  5%|▌         | 254/5000 [1:10:32<18:41:32, 14.18s/it]                                                       {'loss': 183.4702, 'grad_norm': 1184.0, 'learning_rate': 6.99998775185542e-05, 'epoch': 0.32}
  5%|▌         | 254/5000 [1:10:32<18:41:32, 14.18s/it]  5%|▌         | 255/5000 [1:10:48<19:20:55, 14.68s/it]                                                       {'loss': 215.86, 'grad_norm': 1488.0, 'learning_rate': 6.999980862280374e-05, 'epoch': 0.32}
  5%|▌         | 255/5000 [1:10:48<19:20:55, 14.68s/it]  5%|▌         | 256/5000 [1:11:05<20:16:22, 15.38s/it]                                                       {'loss': 161.918, 'grad_norm': 1232.0, 'learning_rate': 6.999972441694789e-05, 'epoch': 0.33}
  5%|▌         | 256/5000 [1:11:05<20:16:22, 15.38s/it]  5%|▌         | 257/5000 [1:11:20<20:06:50, 15.27s/it]                                                       {'loss': 183.5793, 'grad_norm': 1020.0, 'learning_rate': 6.999962490102351e-05, 'epoch': 0.33}
  5%|▌         | 257/5000 [1:11:20<20:06:50, 15.27s/it]  5%|▌         | 258/5000 [1:11:37<20:38:47, 15.67s/it]                                                       {'loss': 187.2587, 'grad_norm': 876.0, 'learning_rate': 6.999951007507409e-05, 'epoch': 0.33}
  5%|▌         | 258/5000 [1:11:37<20:38:47, 15.67s/it]  5%|▌         | 259/5000 [1:11:56<21:54:25, 16.63s/it]                                                       {'loss': 184.9952, 'grad_norm': 1120.0, 'learning_rate': 6.99993799391499e-05, 'epoch': 0.33}
  5%|▌         | 259/5000 [1:11:56<21:54:25, 16.63s/it]  5%|▌         | 260/5000 [1:12:08<20:02:02, 15.22s/it]                                                       {'loss': 174.8697, 'grad_norm': 1320.0, 'learning_rate': 6.999923449330786e-05, 'epoch': 0.33}
  5%|▌         | 260/5000 [1:12:08<20:02:02, 15.22s/it]  5%|▌         | 261/5000 [1:12:20<18:57:44, 14.40s/it]                                                       {'loss': 190.8737, 'grad_norm': 1528.0, 'learning_rate': 6.999907373761158e-05, 'epoch': 0.33}
  5%|▌         | 261/5000 [1:12:20<18:57:44, 14.40s/it]  5%|▌         | 262/5000 [1:12:34<18:45:22, 14.25s/it]                                                       {'loss': 208.2595, 'grad_norm': 1480.0, 'learning_rate': 6.999889767213137e-05, 'epoch': 0.33}
  5%|▌         | 262/5000 [1:12:34<18:45:22, 14.25s/it]  5%|▌         | 263/5000 [1:12:58<22:43:39, 17.27s/it]                                                       {'loss': 179.756, 'grad_norm': 1472.0, 'learning_rate': 6.999870629694427e-05, 'epoch': 0.33}
  5%|▌         | 263/5000 [1:12:58<22:43:39, 17.27s/it]  5%|▌         | 264/5000 [1:13:09<20:10:28, 15.34s/it]                                                       {'loss': 220.9713, 'grad_norm': 1256.0, 'learning_rate': 6.999849961213397e-05, 'epoch': 0.34}
  5%|▌         | 264/5000 [1:13:09<20:10:28, 15.34s/it]  5%|▌         | 265/5000 [1:13:21<19:01:09, 14.46s/it]                                                       {'loss': 202.9835, 'grad_norm': 1320.0, 'learning_rate': 6.999827761779091e-05, 'epoch': 0.34}
  5%|▌         | 265/5000 [1:13:22<19:01:09, 14.46s/it]  5%|▌         | 266/5000 [1:13:40<20:42:00, 15.74s/it]                                                       {'loss': 202.1708, 'grad_norm': 832.0, 'learning_rate': 6.99980403140122e-05, 'epoch': 0.34}
  5%|▌         | 266/5000 [1:13:40<20:42:00, 15.74s/it]  5%|▌         | 267/5000 [1:13:56<20:37:55, 15.69s/it]                                                       {'loss': 176.6991, 'grad_norm': 964.0, 'learning_rate': 6.99977877009016e-05, 'epoch': 0.34}
  5%|▌         | 267/5000 [1:13:56<20:37:55, 15.69s/it]  5%|▌         | 268/5000 [1:14:12<20:49:48, 15.85s/it]                                                       {'loss': 178.4498, 'grad_norm': 1168.0, 'learning_rate': 6.999751977856966e-05, 'epoch': 0.34}
  5%|▌         | 268/5000 [1:14:12<20:49:48, 15.85s/it]  5%|▌         | 269/5000 [1:14:23<18:53:44, 14.38s/it]                                                       {'loss': 207.5336, 'grad_norm': 1184.0, 'learning_rate': 6.999723654713357e-05, 'epoch': 0.34}
  5%|▌         | 269/5000 [1:14:23<18:53:44, 14.38s/it]  5%|▌         | 270/5000 [1:14:46<22:08:20, 16.85s/it]                                                       {'loss': 186.7451, 'grad_norm': 1104.0, 'learning_rate': 6.999693800671718e-05, 'epoch': 0.34}
  5%|▌         | 270/5000 [1:14:46<22:08:20, 16.85s/it]  5%|▌         | 271/5000 [1:14:58<20:22:37, 15.51s/it]                                                       {'loss': 176.6577, 'grad_norm': 2048.0, 'learning_rate': 6.999662415745113e-05, 'epoch': 0.34}
  5%|▌         | 271/5000 [1:14:58<20:22:37, 15.51s/it]  5%|▌         | 272/5000 [1:15:11<19:14:44, 14.65s/it]                                                       {'loss': 176.9308, 'grad_norm': 1216.0, 'learning_rate': 6.99962949994727e-05, 'epoch': 0.35}
  5%|▌         | 272/5000 [1:15:11<19:14:44, 14.65s/it]  5%|▌         | 273/5000 [1:15:26<19:21:50, 14.75s/it]                                                       {'loss': 193.2063, 'grad_norm': 1240.0, 'learning_rate': 6.999595053292587e-05, 'epoch': 0.35}
  5%|▌         | 273/5000 [1:15:26<19:21:50, 14.75s/it]  5%|▌         | 274/5000 [1:15:40<19:24:20, 14.78s/it]                                                       {'loss': 170.4941, 'grad_norm': 920.0, 'learning_rate': 6.99955907579613e-05, 'epoch': 0.35}
  5%|▌         | 274/5000 [1:15:40<19:24:20, 14.78s/it]  6%|▌         | 275/5000 [1:15:52<18:07:58, 13.82s/it]                                                       {'loss': 182.528, 'grad_norm': 996.0, 'learning_rate': 6.999521567473641e-05, 'epoch': 0.35}
  6%|▌         | 275/5000 [1:15:52<18:07:58, 13.82s/it]  6%|▌         | 276/5000 [1:16:05<17:57:03, 13.68s/it]                                                       {'loss': 166.319, 'grad_norm': 1012.0, 'learning_rate': 6.999482528341524e-05, 'epoch': 0.35}
  6%|▌         | 276/5000 [1:16:05<17:57:03, 13.68s/it]  6%|▌         | 277/5000 [1:16:38<25:23:23, 19.35s/it]                                                       {'loss': 152.3726, 'grad_norm': 1336.0, 'learning_rate': 6.999441958416858e-05, 'epoch': 0.35}
  6%|▌         | 277/5000 [1:16:38<25:23:23, 19.35s/it]  6%|▌         | 278/5000 [1:16:59<26:13:57, 20.00s/it]                                                       {'loss': 194.057, 'grad_norm': 1000.0, 'learning_rate': 6.999399857717388e-05, 'epoch': 0.35}
  6%|▌         | 278/5000 [1:16:59<26:13:57, 20.00s/it]  6%|▌         | 279/5000 [1:17:13<23:30:08, 17.92s/it]                                                       {'loss': 188.2684, 'grad_norm': 1408.0, 'learning_rate': 6.999356226261531e-05, 'epoch': 0.35}
  6%|▌         | 279/5000 [1:17:13<23:30:08, 17.92s/it]  6%|▌         | 280/5000 [1:17:30<23:11:25, 17.69s/it]                                                       {'loss': 169.0355, 'grad_norm': 1032.0, 'learning_rate': 6.999311064068374e-05, 'epoch': 0.36}
  6%|▌         | 280/5000 [1:17:30<23:11:25, 17.69s/it]  6%|▌         | 281/5000 [1:17:54<25:43:36, 19.63s/it]                                                       {'loss': 160.1732, 'grad_norm': 924.0, 'learning_rate': 6.999264371157671e-05, 'epoch': 0.36}
  6%|▌         | 281/5000 [1:17:54<25:43:36, 19.63s/it]  6%|▌         | 282/5000 [1:18:04<22:08:48, 16.90s/it]                                                       {'loss': 180.9239, 'grad_norm': 864.0, 'learning_rate': 6.999216147549848e-05, 'epoch': 0.36}
  6%|▌         | 282/5000 [1:18:04<22:08:48, 16.90s/it]  6%|▌         | 283/5000 [1:18:29<25:20:01, 19.33s/it]                                                       {'loss': 160.8013, 'grad_norm': 1056.0, 'learning_rate': 6.999166393265999e-05, 'epoch': 0.36}
  6%|▌         | 283/5000 [1:18:29<25:20:01, 19.33s/it]  6%|▌         | 284/5000 [1:18:42<22:32:43, 17.21s/it]                                                       {'loss': 166.4084, 'grad_norm': 1024.0, 'learning_rate': 6.999115108327888e-05, 'epoch': 0.36}
  6%|▌         | 284/5000 [1:18:42<22:32:43, 17.21s/it]  6%|▌         | 285/5000 [1:19:03<24:15:16, 18.52s/it]                                                       {'loss': 163.4375, 'grad_norm': 996.0, 'learning_rate': 6.99906229275795e-05, 'epoch': 0.36}
  6%|▌         | 285/5000 [1:19:03<24:15:16, 18.52s/it]  6%|▌         | 286/5000 [1:19:18<22:51:22, 17.46s/it]                                                       {'loss': 134.578, 'grad_norm': 640.0, 'learning_rate': 6.999007946579287e-05, 'epoch': 0.36}
  6%|▌         | 286/5000 [1:19:18<22:51:22, 17.46s/it]  6%|▌         | 287/5000 [1:19:34<22:05:17, 16.87s/it]                                                       {'loss': 162.5627, 'grad_norm': 892.0, 'learning_rate': 6.998952069815673e-05, 'epoch': 0.36}
  6%|▌         | 287/5000 [1:19:34<22:05:17, 16.87s/it]  6%|▌         | 288/5000 [1:19:51<22:16:25, 17.02s/it]                                                       {'loss': 115.6079, 'grad_norm': 672.0, 'learning_rate': 6.99889466249155e-05, 'epoch': 0.37}
  6%|▌         | 288/5000 [1:19:51<22:16:25, 17.02s/it]  6%|▌         | 289/5000 [1:20:09<22:29:01, 17.18s/it]                                                       {'loss': 145.2577, 'grad_norm': 924.0, 'learning_rate': 6.998835724632029e-05, 'epoch': 0.37}
  6%|▌         | 289/5000 [1:20:09<22:29:01, 17.18s/it]  6%|▌         | 290/5000 [1:20:25<22:01:29, 16.83s/it]                                                       {'loss': 172.7946, 'grad_norm': 1432.0, 'learning_rate': 6.998775256262892e-05, 'epoch': 0.37}
  6%|▌         | 290/5000 [1:20:25<22:01:29, 16.83s/it]  6%|▌         | 291/5000 [1:20:38<20:34:46, 15.73s/it]                                                       {'loss': 179.6613, 'grad_norm': 1264.0, 'learning_rate': 6.99871325741059e-05, 'epoch': 0.37}
  6%|▌         | 291/5000 [1:20:38<20:34:46, 15.73s/it]  6%|▌         | 292/5000 [1:20:53<20:29:57, 15.67s/it]                                                       {'loss': 144.7414, 'grad_norm': 884.0, 'learning_rate': 6.998649728102243e-05, 'epoch': 0.37}
  6%|▌         | 292/5000 [1:20:53<20:29:57, 15.67s/it]  6%|▌         | 293/5000 [1:21:08<19:59:54, 15.30s/it]                                                       {'loss': 139.9898, 'grad_norm': 848.0, 'learning_rate': 6.998584668365643e-05, 'epoch': 0.37}
  6%|▌         | 293/5000 [1:21:08<19:59:54, 15.30s/it]  6%|▌         | 294/5000 [1:21:27<21:33:29, 16.49s/it]                                                       {'loss': 119.6984, 'grad_norm': 1168.0, 'learning_rate': 6.998518078229247e-05, 'epoch': 0.37}
  6%|▌         | 294/5000 [1:21:27<21:33:29, 16.49s/it]  6%|▌         | 295/5000 [1:21:44<21:48:23, 16.69s/it]                                                       {'loss': 144.6281, 'grad_norm': 888.0, 'learning_rate': 6.998449957722185e-05, 'epoch': 0.37}
  6%|▌         | 295/5000 [1:21:44<21:48:23, 16.69s/it]  6%|▌         | 296/5000 [1:21:57<20:17:30, 15.53s/it]                                                       {'loss': 155.2199, 'grad_norm': 868.0, 'learning_rate': 6.998380306874252e-05, 'epoch': 0.38}
  6%|▌         | 296/5000 [1:21:57<20:17:30, 15.53s/it]  6%|▌         | 297/5000 [1:22:11<19:47:01, 15.14s/it]                                                       {'loss': 163.2863, 'grad_norm': 1072.0, 'learning_rate': 6.998309125715922e-05, 'epoch': 0.38}
  6%|▌         | 297/5000 [1:22:11<19:47:01, 15.14s/it]  6%|▌         | 298/5000 [1:22:23<18:30:57, 14.18s/it]                                                       {'loss': 172.1253, 'grad_norm': 2720.0, 'learning_rate': 6.998236414278326e-05, 'epoch': 0.38}
  6%|▌         | 298/5000 [1:22:23<18:30:57, 14.18s/it]  6%|▌         | 299/5000 [1:22:47<22:13:29, 17.02s/it]                                                       {'loss': 151.7574, 'grad_norm': 1224.0, 'learning_rate': 6.998162172593273e-05, 'epoch': 0.38}
  6%|▌         | 299/5000 [1:22:47<22:13:29, 17.02s/it]  6%|▌         | 300/5000 [1:23:02<21:18:21, 16.32s/it]                                                       {'loss': 207.5924, 'grad_norm': 3520.0, 'learning_rate': 6.998086400693241e-05, 'epoch': 0.38}
  6%|▌         | 300/5000 [1:23:02<21:18:21, 16.32s/it]  6%|▌         | 301/5000 [1:23:20<22:20:37, 17.12s/it]                                                       {'loss': 141.0392, 'grad_norm': 972.0, 'learning_rate': 6.998009098611371e-05, 'epoch': 0.38}
  6%|▌         | 301/5000 [1:23:21<22:20:37, 17.12s/it]  6%|▌         | 302/5000 [1:23:43<24:31:04, 18.79s/it]                                                       {'loss': 152.9285, 'grad_norm': 2032.0, 'learning_rate': 6.997930266381479e-05, 'epoch': 0.38}
  6%|▌         | 302/5000 [1:23:43<24:31:04, 18.79s/it]  6%|▌         | 303/5000 [1:24:09<27:15:14, 20.89s/it]                                                       {'loss': 156.1482, 'grad_norm': 1784.0, 'learning_rate': 6.997849904038051e-05, 'epoch': 0.38}
  6%|▌         | 303/5000 [1:24:09<27:15:14, 20.89s/it]  6%|▌         | 304/5000 [1:24:31<27:32:23, 21.11s/it]                                                       {'loss': 143.6722, 'grad_norm': 2192.0, 'learning_rate': 6.997768011616237e-05, 'epoch': 0.39}
  6%|▌         | 304/5000 [1:24:31<27:32:23, 21.11s/it]  6%|▌         | 305/5000 [1:24:46<25:06:38, 19.25s/it]                                                       {'loss': 164.2178, 'grad_norm': 1624.0, 'learning_rate': 6.997684589151861e-05, 'epoch': 0.39}
  6%|▌         | 305/5000 [1:24:46<25:06:38, 19.25s/it]  6%|▌         | 306/5000 [1:25:00<23:17:17, 17.86s/it]                                                       {'loss': 175.1166, 'grad_norm': 1584.0, 'learning_rate': 6.997599636681416e-05, 'epoch': 0.39}
  6%|▌         | 306/5000 [1:25:00<23:17:17, 17.86s/it]  6%|▌         | 307/5000 [1:25:13<21:27:11, 16.46s/it]                                                       {'loss': 166.0168, 'grad_norm': 1904.0, 'learning_rate': 6.997513154242061e-05, 'epoch': 0.39}
  6%|▌         | 307/5000 [1:25:13<21:27:11, 16.46s/it]  6%|▌         | 308/5000 [1:25:41<25:38:58, 19.68s/it]                                                       {'loss': 111.4814, 'grad_norm': 1120.0, 'learning_rate': 6.997425141871628e-05, 'epoch': 0.39}
  6%|▌         | 308/5000 [1:25:41<25:38:58, 19.68s/it]  6%|▌         | 309/5000 [1:25:53<22:46:52, 17.48s/it]                                                       {'loss': 175.3673, 'grad_norm': 1840.0, 'learning_rate': 6.997335599608617e-05, 'epoch': 0.39}
  6%|▌         | 309/5000 [1:25:53<22:46:52, 17.48s/it]  6%|▌         | 310/5000 [1:26:07<21:33:49, 16.55s/it]                                                       {'loss': 134.5825, 'grad_norm': 1176.0, 'learning_rate': 6.997244527492196e-05, 'epoch': 0.39}
  6%|▌         | 310/5000 [1:26:07<21:33:49, 16.55s/it]  6%|▌         | 311/5000 [1:26:22<20:56:01, 16.07s/it]                                                       {'loss': 98.221, 'grad_norm': 836.0, 'learning_rate': 6.997151925562199e-05, 'epoch': 0.39}
  6%|▌         | 311/5000 [1:26:22<20:56:01, 16.07s/it]  6%|▌         | 312/5000 [1:26:35<19:45:24, 15.17s/it]                                                       {'loss': 174.9106, 'grad_norm': 2160.0, 'learning_rate': 6.99705779385914e-05, 'epoch': 0.4}
  6%|▌         | 312/5000 [1:26:35<19:45:24, 15.17s/it]  6%|▋         | 313/5000 [1:26:55<21:42:45, 16.68s/it]                                                       {'loss': 131.1971, 'grad_norm': 892.0, 'learning_rate': 6.996962132424193e-05, 'epoch': 0.4}
  6%|▋         | 313/5000 [1:26:55<21:42:45, 16.68s/it]  6%|▋         | 314/5000 [1:27:10<20:49:41, 16.00s/it]                                                       {'loss': 140.8908, 'grad_norm': 892.0, 'learning_rate': 6.996864941299201e-05, 'epoch': 0.4}
  6%|▋         | 314/5000 [1:27:10<20:49:41, 16.00s/it]  6%|▋         | 315/5000 [1:27:26<20:45:57, 15.96s/it]                                                       {'loss': 132.8988, 'grad_norm': 768.0, 'learning_rate': 6.996766220526682e-05, 'epoch': 0.4}
  6%|▋         | 315/5000 [1:27:26<20:45:57, 15.96s/it]  6%|▋         | 316/5000 [1:27:49<23:28:05, 18.04s/it]                                                       {'loss': 153.1704, 'grad_norm': 1032.0, 'learning_rate': 6.996665970149818e-05, 'epoch': 0.4}
  6%|▋         | 316/5000 [1:27:49<23:28:05, 18.04s/it]  6%|▋         | 317/5000 [1:28:05<22:47:10, 17.52s/it]                                                       {'loss': 129.6353, 'grad_norm': 1104.0, 'learning_rate': 6.996564190212463e-05, 'epoch': 0.4}
  6%|▋         | 317/5000 [1:28:05<22:47:10, 17.52s/it]  6%|▋         | 318/5000 [1:28:25<23:45:01, 18.26s/it]                                                       {'loss': 108.714, 'grad_norm': 1184.0, 'learning_rate': 6.996460880759139e-05, 'epoch': 0.4}
  6%|▋         | 318/5000 [1:28:25<23:45:01, 18.26s/it]  6%|▋         | 319/5000 [1:28:38<21:46:24, 16.75s/it]                                                       {'loss': 143.3272, 'grad_norm': 1440.0, 'learning_rate': 6.996356041835036e-05, 'epoch': 0.41}
  6%|▋         | 319/5000 [1:28:38<21:46:24, 16.75s/it]  6%|▋         | 320/5000 [1:28:53<20:57:14, 16.12s/it]                                                       {'loss': 181.0604, 'grad_norm': 1776.0, 'learning_rate': 6.996249673486015e-05, 'epoch': 0.41}
  6%|▋         | 320/5000 [1:28:53<20:57:14, 16.12s/it]  6%|▋         | 321/5000 [1:29:08<20:34:54, 15.84s/it]                                                       {'loss': 132.2544, 'grad_norm': 1168.0, 'learning_rate': 6.996141775758604e-05, 'epoch': 0.41}
  6%|▋         | 321/5000 [1:29:08<20:34:54, 15.84s/it]  6%|▋         | 322/5000 [1:29:22<19:47:52, 15.24s/it]                                                       {'loss': 128.3412, 'grad_norm': 812.0, 'learning_rate': 6.996032348700003e-05, 'epoch': 0.41}
  6%|▋         | 322/5000 [1:29:22<19:47:52, 15.24s/it]  6%|▋         | 323/5000 [1:29:37<19:44:42, 15.20s/it]                                                       {'loss': 134.4985, 'grad_norm': 992.0, 'learning_rate': 6.995921392358075e-05, 'epoch': 0.41}
  6%|▋         | 323/5000 [1:29:37<19:44:42, 15.20s/it]  6%|▋         | 324/5000 [1:29:47<17:50:25, 13.74s/it]                                                       {'loss': 177.1308, 'grad_norm': 1656.0, 'learning_rate': 6.995808906781363e-05, 'epoch': 0.41}
  6%|▋         | 324/5000 [1:29:47<17:50:25, 13.74s/it]  6%|▋         | 325/5000 [1:30:01<18:01:09, 13.88s/it]                                                       {'loss': 152.4392, 'grad_norm': 3632.0, 'learning_rate': 6.995694892019065e-05, 'epoch': 0.41}
  6%|▋         | 325/5000 [1:30:01<18:01:09, 13.88s/it]  7%|▋         | 326/5000 [1:30:19<19:16:21, 14.84s/it]                                                       {'loss': 183.8861, 'grad_norm': 2384.0, 'learning_rate': 6.99557934812106e-05, 'epoch': 0.41}
  7%|▋         | 326/5000 [1:30:19<19:16:21, 14.84s/it]  7%|▋         | 327/5000 [1:30:29<17:28:59, 13.47s/it]                                                       {'loss': 196.2258, 'grad_norm': 1976.0, 'learning_rate': 6.99546227513789e-05, 'epoch': 0.42}
  7%|▋         | 327/5000 [1:30:29<17:28:59, 13.47s/it]  7%|▋         | 328/5000 [1:30:41<17:04:33, 13.16s/it]                                                       {'loss': 152.2052, 'grad_norm': 1200.0, 'learning_rate': 6.995343673120762e-05, 'epoch': 0.42}
  7%|▋         | 328/5000 [1:30:41<17:04:33, 13.16s/it]  7%|▋         | 329/5000 [1:30:57<18:07:31, 13.97s/it]                                                       {'loss': 119.7733, 'grad_norm': 916.0, 'learning_rate': 6.995223542121563e-05, 'epoch': 0.42}
  7%|▋         | 329/5000 [1:30:57<18:07:31, 13.97s/it]  7%|▋         | 330/5000 [1:31:16<20:05:51, 15.49s/it]                                                       {'loss': 119.9874, 'grad_norm': 948.0, 'learning_rate': 6.995101882192839e-05, 'epoch': 0.42}
  7%|▋         | 330/5000 [1:31:16<20:05:51, 15.49s/it]  7%|▋         | 331/5000 [1:31:38<22:41:32, 17.50s/it]                                                       {'loss': 117.2635, 'grad_norm': 1032.0, 'learning_rate': 6.994978693387808e-05, 'epoch': 0.42}
  7%|▋         | 331/5000 [1:31:38<22:41:32, 17.50s/it]  7%|▋         | 332/5000 [1:31:51<20:53:46, 16.12s/it]                                                       {'loss': 128.4693, 'grad_norm': 1960.0, 'learning_rate': 6.994853975760358e-05, 'epoch': 0.42}
  7%|▋         | 332/5000 [1:31:51<20:53:46, 16.12s/it]  7%|▋         | 333/5000 [1:32:02<18:56:42, 14.61s/it]                                                       {'loss': 218.004, 'grad_norm': 2608.0, 'learning_rate': 6.994727729365044e-05, 'epoch': 0.42}
  7%|▋         | 333/5000 [1:32:02<18:56:42, 14.61s/it]  7%|▋         | 334/5000 [1:32:15<18:05:34, 13.96s/it]                                                       {'loss': 197.4475, 'grad_norm': 2016.0, 'learning_rate': 6.99459995425709e-05, 'epoch': 0.42}
  7%|▋         | 334/5000 [1:32:15<18:05:34, 13.96s/it]  7%|▋         | 335/5000 [1:32:31<18:52:07, 14.56s/it]                                                       {'loss': 122.0317, 'grad_norm': 880.0, 'learning_rate': 6.994470650492392e-05, 'epoch': 0.43}
  7%|▋         | 335/5000 [1:32:31<18:52:07, 14.56s/it]  7%|▋         | 336/5000 [1:32:48<20:03:03, 15.48s/it]                                                       {'loss': 113.5679, 'grad_norm': 956.0, 'learning_rate': 6.994339818127509e-05, 'epoch': 0.43}
  7%|▋         | 336/5000 [1:32:48<20:03:03, 15.48s/it]  7%|▋         | 337/5000 [1:33:00<18:33:27, 14.33s/it]                                                       {'loss': 141.2393, 'grad_norm': 844.0, 'learning_rate': 6.994207457219673e-05, 'epoch': 0.43}
  7%|▋         | 337/5000 [1:33:00<18:33:27, 14.33s/it]  7%|▋         | 338/5000 [1:33:17<19:40:41, 15.20s/it]                                                       {'loss': 144.7049, 'grad_norm': 1096.0, 'learning_rate': 6.99407356782678e-05, 'epoch': 0.43}
  7%|▋         | 338/5000 [1:33:17<19:40:41, 15.20s/it]  7%|▋         | 339/5000 [1:33:28<17:55:38, 13.85s/it]                                                       {'loss': 115.9899, 'grad_norm': 772.0, 'learning_rate': 6.993938150007404e-05, 'epoch': 0.43}
  7%|▋         | 339/5000 [1:33:28<17:55:38, 13.85s/it]  7%|▋         | 340/5000 [1:33:45<19:09:16, 14.80s/it]                                                       {'loss': 111.5017, 'grad_norm': 1184.0, 'learning_rate': 6.993801203820775e-05, 'epoch': 0.43}
  7%|▋         | 340/5000 [1:33:45<19:09:16, 14.80s/it]  7%|▋         | 341/5000 [1:33:57<18:00:31, 13.92s/it]                                                       {'loss': 159.3804, 'grad_norm': 1136.0, 'learning_rate': 6.993662729326802e-05, 'epoch': 0.43}
  7%|▋         | 341/5000 [1:33:57<18:00:31, 13.92s/it]  7%|▋         | 342/5000 [1:34:13<18:59:23, 14.68s/it]                                                       {'loss': 111.8466, 'grad_norm': 716.0, 'learning_rate': 6.993522726586056e-05, 'epoch': 0.43}
  7%|▋         | 342/5000 [1:34:13<18:59:23, 14.68s/it]  7%|▋         | 343/5000 [1:34:38<22:58:12, 17.76s/it]                                                       {'loss': 111.9778, 'grad_norm': 664.0, 'learning_rate': 6.99338119565978e-05, 'epoch': 0.44}
  7%|▋         | 343/5000 [1:34:38<22:58:12, 17.76s/it]  7%|▋         | 344/5000 [1:34:56<22:58:26, 17.76s/it]                                                       {'loss': 82.2085, 'grad_norm': 476.0, 'learning_rate': 6.993238136609885e-05, 'epoch': 0.44}
  7%|▋         | 344/5000 [1:34:56<22:58:26, 17.76s/it]  7%|▋         | 345/5000 [1:35:11<22:00:04, 17.01s/it]                                                       {'loss': 94.6165, 'grad_norm': 466.0, 'learning_rate': 6.99309354949895e-05, 'epoch': 0.44}
  7%|▋         | 345/5000 [1:35:11<22:00:04, 17.01s/it]  7%|▋         | 346/5000 [1:35:25<20:37:24, 15.95s/it]                                                       {'loss': 148.9536, 'grad_norm': 864.0, 'learning_rate': 6.992947434390221e-05, 'epoch': 0.44}
  7%|▋         | 346/5000 [1:35:25<20:37:24, 15.95s/it]  7%|▋         | 347/5000 [1:35:48<23:25:01, 18.12s/it]                                                       {'loss': 94.7481, 'grad_norm': 488.0, 'learning_rate': 6.992799791347615e-05, 'epoch': 0.44}
  7%|▋         | 347/5000 [1:35:48<23:25:01, 18.12s/it]  7%|▋         | 348/5000 [1:36:01<21:39:15, 16.76s/it]                                                       {'loss': 95.7101, 'grad_norm': 430.0, 'learning_rate': 6.992650620435714e-05, 'epoch': 0.44}
  7%|▋         | 348/5000 [1:36:01<21:39:15, 16.76s/it]  7%|▋         | 349/5000 [1:36:15<20:14:45, 15.67s/it]                                                       {'loss': 110.5696, 'grad_norm': 478.0, 'learning_rate': 6.992499921719773e-05, 'epoch': 0.44}
  7%|▋         | 349/5000 [1:36:15<20:14:45, 15.67s/it]  7%|▋         | 350/5000 [1:36:27<18:47:44, 14.55s/it]                                                       {'loss': 124.6629, 'grad_norm': 964.0, 'learning_rate': 6.99234769526571e-05, 'epoch': 0.44}
  7%|▋         | 350/5000 [1:36:27<18:47:44, 14.55s/it]  7%|▋         | 351/5000 [1:36:40<18:20:57, 14.21s/it]                                                       {'loss': 124.4826, 'grad_norm': 1640.0, 'learning_rate': 6.992193941140116e-05, 'epoch': 0.45}
  7%|▋         | 351/5000 [1:36:40<18:20:57, 14.21s/it]  7%|▋         | 352/5000 [1:36:54<18:25:22, 14.27s/it]                                                       {'loss': 115.8661, 'grad_norm': 740.0, 'learning_rate': 6.99203865941025e-05, 'epoch': 0.45}
  7%|▋         | 352/5000 [1:36:54<18:25:22, 14.27s/it]  7%|▋         | 353/5000 [1:37:17<21:43:45, 16.83s/it]                                                       {'loss': 104.0074, 'grad_norm': 1608.0, 'learning_rate': 6.991881850144033e-05, 'epoch': 0.45}
  7%|▋         | 353/5000 [1:37:17<21:43:45, 16.83s/it]  7%|▋         | 354/5000 [1:37:29<19:44:22, 15.30s/it]                                                       {'loss': 177.2007, 'grad_norm': 1040.0, 'learning_rate': 6.991723513410063e-05, 'epoch': 0.45}
  7%|▋         | 354/5000 [1:37:29<19:44:22, 15.30s/it]  7%|▋         | 355/5000 [1:37:39<17:55:26, 13.89s/it]                                                       {'loss': 102.8869, 'grad_norm': 752.0, 'learning_rate': 6.991563649277599e-05, 'epoch': 0.45}
  7%|▋         | 355/5000 [1:37:39<17:55:26, 13.89s/it]  7%|▋         | 356/5000 [1:37:50<16:47:00, 13.01s/it]                                                       {'loss': 160.9098, 'grad_norm': 2832.0, 'learning_rate': 6.991402257816572e-05, 'epoch': 0.45}
  7%|▋         | 356/5000 [1:37:50<16:47:00, 13.01s/it]  7%|▋         | 357/5000 [1:38:03<16:43:19, 12.97s/it]                                                       {'loss': 119.7743, 'grad_norm': 824.0, 'learning_rate': 6.99123933909758e-05, 'epoch': 0.45}
  7%|▋         | 357/5000 [1:38:03<16:43:19, 12.97s/it]  7%|▋         | 358/5000 [1:38:14<15:58:26, 12.39s/it]                                                       {'loss': 97.0862, 'grad_norm': 592.0, 'learning_rate': 6.99107489319189e-05, 'epoch': 0.45}
  7%|▋         | 358/5000 [1:38:14<15:58:26, 12.39s/it]  7%|▋         | 359/5000 [1:38:26<15:44:13, 12.21s/it]                                                       {'loss': 134.9744, 'grad_norm': 816.0, 'learning_rate': 6.990908920171434e-05, 'epoch': 0.46}
  7%|▋         | 359/5000 [1:38:26<15:44:13, 12.21s/it]  7%|▋         | 360/5000 [1:38:43<17:30:35, 13.59s/it]                                                       {'loss': 74.3905, 'grad_norm': 560.0, 'learning_rate': 6.990741420108816e-05, 'epoch': 0.46}
  7%|▋         | 360/5000 [1:38:43<17:30:35, 13.59s/it]  7%|▋         | 361/5000 [1:38:59<18:34:56, 14.42s/it]                                                       {'loss': 99.8797, 'grad_norm': 462.0, 'learning_rate': 6.990572393077308e-05, 'epoch': 0.46}
  7%|▋         | 361/5000 [1:38:59<18:34:56, 14.42s/it]  7%|▋         | 362/5000 [1:39:13<18:07:39, 14.07s/it]                                                       {'loss': 95.1588, 'grad_norm': 1064.0, 'learning_rate': 6.990401839150844e-05, 'epoch': 0.46}
  7%|▋         | 362/5000 [1:39:13<18:07:39, 14.07s/it]  7%|▋         | 363/5000 [1:39:30<19:21:55, 15.03s/it]                                                       {'loss': 117.5183, 'grad_norm': 1744.0, 'learning_rate': 6.990229758404033e-05, 'epoch': 0.46}
  7%|▋         | 363/5000 [1:39:30<19:21:55, 15.03s/it]  7%|▋         | 364/5000 [1:39:42<18:05:18, 14.05s/it]                                                       {'loss': 122.1785, 'grad_norm': 680.0, 'learning_rate': 6.990056150912147e-05, 'epoch': 0.46}
  7%|▋         | 364/5000 [1:39:42<18:05:18, 14.05s/it]  7%|▋         | 365/5000 [1:39:58<19:03:44, 14.81s/it]                                                       {'loss': 87.853, 'grad_norm': 480.0, 'learning_rate': 6.989881016751131e-05, 'epoch': 0.46}
  7%|▋         | 365/5000 [1:39:58<19:03:44, 14.81s/it]  7%|▋         | 366/5000 [1:40:11<18:24:43, 14.30s/it]                                                       {'loss': 87.8339, 'grad_norm': 472.0, 'learning_rate': 6.989704355997592e-05, 'epoch': 0.46}
  7%|▋         | 366/5000 [1:40:11<18:24:43, 14.30s/it]  7%|▋         | 367/5000 [1:40:30<20:15:16, 15.74s/it]                                                       {'loss': 107.1108, 'grad_norm': 912.0, 'learning_rate': 6.989526168728807e-05, 'epoch': 0.47}
  7%|▋         | 367/5000 [1:40:30<20:15:16, 15.74s/it]  7%|▋         | 368/5000 [1:40:43<19:13:10, 14.94s/it]                                                       {'loss': 129.8439, 'grad_norm': 1544.0, 'learning_rate': 6.989346455022724e-05, 'epoch': 0.47}
  7%|▋         | 368/5000 [1:40:43<19:13:10, 14.94s/it]  7%|▋         | 369/5000 [1:40:56<18:21:31, 14.27s/it]                                                       {'loss': 120.117, 'grad_norm': 1192.0, 'learning_rate': 6.989165214957951e-05, 'epoch': 0.47}
  7%|▋         | 369/5000 [1:40:56<18:21:31, 14.27s/it]  7%|▋         | 370/5000 [1:41:09<17:45:55, 13.81s/it]                                                       {'loss': 93.3938, 'grad_norm': 1312.0, 'learning_rate': 6.988982448613774e-05, 'epoch': 0.47}
  7%|▋         | 370/5000 [1:41:09<17:45:55, 13.81s/it]  7%|▋         | 371/5000 [1:41:22<17:39:37, 13.73s/it]                                                       {'loss': 79.4411, 'grad_norm': 544.0, 'learning_rate': 6.988798156070139e-05, 'epoch': 0.47}
  7%|▋         | 371/5000 [1:41:22<17:39:37, 13.73s/it]  7%|▋         | 372/5000 [1:41:36<17:42:40, 13.78s/it]                                                       {'loss': 80.3765, 'grad_norm': 1600.0, 'learning_rate': 6.988612337407661e-05, 'epoch': 0.47}
  7%|▋         | 372/5000 [1:41:36<17:42:40, 13.78s/it]  7%|▋         | 373/5000 [1:41:48<16:44:08, 13.02s/it]                                                       {'loss': 98.6398, 'grad_norm': 568.0, 'learning_rate': 6.988424992707622e-05, 'epoch': 0.47}
  7%|▋         | 373/5000 [1:41:48<16:44:08, 13.02s/it]  7%|▋         | 374/5000 [1:41:59<16:10:30, 12.59s/it]                                                       {'loss': 97.1255, 'grad_norm': 644.0, 'learning_rate': 6.988236122051978e-05, 'epoch': 0.47}
  7%|▋         | 374/5000 [1:41:59<16:10:30, 12.59s/it]  8%|▊         | 375/5000 [1:42:11<15:42:04, 12.22s/it]                                                       {'loss': 122.8083, 'grad_norm': 3120.0, 'learning_rate': 6.988045725523343e-05, 'epoch': 0.48}
  8%|▊         | 375/5000 [1:42:11<15:42:04, 12.22s/it]  8%|▊         | 376/5000 [1:42:31<18:52:35, 14.70s/it]                                                       {'loss': 68.8137, 'grad_norm': 454.0, 'learning_rate': 6.987853803205005e-05, 'epoch': 0.48}
  8%|▊         | 376/5000 [1:42:31<18:52:35, 14.70s/it]  8%|▊         | 377/5000 [1:42:45<18:38:11, 14.51s/it]                                                       {'loss': 74.9639, 'grad_norm': 430.0, 'learning_rate': 6.987660355180916e-05, 'epoch': 0.48}
  8%|▊         | 377/5000 [1:42:45<18:38:11, 14.51s/it]  8%|▊         | 378/5000 [1:43:00<18:42:20, 14.57s/it]                                                       {'loss': 124.7796, 'grad_norm': 3648.0, 'learning_rate': 6.987465381535699e-05, 'epoch': 0.48}
  8%|▊         | 378/5000 [1:43:00<18:42:20, 14.57s/it]  8%|▊         | 379/5000 [1:43:15<18:49:15, 14.66s/it]                                                       {'loss': 100.9458, 'grad_norm': 1496.0, 'learning_rate': 6.987268882354639e-05, 'epoch': 0.48}
  8%|▊         | 379/5000 [1:43:15<18:49:15, 14.66s/it]  8%|▊         | 380/5000 [1:43:25<17:20:07, 13.51s/it]                                                       {'loss': 127.4846, 'grad_norm': 1392.0, 'learning_rate': 6.987070857723693e-05, 'epoch': 0.48}
  8%|▊         | 380/5000 [1:43:25<17:20:07, 13.51s/it]  8%|▊         | 381/5000 [1:43:40<17:34:01, 13.69s/it]                                                       {'loss': 83.7254, 'grad_norm': 444.0, 'learning_rate': 6.986871307729485e-05, 'epoch': 0.48}
  8%|▊         | 381/5000 [1:43:40<17:34:01, 13.69s/it]  8%|▊         | 382/5000 [1:43:52<17:13:56, 13.43s/it]                                                       {'loss': 102.795, 'grad_norm': 716.0, 'learning_rate': 6.986670232459301e-05, 'epoch': 0.49}
  8%|▊         | 382/5000 [1:43:52<17:13:56, 13.43s/it]  8%|▊         | 383/5000 [1:44:07<17:34:44, 13.71s/it]                                                       {'loss': 92.8337, 'grad_norm': 640.0, 'learning_rate': 6.986467632001103e-05, 'epoch': 0.49}
  8%|▊         | 383/5000 [1:44:07<17:34:44, 13.71s/it]  8%|▊         | 384/5000 [1:44:18<16:47:52, 13.10s/it]                                                       {'loss': 118.7278, 'grad_norm': 1368.0, 'learning_rate': 6.986263506443512e-05, 'epoch': 0.49}
  8%|▊         | 384/5000 [1:44:18<16:47:52, 13.10s/it]  8%|▊         | 385/5000 [1:44:35<18:09:14, 14.16s/it]                                                       {'loss': 99.7397, 'grad_norm': 640.0, 'learning_rate': 6.986057855875822e-05, 'epoch': 0.49}
  8%|▊         | 385/5000 [1:44:35<18:09:14, 14.16s/it]  8%|▊         | 386/5000 [1:44:51<18:42:06, 14.59s/it]                                                       {'loss': 102.3878, 'grad_norm': 1656.0, 'learning_rate': 6.985850680387988e-05, 'epoch': 0.49}
  8%|▊         | 386/5000 [1:44:51<18:42:06, 14.59s/it]  8%|▊         | 387/5000 [1:45:04<18:18:45, 14.29s/it]                                                       {'loss': 120.8213, 'grad_norm': 980.0, 'learning_rate': 6.985641980070638e-05, 'epoch': 0.49}
  8%|▊         | 387/5000 [1:45:04<18:18:45, 14.29s/it]  8%|▊         | 388/5000 [1:45:29<22:11:02, 17.32s/it]                                                       {'loss': 146.7237, 'grad_norm': 1304.0, 'learning_rate': 6.985431755015066e-05, 'epoch': 0.49}
  8%|▊         | 388/5000 [1:45:29<22:11:02, 17.32s/it]  8%|▊         | 389/5000 [1:45:42<20:46:51, 16.22s/it]                                                       {'loss': 98.1096, 'grad_norm': 820.0, 'learning_rate': 6.985220005313228e-05, 'epoch': 0.49}
  8%|▊         | 389/5000 [1:45:42<20:46:51, 16.22s/it]  8%|▊         | 390/5000 [1:45:58<20:25:48, 15.95s/it]                                                       {'loss': 108.2157, 'grad_norm': 1392.0, 'learning_rate': 6.985006731057753e-05, 'epoch': 0.5}
  8%|▊         | 390/5000 [1:45:58<20:25:48, 15.95s/it]  8%|▊         | 391/5000 [1:46:19<22:34:58, 17.64s/it]                                                       {'loss': 89.1585, 'grad_norm': 616.0, 'learning_rate': 6.984791932341934e-05, 'epoch': 0.5}
  8%|▊         | 391/5000 [1:46:19<22:34:58, 17.64s/it]  8%|▊         | 392/5000 [1:46:31<20:25:55, 15.96s/it]                                                       {'loss': 123.4209, 'grad_norm': 756.0, 'learning_rate': 6.98457560925973e-05, 'epoch': 0.5}
  8%|▊         | 392/5000 [1:46:31<20:25:55, 15.96s/it]  8%|▊         | 393/5000 [1:47:00<25:26:10, 19.88s/it]                                                       {'loss': 164.3383, 'grad_norm': 10432.0, 'learning_rate': 6.98435776190577e-05, 'epoch': 0.5}
  8%|▊         | 393/5000 [1:47:00<25:26:10, 19.88s/it]  8%|▊         | 394/5000 [1:47:13<22:32:31, 17.62s/it]                                                       {'loss': 125.8583, 'grad_norm': 9280.0, 'learning_rate': 6.984138390375347e-05, 'epoch': 0.5}
  8%|▊         | 394/5000 [1:47:13<22:32:31, 17.62s/it]  8%|▊         | 395/5000 [1:47:29<22:13:34, 17.38s/it]                                                       {'loss': 148.7245, 'grad_norm': 1104.0, 'learning_rate': 6.983917494764421e-05, 'epoch': 0.5}
  8%|▊         | 395/5000 [1:47:29<22:13:34, 17.38s/it]  8%|▊         | 396/5000 [1:47:46<21:49:15, 17.06s/it]                                                       {'loss': 109.7103, 'grad_norm': 892.0, 'learning_rate': 6.98369507516962e-05, 'epoch': 0.5}
  8%|▊         | 396/5000 [1:47:46<21:49:15, 17.06s/it]  8%|▊         | 397/5000 [1:48:01<21:01:46, 16.45s/it]                                                       {'loss': 98.1257, 'grad_norm': 2304.0, 'learning_rate': 6.983471131688237e-05, 'epoch': 0.5}
  8%|▊         | 397/5000 [1:48:01<21:01:46, 16.45s/it]  8%|▊         | 398/5000 [1:48:14<19:49:57, 15.51s/it]                                                       {'loss': 77.9196, 'grad_norm': 652.0, 'learning_rate': 6.983245664418234e-05, 'epoch': 0.51}
  8%|▊         | 398/5000 [1:48:14<19:49:57, 15.51s/it]  8%|▊         | 399/5000 [1:48:28<19:20:39, 15.14s/it]                                                       {'loss': 115.7588, 'grad_norm': 1032.0, 'learning_rate': 6.983018673458237e-05, 'epoch': 0.51}
  8%|▊         | 399/5000 [1:48:28<19:20:39, 15.14s/it]  8%|▊         | 400/5000 [1:48:43<19:06:54, 14.96s/it]                                                       {'loss': 90.4186, 'grad_norm': 532.0, 'learning_rate': 6.982790158907539e-05, 'epoch': 0.51}
  8%|▊         | 400/5000 [1:48:43<19:06:54, 14.96s/it]  8%|▊         | 401/5000 [1:49:04<21:33:27, 16.87s/it]                                                       {'loss': 118.6624, 'grad_norm': 704.0, 'learning_rate': 6.982560120866101e-05, 'epoch': 0.51}
  8%|▊         | 401/5000 [1:49:04<21:33:27, 16.87s/it]  8%|▊         | 402/5000 [1:49:19<20:49:30, 16.31s/it]                                                       {'loss': 95.9345, 'grad_norm': 604.0, 'learning_rate': 6.982328559434551e-05, 'epoch': 0.51}
  8%|▊         | 402/5000 [1:49:19<20:49:30, 16.31s/it]  8%|▊         | 403/5000 [1:49:30<18:32:58, 14.53s/it]                                                       {'loss': 96.041, 'grad_norm': 324.0, 'learning_rate': 6.982095474714177e-05, 'epoch': 0.51}
  8%|▊         | 403/5000 [1:49:30<18:32:58, 14.53s/it]  8%|▊         | 404/5000 [1:49:49<20:19:31, 15.92s/it]                                                       {'loss': 66.6951, 'grad_norm': 260.0, 'learning_rate': 6.981860866806943e-05, 'epoch': 0.51}
  8%|▊         | 404/5000 [1:49:49<20:19:31, 15.92s/it]  8%|▊         | 405/5000 [1:50:00<18:19:51, 14.36s/it]                                                       {'loss': 87.4244, 'grad_norm': 864.0, 'learning_rate': 6.981624735815472e-05, 'epoch': 0.51}
  8%|▊         | 405/5000 [1:50:00<18:19:51, 14.36s/it]  8%|▊         | 406/5000 [1:50:16<19:13:20, 15.06s/it]                                                       {'loss': 68.8285, 'grad_norm': 390.0, 'learning_rate': 6.981387081843057e-05, 'epoch': 0.52}
  8%|▊         | 406/5000 [1:50:16<19:13:20, 15.06s/it]  8%|▊         | 407/5000 [1:50:32<19:22:25, 15.19s/it]                                                       {'loss': 59.616, 'grad_norm': 254.0, 'learning_rate': 6.981147904993655e-05, 'epoch': 0.52}
  8%|▊         | 407/5000 [1:50:32<19:22:25, 15.19s/it]  8%|▊         | 408/5000 [1:50:49<20:16:26, 15.89s/it]                                                       {'loss': 80.7891, 'grad_norm': 708.0, 'learning_rate': 6.98090720537189e-05, 'epoch': 0.52}
  8%|▊         | 408/5000 [1:50:49<20:16:26, 15.89s/it]  8%|▊         | 409/5000 [1:51:02<19:14:31, 15.09s/it]                                                       {'loss': 75.5965, 'grad_norm': 474.0, 'learning_rate': 6.980664983083052e-05, 'epoch': 0.52}
  8%|▊         | 409/5000 [1:51:02<19:14:31, 15.09s/it]  8%|▊         | 410/5000 [1:51:16<18:44:17, 14.70s/it]                                                       {'loss': 86.5707, 'grad_norm': 512.0, 'learning_rate': 6.9804212382331e-05, 'epoch': 0.52}
  8%|▊         | 410/5000 [1:51:16<18:44:17, 14.70s/it]  8%|▊         | 411/5000 [1:51:28<17:36:35, 13.81s/it]                                                       {'loss': 73.4291, 'grad_norm': 422.0, 'learning_rate': 6.980175970928653e-05, 'epoch': 0.52}
  8%|▊         | 411/5000 [1:51:28<17:36:35, 13.81s/it]  8%|▊         | 412/5000 [1:51:43<17:59:06, 14.11s/it]                                                       {'loss': 79.9959, 'grad_norm': 776.0, 'learning_rate': 6.979929181277e-05, 'epoch': 0.52}
  8%|▊         | 412/5000 [1:51:43<17:59:06, 14.11s/it]  8%|▊         | 413/5000 [1:51:58<18:14:13, 14.31s/it]                                                       {'loss': 77.1847, 'grad_norm': 454.0, 'learning_rate': 6.979680869386095e-05, 'epoch': 0.52}
  8%|▊         | 413/5000 [1:51:58<18:14:13, 14.31s/it]  8%|▊         | 414/5000 [1:52:13<18:47:15, 14.75s/it]                                                       {'loss': 77.4572, 'grad_norm': 410.0, 'learning_rate': 6.97943103536456e-05, 'epoch': 0.53}
  8%|▊         | 414/5000 [1:52:13<18:47:15, 14.75s/it]  8%|▊         | 415/5000 [1:52:37<22:04:52, 17.34s/it]                                                       {'loss': 71.7798, 'grad_norm': 932.0, 'learning_rate': 6.97917967932168e-05, 'epoch': 0.53}
  8%|▊         | 415/5000 [1:52:37<22:04:52, 17.34s/it]  8%|▊         | 416/5000 [1:52:48<19:37:02, 15.41s/it]                                                       {'loss': 93.7961, 'grad_norm': 632.0, 'learning_rate': 6.978926801367404e-05, 'epoch': 0.53}
  8%|▊         | 416/5000 [1:52:48<19:37:02, 15.41s/it]  8%|▊         | 417/5000 [1:53:12<22:56:54, 18.03s/it]                                                       {'loss': 85.023, 'grad_norm': 680.0, 'learning_rate': 6.978672401612353e-05, 'epoch': 0.53}
  8%|▊         | 417/5000 [1:53:12<22:56:54, 18.03s/it]  8%|▊         | 418/5000 [1:53:31<23:31:51, 18.49s/it]                                                       {'loss': 63.5928, 'grad_norm': 296.0, 'learning_rate': 6.978416480167808e-05, 'epoch': 0.53}
  8%|▊         | 418/5000 [1:53:31<23:31:51, 18.49s/it]  8%|▊         | 419/5000 [1:53:50<23:45:22, 18.67s/it]                                                       {'loss': 88.851, 'grad_norm': 808.0, 'learning_rate': 6.978159037145718e-05, 'epoch': 0.53}
  8%|▊         | 419/5000 [1:53:50<23:45:22, 18.67s/it]  8%|▊         | 420/5000 [1:54:03<21:36:39, 16.99s/it]                                                       {'loss': 89.4254, 'grad_norm': 980.0, 'learning_rate': 6.977900072658699e-05, 'epoch': 0.53}
  8%|▊         | 420/5000 [1:54:03<21:36:39, 16.99s/it]  8%|▊         | 421/5000 [1:54:14<19:18:00, 15.17s/it]                                                       {'loss': 71.9975, 'grad_norm': 520.0, 'learning_rate': 6.977639586820028e-05, 'epoch': 0.53}
  8%|▊         | 421/5000 [1:54:14<19:18:00, 15.17s/it]  8%|▊         | 422/5000 [1:54:32<20:02:24, 15.76s/it]                                                       {'loss': 79.012, 'grad_norm': 676.0, 'learning_rate': 6.977377579743653e-05, 'epoch': 0.54}
  8%|▊         | 422/5000 [1:54:32<20:02:24, 15.76s/it]  8%|▊         | 423/5000 [1:54:46<19:42:57, 15.51s/it]                                                       {'loss': 85.6959, 'grad_norm': 1064.0, 'learning_rate': 6.977114051544185e-05, 'epoch': 0.54}
  8%|▊         | 423/5000 [1:54:46<19:42:57, 15.51s/it]  8%|▊         | 424/5000 [1:55:03<20:00:38, 15.74s/it]                                                       {'loss': 108.9411, 'grad_norm': 1264.0, 'learning_rate': 6.976849002336897e-05, 'epoch': 0.54}
  8%|▊         | 424/5000 [1:55:03<20:00:38, 15.74s/it]  8%|▊         | 425/5000 [1:55:24<22:16:00, 17.52s/it]                                                       {'loss': 113.1856, 'grad_norm': 1536.0, 'learning_rate': 6.976582432237733e-05, 'epoch': 0.54}
  8%|▊         | 425/5000 [1:55:24<22:16:00, 17.52s/it]  9%|▊         | 426/5000 [1:55:37<20:18:23, 15.98s/it]                                                       {'loss': 94.2243, 'grad_norm': 608.0, 'learning_rate': 6.9763143413633e-05, 'epoch': 0.54}
  9%|▊         | 426/5000 [1:55:37<20:18:23, 15.98s/it]  9%|▊         | 427/5000 [1:55:49<18:57:12, 14.92s/it]                                                       {'loss': 84.4392, 'grad_norm': 524.0, 'learning_rate': 6.976044729830867e-05, 'epoch': 0.54}
  9%|▊         | 427/5000 [1:55:49<18:57:12, 14.92s/it]  9%|▊         | 428/5000 [1:56:05<19:12:20, 15.12s/it]                                                       {'loss': 80.7252, 'grad_norm': 474.0, 'learning_rate': 6.975773597758377e-05, 'epoch': 0.54}
  9%|▊         | 428/5000 [1:56:05<19:12:20, 15.12s/it]  9%|▊         | 429/5000 [1:56:17<17:55:31, 14.12s/it]                                                       {'loss': 66.9215, 'grad_norm': 496.0, 'learning_rate': 6.975500945264427e-05, 'epoch': 0.54}
  9%|▊         | 429/5000 [1:56:17<17:55:31, 14.12s/it]  9%|▊         | 430/5000 [1:56:28<16:50:15, 13.26s/it]                                                       {'loss': 83.7634, 'grad_norm': 820.0, 'learning_rate': 6.975226772468286e-05, 'epoch': 0.55}
  9%|▊         | 430/5000 [1:56:28<16:50:15, 13.26s/it]  9%|▊         | 431/5000 [1:56:39<16:00:02, 12.61s/it]                                                       {'loss': 76.9557, 'grad_norm': 568.0, 'learning_rate': 6.974951079489888e-05, 'epoch': 0.55}
  9%|▊         | 431/5000 [1:56:39<16:00:02, 12.61s/it]  9%|▊         | 432/5000 [1:56:51<15:49:57, 12.48s/it]                                                       {'loss': 69.2368, 'grad_norm': 460.0, 'learning_rate': 6.974673866449828e-05, 'epoch': 0.55}
  9%|▊         | 432/5000 [1:56:51<15:49:57, 12.48s/it]  9%|▊         | 433/5000 [1:57:09<17:51:26, 14.08s/it]                                                       {'loss': 65.5199, 'grad_norm': 338.0, 'learning_rate': 6.97439513346937e-05, 'epoch': 0.55}
  9%|▊         | 433/5000 [1:57:09<17:51:26, 14.08s/it]  9%|▊         | 434/5000 [1:57:27<19:21:28, 15.26s/it]                                                       {'loss': 72.5831, 'grad_norm': 532.0, 'learning_rate': 6.974114880670442e-05, 'epoch': 0.55}
  9%|▊         | 434/5000 [1:57:27<19:21:28, 15.26s/it]  9%|▊         | 435/5000 [1:57:41<18:49:59, 14.85s/it]                                                       {'loss': 121.0738, 'grad_norm': 608.0, 'learning_rate': 6.973833108175636e-05, 'epoch': 0.55}
  9%|▊         | 435/5000 [1:57:41<18:49:59, 14.85s/it]  9%|▊         | 436/5000 [1:57:52<17:22:33, 13.71s/it]                                                       {'loss': 76.6644, 'grad_norm': 1560.0, 'learning_rate': 6.973549816108206e-05, 'epoch': 0.55}
  9%|▊         | 436/5000 [1:57:52<17:22:33, 13.71s/it]  9%|▊         | 437/5000 [1:58:04<16:40:26, 13.16s/it]                                                       {'loss': 85.7547, 'grad_norm': 5504.0, 'learning_rate': 6.973265004592078e-05, 'epoch': 0.55}
  9%|▊         | 437/5000 [1:58:04<16:40:26, 13.16s/it]  9%|▉         | 438/5000 [1:58:15<15:44:36, 12.42s/it]                                                       {'loss': 90.8637, 'grad_norm': 1096.0, 'learning_rate': 6.972978673751834e-05, 'epoch': 0.56}
  9%|▉         | 438/5000 [1:58:15<15:44:36, 12.42s/it]  9%|▉         | 439/5000 [1:58:26<15:31:20, 12.25s/it]                                                       {'loss': 69.0833, 'grad_norm': 412.0, 'learning_rate': 6.972690823712727e-05, 'epoch': 0.56}
  9%|▉         | 439/5000 [1:58:26<15:31:20, 12.25s/it]  9%|▉         | 440/5000 [1:58:42<16:56:25, 13.37s/it]                                                       {'loss': 72.5861, 'grad_norm': 510.0, 'learning_rate': 6.972401454600672e-05, 'epoch': 0.56}
  9%|▉         | 440/5000 [1:58:42<16:56:25, 13.37s/it]  9%|▉         | 441/5000 [1:58:58<17:41:25, 13.97s/it]                                                       {'loss': 57.7715, 'grad_norm': 396.0, 'learning_rate': 6.972110566542249e-05, 'epoch': 0.56}
  9%|▉         | 441/5000 [1:58:58<17:41:25, 13.97s/it]  9%|▉         | 442/5000 [1:59:11<17:28:49, 13.81s/it]                                                       {'loss': 73.237, 'grad_norm': 474.0, 'learning_rate': 6.971818159664702e-05, 'epoch': 0.56}
  9%|▉         | 442/5000 [1:59:11<17:28:49, 13.81s/it]  9%|▉         | 443/5000 [1:59:25<17:26:11, 13.77s/it]                                                       {'loss': 99.5251, 'grad_norm': 748.0, 'learning_rate': 6.97152423409594e-05, 'epoch': 0.56}
  9%|▉         | 443/5000 [1:59:25<17:26:11, 13.77s/it]  9%|▉         | 444/5000 [1:59:36<16:15:20, 12.84s/it]                                                       {'loss': 99.3038, 'grad_norm': 776.0, 'learning_rate': 6.971228789964537e-05, 'epoch': 0.56}
  9%|▉         | 444/5000 [1:59:36<16:15:20, 12.84s/it]  9%|▉         | 445/5000 [1:59:47<15:42:32, 12.42s/it]                                                       {'loss': 65.3931, 'grad_norm': 478.0, 'learning_rate': 6.970931827399728e-05, 'epoch': 0.57}
  9%|▉         | 445/5000 [1:59:47<15:42:32, 12.42s/it]  9%|▉         | 446/5000 [1:59:59<15:29:20, 12.24s/it]                                                       {'loss': 95.0625, 'grad_norm': 876.0, 'learning_rate': 6.970633346531416e-05, 'epoch': 0.57}
  9%|▉         | 446/5000 [1:59:59<15:29:20, 12.24s/it]  9%|▉         | 447/5000 [2:00:28<21:44:59, 17.20s/it]                                                       {'loss': 59.1828, 'grad_norm': 255.0, 'learning_rate': 6.970333347490167e-05, 'epoch': 0.57}
  9%|▉         | 447/5000 [2:00:28<21:44:59, 17.20s/it]  9%|▉         | 448/5000 [2:00:42<20:41:57, 16.37s/it]                                                       {'loss': 83.2722, 'grad_norm': 836.0, 'learning_rate': 6.97003183040721e-05, 'epoch': 0.57}
  9%|▉         | 448/5000 [2:00:42<20:41:57, 16.37s/it]  9%|▉         | 449/5000 [2:00:57<20:19:27, 16.08s/it]                                                       {'loss': 49.8757, 'grad_norm': 156.0, 'learning_rate': 6.96972879541444e-05, 'epoch': 0.57}
  9%|▉         | 449/5000 [2:00:57<20:19:27, 16.08s/it]  9%|▉         | 450/5000 [2:01:11<19:16:32, 15.25s/it]                                                       {'loss': 66.0736, 'grad_norm': 632.0, 'learning_rate': 6.969424242644413e-05, 'epoch': 0.57}
  9%|▉         | 450/5000 [2:01:11<19:16:32, 15.25s/it]  9%|▉         | 451/5000 [2:01:28<20:00:23, 15.83s/it]                                                       {'loss': 52.3505, 'grad_norm': 183.0, 'learning_rate': 6.969118172230351e-05, 'epoch': 0.57}
  9%|▉         | 451/5000 [2:01:28<20:00:23, 15.83s/it]  9%|▉         | 452/5000 [2:01:40<18:36:28, 14.73s/it]                                                       {'loss': 79.9569, 'grad_norm': 424.0, 'learning_rate': 6.968810584306142e-05, 'epoch': 0.57}
  9%|▉         | 452/5000 [2:01:40<18:36:28, 14.73s/it]  9%|▉         | 453/5000 [2:01:55<18:44:51, 14.84s/it]                                                       {'loss': 65.582, 'grad_norm': 1392.0, 'learning_rate': 6.968501479006332e-05, 'epoch': 0.58}
  9%|▉         | 453/5000 [2:01:55<18:44:51, 14.84s/it]  9%|▉         | 454/5000 [2:02:17<21:14:47, 16.83s/it]                                                       {'loss': 50.5343, 'grad_norm': 318.0, 'learning_rate': 6.968190856466136e-05, 'epoch': 0.58}
  9%|▉         | 454/5000 [2:02:17<21:14:47, 16.83s/it]  9%|▉         | 455/5000 [2:02:32<20:53:31, 16.55s/it]                                                       {'loss': 63.1938, 'grad_norm': 288.0, 'learning_rate': 6.96787871682143e-05, 'epoch': 0.58}
  9%|▉         | 455/5000 [2:02:32<20:53:31, 16.55s/it]  9%|▉         | 456/5000 [2:02:43<18:41:47, 14.81s/it]                                                       {'loss': 69.0162, 'grad_norm': 324.0, 'learning_rate': 6.967565060208757e-05, 'epoch': 0.58}
  9%|▉         | 456/5000 [2:02:43<18:41:47, 14.81s/it]  9%|▉         | 457/5000 [2:03:07<22:07:31, 17.53s/it]                                                       {'loss': 62.5591, 'grad_norm': 326.0, 'learning_rate': 6.967249886765317e-05, 'epoch': 0.58}
  9%|▉         | 457/5000 [2:03:07<22:07:31, 17.53s/it]  9%|▉         | 458/5000 [2:03:18<19:24:59, 15.39s/it]                                                       {'loss': 68.3165, 'grad_norm': 632.0, 'learning_rate': 6.966933196628981e-05, 'epoch': 0.58}
  9%|▉         | 458/5000 [2:03:18<19:24:59, 15.39s/it]  9%|▉         | 459/5000 [2:03:32<19:06:04, 15.14s/it]                                                       {'loss': 52.2413, 'grad_norm': 227.0, 'learning_rate': 6.966614989938278e-05, 'epoch': 0.58}
  9%|▉         | 459/5000 [2:03:32<19:06:04, 15.14s/it]  9%|▉         | 460/5000 [2:03:46<18:28:15, 14.65s/it]                                                       {'loss': 77.9242, 'grad_norm': 446.0, 'learning_rate': 6.966295266832404e-05, 'epoch': 0.58}
  9%|▉         | 460/5000 [2:03:46<18:28:15, 14.65s/it]  9%|▉         | 461/5000 [2:04:04<20:02:49, 15.90s/it]                                                       {'loss': 61.3029, 'grad_norm': 672.0, 'learning_rate': 6.965974027451216e-05, 'epoch': 0.59}
  9%|▉         | 461/5000 [2:04:04<20:02:49, 15.90s/it]  9%|▉         | 462/5000 [2:04:14<17:49:42, 14.14s/it]                                                       {'loss': 65.4557, 'grad_norm': 2528.0, 'learning_rate': 6.965651271935234e-05, 'epoch': 0.59}
  9%|▉         | 462/5000 [2:04:14<17:49:42, 14.14s/it]  9%|▉         | 463/5000 [2:04:27<17:04:08, 13.54s/it]                                                       {'loss': 58.6415, 'grad_norm': 544.0, 'learning_rate': 6.965327000425644e-05, 'epoch': 0.59}
  9%|▉         | 463/5000 [2:04:27<17:04:08, 13.54s/it]  9%|▉         | 464/5000 [2:04:47<19:39:33, 15.60s/it]                                                       {'loss': 56.0517, 'grad_norm': 324.0, 'learning_rate': 6.965001213064292e-05, 'epoch': 0.59}
  9%|▉         | 464/5000 [2:04:47<19:39:33, 15.60s/it]  9%|▉         | 465/5000 [2:05:08<21:47:40, 17.30s/it]                                                       {'loss': 82.8289, 'grad_norm': 11456.0, 'learning_rate': 6.96467390999369e-05, 'epoch': 0.59}
  9%|▉         | 465/5000 [2:05:08<21:47:40, 17.30s/it]  9%|▉         | 466/5000 [2:05:26<21:55:41, 17.41s/it]                                                       {'loss': 64.8104, 'grad_norm': 5536.0, 'learning_rate': 6.96434509135701e-05, 'epoch': 0.59}
  9%|▉         | 466/5000 [2:05:26<21:55:41, 17.41s/it]  9%|▉         | 467/5000 [2:05:47<23:08:59, 18.39s/it]                                                       {'loss': 77.0612, 'grad_norm': 3680.0, 'learning_rate': 6.964014757298088e-05, 'epoch': 0.59}
  9%|▉         | 467/5000 [2:05:47<23:08:59, 18.39s/it]  9%|▉         | 468/5000 [2:05:59<20:57:39, 16.65s/it]                                                       {'loss': 74.5211, 'grad_norm': 1792.0, 'learning_rate': 6.963682907961426e-05, 'epoch': 0.59}
  9%|▉         | 468/5000 [2:05:59<20:57:39, 16.65s/it]  9%|▉         | 469/5000 [2:06:13<20:00:53, 15.90s/it]                                                       {'loss': 47.5115, 'grad_norm': 2240.0, 'learning_rate': 6.963349543492182e-05, 'epoch': 0.6}
  9%|▉         | 469/5000 [2:06:13<20:00:53, 15.90s/it]  9%|▉         | 470/5000 [2:06:27<19:03:35, 15.15s/it]                                                       {'loss': 54.3383, 'grad_norm': 464.0, 'learning_rate': 6.963014664036185e-05, 'epoch': 0.6}
  9%|▉         | 470/5000 [2:06:27<19:03:35, 15.15s/it]  9%|▉         | 471/5000 [2:06:41<18:41:26, 14.86s/it]                                                       {'loss': 56.4564, 'grad_norm': 336.0, 'learning_rate': 6.962678269739922e-05, 'epoch': 0.6}
  9%|▉         | 471/5000 [2:06:41<18:41:26, 14.86s/it]  9%|▉         | 472/5000 [2:06:51<16:54:18, 13.44s/it]                                                       {'loss': 92.7671, 'grad_norm': 4320.0, 'learning_rate': 6.96234036075054e-05, 'epoch': 0.6}
  9%|▉         | 472/5000 [2:06:51<16:54:18, 13.44s/it]  9%|▉         | 473/5000 [2:07:03<16:28:35, 13.10s/it]                                                       {'loss': 71.9667, 'grad_norm': 744.0, 'learning_rate': 6.962000937215855e-05, 'epoch': 0.6}
  9%|▉         | 473/5000 [2:07:03<16:28:35, 13.10s/it]  9%|▉         | 474/5000 [2:07:26<19:54:40, 15.84s/it]                                                       {'loss': 46.5178, 'grad_norm': 179.0, 'learning_rate': 6.961659999284343e-05, 'epoch': 0.6}
  9%|▉         | 474/5000 [2:07:26<19:54:40, 15.84s/it] 10%|▉         | 475/5000 [2:07:46<21:32:04, 17.13s/it]                                                       {'loss': 63.4739, 'grad_norm': 4128.0, 'learning_rate': 6.961317547105138e-05, 'epoch': 0.6}
 10%|▉         | 475/5000 [2:07:46<21:32:04, 17.13s/it] 10%|▉         | 476/5000 [2:07:57<19:11:55, 15.28s/it]                                                       {'loss': 65.4282, 'grad_norm': 524.0, 'learning_rate': 6.960973580828045e-05, 'epoch': 0.6}
 10%|▉         | 476/5000 [2:07:57<19:11:55, 15.28s/it] 10%|▉         | 477/5000 [2:08:09<18:11:55, 14.48s/it]                                                       {'loss': 82.0685, 'grad_norm': 564.0, 'learning_rate': 6.960628100603524e-05, 'epoch': 0.61}
 10%|▉         | 477/5000 [2:08:09<18:11:55, 14.48s/it] 10%|▉         | 478/5000 [2:08:26<19:06:34, 15.21s/it]                                                       {'loss': 56.1568, 'grad_norm': 272.0, 'learning_rate': 6.960281106582698e-05, 'epoch': 0.61}
 10%|▉         | 478/5000 [2:08:26<19:06:34, 15.21s/it] 10%|▉         | 479/5000 [2:08:48<21:31:18, 17.14s/it]                                                       {'loss': 52.7993, 'grad_norm': 197.0, 'learning_rate': 6.959932598917358e-05, 'epoch': 0.61}
 10%|▉         | 479/5000 [2:08:48<21:31:18, 17.14s/it] 10%|▉         | 480/5000 [2:09:00<19:30:47, 15.54s/it]                                                       {'loss': 50.2257, 'grad_norm': 488.0, 'learning_rate': 6.95958257775995e-05, 'epoch': 0.61}
 10%|▉         | 480/5000 [2:09:00<19:30:47, 15.54s/it] 10%|▉         | 481/5000 [2:09:14<19:06:47, 15.23s/it]                                                       {'loss': 59.6617, 'grad_norm': 258.0, 'learning_rate': 6.959231043263586e-05, 'epoch': 0.61}
 10%|▉         | 481/5000 [2:09:14<19:06:47, 15.23s/it] 10%|▉         | 482/5000 [2:09:25<17:30:39, 13.95s/it]                                                       {'loss': 61.2956, 'grad_norm': 584.0, 'learning_rate': 6.95887799558204e-05, 'epoch': 0.61}
 10%|▉         | 482/5000 [2:09:25<17:30:39, 13.95s/it] 10%|▉         | 483/5000 [2:09:38<16:57:48, 13.52s/it]                                                       {'loss': 63.5415, 'grad_norm': 628.0, 'learning_rate': 6.958523434869746e-05, 'epoch': 0.61}
 10%|▉         | 483/5000 [2:09:38<16:57:48, 13.52s/it] 10%|▉         | 484/5000 [2:09:50<16:25:07, 13.09s/it]                                                       {'loss': 77.1542, 'grad_norm': 366.0, 'learning_rate': 6.9581673612818e-05, 'epoch': 0.61}
 10%|▉         | 484/5000 [2:09:50<16:25:07, 13.09s/it] 10%|▉         | 485/5000 [2:10:10<19:18:00, 15.39s/it]                                                       {'loss': 80.6793, 'grad_norm': 448.0, 'learning_rate': 6.957809774973965e-05, 'epoch': 0.62}
 10%|▉         | 485/5000 [2:10:10<19:18:00, 15.39s/it] 10%|▉         | 486/5000 [2:10:33<22:05:20, 17.62s/it]                                                       {'loss': 44.3523, 'grad_norm': 208.0, 'learning_rate': 6.957450676102656e-05, 'epoch': 0.62}
 10%|▉         | 486/5000 [2:10:33<22:05:20, 17.62s/it] 10%|▉         | 487/5000 [2:10:47<20:38:37, 16.47s/it]                                                       {'loss': 50.2205, 'grad_norm': 237.0, 'learning_rate': 6.957090064824957e-05, 'epoch': 0.62}
 10%|▉         | 487/5000 [2:10:47<20:38:37, 16.47s/it] 10%|▉         | 488/5000 [2:11:00<19:11:13, 15.31s/it]                                                       {'loss': 42.4003, 'grad_norm': 99.5, 'learning_rate': 6.956727941298614e-05, 'epoch': 0.62}
 10%|▉         | 488/5000 [2:11:00<19:11:13, 15.31s/it] 10%|▉         | 489/5000 [2:11:10<17:23:40, 13.88s/it]                                                       {'loss': 77.4549, 'grad_norm': 656.0, 'learning_rate': 6.956364305682029e-05, 'epoch': 0.62}
 10%|▉         | 489/5000 [2:11:10<17:23:40, 13.88s/it] 10%|▉         | 490/5000 [2:11:23<17:02:07, 13.60s/it]                                                       {'loss': 53.4402, 'grad_norm': 876.0, 'learning_rate': 6.955999158134269e-05, 'epoch': 0.62}
 10%|▉         | 490/5000 [2:11:23<17:02:07, 13.60s/it] 10%|▉         | 491/5000 [2:11:36<16:44:54, 13.37s/it]                                                       {'loss': 64.0549, 'grad_norm': 378.0, 'learning_rate': 6.955632498815063e-05, 'epoch': 0.62}
 10%|▉         | 491/5000 [2:11:36<16:44:54, 13.37s/it] 10%|▉         | 492/5000 [2:11:50<16:59:05, 13.56s/it]                                                       {'loss': 77.4487, 'grad_norm': 928.0, 'learning_rate': 6.955264327884801e-05, 'epoch': 0.62}
 10%|▉         | 492/5000 [2:11:50<16:59:05, 13.56s/it] 10%|▉         | 493/5000 [2:12:01<16:02:20, 12.81s/it]                                                       {'loss': 48.4463, 'grad_norm': 162.0, 'learning_rate': 6.954894645504532e-05, 'epoch': 0.63}
 10%|▉         | 493/5000 [2:12:01<16:02:20, 12.81s/it] 10%|▉         | 494/5000 [2:12:18<17:31:57, 14.01s/it]                                                       {'loss': 54.2993, 'grad_norm': 358.0, 'learning_rate': 6.954523451835967e-05, 'epoch': 0.63}
 10%|▉         | 494/5000 [2:12:18<17:31:57, 14.01s/it] 10%|▉         | 495/5000 [2:12:35<18:50:12, 15.05s/it]                                                       {'loss': 103.0249, 'grad_norm': 1016.0, 'learning_rate': 6.954150747041481e-05, 'epoch': 0.63}
 10%|▉         | 495/5000 [2:12:35<18:50:12, 15.05s/it] 10%|▉         | 496/5000 [2:12:55<20:27:49, 16.36s/it]                                                       {'loss': 71.0195, 'grad_norm': 1992.0, 'learning_rate': 6.953776531284106e-05, 'epoch': 0.63}
 10%|▉         | 496/5000 [2:12:55<20:27:49, 16.36s/it] 10%|▉         | 497/5000 [2:13:08<19:22:17, 15.49s/it]                                                       {'loss': 49.7348, 'grad_norm': 544.0, 'learning_rate': 6.953400804727537e-05, 'epoch': 0.63}
 10%|▉         | 497/5000 [2:13:08<19:22:17, 15.49s/it] 10%|▉         | 498/5000 [2:13:22<18:36:03, 14.87s/it]                                                       {'loss': 58.5502, 'grad_norm': 378.0, 'learning_rate': 6.953023567536131e-05, 'epoch': 0.63}
 10%|▉         | 498/5000 [2:13:22<18:36:03, 14.87s/it] 10%|▉         | 499/5000 [2:13:37<18:55:23, 15.14s/it]                                                       {'loss': 61.9703, 'grad_norm': 312.0, 'learning_rate': 6.952644819874903e-05, 'epoch': 0.63}
 10%|▉         | 499/5000 [2:13:37<18:55:23, 15.14s/it] 10%|█         | 500/5000 [2:13:50<17:59:20, 14.39s/it]                                                       {'loss': 60.3273, 'grad_norm': 716.0, 'learning_rate': 6.952264561909527e-05, 'epoch': 0.63}
 10%|█         | 500/5000 [2:13:50<17:59:20, 14.39s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:07,  4.27s/it][A
  3%|▎         | 3/88 [00:16<07:58,  5.63s/it][A
  5%|▍         | 4/88 [00:21<07:33,  5.40s/it][A
  6%|▌         | 5/88 [00:24<06:43,  4.86s/it][A
  7%|▋         | 6/88 [00:31<07:12,  5.28s/it][A
  8%|▊         | 7/88 [00:34<06:26,  4.78s/it][A
  9%|▉         | 8/88 [00:38<05:55,  4.44s/it][A
 10%|█         | 9/88 [00:42<05:29,  4.17s/it][A
 11%|█▏        | 10/88 [00:44<04:49,  3.71s/it][A
 12%|█▎        | 11/88 [00:47<04:20,  3.38s/it][A
 14%|█▎        | 12/88 [00:49<03:46,  2.99s/it][A
 15%|█▍        | 13/88 [00:51<03:27,  2.77s/it][A
 16%|█▌        | 14/88 [00:56<04:21,  3.53s/it][A
 17%|█▋        | 15/88 [01:02<04:54,  4.03s/it][A
 18%|█▊        | 16/88 [01:05<04:31,  3.77s/it][A
 19%|█▉        | 17/88 [01:10<04:55,  4.16s/it][A
 20%|██        | 18/88 [01:13<04:25,  3.79s/it][A
 22%|██▏       | 19/88 [01:17<04:26,  3.87s/it][A
 23%|██▎       | 20/88 [01:20<04:16,  3.78s/it][A
 24%|██▍       | 21/88 [01:23<03:56,  3.53s/it][A
 25%|██▌       | 22/88 [01:27<03:52,  3.53s/it][A
 26%|██▌       | 23/88 [01:29<03:27,  3.19s/it][A
 27%|██▋       | 24/88 [01:38<05:02,  4.73s/it][A
 28%|██▊       | 25/88 [01:41<04:28,  4.26s/it][A
 30%|██▉       | 26/88 [01:47<04:59,  4.84s/it][A
 31%|███       | 27/88 [01:51<04:40,  4.60s/it][A
 32%|███▏      | 28/88 [01:59<05:44,  5.74s/it][A
 33%|███▎      | 29/88 [02:06<05:50,  5.95s/it][A
 34%|███▍      | 30/88 [02:11<05:27,  5.65s/it][A
 35%|███▌      | 31/88 [02:15<04:51,  5.12s/it][A
 36%|███▋      | 32/88 [02:23<05:47,  6.20s/it][A
 38%|███▊      | 33/88 [02:28<05:11,  5.66s/it][A
 39%|███▊      | 34/88 [02:36<05:50,  6.50s/it][A
 40%|███▉      | 35/88 [02:39<04:44,  5.36s/it][A
 41%|████      | 36/88 [02:44<04:31,  5.23s/it][A
 42%|████▏     | 37/88 [02:47<03:58,  4.68s/it][A
 43%|████▎     | 38/88 [02:51<03:42,  4.46s/it][A
 44%|████▍     | 39/88 [02:54<03:14,  3.98s/it][A
 45%|████▌     | 40/88 [03:00<03:45,  4.70s/it][A
 47%|████▋     | 41/88 [03:10<04:44,  6.05s/it][A
 48%|████▊     | 42/88 [03:13<04:01,  5.26s/it][A
 49%|████▉     | 43/88 [03:21<04:38,  6.19s/it][A
 50%|█████     | 44/88 [03:24<03:50,  5.23s/it][A
 51%|█████     | 45/88 [03:29<03:31,  4.93s/it][A
 52%|█████▏    | 46/88 [03:32<03:10,  4.54s/it][A
 53%|█████▎    | 47/88 [03:35<02:37,  3.84s/it][A
 55%|█████▍    | 48/88 [03:37<02:12,  3.31s/it][A
 56%|█████▌    | 49/88 [03:41<02:25,  3.73s/it][A
 57%|█████▋    | 50/88 [03:45<02:23,  3.77s/it][A
 58%|█████▊    | 51/88 [03:49<02:21,  3.82s/it][A
 59%|█████▉    | 52/88 [03:55<02:36,  4.34s/it][A
 60%|██████    | 53/88 [03:59<02:35,  4.45s/it][A
 61%|██████▏   | 54/88 [04:09<03:20,  5.91s/it][A
 62%|██████▎   | 55/88 [04:13<03:02,  5.53s/it][A
 64%|██████▎   | 56/88 [04:16<02:25,  4.56s/it][A
 65%|██████▍   | 57/88 [04:19<02:12,  4.27s/it][A
 66%|██████▌   | 58/88 [04:28<02:46,  5.54s/it][A
 67%|██████▋   | 59/88 [04:33<02:42,  5.59s/it][A
 68%|██████▊   | 60/88 [04:37<02:15,  4.85s/it][A
 69%|██████▉   | 61/88 [04:40<02:02,  4.53s/it][A
 70%|███████   | 62/88 [04:43<01:43,  3.98s/it][A
 72%|███████▏  | 63/88 [04:48<01:49,  4.37s/it][A
 73%|███████▎  | 64/88 [04:52<01:41,  4.23s/it][A
 74%|███████▍  | 65/88 [04:56<01:35,  4.17s/it][A
 75%|███████▌  | 66/88 [05:00<01:27,  3.98s/it][A
 76%|███████▌  | 67/88 [05:03<01:16,  3.62s/it][A
 77%|███████▋  | 68/88 [05:07<01:20,  4.02s/it][A
 78%|███████▊  | 69/88 [05:14<01:27,  4.62s/it][A
 80%|███████▉  | 70/88 [05:18<01:19,  4.44s/it][A
 81%|████████  | 71/88 [05:21<01:11,  4.18s/it][A
 82%|████████▏ | 72/88 [05:25<01:04,  4.01s/it][A
 83%|████████▎ | 73/88 [05:28<00:54,  3.64s/it][A
 84%|████████▍ | 74/88 [05:30<00:46,  3.30s/it][A
 85%|████████▌ | 75/88 [05:35<00:47,  3.68s/it][A
 86%|████████▋ | 76/88 [05:38<00:43,  3.64s/it][A
 88%|████████▊ | 77/88 [05:45<00:50,  4.59s/it][A
 89%|████████▊ | 78/88 [05:49<00:42,  4.29s/it][A
 90%|████████▉ | 79/88 [05:53<00:39,  4.34s/it][A
 91%|█████████ | 80/88 [05:56<00:30,  3.85s/it][A
 92%|█████████▏| 81/88 [06:00<00:28,  4.13s/it][A
 93%|█████████▎| 82/88 [06:04<00:23,  3.99s/it][A
 94%|█████████▍| 83/88 [06:08<00:19,  3.83s/it][A
 95%|█████████▌| 84/88 [06:11<00:14,  3.75s/it][A
 97%|█████████▋| 85/88 [06:14<00:10,  3.43s/it][A
 98%|█████████▊| 86/88 [06:16<00:06,  3.18s/it][A
 99%|█████████▉| 87/88 [06:20<00:03,  3.36s/it][A
100%|██████████| 88/88 [06:24<00:00,  3.43s/it][A                                                       
                                               [A{'eval_loss': 58.074676513671875, 'eval_runtime': 388.0237, 'eval_samples_per_second': 7.216, 'eval_steps_per_second': 0.227, 'epoch': 0.63}
 10%|█         | 500/5000 [2:20:18<17:59:20, 14.39s/it]
100%|██████████| 88/88 [06:24<00:00,  3.43s/it][A
                                               [A2024-06-13 11:55:55,918 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-13 11:56:07,291 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 10%|█         | 501/5000 [2:20:50<170:00:29, 136.04s/it]                                                         {'loss': 47.1846, 'grad_norm': 177.0, 'learning_rate': 6.951882793806347e-05, 'epoch': 0.64}
 10%|█         | 501/5000 [2:20:50<170:00:29, 136.04s/it] 10%|█         | 502/5000 [2:21:03<123:48:44, 99.09s/it]                                                         {'loss': 70.4831, 'grad_norm': 1488.0, 'learning_rate': 6.951499515732357e-05, 'epoch': 0.64}
 10%|█         | 502/5000 [2:21:03<123:48:44, 99.09s/it] 10%|█         | 503/5000 [2:21:24<94:39:04, 75.77s/it]                                                        {'loss': 93.8183, 'grad_norm': 740.0, 'learning_rate': 6.951114727855218e-05, 'epoch': 0.64}
 10%|█         | 503/5000 [2:21:24<94:39:04, 75.77s/it] 10%|█         | 504/5000 [2:21:41<72:42:58, 58.22s/it]                                                       {'loss': 68.6515, 'grad_norm': 572.0, 'learning_rate': 6.950728430343248e-05, 'epoch': 0.64}
 10%|█         | 504/5000 [2:21:41<72:42:58, 58.22s/it] 10%|█         | 505/5000 [2:21:57<56:36:50, 45.34s/it]                                                       {'loss': 90.1056, 'grad_norm': 1552.0, 'learning_rate': 6.950340623365426e-05, 'epoch': 0.64}
 10%|█         | 505/5000 [2:21:57<56:36:50, 45.34s/it] 10%|█         | 506/5000 [2:22:11<45:04:39, 36.11s/it]                                                       {'loss': 63.8903, 'grad_norm': 812.0, 'learning_rate': 6.949951307091394e-05, 'epoch': 0.64}
 10%|█         | 506/5000 [2:22:11<45:04:39, 36.11s/it] 10%|█         | 507/5000 [2:22:35<40:23:18, 32.36s/it]                                                       {'loss': 67.8651, 'grad_norm': 712.0, 'learning_rate': 6.949560481691451e-05, 'epoch': 0.64}
 10%|█         | 507/5000 [2:22:35<40:23:18, 32.36s/it] 10%|█         | 508/5000 [2:22:47<32:35:20, 26.12s/it]                                                       {'loss': 61.5594, 'grad_norm': 844.0, 'learning_rate': 6.949168147336559e-05, 'epoch': 0.65}
 10%|█         | 508/5000 [2:22:47<32:35:20, 26.12s/it] 10%|█         | 509/5000 [2:22:58<27:17:21, 21.88s/it]                                                       {'loss': 51.1338, 'grad_norm': 844.0, 'learning_rate': 6.948774304198335e-05, 'epoch': 0.65}
 10%|█         | 509/5000 [2:22:58<27:17:21, 21.88s/it] 10%|█         | 510/5000 [2:23:21<27:40:31, 22.19s/it]                                                       {'loss': 56.8864, 'grad_norm': 268.0, 'learning_rate': 6.948378952449062e-05, 'epoch': 0.65}
 10%|█         | 510/5000 [2:23:21<27:40:31, 22.19s/it] 10%|█         | 511/5000 [2:23:33<23:49:53, 19.11s/it]                                                       {'loss': 68.1061, 'grad_norm': 1600.0, 'learning_rate': 6.947982092261678e-05, 'epoch': 0.65}
 10%|█         | 511/5000 [2:23:33<23:49:53, 19.11s/it] 10%|█         | 512/5000 [2:23:49<22:33:41, 18.10s/it]                                                       {'loss': 47.1712, 'grad_norm': 352.0, 'learning_rate': 6.947583723809787e-05, 'epoch': 0.65}
 10%|█         | 512/5000 [2:23:49<22:33:41, 18.10s/it] 10%|█         | 513/5000 [2:24:09<23:18:14, 18.70s/it]                                                       {'loss': 59.6444, 'grad_norm': 358.0, 'learning_rate': 6.947183847267646e-05, 'epoch': 0.65}
 10%|█         | 513/5000 [2:24:09<23:18:14, 18.70s/it] 10%|█         | 514/5000 [2:24:24<21:51:57, 17.55s/it]                                                       {'loss': 51.5552, 'grad_norm': 167.0, 'learning_rate': 6.946782462810174e-05, 'epoch': 0.65}
 10%|█         | 514/5000 [2:24:24<21:51:57, 17.55s/it] 10%|█         | 515/5000 [2:24:36<19:48:30, 15.90s/it]                                                       {'loss': 70.9366, 'grad_norm': 424.0, 'learning_rate': 6.946379570612952e-05, 'epoch': 0.65}
 10%|█         | 515/5000 [2:24:36<19:48:30, 15.90s/it] 10%|█         | 516/5000 [2:24:49<18:47:18, 15.08s/it]                                                       {'loss': 125.6682, 'grad_norm': 836.0, 'learning_rate': 6.945975170852218e-05, 'epoch': 0.66}
 10%|█         | 516/5000 [2:24:49<18:47:18, 15.08s/it] 10%|█         | 517/5000 [2:25:00<17:09:55, 13.78s/it]                                                       {'loss': 73.0249, 'grad_norm': 668.0, 'learning_rate': 6.94556926370487e-05, 'epoch': 0.66}
 10%|█         | 517/5000 [2:25:00<17:09:55, 13.78s/it] 10%|█         | 518/5000 [2:25:11<16:00:14, 12.85s/it]                                                       {'loss': 75.2743, 'grad_norm': 482.0, 'learning_rate': 6.945161849348467e-05, 'epoch': 0.66}
 10%|█         | 518/5000 [2:25:11<16:00:14, 12.85s/it] 10%|█         | 519/5000 [2:25:23<15:55:59, 12.80s/it]                                                       {'loss': 67.5383, 'grad_norm': 210.0, 'learning_rate': 6.944752927961225e-05, 'epoch': 0.66}
 10%|█         | 519/5000 [2:25:23<15:55:59, 12.80s/it] 10%|█         | 520/5000 [2:25:35<15:27:59, 12.43s/it]                                                       {'loss': 86.2247, 'grad_norm': 772.0, 'learning_rate': 6.944342499722018e-05, 'epoch': 0.66}
 10%|█         | 520/5000 [2:25:35<15:27:59, 12.43s/it] 10%|█         | 521/5000 [2:25:52<17:04:13, 13.72s/it]                                                       {'loss': 58.2183, 'grad_norm': 404.0, 'learning_rate': 6.943930564810383e-05, 'epoch': 0.66}
 10%|█         | 521/5000 [2:25:52<17:04:13, 13.72s/it] 10%|█         | 522/5000 [2:26:04<16:29:25, 13.26s/it]                                                       {'loss': 46.1156, 'grad_norm': 149.0, 'learning_rate': 6.943517123406515e-05, 'epoch': 0.66}
 10%|█         | 522/5000 [2:26:04<16:29:25, 13.26s/it] 10%|█         | 523/5000 [2:26:21<17:52:57, 14.38s/it]                                                       {'loss': 52.0101, 'grad_norm': 304.0, 'learning_rate': 6.943102175691267e-05, 'epoch': 0.66}
 10%|█         | 523/5000 [2:26:21<17:52:57, 14.38s/it] 10%|█         | 524/5000 [2:26:36<18:04:32, 14.54s/it]                                                       {'loss': 44.7532, 'grad_norm': 180.0, 'learning_rate': 6.94268572184615e-05, 'epoch': 0.67}
 10%|█         | 524/5000 [2:26:36<18:04:32, 14.54s/it] 10%|█         | 525/5000 [2:26:47<16:42:15, 13.44s/it]                                                       {'loss': 60.4767, 'grad_norm': 624.0, 'learning_rate': 6.942267762053337e-05, 'epoch': 0.67}
 10%|█         | 525/5000 [2:26:47<16:42:15, 13.44s/it] 11%|█         | 526/5000 [2:27:02<17:29:23, 14.07s/it]                                                       {'loss': 61.5846, 'grad_norm': 286.0, 'learning_rate': 6.941848296495658e-05, 'epoch': 0.67}
 11%|█         | 526/5000 [2:27:02<17:29:23, 14.07s/it] 11%|█         | 527/5000 [2:27:16<17:34:43, 14.15s/it]                                                       {'loss': 45.762, 'grad_norm': 185.0, 'learning_rate': 6.941427325356598e-05, 'epoch': 0.67}
 11%|█         | 527/5000 [2:27:17<17:34:43, 14.15s/it] 11%|█         | 528/5000 [2:27:38<20:25:16, 16.44s/it]                                                       {'loss': 43.759, 'grad_norm': 426.0, 'learning_rate': 6.941004848820308e-05, 'epoch': 0.67}
 11%|█         | 528/5000 [2:27:38<20:25:16, 16.44s/it] 11%|█         | 529/5000 [2:27:51<18:53:27, 15.21s/it]                                                       {'loss': 46.5404, 'grad_norm': 536.0, 'learning_rate': 6.940580867071591e-05, 'epoch': 0.67}
 11%|█         | 529/5000 [2:27:51<18:53:27, 15.21s/it] 11%|█         | 530/5000 [2:28:04<18:03:19, 14.54s/it]                                                       {'loss': 44.1842, 'grad_norm': 219.0, 'learning_rate': 6.940155380295912e-05, 'epoch': 0.67}
 11%|█         | 530/5000 [2:28:04<18:03:19, 14.54s/it] 11%|█         | 531/5000 [2:28:14<16:33:31, 13.34s/it]                                                       {'loss': 54.8978, 'grad_norm': 304.0, 'learning_rate': 6.939728388679394e-05, 'epoch': 0.67}
 11%|█         | 531/5000 [2:28:14<16:33:31, 13.34s/it] 11%|█         | 532/5000 [2:28:26<15:53:49, 12.81s/it]                                                       {'loss': 46.6179, 'grad_norm': 141.0, 'learning_rate': 6.939299892408816e-05, 'epoch': 0.68}
 11%|█         | 532/5000 [2:28:26<15:53:49, 12.81s/it] 11%|█         | 533/5000 [2:28:39<16:04:07, 12.95s/it]                                                       {'loss': 52.8749, 'grad_norm': 178.0, 'learning_rate': 6.938869891671619e-05, 'epoch': 0.68}
 11%|█         | 533/5000 [2:28:39<16:04:07, 12.95s/it] 11%|█         | 534/5000 [2:28:49<15:04:17, 12.15s/it]                                                       {'loss': 48.4844, 'grad_norm': 252.0, 'learning_rate': 6.938438386655898e-05, 'epoch': 0.68}
 11%|█         | 534/5000 [2:28:49<15:04:17, 12.15s/it] 11%|█         | 535/5000 [2:29:02<15:15:12, 12.30s/it]                                                       {'loss': 57.2225, 'grad_norm': 280.0, 'learning_rate': 6.93800537755041e-05, 'epoch': 0.68}
 11%|█         | 535/5000 [2:29:02<15:15:12, 12.30s/it] 11%|█         | 536/5000 [2:29:12<14:26:21, 11.64s/it]                                                       {'loss': 58.3173, 'grad_norm': 237.0, 'learning_rate': 6.937570864544565e-05, 'epoch': 0.68}
 11%|█         | 536/5000 [2:29:12<14:26:21, 11.64s/it] 11%|█         | 537/5000 [2:29:34<18:10:24, 14.66s/it]                                                       {'loss': 46.4624, 'grad_norm': 161.0, 'learning_rate': 6.937134847828437e-05, 'epoch': 0.68}
 11%|█         | 537/5000 [2:29:34<18:10:24, 14.66s/it] 11%|█         | 538/5000 [2:29:46<17:06:40, 13.81s/it]                                                       {'loss': 135.2063, 'grad_norm': 3072.0, 'learning_rate': 6.93669732759275e-05, 'epoch': 0.68}
 11%|█         | 538/5000 [2:29:46<17:06:40, 13.81s/it] 11%|█         | 539/5000 [2:29:57<16:24:47, 13.25s/it]                                                       {'loss': 65.9987, 'grad_norm': 724.0, 'learning_rate': 6.936258304028897e-05, 'epoch': 0.68}
 11%|█         | 539/5000 [2:29:57<16:24:47, 13.25s/it] 11%|█         | 540/5000 [2:30:12<16:57:00, 13.68s/it]                                                       {'loss': 43.865, 'grad_norm': 182.0, 'learning_rate': 6.935817777328915e-05, 'epoch': 0.69}
 11%|█         | 540/5000 [2:30:12<16:57:00, 13.68s/it] 11%|█         | 541/5000 [2:30:27<17:16:13, 13.94s/it]                                                       {'loss': 59.0393, 'grad_norm': 3360.0, 'learning_rate': 6.93537574768551e-05, 'epoch': 0.69}
 11%|█         | 541/5000 [2:30:27<17:16:13, 13.94s/it] 11%|█         | 542/5000 [2:30:38<16:18:16, 13.17s/it]                                                       {'loss': 58.9383, 'grad_norm': 620.0, 'learning_rate': 6.934932215292038e-05, 'epoch': 0.69}
 11%|█         | 542/5000 [2:30:38<16:18:16, 13.17s/it] 11%|█         | 543/5000 [2:30:52<16:32:03, 13.35s/it]                                                       {'loss': 47.1341, 'grad_norm': 164.0, 'learning_rate': 6.934487180342517e-05, 'epoch': 0.69}
 11%|█         | 543/5000 [2:30:52<16:32:03, 13.35s/it] 11%|█         | 544/5000 [2:31:06<16:48:02, 13.57s/it]                                                       {'loss': 59.0024, 'grad_norm': 356.0, 'learning_rate': 6.934040643031618e-05, 'epoch': 0.69}
 11%|█         | 544/5000 [2:31:06<16:48:02, 13.57s/it] 11%|█         | 545/5000 [2:31:16<15:35:56, 12.61s/it]                                                       {'loss': 66.0692, 'grad_norm': 482.0, 'learning_rate': 6.933592603554675e-05, 'epoch': 0.69}
 11%|█         | 545/5000 [2:31:16<15:35:56, 12.61s/it] 11%|█         | 546/5000 [2:31:28<15:10:32, 12.27s/it]                                                       {'loss': 53.2706, 'grad_norm': 372.0, 'learning_rate': 6.933143062107674e-05, 'epoch': 0.69}
 11%|█         | 546/5000 [2:31:28<15:10:32, 12.27s/it] 11%|█         | 547/5000 [2:31:39<14:45:15, 11.93s/it]                                                       {'loss': 41.2996, 'grad_norm': 155.0, 'learning_rate': 6.932692018887258e-05, 'epoch': 0.69}
 11%|█         | 547/5000 [2:31:39<14:45:15, 11.93s/it] 11%|█         | 548/5000 [2:31:53<15:30:03, 12.53s/it]                                                       {'loss': 40.0467, 'grad_norm': 106.0, 'learning_rate': 6.93223947409073e-05, 'epoch': 0.7}
 11%|█         | 548/5000 [2:31:53<15:30:03, 12.53s/it] 11%|█         | 549/5000 [2:32:04<15:02:05, 12.16s/it]                                                       {'loss': 43.2203, 'grad_norm': 114.0, 'learning_rate': 6.93178542791605e-05, 'epoch': 0.7}
 11%|█         | 549/5000 [2:32:04<15:02:05, 12.16s/it] 11%|█         | 550/5000 [2:32:19<15:51:51, 12.83s/it]                                                       {'loss': 62.9259, 'grad_norm': 676.0, 'learning_rate': 6.931329880561832e-05, 'epoch': 0.7}
 11%|█         | 550/5000 [2:32:19<15:51:51, 12.83s/it] 11%|█         | 551/5000 [2:32:30<15:11:29, 12.29s/it]                                                       {'loss': 46.4439, 'grad_norm': 140.0, 'learning_rate': 6.930872832227347e-05, 'epoch': 0.7}
 11%|█         | 551/5000 [2:32:30<15:11:29, 12.29s/it] 11%|█         | 552/5000 [2:32:42<15:05:00, 12.21s/it]                                                       {'loss': 44.3789, 'grad_norm': 700.0, 'learning_rate': 6.930414283112525e-05, 'epoch': 0.7}
 11%|█         | 552/5000 [2:32:42<15:05:00, 12.21s/it] 11%|█         | 553/5000 [2:32:57<16:17:06, 13.18s/it]                                                       {'loss': 40.2081, 'grad_norm': 118.5, 'learning_rate': 6.929954233417949e-05, 'epoch': 0.7}
 11%|█         | 553/5000 [2:32:57<16:17:06, 13.18s/it] 11%|█         | 554/5000 [2:33:09<15:42:50, 12.72s/it]                                                       {'loss': 54.3102, 'grad_norm': 446.0, 'learning_rate': 6.929492683344863e-05, 'epoch': 0.7}
 11%|█         | 554/5000 [2:33:09<15:42:50, 12.72s/it] 11%|█         | 555/5000 [2:33:30<19:02:21, 15.42s/it]                                                       {'loss': 44.9779, 'grad_norm': 206.0, 'learning_rate': 6.929029633095163e-05, 'epoch': 0.7}
 11%|█         | 555/5000 [2:33:30<19:02:21, 15.42s/it] 11%|█         | 556/5000 [2:33:50<20:31:11, 16.62s/it]                                                       {'loss': 48.0283, 'grad_norm': 220.0, 'learning_rate': 6.928565082871403e-05, 'epoch': 0.71}
 11%|█         | 556/5000 [2:33:50<20:31:11, 16.62s/it] 11%|█         | 557/5000 [2:34:02<18:43:45, 15.18s/it]                                                       {'loss': 41.7258, 'grad_norm': 228.0, 'learning_rate': 6.928099032876793e-05, 'epoch': 0.71}
 11%|█         | 557/5000 [2:34:02<18:43:45, 15.18s/it] 11%|█         | 558/5000 [2:34:12<17:04:44, 13.84s/it]                                                       {'loss': 63.4124, 'grad_norm': 1312.0, 'learning_rate': 6.9276314833152e-05, 'epoch': 0.71}
 11%|█         | 558/5000 [2:34:12<17:04:44, 13.84s/it] 11%|█         | 559/5000 [2:34:25<16:31:30, 13.40s/it]                                                       {'loss': 40.1451, 'grad_norm': 125.5, 'learning_rate': 6.927162434391144e-05, 'epoch': 0.71}
 11%|█         | 559/5000 [2:34:25<16:31:30, 13.40s/it] 11%|█         | 560/5000 [2:34:46<19:20:44, 15.69s/it]                                                       {'loss': 57.5233, 'grad_norm': 260.0, 'learning_rate': 6.926691886309806e-05, 'epoch': 0.71}
 11%|█         | 560/5000 [2:34:46<19:20:44, 15.69s/it] 11%|█         | 561/5000 [2:34:58<18:03:38, 14.65s/it]                                                       {'loss': 60.9812, 'grad_norm': 436.0, 'learning_rate': 6.926219839277018e-05, 'epoch': 0.71}
 11%|█         | 561/5000 [2:34:58<18:03:38, 14.65s/it] 11%|█         | 562/5000 [2:35:11<17:33:21, 14.24s/it]                                                       {'loss': 43.8736, 'grad_norm': 131.0, 'learning_rate': 6.925746293499269e-05, 'epoch': 0.71}
 11%|█         | 562/5000 [2:35:11<17:33:21, 14.24s/it] 11%|█▏        | 563/5000 [2:35:25<17:12:07, 13.96s/it]                                                       {'loss': 47.2608, 'grad_norm': 148.0, 'learning_rate': 6.925271249183703e-05, 'epoch': 0.71}
 11%|█▏        | 563/5000 [2:35:25<17:12:07, 13.96s/it] 11%|█▏        | 564/5000 [2:35:38<17:09:55, 13.93s/it]                                                       {'loss': 40.2961, 'grad_norm': 704.0, 'learning_rate': 6.924794706538123e-05, 'epoch': 0.72}
 11%|█▏        | 564/5000 [2:35:38<17:09:55, 13.93s/it] 11%|█▏        | 565/5000 [2:35:52<17:10:03, 13.94s/it]                                                       {'loss': 39.2444, 'grad_norm': 107.5, 'learning_rate': 6.924316665770984e-05, 'epoch': 0.72}
 11%|█▏        | 565/5000 [2:35:52<17:10:03, 13.94s/it] 11%|█▏        | 566/5000 [2:36:06<17:00:06, 13.80s/it]                                                       {'loss': 38.3483, 'grad_norm': 110.0, 'learning_rate': 6.923837127091399e-05, 'epoch': 0.72}
 11%|█▏        | 566/5000 [2:36:06<17:00:06, 13.80s/it] 11%|█▏        | 567/5000 [2:36:17<16:06:55, 13.09s/it]                                                       {'loss': 42.8056, 'grad_norm': 201.0, 'learning_rate': 6.92335609070913e-05, 'epoch': 0.72}
 11%|█▏        | 567/5000 [2:36:17<16:06:55, 13.09s/it] 11%|█▏        | 568/5000 [2:36:29<15:26:41, 12.55s/it]                                                       {'loss': 42.2612, 'grad_norm': 185.0, 'learning_rate': 6.922873556834603e-05, 'epoch': 0.72}
 11%|█▏        | 568/5000 [2:36:29<15:26:41, 12.55s/it] 11%|█▏        | 569/5000 [2:36:43<16:12:43, 13.17s/it]                                                       {'loss': 46.7126, 'grad_norm': 306.0, 'learning_rate': 6.922389525678892e-05, 'epoch': 0.72}
 11%|█▏        | 569/5000 [2:36:43<16:12:43, 13.17s/it] 11%|█▏        | 570/5000 [2:36:56<16:04:22, 13.06s/it]                                                       {'loss': 42.0591, 'grad_norm': 116.5, 'learning_rate': 6.92190399745373e-05, 'epoch': 0.72}
 11%|█▏        | 570/5000 [2:36:56<16:04:22, 13.06s/it] 11%|█▏        | 571/5000 [2:37:08<15:48:54, 12.85s/it]                                                       {'loss': 43.1338, 'grad_norm': 205.0, 'learning_rate': 6.921416972371503e-05, 'epoch': 0.73}
 11%|█▏        | 571/5000 [2:37:08<15:48:54, 12.85s/it] 11%|█▏        | 572/5000 [2:37:19<14:53:02, 12.10s/it]                                                       {'loss': 44.0057, 'grad_norm': 302.0, 'learning_rate': 6.920928450645254e-05, 'epoch': 0.73}
 11%|█▏        | 572/5000 [2:37:19<14:53:02, 12.10s/it] 11%|█▏        | 573/5000 [2:37:32<15:16:55, 12.43s/it]                                                       {'loss': 38.9442, 'grad_norm': 960.0, 'learning_rate': 6.920438432488677e-05, 'epoch': 0.73}
 11%|█▏        | 573/5000 [2:37:32<15:16:55, 12.43s/it] 11%|█▏        | 574/5000 [2:37:55<19:15:49, 15.67s/it]                                                       {'loss': 58.3538, 'grad_norm': 14784.0, 'learning_rate': 6.919946918116122e-05, 'epoch': 0.73}
 11%|█▏        | 574/5000 [2:37:55<19:15:49, 15.67s/it] 12%|█▏        | 575/5000 [2:38:06<17:27:13, 14.20s/it]                                                       {'loss': 46.3082, 'grad_norm': 446.0, 'learning_rate': 6.919453907742597e-05, 'epoch': 0.73}
 12%|█▏        | 575/5000 [2:38:06<17:27:13, 14.20s/it] 12%|█▏        | 576/5000 [2:38:20<17:29:16, 14.23s/it]                                                       {'loss': 53.5217, 'grad_norm': 572.0, 'learning_rate': 6.91895940158376e-05, 'epoch': 0.73}
 12%|█▏        | 576/5000 [2:38:20<17:29:16, 14.23s/it] 12%|█▏        | 577/5000 [2:38:43<20:36:14, 16.77s/it]                                                       {'loss': 49.9853, 'grad_norm': 1536.0, 'learning_rate': 6.918463399855923e-05, 'epoch': 0.73}
 12%|█▏        | 577/5000 [2:38:43<20:36:14, 16.77s/it] 12%|█▏        | 578/5000 [2:38:57<19:28:55, 15.86s/it]                                                       {'loss': 62.016, 'grad_norm': 1352.0, 'learning_rate': 6.917965902776056e-05, 'epoch': 0.73}
 12%|█▏        | 578/5000 [2:38:57<19:28:55, 15.86s/it] 12%|█▏        | 579/5000 [2:39:07<17:26:11, 14.20s/it]                                                       {'loss': 60.575, 'grad_norm': 350.0, 'learning_rate': 6.917466910561781e-05, 'epoch': 0.74}
 12%|█▏        | 579/5000 [2:39:07<17:26:11, 14.20s/it] 12%|█▏        | 580/5000 [2:39:21<17:27:19, 14.22s/it]                                                       {'loss': 43.0381, 'grad_norm': 1896.0, 'learning_rate': 6.916966423431374e-05, 'epoch': 0.74}
 12%|█▏        | 580/5000 [2:39:21<17:27:19, 14.22s/it] 12%|█▏        | 581/5000 [2:39:42<20:00:51, 16.30s/it]                                                       {'loss': 65.2552, 'grad_norm': 4048.0, 'learning_rate': 6.916464441603765e-05, 'epoch': 0.74}
 12%|█▏        | 581/5000 [2:39:42<20:00:51, 16.30s/it] 12%|█▏        | 582/5000 [2:39:55<18:39:10, 15.20s/it]                                                       {'loss': 46.9337, 'grad_norm': 348.0, 'learning_rate': 6.915960965298537e-05, 'epoch': 0.74}
 12%|█▏        | 582/5000 [2:39:55<18:39:10, 15.20s/it] 12%|█▏        | 583/5000 [2:40:11<18:52:59, 15.39s/it]                                                       {'loss': 50.0559, 'grad_norm': 412.0, 'learning_rate': 6.91545599473593e-05, 'epoch': 0.74}
 12%|█▏        | 583/5000 [2:40:11<18:52:59, 15.39s/it] 12%|█▏        | 584/5000 [2:40:25<18:23:15, 14.99s/it]                                                       {'loss': 102.3509, 'grad_norm': 1736.0, 'learning_rate': 6.914949530136832e-05, 'epoch': 0.74}
 12%|█▏        | 584/5000 [2:40:25<18:23:15, 14.99s/it] 12%|█▏        | 585/5000 [2:40:46<20:31:45, 16.74s/it]                                                       {'loss': 71.6788, 'grad_norm': 516.0, 'learning_rate': 6.914441571722791e-05, 'epoch': 0.74}
 12%|█▏        | 585/5000 [2:40:46<20:31:45, 16.74s/it] 12%|█▏        | 586/5000 [2:41:00<19:30:13, 15.91s/it]                                                       {'loss': 43.1136, 'grad_norm': 302.0, 'learning_rate': 6.913932119716003e-05, 'epoch': 0.74}
 12%|█▏        | 586/5000 [2:41:00<19:30:13, 15.91s/it] 12%|█▏        | 587/5000 [2:41:11<17:41:26, 14.43s/it]                                                       {'loss': 41.893, 'grad_norm': 216.0, 'learning_rate': 6.91342117433932e-05, 'epoch': 0.75}
 12%|█▏        | 587/5000 [2:41:11<17:41:26, 14.43s/it] 12%|█▏        | 588/5000 [2:41:21<16:10:22, 13.20s/it]                                                       {'loss': 62.1739, 'grad_norm': 204.0, 'learning_rate': 6.912908735816249e-05, 'epoch': 0.75}
 12%|█▏        | 588/5000 [2:41:21<16:10:22, 13.20s/it] 12%|█▏        | 589/5000 [2:41:32<15:25:38, 12.59s/it]                                                       {'loss': 47.443, 'grad_norm': 620.0, 'learning_rate': 6.912394804370944e-05, 'epoch': 0.75}
 12%|█▏        | 589/5000 [2:41:32<15:25:38, 12.59s/it] 12%|█▏        | 590/5000 [2:41:53<18:30:14, 15.11s/it]                                                       {'loss': 38.5006, 'grad_norm': 100.0, 'learning_rate': 6.91187938022822e-05, 'epoch': 0.75}
 12%|█▏        | 590/5000 [2:41:53<18:30:14, 15.11s/it] 12%|█▏        | 591/5000 [2:42:05<17:09:48, 14.01s/it]                                                       {'loss': 57.2641, 'grad_norm': 856.0, 'learning_rate': 6.911362463613539e-05, 'epoch': 0.75}
 12%|█▏        | 591/5000 [2:42:05<17:09:48, 14.01s/it] 12%|█▏        | 592/5000 [2:42:19<17:08:56, 14.01s/it]                                                       {'loss': 36.8255, 'grad_norm': 121.5, 'learning_rate': 6.91084405475302e-05, 'epoch': 0.75}
 12%|█▏        | 592/5000 [2:42:19<17:08:56, 14.01s/it] 12%|█▏        | 593/5000 [2:42:40<19:45:16, 16.14s/it]                                                       {'loss': 81.4899, 'grad_norm': 3568.0, 'learning_rate': 6.910324153873429e-05, 'epoch': 0.75}
 12%|█▏        | 593/5000 [2:42:40<19:45:16, 16.14s/it] 12%|█▏        | 594/5000 [2:42:59<20:44:25, 16.95s/it]                                                       {'loss': 46.7946, 'grad_norm': 1744.0, 'learning_rate': 6.90980276120219e-05, 'epoch': 0.75}
 12%|█▏        | 594/5000 [2:42:59<20:44:25, 16.95s/it] 12%|█▏        | 595/5000 [2:43:11<19:07:04, 15.62s/it]                                                       {'loss': 51.6359, 'grad_norm': 426.0, 'learning_rate': 6.90927987696738e-05, 'epoch': 0.76}
 12%|█▏        | 595/5000 [2:43:11<19:07:04, 15.62s/it] 12%|█▏        | 596/5000 [2:43:33<21:29:30, 17.57s/it]                                                       {'loss': 40.6686, 'grad_norm': 988.0, 'learning_rate': 6.908755501397724e-05, 'epoch': 0.76}
 12%|█▏        | 596/5000 [2:43:33<21:29:30, 17.57s/it] 12%|█▏        | 597/5000 [2:43:44<19:06:33, 15.62s/it]                                                       {'loss': 46.1391, 'grad_norm': 1064.0, 'learning_rate': 6.908229634722602e-05, 'epoch': 0.76}
 12%|█▏        | 597/5000 [2:43:44<19:06:33, 15.62s/it] 12%|█▏        | 598/5000 [2:44:06<21:28:02, 17.56s/it]                                                       {'loss': 42.3124, 'grad_norm': 166.0, 'learning_rate': 6.907702277172045e-05, 'epoch': 0.76}
 12%|█▏        | 598/5000 [2:44:06<21:28:02, 17.56s/it] 12%|█▏        | 599/5000 [2:44:20<19:52:36, 16.26s/it]                                                       {'loss': 62.5833, 'grad_norm': 2352.0, 'learning_rate': 6.90717342897674e-05, 'epoch': 0.76}
 12%|█▏        | 599/5000 [2:44:20<19:52:36, 16.26s/it] 12%|█▏        | 600/5000 [2:44:32<18:22:00, 15.03s/it]                                                       {'loss': 44.3349, 'grad_norm': 145.0, 'learning_rate': 6.90664309036802e-05, 'epoch': 0.76}
 12%|█▏        | 600/5000 [2:44:32<18:22:00, 15.03s/it] 12%|█▏        | 601/5000 [2:44:48<18:41:35, 15.30s/it]                                                       {'loss': 34.9463, 'grad_norm': 179.0, 'learning_rate': 6.906111261577876e-05, 'epoch': 0.76}
 12%|█▏        | 601/5000 [2:44:48<18:41:35, 15.30s/it] 12%|█▏        | 602/5000 [2:45:01<18:01:02, 14.75s/it]                                                       {'loss': 41.0247, 'grad_norm': 572.0, 'learning_rate': 6.905577942838945e-05, 'epoch': 0.76}
 12%|█▏        | 602/5000 [2:45:01<18:01:02, 14.75s/it] 12%|█▏        | 603/5000 [2:45:15<17:44:13, 14.52s/it]                                                       {'loss': 54.4695, 'grad_norm': 16000.0, 'learning_rate': 6.90504313438452e-05, 'epoch': 0.77}
 12%|█▏        | 603/5000 [2:45:15<17:44:13, 14.52s/it] 12%|█▏        | 604/5000 [2:45:35<19:31:12, 15.99s/it]                                                       {'loss': 37.9027, 'grad_norm': 122.0, 'learning_rate': 6.904506836448545e-05, 'epoch': 0.77}
 12%|█▏        | 604/5000 [2:45:35<19:31:12, 15.99s/it] 12%|█▏        | 605/5000 [2:45:49<19:07:08, 15.66s/it]                                                       {'loss': 44.679, 'grad_norm': 201.0, 'learning_rate': 6.903969049265615e-05, 'epoch': 0.77}
 12%|█▏        | 605/5000 [2:45:49<19:07:08, 15.66s/it] 12%|█▏        | 606/5000 [2:46:01<17:33:42, 14.39s/it]                                                       {'loss': 78.364, 'grad_norm': 1004.0, 'learning_rate': 6.903429773070977e-05, 'epoch': 0.77}
 12%|█▏        | 606/5000 [2:46:01<17:33:42, 14.39s/it] 12%|█▏        | 607/5000 [2:46:22<20:01:40, 16.41s/it]                                                       {'loss': 45.2016, 'grad_norm': 466.0, 'learning_rate': 6.902889008100527e-05, 'epoch': 0.77}
 12%|█▏        | 607/5000 [2:46:22<20:01:40, 16.41s/it] 12%|█▏        | 608/5000 [2:46:33<17:53:38, 14.67s/it]                                                       {'loss': 51.2534, 'grad_norm': 233.0, 'learning_rate': 6.902346754590814e-05, 'epoch': 0.77}
 12%|█▏        | 608/5000 [2:46:33<17:53:38, 14.67s/it] 12%|█▏        | 609/5000 [2:46:57<21:20:16, 17.49s/it]                                                       {'loss': 67.5245, 'grad_norm': 392.0, 'learning_rate': 6.90180301277904e-05, 'epoch': 0.77}
 12%|█▏        | 609/5000 [2:46:57<21:20:16, 17.49s/it] 12%|█▏        | 610/5000 [2:47:09<19:29:37, 15.99s/it]                                                       {'loss': 94.3787, 'grad_norm': 1672.0, 'learning_rate': 6.901257782903054e-05, 'epoch': 0.77}
 12%|█▏        | 610/5000 [2:47:09<19:29:37, 15.99s/it] 12%|█▏        | 611/5000 [2:47:22<18:10:35, 14.91s/it]                                                       {'loss': 145.6071, 'grad_norm': 4192.0, 'learning_rate': 6.900711065201358e-05, 'epoch': 0.78}
 12%|█▏        | 611/5000 [2:47:22<18:10:35, 14.91s/it] 12%|█▏        | 612/5000 [2:47:34<17:07:31, 14.05s/it]                                                       {'loss': 164.2791, 'grad_norm': 1752.0, 'learning_rate': 6.900162859913107e-05, 'epoch': 0.78}
 12%|█▏        | 612/5000 [2:47:34<17:07:31, 14.05s/it] 12%|█▏        | 613/5000 [2:47:50<18:02:43, 14.81s/it]                                                       {'loss': 75.7513, 'grad_norm': 1032.0, 'learning_rate': 6.899613167278104e-05, 'epoch': 0.78}
 12%|█▏        | 613/5000 [2:47:50<18:02:43, 14.81s/it] 12%|█▏        | 614/5000 [2:48:00<16:21:25, 13.43s/it]                                                       {'loss': 106.7832, 'grad_norm': 1104.0, 'learning_rate': 6.899061987536802e-05, 'epoch': 0.78}
 12%|█▏        | 614/5000 [2:48:00<16:21:25, 13.43s/it] 12%|█▏        | 615/5000 [2:48:13<16:06:13, 13.22s/it]                                                       {'loss': 64.4418, 'grad_norm': 464.0, 'learning_rate': 6.898509320930306e-05, 'epoch': 0.78}
 12%|█▏        | 615/5000 [2:48:13<16:06:13, 13.22s/it] 12%|█▏        | 616/5000 [2:48:34<18:59:52, 15.60s/it]                                                       {'loss': 196.8559, 'grad_norm': 33536.0, 'learning_rate': 6.897955167700373e-05, 'epoch': 0.78}
 12%|█▏        | 616/5000 [2:48:34<18:59:52, 15.60s/it] 12%|█▏        | 617/5000 [2:48:49<18:35:38, 15.27s/it]                                                       {'loss': 119.4362, 'grad_norm': 1136.0, 'learning_rate': 6.897399528089407e-05, 'epoch': 0.78}
 12%|█▏        | 617/5000 [2:48:49<18:35:38, 15.27s/it] 12%|█▏        | 618/5000 [2:49:04<18:30:46, 15.21s/it]                                                       {'loss': 51.2586, 'grad_norm': 316.0, 'learning_rate': 6.896842402340461e-05, 'epoch': 0.78}
 12%|█▏        | 618/5000 [2:49:04<18:30:46, 15.21s/it] 12%|█▏        | 619/5000 [2:49:15<17:05:38, 14.05s/it]                                                       {'loss': 77.9445, 'grad_norm': 1400.0, 'learning_rate': 6.896283790697247e-05, 'epoch': 0.79}
 12%|█▏        | 619/5000 [2:49:15<17:05:38, 14.05s/it] 12%|█▏        | 620/5000 [2:49:30<17:15:10, 14.18s/it]                                                       {'loss': 77.1219, 'grad_norm': 1384.0, 'learning_rate': 6.895723693404115e-05, 'epoch': 0.79}
 12%|█▏        | 620/5000 [2:49:30<17:15:10, 14.18s/it] 12%|█▏        | 621/5000 [2:49:47<18:25:22, 15.15s/it]                                                       {'loss': 66.3816, 'grad_norm': 442.0, 'learning_rate': 6.895162110706073e-05, 'epoch': 0.79}
 12%|█▏        | 621/5000 [2:49:47<18:25:22, 15.15s/it] 12%|█▏        | 622/5000 [2:49:58<16:51:49, 13.87s/it]                                                       {'loss': 53.9367, 'grad_norm': 486.0, 'learning_rate': 6.894599042848777e-05, 'epoch': 0.79}
 12%|█▏        | 622/5000 [2:49:58<16:51:49, 13.87s/it] 12%|█▏        | 623/5000 [2:50:10<16:21:13, 13.45s/it]                                                       {'loss': 71.9662, 'grad_norm': 3296.0, 'learning_rate': 6.89403449007853e-05, 'epoch': 0.79}
 12%|█▏        | 623/5000 [2:50:10<16:21:13, 13.45s/it] 12%|█▏        | 624/5000 [2:50:35<20:16:55, 16.69s/it]                                                       {'loss': 93.3182, 'grad_norm': 1528.0, 'learning_rate': 6.893468452642289e-05, 'epoch': 0.79}
 12%|█▏        | 624/5000 [2:50:35<20:16:55, 16.69s/it] 12%|█▎        | 625/5000 [2:50:47<18:52:20, 15.53s/it]                                                       {'loss': 57.7178, 'grad_norm': 524.0, 'learning_rate': 6.892900930787656e-05, 'epoch': 0.79}
 12%|█▎        | 625/5000 [2:50:48<18:52:20, 15.53s/it] 13%|█▎        | 626/5000 [2:51:12<22:14:42, 18.31s/it]                                                       {'loss': 50.177, 'grad_norm': 1856.0, 'learning_rate': 6.892331924762885e-05, 'epoch': 0.79}
 13%|█▎        | 626/5000 [2:51:12<22:14:42, 18.31s/it] 13%|█▎        | 627/5000 [2:51:28<21:27:40, 17.67s/it]                                                       {'loss': 45.182, 'grad_norm': 201.0, 'learning_rate': 6.891761434816878e-05, 'epoch': 0.8}
 13%|█▎        | 627/5000 [2:51:28<21:27:40, 17.67s/it] 13%|█▎        | 628/5000 [2:51:40<19:18:51, 15.90s/it]                                                       {'loss': 56.2191, 'grad_norm': 196.0, 'learning_rate': 6.891189461199189e-05, 'epoch': 0.8}
 13%|█▎        | 628/5000 [2:51:40<19:18:51, 15.90s/it] 13%|█▎        | 629/5000 [2:51:53<18:20:04, 15.10s/it]                                                       {'loss': 46.5551, 'grad_norm': 370.0, 'learning_rate': 6.890616004160015e-05, 'epoch': 0.8}
 13%|█▎        | 629/5000 [2:51:53<18:20:04, 15.10s/it] 13%|█▎        | 630/5000 [2:52:13<19:53:29, 16.39s/it]                                                       {'loss': 55.3382, 'grad_norm': 628.0, 'learning_rate': 6.890041063950208e-05, 'epoch': 0.8}
 13%|█▎        | 630/5000 [2:52:13<19:53:29, 16.39s/it] 13%|█▎        | 631/5000 [2:52:26<18:49:42, 15.51s/it]                                                       {'loss': 86.1594, 'grad_norm': 3872.0, 'learning_rate': 6.889464640821266e-05, 'epoch': 0.8}
 13%|█▎        | 631/5000 [2:52:26<18:49:42, 15.51s/it] 13%|█▎        | 632/5000 [2:52:36<16:50:59, 13.89s/it]                                                       {'loss': 46.4409, 'grad_norm': 227.0, 'learning_rate': 6.888886735025337e-05, 'epoch': 0.8}
 13%|█▎        | 632/5000 [2:52:36<16:50:59, 13.89s/it] 13%|█▎        | 633/5000 [2:52:58<19:41:17, 16.23s/it]                                                       {'loss': 37.6671, 'grad_norm': 103.5, 'learning_rate': 6.888307346815215e-05, 'epoch': 0.8}
 13%|█▎        | 633/5000 [2:52:58<19:41:17, 16.23s/it] 13%|█▎        | 634/5000 [2:53:10<18:14:24, 15.04s/it]                                                       {'loss': 46.748, 'grad_norm': 235.0, 'learning_rate': 6.887726476444345e-05, 'epoch': 0.81}
 13%|█▎        | 634/5000 [2:53:10<18:14:24, 15.04s/it] 13%|█▎        | 635/5000 [2:53:21<16:28:32, 13.59s/it]                                                       {'loss': 44.3351, 'grad_norm': 106.5, 'learning_rate': 6.887144124166817e-05, 'epoch': 0.81}
 13%|█▎        | 635/5000 [2:53:21<16:28:32, 13.59s/it] 13%|█▎        | 636/5000 [2:53:37<17:23:02, 14.34s/it]                                                       {'loss': 49.9099, 'grad_norm': 784.0, 'learning_rate': 6.886560290237376e-05, 'epoch': 0.81}
 13%|█▎        | 636/5000 [2:53:37<17:23:02, 14.34s/it] 13%|█▎        | 637/5000 [2:53:53<17:58:03, 14.83s/it]                                                       {'loss': 36.9409, 'grad_norm': 209.0, 'learning_rate': 6.885974974911408e-05, 'epoch': 0.81}
 13%|█▎        | 637/5000 [2:53:53<17:58:03, 14.83s/it] 13%|█▎        | 638/5000 [2:54:08<18:14:08, 15.05s/it]                                                       {'loss': 49.0404, 'grad_norm': 3392.0, 'learning_rate': 6.885388178444952e-05, 'epoch': 0.81}
 13%|█▎        | 638/5000 [2:54:08<18:14:08, 15.05s/it] 13%|█▎        | 639/5000 [2:54:21<17:22:26, 14.34s/it]                                                       {'loss': 49.8558, 'grad_norm': 442.0, 'learning_rate': 6.88479990109469e-05, 'epoch': 0.81}
 13%|█▎        | 639/5000 [2:54:21<17:22:26, 14.34s/it] 13%|█▎        | 640/5000 [2:54:34<16:44:49, 13.83s/it]                                                       {'loss': 37.6502, 'grad_norm': 82.5, 'learning_rate': 6.884210143117954e-05, 'epoch': 0.81}
 13%|█▎        | 640/5000 [2:54:34<16:44:49, 13.83s/it] 13%|█▎        | 641/5000 [2:54:45<15:49:59, 13.08s/it]                                                       {'loss': 53.0728, 'grad_norm': 952.0, 'learning_rate': 6.883618904772728e-05, 'epoch': 0.81}
 13%|█▎        | 641/5000 [2:54:45<15:49:59, 13.08s/it] 13%|█▎        | 642/5000 [2:55:00<16:31:02, 13.64s/it]                                                       {'loss': 36.4664, 'grad_norm': 69.5, 'learning_rate': 6.883026186317637e-05, 'epoch': 0.82}
 13%|█▎        | 642/5000 [2:55:00<16:31:02, 13.64s/it] 13%|█▎        | 643/5000 [2:55:16<17:16:20, 14.27s/it]                                                       {'loss': 41.4799, 'grad_norm': 96.5, 'learning_rate': 6.882431988011955e-05, 'epoch': 0.82}
 13%|█▎        | 643/5000 [2:55:16<17:16:20, 14.27s/it] 13%|█▎        | 644/5000 [2:55:31<17:38:15, 14.58s/it]                                                       {'loss': 40.3894, 'grad_norm': 105.5, 'learning_rate': 6.881836310115608e-05, 'epoch': 0.82}
 13%|█▎        | 644/5000 [2:55:31<17:38:15, 14.58s/it] 13%|█▎        | 645/5000 [2:55:47<18:22:35, 15.19s/it]                                                       {'loss': 42.0552, 'grad_norm': 334.0, 'learning_rate': 6.881239152889164e-05, 'epoch': 0.82}
 13%|█▎        | 645/5000 [2:55:47<18:22:35, 15.19s/it] 13%|█▎        | 646/5000 [2:56:00<17:15:58, 14.28s/it]                                                       {'loss': 44.8291, 'grad_norm': 187.0, 'learning_rate': 6.880640516593839e-05, 'epoch': 0.82}
 13%|█▎        | 646/5000 [2:56:00<17:15:58, 14.28s/it] 13%|█▎        | 647/5000 [2:56:18<18:54:16, 15.63s/it]                                                       {'loss': 37.2274, 'grad_norm': 86.5, 'learning_rate': 6.880040401491498e-05, 'epoch': 0.82}
 13%|█▎        | 647/5000 [2:56:18<18:54:16, 15.63s/it] 13%|█▎        | 648/5000 [2:56:32<18:06:07, 14.97s/it]                                                       {'loss': 42.7117, 'grad_norm': 434.0, 'learning_rate': 6.879438807844653e-05, 'epoch': 0.82}
 13%|█▎        | 648/5000 [2:56:32<18:06:07, 14.97s/it] 13%|█▎        | 649/5000 [2:56:44<17:06:24, 14.15s/it]                                                       {'loss': 54.6753, 'grad_norm': 1720.0, 'learning_rate': 6.878835735916458e-05, 'epoch': 0.82}
 13%|█▎        | 649/5000 [2:56:44<17:06:24, 14.15s/it] 13%|█▎        | 650/5000 [2:56:58<17:03:51, 14.12s/it]                                                       {'loss': 48.6965, 'grad_norm': 344.0, 'learning_rate': 6.87823118597072e-05, 'epoch': 0.83}
 13%|█▎        | 650/5000 [2:56:58<17:03:51, 14.12s/it] 13%|█▎        | 651/5000 [2:57:20<19:55:45, 16.50s/it]                                                       {'loss': 38.8336, 'grad_norm': 172.0, 'learning_rate': 6.877625158271888e-05, 'epoch': 0.83}
 13%|█▎        | 651/5000 [2:57:20<19:55:45, 16.50s/it] 13%|█▎        | 652/5000 [2:57:36<19:30:52, 16.16s/it]                                                       {'loss': 42.0488, 'grad_norm': 478.0, 'learning_rate': 6.877017653085061e-05, 'epoch': 0.83}
 13%|█▎        | 652/5000 [2:57:36<19:30:52, 16.16s/it] 13%|█▎        | 653/5000 [2:57:56<21:14:01, 17.58s/it]                                                       {'loss': 35.0623, 'grad_norm': 82.5, 'learning_rate': 6.876408670675981e-05, 'epoch': 0.83}
 13%|█▎        | 653/5000 [2:57:56<21:14:01, 17.58s/it] 13%|█▎        | 654/5000 [2:58:13<20:46:56, 17.22s/it]                                                       {'loss': 36.6521, 'grad_norm': 78.0, 'learning_rate': 6.875798211311037e-05, 'epoch': 0.83}
 13%|█▎        | 654/5000 [2:58:13<20:46:56, 17.22s/it] 13%|█▎        | 655/5000 [2:58:25<19:03:30, 15.79s/it]                                                       {'loss': 67.7842, 'grad_norm': 2384.0, 'learning_rate': 6.875186275257266e-05, 'epoch': 0.83}
 13%|█▎        | 655/5000 [2:58:25<19:03:30, 15.79s/it] 13%|█▎        | 656/5000 [2:58:38<17:52:33, 14.81s/it]                                                       {'loss': 43.1702, 'grad_norm': 1256.0, 'learning_rate': 6.87457286278235e-05, 'epoch': 0.83}
 13%|█▎        | 656/5000 [2:58:38<17:52:33, 14.81s/it] 13%|█▎        | 657/5000 [2:58:51<17:09:10, 14.22s/it]                                                       {'loss': 47.6692, 'grad_norm': 112.5, 'learning_rate': 6.873957974154614e-05, 'epoch': 0.83}
 13%|█▎        | 657/5000 [2:58:51<17:09:10, 14.22s/it] 13%|█▎        | 658/5000 [2:59:06<17:29:43, 14.51s/it]                                                       {'loss': 39.0232, 'grad_norm': 95.5, 'learning_rate': 6.873341609643035e-05, 'epoch': 0.84}
 13%|█▎        | 658/5000 [2:59:06<17:29:43, 14.51s/it] 13%|█▎        | 659/5000 [2:59:29<20:32:35, 17.04s/it]                                                       {'loss': 37.2375, 'grad_norm': 88.5, 'learning_rate': 6.872723769517228e-05, 'epoch': 0.84}
 13%|█▎        | 659/5000 [2:59:29<20:32:35, 17.04s/it] 13%|█▎        | 660/5000 [2:59:50<22:00:11, 18.25s/it]                                                       {'loss': 80.8446, 'grad_norm': 612.0, 'learning_rate': 6.87210445404746e-05, 'epoch': 0.84}
 13%|█▎        | 660/5000 [2:59:50<22:00:11, 18.25s/it] 13%|█▎        | 661/5000 [3:00:11<22:57:59, 19.05s/it]                                                       {'loss': 43.2099, 'grad_norm': 450.0, 'learning_rate': 6.871483663504639e-05, 'epoch': 0.84}
 13%|█▎        | 661/5000 [3:00:11<22:57:59, 19.05s/it] 13%|█▎        | 662/5000 [3:00:28<22:24:41, 18.60s/it]                                                       {'loss': 49.6413, 'grad_norm': 223.0, 'learning_rate': 6.87086139816032e-05, 'epoch': 0.84}
 13%|█▎        | 662/5000 [3:00:28<22:24:41, 18.60s/it] 13%|█▎        | 663/5000 [3:00:42<20:28:19, 16.99s/it]                                                       {'loss': 40.2047, 'grad_norm': 150.0, 'learning_rate': 6.870237658286703e-05, 'epoch': 0.84}
 13%|█▎        | 663/5000 [3:00:42<20:28:19, 16.99s/it] 13%|█▎        | 664/5000 [3:00:55<19:12:23, 15.95s/it]                                                       {'loss': 34.5851, 'grad_norm': 123.5, 'learning_rate': 6.869612444156634e-05, 'epoch': 0.84}
 13%|█▎        | 664/5000 [3:00:55<19:12:23, 15.95s/it] 13%|█▎        | 665/5000 [3:01:09<18:26:05, 15.31s/it]                                                       {'loss': 40.4465, 'grad_norm': 1856.0, 'learning_rate': 6.868985756043601e-05, 'epoch': 0.84}
 13%|█▎        | 665/5000 [3:01:09<18:26:05, 15.31s/it] 13%|█▎        | 666/5000 [3:01:22<17:35:30, 14.61s/it]                                                       {'loss': 37.2435, 'grad_norm': 76.5, 'learning_rate': 6.868357594221743e-05, 'epoch': 0.85}
 13%|█▎        | 666/5000 [3:01:22<17:35:30, 14.61s/it] 13%|█▎        | 667/5000 [3:01:35<17:02:24, 14.16s/it]                                                       {'loss': 40.1664, 'grad_norm': 143.0, 'learning_rate': 6.867727958965833e-05, 'epoch': 0.85}
 13%|█▎        | 667/5000 [3:01:35<17:02:24, 14.16s/it] 13%|█▎        | 668/5000 [3:01:49<16:54:56, 14.06s/it]                                                       {'loss': 35.4945, 'grad_norm': 55.5, 'learning_rate': 6.8670968505513e-05, 'epoch': 0.85}
 13%|█▎        | 668/5000 [3:01:49<16:54:56, 14.06s/it] 13%|█▎        | 669/5000 [3:02:05<17:32:52, 14.59s/it]                                                       {'loss': 35.7216, 'grad_norm': 80.5, 'learning_rate': 6.866464269254211e-05, 'epoch': 0.85}
 13%|█▎        | 669/5000 [3:02:05<17:32:52, 14.59s/it] 13%|█▎        | 670/5000 [3:02:20<17:48:11, 14.80s/it]                                                       {'loss': 32.2149, 'grad_norm': 55.5, 'learning_rate': 6.865830215351276e-05, 'epoch': 0.85}
 13%|█▎        | 670/5000 [3:02:20<17:48:11, 14.80s/it] 13%|█▎        | 671/5000 [3:02:39<19:24:05, 16.13s/it]                                                       {'loss': 31.9609, 'grad_norm': 55.5, 'learning_rate': 6.865194689119855e-05, 'epoch': 0.85}
 13%|█▎        | 671/5000 [3:02:39<19:24:05, 16.13s/it] 13%|█▎        | 672/5000 [3:02:51<17:46:08, 14.78s/it]                                                       {'loss': 38.6098, 'grad_norm': 115.0, 'learning_rate': 6.864557690837946e-05, 'epoch': 0.85}
 13%|█▎        | 672/5000 [3:02:51<17:46:08, 14.78s/it] 13%|█▎        | 673/5000 [3:03:05<17:23:55, 14.48s/it]                                                       {'loss': 33.2901, 'grad_norm': 103.0, 'learning_rate': 6.863919220784196e-05, 'epoch': 0.85}
 13%|█▎        | 673/5000 [3:03:05<17:23:55, 14.48s/it] 13%|█▎        | 674/5000 [3:03:15<15:55:29, 13.25s/it]                                                       {'loss': 38.4732, 'grad_norm': 79.0, 'learning_rate': 6.863279279237892e-05, 'epoch': 0.86}
 13%|█▎        | 674/5000 [3:03:15<15:55:29, 13.25s/it] 14%|█▎        | 675/5000 [3:03:27<15:28:47, 12.88s/it]                                                       {'loss': 34.3786, 'grad_norm': 101.0, 'learning_rate': 6.862637866478969e-05, 'epoch': 0.86}
 14%|█▎        | 675/5000 [3:03:27<15:28:47, 12.88s/it] 14%|█▎        | 676/5000 [3:03:41<15:50:39, 13.19s/it]                                                       {'loss': 35.7391, 'grad_norm': 66.5, 'learning_rate': 6.861994982787998e-05, 'epoch': 0.86}
 14%|█▎        | 676/5000 [3:03:41<15:50:39, 13.19s/it] 14%|█▎        | 677/5000 [3:04:09<21:06:08, 17.57s/it]                                                       {'loss': 33.5724, 'grad_norm': 90.0, 'learning_rate': 6.861350628446201e-05, 'epoch': 0.86}
 14%|█▎        | 677/5000 [3:04:09<21:06:08, 17.57s/it] 14%|█▎        | 678/5000 [3:04:22<19:32:59, 16.28s/it]                                                       {'loss': 41.0832, 'grad_norm': 162.0, 'learning_rate': 6.860704803735441e-05, 'epoch': 0.86}
 14%|█▎        | 678/5000 [3:04:22<19:32:59, 16.28s/it] 14%|█▎        | 679/5000 [3:04:33<17:34:30, 14.64s/it]                                                       {'loss': 44.4977, 'grad_norm': 173.0, 'learning_rate': 6.860057508938223e-05, 'epoch': 0.86}
 14%|█▎        | 679/5000 [3:04:33<17:34:30, 14.64s/it] 14%|█▎        | 680/5000 [3:04:46<16:54:04, 14.08s/it]                                                       {'loss': 35.3633, 'grad_norm': 54.0, 'learning_rate': 6.859408744337694e-05, 'epoch': 0.86}
 14%|█▎        | 680/5000 [3:04:46<16:54:04, 14.08s/it] 14%|█▎        | 681/5000 [3:04:57<15:48:02, 13.17s/it]                                                       {'loss': 32.6028, 'grad_norm': 48.0, 'learning_rate': 6.858758510217649e-05, 'epoch': 0.86}
 14%|█▎        | 681/5000 [3:04:57<15:48:02, 13.17s/it] 14%|█▎        | 682/5000 [3:05:17<18:14:41, 15.21s/it]                                                       {'loss': 39.3934, 'grad_norm': 316.0, 'learning_rate': 6.85810680686252e-05, 'epoch': 0.87}
 14%|█▎        | 682/5000 [3:05:17<18:14:41, 15.21s/it] 14%|█▎        | 683/5000 [3:05:35<19:27:23, 16.22s/it]                                                       {'loss': 39.0762, 'grad_norm': 488.0, 'learning_rate': 6.857453634557385e-05, 'epoch': 0.87}
 14%|█▎        | 683/5000 [3:05:35<19:27:23, 16.22s/it] 14%|█▎        | 684/5000 [3:05:45<17:16:41, 14.41s/it]                                                       {'loss': 41.6261, 'grad_norm': 201.0, 'learning_rate': 6.856798993587964e-05, 'epoch': 0.87}
 14%|█▎        | 684/5000 [3:05:45<17:16:41, 14.41s/it] 14%|█▎        | 685/5000 [3:05:59<17:09:18, 14.31s/it]                                                       {'loss': 77.3531, 'grad_norm': 1288.0, 'learning_rate': 6.856142884240618e-05, 'epoch': 0.87}
 14%|█▎        | 685/5000 [3:05:59<17:09:18, 14.31s/it] 14%|█▎        | 686/5000 [3:06:12<16:37:05, 13.87s/it]                                                       {'loss': 33.7901, 'grad_norm': 64.0, 'learning_rate': 6.855485306802352e-05, 'epoch': 0.87}
 14%|█▎        | 686/5000 [3:06:12<16:37:05, 13.87s/it] 14%|█▎        | 687/5000 [3:06:27<16:56:20, 14.14s/it]                                                       {'loss': 32.4943, 'grad_norm': 102.0, 'learning_rate': 6.854826261560814e-05, 'epoch': 0.87}
 14%|█▎        | 687/5000 [3:06:27<16:56:20, 14.14s/it] 14%|█▍        | 688/5000 [3:06:42<17:22:20, 14.50s/it]                                                       {'loss': 30.4253, 'grad_norm': 65.5, 'learning_rate': 6.854165748804293e-05, 'epoch': 0.87}
 14%|█▍        | 688/5000 [3:06:42<17:22:20, 14.50s/it] 14%|█▍        | 689/5000 [3:06:55<16:49:47, 14.05s/it]                                                       {'loss': 41.0657, 'grad_norm': 165.0, 'learning_rate': 6.853503768821718e-05, 'epoch': 0.87}
 14%|█▍        | 689/5000 [3:06:55<16:49:47, 14.05s/it] 14%|█▍        | 690/5000 [3:07:18<19:59:28, 16.70s/it]                                                       {'loss': 35.2351, 'grad_norm': 93.5, 'learning_rate': 6.852840321902661e-05, 'epoch': 0.88}
 14%|█▍        | 690/5000 [3:07:18<19:59:28, 16.70s/it] 14%|█▍        | 691/5000 [3:07:40<21:39:39, 18.10s/it]                                                       {'loss': 35.4861, 'grad_norm': 77.0, 'learning_rate': 6.852175408337338e-05, 'epoch': 0.88}
 14%|█▍        | 691/5000 [3:07:40<21:39:39, 18.10s/it] 14%|█▍        | 692/5000 [3:08:03<23:29:54, 19.64s/it]                                                       {'loss': 36.1641, 'grad_norm': 1072.0, 'learning_rate': 6.851509028416605e-05, 'epoch': 0.88}
 14%|█▍        | 692/5000 [3:08:03<23:29:54, 19.64s/it] 14%|█▍        | 693/5000 [3:08:21<22:49:44, 19.08s/it]                                                       {'loss': 33.6188, 'grad_norm': 266.0, 'learning_rate': 6.850841182431958e-05, 'epoch': 0.88}
 14%|█▍        | 693/5000 [3:08:21<22:49:44, 19.08s/it] 14%|█▍        | 694/5000 [3:08:32<20:08:39, 16.84s/it]                                                       {'loss': 86.0412, 'grad_norm': 1392.0, 'learning_rate': 6.850171870675534e-05, 'epoch': 0.88}
 14%|█▍        | 694/5000 [3:08:32<20:08:39, 16.84s/it] 14%|█▍        | 695/5000 [3:08:44<18:26:49, 15.43s/it]                                                       {'loss': 36.8657, 'grad_norm': 298.0, 'learning_rate': 6.849501093440116e-05, 'epoch': 0.88}
 14%|█▍        | 695/5000 [3:08:44<18:26:49, 15.43s/it] 14%|█▍        | 696/5000 [3:08:55<16:37:42, 13.91s/it]                                                       {'loss': 93.0616, 'grad_norm': 290.0, 'learning_rate': 6.848828851019123e-05, 'epoch': 0.88}
 14%|█▍        | 696/5000 [3:08:55<16:37:42, 13.91s/it] 14%|█▍        | 697/5000 [3:09:06<15:47:17, 13.21s/it]                                                       {'loss': 36.5704, 'grad_norm': 90.0, 'learning_rate': 6.848155143706619e-05, 'epoch': 0.89}
 14%|█▍        | 697/5000 [3:09:06<15:47:17, 13.21s/it] 14%|█▍        | 698/5000 [3:09:20<16:04:29, 13.45s/it]                                                       {'loss': 42.8188, 'grad_norm': 157.0, 'learning_rate': 6.847479971797302e-05, 'epoch': 0.89}
 14%|█▍        | 698/5000 [3:09:20<16:04:29, 13.45s/it] 14%|█▍        | 699/5000 [3:09:33<15:53:05, 13.30s/it]                                                       {'loss': 34.3312, 'grad_norm': 270.0, 'learning_rate': 6.846803335586518e-05, 'epoch': 0.89}
 14%|█▍        | 699/5000 [3:09:33<15:53:05, 13.30s/it] 14%|█▍        | 700/5000 [3:09:48<16:34:11, 13.87s/it]                                                       {'loss': 33.2762, 'grad_norm': 67.0, 'learning_rate': 6.846125235370252e-05, 'epoch': 0.89}
 14%|█▍        | 700/5000 [3:09:48<16:34:11, 13.87s/it] 14%|█▍        | 701/5000 [3:10:03<16:49:25, 14.09s/it]                                                       {'loss': 31.0767, 'grad_norm': 85.0, 'learning_rate': 6.845445671445124e-05, 'epoch': 0.89}
 14%|█▍        | 701/5000 [3:10:03<16:49:25, 14.09s/it] 14%|█▍        | 702/5000 [3:10:24<19:21:34, 16.22s/it]                                                       {'loss': 37.9476, 'grad_norm': 370.0, 'learning_rate': 6.844764644108402e-05, 'epoch': 0.89}
 14%|█▍        | 702/5000 [3:10:24<19:21:34, 16.22s/it] 14%|█▍        | 703/5000 [3:10:41<19:32:41, 16.37s/it]                                                       {'loss': 34.2461, 'grad_norm': 70.0, 'learning_rate': 6.84408215365799e-05, 'epoch': 0.89}
 14%|█▍        | 703/5000 [3:10:41<19:32:41, 16.37s/it] 14%|█▍        | 704/5000 [3:11:07<23:08:35, 19.39s/it]                                                       {'loss': 31.0671, 'grad_norm': 63.0, 'learning_rate': 6.84339820039243e-05, 'epoch': 0.89}
 14%|█▍        | 704/5000 [3:11:07<23:08:35, 19.39s/it] 14%|█▍        | 705/5000 [3:11:20<20:48:26, 17.44s/it]                                                       {'loss': 34.4408, 'grad_norm': 97.5, 'learning_rate': 6.842712784610909e-05, 'epoch': 0.9}
 14%|█▍        | 705/5000 [3:11:20<20:48:26, 17.44s/it] 14%|█▍        | 706/5000 [3:11:32<18:36:24, 15.60s/it]                                                       {'loss': 35.5118, 'grad_norm': 166.0, 'learning_rate': 6.84202590661325e-05, 'epoch': 0.9}
 14%|█▍        | 706/5000 [3:11:32<18:36:24, 15.60s/it] 14%|█▍        | 707/5000 [3:11:56<21:34:39, 18.09s/it]                                                       {'loss': 37.3454, 'grad_norm': 1072.0, 'learning_rate': 6.841337566699916e-05, 'epoch': 0.9}
 14%|█▍        | 707/5000 [3:11:56<21:34:39, 18.09s/it] 14%|█▍        | 708/5000 [3:12:11<20:31:05, 17.21s/it]                                                       {'loss': 43.1206, 'grad_norm': 474.0, 'learning_rate': 6.840647765172013e-05, 'epoch': 0.9}
 14%|█▍        | 708/5000 [3:12:11<20:31:05, 17.21s/it] 14%|█▍        | 709/5000 [3:12:27<20:03:32, 16.83s/it]                                                       {'loss': 52.0839, 'grad_norm': 2480.0, 'learning_rate': 6.839956502331281e-05, 'epoch': 0.9}
 14%|█▍        | 709/5000 [3:12:27<20:03:32, 16.83s/it] 14%|█▍        | 710/5000 [3:12:40<18:46:06, 15.75s/it]                                                       {'loss': 33.8938, 'grad_norm': 126.5, 'learning_rate': 6.839263778480104e-05, 'epoch': 0.9}
 14%|█▍        | 710/5000 [3:12:40<18:46:06, 15.75s/it] 14%|█▍        | 711/5000 [3:13:01<20:38:23, 17.32s/it]                                                       {'loss': 36.0118, 'grad_norm': 230.0, 'learning_rate': 6.8385695939215e-05, 'epoch': 0.9}
 14%|█▍        | 711/5000 [3:13:01<20:38:23, 17.32s/it] 14%|█▍        | 712/5000 [3:13:15<19:30:36, 16.38s/it]                                                       {'loss': 40.9445, 'grad_norm': 1088.0, 'learning_rate': 6.837873948959132e-05, 'epoch': 0.9}
 14%|█▍        | 712/5000 [3:13:15<19:30:36, 16.38s/it] 14%|█▍        | 713/5000 [3:13:26<17:31:31, 14.72s/it]                                                       {'loss': 34.7389, 'grad_norm': 241.0, 'learning_rate': 6.837176843897298e-05, 'epoch': 0.91}
 14%|█▍        | 713/5000 [3:13:26<17:31:31, 14.72s/it] 14%|█▍        | 714/5000 [3:13:38<16:39:45, 14.00s/it]                                                       {'loss': 34.5615, 'grad_norm': 304.0, 'learning_rate': 6.836478279040931e-05, 'epoch': 0.91}
 14%|█▍        | 714/5000 [3:13:38<16:39:45, 14.00s/it] 14%|█▍        | 715/5000 [3:14:02<20:02:38, 16.84s/it]                                                       {'loss': 33.6125, 'grad_norm': 187.0, 'learning_rate': 6.835778254695615e-05, 'epoch': 0.91}
 14%|█▍        | 715/5000 [3:14:02<20:02:38, 16.84s/it] 14%|█▍        | 716/5000 [3:14:17<19:40:50, 16.54s/it]                                                       {'loss': 36.3706, 'grad_norm': 80.5, 'learning_rate': 6.835076771167558e-05, 'epoch': 0.91}
 14%|█▍        | 716/5000 [3:14:17<19:40:50, 16.54s/it] 14%|█▍        | 717/5000 [3:14:39<21:20:49, 17.94s/it]                                                       {'loss': 35.3649, 'grad_norm': 1816.0, 'learning_rate': 6.834373828763615e-05, 'epoch': 0.91}
 14%|█▍        | 717/5000 [3:14:39<21:20:49, 17.94s/it] 14%|█▍        | 718/5000 [3:14:56<21:13:48, 17.85s/it]                                                       {'loss': 33.2922, 'grad_norm': 105.5, 'learning_rate': 6.833669427791278e-05, 'epoch': 0.91}
 14%|█▍        | 718/5000 [3:14:56<21:13:48, 17.85s/it] 14%|█▍        | 719/5000 [3:15:08<19:04:05, 16.03s/it]                                                       {'loss': 42.4313, 'grad_norm': 326.0, 'learning_rate': 6.832963568558675e-05, 'epoch': 0.91}
 14%|█▍        | 719/5000 [3:15:08<19:04:05, 16.03s/it] 14%|█▍        | 720/5000 [3:15:23<18:41:08, 15.72s/it]                                                       {'loss': 31.5019, 'grad_norm': 60.25, 'learning_rate': 6.832256251374571e-05, 'epoch': 0.91}
 14%|█▍        | 720/5000 [3:15:23<18:41:08, 15.72s/it] 14%|█▍        | 721/5000 [3:15:34<17:06:57, 14.40s/it]                                                       {'loss': 35.7173, 'grad_norm': 62.25, 'learning_rate': 6.831547476548372e-05, 'epoch': 0.92}
 14%|█▍        | 721/5000 [3:15:34<17:06:57, 14.40s/it] 14%|█▍        | 722/5000 [3:15:52<18:18:47, 15.41s/it]                                                       {'loss': 42.2102, 'grad_norm': 240.0, 'learning_rate': 6.830837244390122e-05, 'epoch': 0.92}
 14%|█▍        | 722/5000 [3:15:52<18:18:47, 15.41s/it] 14%|█▍        | 723/5000 [3:16:14<20:32:49, 17.29s/it]                                                       {'loss': 33.8593, 'grad_norm': 51.0, 'learning_rate': 6.830125555210497e-05, 'epoch': 0.92}
 14%|█▍        | 723/5000 [3:16:14<20:32:49, 17.29s/it] 14%|█▍        | 724/5000 [3:16:28<19:27:13, 16.38s/it]                                                       {'loss': 32.8374, 'grad_norm': 63.5, 'learning_rate': 6.829412409320818e-05, 'epoch': 0.92}
 14%|█▍        | 724/5000 [3:16:28<19:27:13, 16.38s/it] 14%|█▍        | 725/5000 [3:16:43<19:00:21, 16.01s/it]                                                       {'loss': 43.2543, 'grad_norm': 332.0, 'learning_rate': 6.828697807033038e-05, 'epoch': 0.92}
 14%|█▍        | 725/5000 [3:16:43<19:00:21, 16.01s/it] 15%|█▍        | 726/5000 [3:17:04<20:51:06, 17.56s/it]                                                       {'loss': 36.0088, 'grad_norm': 173.0, 'learning_rate': 6.827981748659745e-05, 'epoch': 0.92}
 15%|█▍        | 726/5000 [3:17:04<20:51:06, 17.56s/it] 15%|█▍        | 727/5000 [3:17:17<19:00:34, 16.02s/it]                                                       {'loss': 36.3202, 'grad_norm': 370.0, 'learning_rate': 6.82726423451417e-05, 'epoch': 0.92}
 15%|█▍        | 727/5000 [3:17:17<19:00:34, 16.02s/it] 15%|█▍        | 728/5000 [3:17:30<18:07:52, 15.28s/it]                                                       {'loss': 51.8257, 'grad_norm': 284.0, 'learning_rate': 6.82654526491018e-05, 'epoch': 0.92}
 15%|█▍        | 728/5000 [3:17:30<18:07:52, 15.28s/it] 15%|█▍        | 729/5000 [3:17:43<17:12:16, 14.50s/it]                                                       {'loss': 34.06, 'grad_norm': 77.0, 'learning_rate': 6.825824840162273e-05, 'epoch': 0.93}
 15%|█▍        | 729/5000 [3:17:43<17:12:16, 14.50s/it] 15%|█▍        | 730/5000 [3:17:55<16:22:48, 13.81s/it]                                                       {'loss': 37.3888, 'grad_norm': 98.0, 'learning_rate': 6.82510296058559e-05, 'epoch': 0.93}
 15%|█▍        | 730/5000 [3:17:55<16:22:48, 13.81s/it] 15%|█▍        | 731/5000 [3:18:08<15:52:01, 13.38s/it]                                                       {'loss': 38.69, 'grad_norm': 195.0, 'learning_rate': 6.824379626495904e-05, 'epoch': 0.93}
 15%|█▍        | 731/5000 [3:18:08<15:52:01, 13.38s/it] 15%|█▍        | 732/5000 [3:18:23<16:23:40, 13.83s/it]                                                       {'loss': 36.0529, 'grad_norm': 402.0, 'learning_rate': 6.823654838209626e-05, 'epoch': 0.93}
 15%|█▍        | 732/5000 [3:18:23<16:23:40, 13.83s/it] 15%|█▍        | 733/5000 [3:18:43<18:39:16, 15.74s/it]                                                       {'loss': 34.2864, 'grad_norm': 83.5, 'learning_rate': 6.822928596043804e-05, 'epoch': 0.93}
 15%|█▍        | 733/5000 [3:18:43<18:39:16, 15.74s/it] 15%|█▍        | 734/5000 [3:19:02<19:45:51, 16.68s/it]                                                       {'loss': 35.9746, 'grad_norm': 314.0, 'learning_rate': 6.822200900316121e-05, 'epoch': 0.93}
 15%|█▍        | 734/5000 [3:19:02<19:45:51, 16.68s/it] 15%|█▍        | 735/5000 [3:19:13<17:49:35, 15.05s/it]                                                       {'loss': 35.91, 'grad_norm': 129.0, 'learning_rate': 6.821471751344895e-05, 'epoch': 0.93}
 15%|█▍        | 735/5000 [3:19:13<17:49:35, 15.05s/it] 15%|█▍        | 736/5000 [3:19:27<17:19:15, 14.62s/it]                                                       {'loss': 33.4707, 'grad_norm': 75.0, 'learning_rate': 6.82074114944908e-05, 'epoch': 0.93}
 15%|█▍        | 736/5000 [3:19:27<17:19:15, 14.62s/it] 15%|█▍        | 737/5000 [3:19:39<16:41:02, 14.09s/it]                                                       {'loss': 40.0651, 'grad_norm': 382.0, 'learning_rate': 6.820009094948268e-05, 'epoch': 0.94}
 15%|█▍        | 737/5000 [3:19:39<16:41:02, 14.09s/it] 15%|█▍        | 738/5000 [3:19:50<15:17:11, 12.91s/it]                                                       {'loss': 41.522, 'grad_norm': 520.0, 'learning_rate': 6.819275588162682e-05, 'epoch': 0.94}
 15%|█▍        | 738/5000 [3:19:50<15:17:11, 12.91s/it] 15%|█▍        | 739/5000 [3:20:10<17:52:21, 15.10s/it]                                                       {'loss': 34.0591, 'grad_norm': 59.25, 'learning_rate': 6.818540629413185e-05, 'epoch': 0.94}
 15%|█▍        | 739/5000 [3:20:10<17:52:21, 15.10s/it] 15%|█▍        | 740/5000 [3:20:23<17:07:49, 14.48s/it]                                                       {'loss': 32.5555, 'grad_norm': 123.0, 'learning_rate': 6.817804219021273e-05, 'epoch': 0.94}
 15%|█▍        | 740/5000 [3:20:23<17:07:49, 14.48s/it] 15%|█▍        | 741/5000 [3:20:35<16:20:40, 13.82s/it]                                                       {'loss': 44.0698, 'grad_norm': 182.0, 'learning_rate': 6.817066357309074e-05, 'epoch': 0.94}
 15%|█▍        | 741/5000 [3:20:35<16:20:40, 13.82s/it] 15%|█▍        | 742/5000 [3:20:45<15:02:26, 12.72s/it]                                                       {'loss': 35.7742, 'grad_norm': 588.0, 'learning_rate': 6.81632704459936e-05, 'epoch': 0.94}
 15%|█▍        | 742/5000 [3:20:45<15:02:26, 12.72s/it] 15%|█▍        | 743/5000 [3:21:00<15:40:34, 13.26s/it]                                                       {'loss': 34.6335, 'grad_norm': 106.5, 'learning_rate': 6.815586281215523e-05, 'epoch': 0.94}
 15%|█▍        | 743/5000 [3:21:00<15:40:34, 13.26s/it] 15%|█▍        | 744/5000 [3:21:14<16:11:38, 13.70s/it]                                                       {'loss': 29.5508, 'grad_norm': 48.75, 'learning_rate': 6.814844067481604e-05, 'epoch': 0.94}
 15%|█▍        | 744/5000 [3:21:14<16:11:38, 13.70s/it] 15%|█▍        | 745/5000 [3:21:29<16:26:12, 13.91s/it]                                                       {'loss': 43.3489, 'grad_norm': 3984.0, 'learning_rate': 6.814100403722272e-05, 'epoch': 0.95}
 15%|█▍        | 745/5000 [3:21:29<16:26:12, 13.91s/it] 15%|█▍        | 746/5000 [3:21:49<18:34:52, 15.72s/it]                                                       {'loss': 39.7064, 'grad_norm': 476.0, 'learning_rate': 6.813355290262828e-05, 'epoch': 0.95}
 15%|█▍        | 746/5000 [3:21:49<18:34:52, 15.72s/it] 15%|█▍        | 747/5000 [3:22:01<17:10:51, 14.54s/it]                                                       {'loss': 49.988, 'grad_norm': 9152.0, 'learning_rate': 6.812608727429213e-05, 'epoch': 0.95}
 15%|█▍        | 747/5000 [3:22:01<17:10:51, 14.54s/it] 15%|█▍        | 748/5000 [3:22:12<15:57:45, 13.51s/it]                                                       {'loss': 39.5004, 'grad_norm': 528.0, 'learning_rate': 6.811860715547997e-05, 'epoch': 0.95}
 15%|█▍        | 748/5000 [3:22:12<15:57:45, 13.51s/it] 15%|█▍        | 749/5000 [3:22:32<18:26:21, 15.62s/it]                                                       {'loss': 35.438, 'grad_norm': 5536.0, 'learning_rate': 6.811111254946386e-05, 'epoch': 0.95}
 15%|█▍        | 749/5000 [3:22:32<18:26:21, 15.62s/it] 15%|█▌        | 750/5000 [3:22:45<17:23:22, 14.73s/it]                                                       {'loss': 38.2177, 'grad_norm': 153.0, 'learning_rate': 6.81036034595222e-05, 'epoch': 0.95}
 15%|█▌        | 750/5000 [3:22:45<17:23:22, 14.73s/it] 15%|█▌        | 751/5000 [3:23:01<17:52:40, 15.15s/it]                                                       {'loss': 43.353, 'grad_norm': 189.0, 'learning_rate': 6.809607988893974e-05, 'epoch': 0.95}
 15%|█▌        | 751/5000 [3:23:01<17:52:40, 15.15s/it] 15%|█▌        | 752/5000 [3:23:32<23:37:41, 20.02s/it]                                                       {'loss': 32.2487, 'grad_norm': 73.0, 'learning_rate': 6.80885418410075e-05, 'epoch': 0.95}
 15%|█▌        | 752/5000 [3:23:32<23:37:41, 20.02s/it] 15%|█▌        | 753/5000 [3:23:46<21:14:38, 18.01s/it]                                                       {'loss': 34.2549, 'grad_norm': 108.0, 'learning_rate': 6.808098931902292e-05, 'epoch': 0.96}
 15%|█▌        | 753/5000 [3:23:46<21:14:38, 18.01s/it] 15%|█▌        | 754/5000 [3:24:01<20:14:40, 17.16s/it]                                                       {'loss': 33.8755, 'grad_norm': 79.5, 'learning_rate': 6.80734223262897e-05, 'epoch': 0.96}
 15%|█▌        | 754/5000 [3:24:01<20:14:40, 17.16s/it] 15%|█▌        | 755/5000 [3:24:12<18:03:52, 15.32s/it]                                                       {'loss': 38.8466, 'grad_norm': 154.0, 'learning_rate': 6.806584086611793e-05, 'epoch': 0.96}
 15%|█▌        | 755/5000 [3:24:12<18:03:52, 15.32s/it] 15%|█▌        | 756/5000 [3:24:25<17:25:45, 14.78s/it]                                                       {'loss': 32.0123, 'grad_norm': 70.5, 'learning_rate': 6.805824494182398e-05, 'epoch': 0.96}
 15%|█▌        | 756/5000 [3:24:25<17:25:45, 14.78s/it] 15%|█▌        | 757/5000 [3:24:39<17:02:04, 14.45s/it]                                                       {'loss': 32.4991, 'grad_norm': 61.75, 'learning_rate': 6.805063455673057e-05, 'epoch': 0.96}
 15%|█▌        | 757/5000 [3:24:39<17:02:04, 14.45s/it] 15%|█▌        | 758/5000 [3:24:52<16:20:09, 13.86s/it]                                                       {'loss': 36.3864, 'grad_norm': 75.5, 'learning_rate': 6.804300971416673e-05, 'epoch': 0.96}
 15%|█▌        | 758/5000 [3:24:52<16:20:09, 13.86s/it] 15%|█▌        | 759/5000 [3:25:10<17:54:55, 15.21s/it]                                                       {'loss': 59.0108, 'grad_norm': 4608.0, 'learning_rate': 6.803537041746783e-05, 'epoch': 0.96}
 15%|█▌        | 759/5000 [3:25:10<17:54:55, 15.21s/it] 15%|█▌        | 760/5000 [3:25:23<16:59:43, 14.43s/it]                                                       {'loss': 39.2744, 'grad_norm': 370.0, 'learning_rate': 6.802771666997557e-05, 'epoch': 0.97}
 15%|█▌        | 760/5000 [3:25:23<16:59:43, 14.43s/it] 15%|█▌        | 761/5000 [3:25:44<19:20:10, 16.42s/it]                                                       {'loss': 33.8121, 'grad_norm': 122.0, 'learning_rate': 6.802004847503794e-05, 'epoch': 0.97}
 15%|█▌        | 761/5000 [3:25:44<19:20:10, 16.42s/it] 15%|█▌        | 762/5000 [3:25:56<17:55:40, 15.23s/it]                                                       {'loss': 36.2491, 'grad_norm': 396.0, 'learning_rate': 6.80123658360093e-05, 'epoch': 0.97}
 15%|█▌        | 762/5000 [3:25:56<17:55:40, 15.23s/it] 15%|█▌        | 763/5000 [3:26:09<17:16:18, 14.68s/it]                                                       {'loss': 34.2589, 'grad_norm': 165.0, 'learning_rate': 6.800466875625026e-05, 'epoch': 0.97}
 15%|█▌        | 763/5000 [3:26:09<17:16:18, 14.68s/it] 15%|█▌        | 764/5000 [3:26:41<23:21:00, 19.84s/it]                                                       {'loss': 38.6635, 'grad_norm': 940.0, 'learning_rate': 6.79969572391278e-05, 'epoch': 0.97}
 15%|█▌        | 764/5000 [3:26:41<23:21:00, 19.84s/it] 15%|█▌        | 765/5000 [3:27:05<24:34:48, 20.89s/it]                                                       {'loss': 35.6779, 'grad_norm': 81.5, 'learning_rate': 6.798923128801519e-05, 'epoch': 0.97}
 15%|█▌        | 765/5000 [3:27:05<24:34:48, 20.89s/it] 15%|█▌        | 766/5000 [3:27:29<25:53:27, 22.01s/it]                                                       {'loss': 36.8108, 'grad_norm': 97.5, 'learning_rate': 6.798149090629206e-05, 'epoch': 0.97}
 15%|█▌        | 766/5000 [3:27:29<25:53:27, 22.01s/it] 15%|█▌        | 767/5000 [3:27:45<23:47:04, 20.23s/it]                                                       {'loss': 29.7401, 'grad_norm': 55.5, 'learning_rate': 6.797373609734429e-05, 'epoch': 0.97}
 15%|█▌        | 767/5000 [3:27:45<23:47:04, 20.23s/it] 15%|█▌        | 768/5000 [3:27:59<21:20:35, 18.16s/it]                                                       {'loss': 30.6604, 'grad_norm': 55.75, 'learning_rate': 6.796596686456407e-05, 'epoch': 0.98}
 15%|█▌        | 768/5000 [3:27:59<21:20:35, 18.16s/it] 15%|█▌        | 769/5000 [3:28:13<20:07:16, 17.12s/it]                                                       {'loss': 30.9389, 'grad_norm': 69.0, 'learning_rate': 6.795818321134998e-05, 'epoch': 0.98}
 15%|█▌        | 769/5000 [3:28:13<20:07:16, 17.12s/it] 15%|█▌        | 770/5000 [3:28:23<17:34:54, 14.96s/it]                                                       {'loss': 39.55, 'grad_norm': 149.0, 'learning_rate': 6.79503851411068e-05, 'epoch': 0.98}
 15%|█▌        | 770/5000 [3:28:23<17:34:54, 14.96s/it] 15%|█▌        | 771/5000 [3:28:36<16:45:51, 14.27s/it]                                                       {'loss': 32.4523, 'grad_norm': 59.0, 'learning_rate': 6.794257265724573e-05, 'epoch': 0.98}
 15%|█▌        | 771/5000 [3:28:36<16:45:51, 14.27s/it] 15%|█▌        | 772/5000 [3:28:50<16:34:00, 14.11s/it]                                                       {'loss': 31.7924, 'grad_norm': 100.0, 'learning_rate': 6.793474576318417e-05, 'epoch': 0.98}
 15%|█▌        | 772/5000 [3:28:50<16:34:00, 14.11s/it] 15%|█▌        | 773/5000 [3:29:00<15:16:48, 13.01s/it]                                                       {'loss': 30.9297, 'grad_norm': 88.5, 'learning_rate': 6.792690446234588e-05, 'epoch': 0.98}
 15%|█▌        | 773/5000 [3:29:00<15:16:48, 13.01s/it] 15%|█▌        | 774/5000 [3:29:14<15:42:44, 13.38s/it]                                                       {'loss': 36.406, 'grad_norm': 226.0, 'learning_rate': 6.791904875816092e-05, 'epoch': 0.98}
 15%|█▌        | 774/5000 [3:29:14<15:42:44, 13.38s/it] 16%|█▌        | 775/5000 [3:29:27<15:25:17, 13.14s/it]                                                       {'loss': 30.2223, 'grad_norm': 43.5, 'learning_rate': 6.791117865406564e-05, 'epoch': 0.98}
 16%|█▌        | 775/5000 [3:29:27<15:25:17, 13.14s/it] 16%|█▌        | 776/5000 [3:29:38<14:32:23, 12.39s/it]                                                       {'loss': 33.2982, 'grad_norm': 54.25, 'learning_rate': 6.790329415350268e-05, 'epoch': 0.99}
 16%|█▌        | 776/5000 [3:29:38<14:32:23, 12.39s/it] 16%|█▌        | 777/5000 [3:29:49<14:19:32, 12.21s/it]                                                       {'loss': 47.3882, 'grad_norm': 768.0, 'learning_rate': 6.789539525992101e-05, 'epoch': 0.99}
 16%|█▌        | 777/5000 [3:29:49<14:19:32, 12.21s/it] 16%|█▌        | 778/5000 [3:30:01<14:04:22, 12.00s/it]                                                       {'loss': 33.5928, 'grad_norm': 124.0, 'learning_rate': 6.788748197677584e-05, 'epoch': 0.99}
 16%|█▌        | 778/5000 [3:30:01<14:04:22, 12.00s/it] 16%|█▌        | 779/5000 [3:30:16<15:15:24, 13.01s/it]                                                       {'loss': 27.7692, 'grad_norm': 36.0, 'learning_rate': 6.787955430752874e-05, 'epoch': 0.99}
 16%|█▌        | 779/5000 [3:30:16<15:15:24, 13.01s/it] 16%|█▌        | 780/5000 [3:30:31<15:43:29, 13.41s/it]                                                       {'loss': 33.1434, 'grad_norm': 67.5, 'learning_rate': 6.787161225564754e-05, 'epoch': 0.99}
 16%|█▌        | 780/5000 [3:30:31<15:43:29, 13.41s/it] 16%|█▌        | 781/5000 [3:30:43<15:16:31, 13.03s/it]                                                       {'loss': 54.0511, 'grad_norm': 512.0, 'learning_rate': 6.786365582460636e-05, 'epoch': 0.99}
 16%|█▌        | 781/5000 [3:30:43<15:16:31, 13.03s/it] 16%|█▌        | 782/5000 [3:30:56<15:14:27, 13.01s/it]                                                       {'loss': 28.7098, 'grad_norm': 65.0, 'learning_rate': 6.785568501788559e-05, 'epoch': 0.99}
 16%|█▌        | 782/5000 [3:30:56<15:14:27, 13.01s/it] 16%|█▌        | 783/5000 [3:31:08<15:02:28, 12.84s/it]                                                       {'loss': 38.4568, 'grad_norm': 362.0, 'learning_rate': 6.784769983897196e-05, 'epoch': 0.99}
 16%|█▌        | 783/5000 [3:31:08<15:02:28, 12.84s/it] 16%|█▌        | 784/5000 [3:31:20<14:29:28, 12.37s/it]                                                       {'loss': 60.3228, 'grad_norm': 18688.0, 'learning_rate': 6.783970029135844e-05, 'epoch': 1.0}
 16%|█▌        | 784/5000 [3:31:20<14:29:28, 12.37s/it] 16%|█▌        | 785/5000 [3:31:40<17:16:43, 14.76s/it]                                                       {'loss': 36.2486, 'grad_norm': 408.0, 'learning_rate': 6.783168637854431e-05, 'epoch': 1.0}
 16%|█▌        | 785/5000 [3:31:40<17:16:43, 14.76s/it] 16%|█▌        | 786/5000 [3:31:57<17:58:31, 15.36s/it]                                                       {'loss': 31.4107, 'grad_norm': 94.0, 'learning_rate': 6.782365810403514e-05, 'epoch': 1.0}
 16%|█▌        | 786/5000 [3:31:57<17:58:31, 15.36s/it] 16%|█▌        | 787/5000 [3:32:10<17:09:51, 14.67s/it]                                                       {'loss': 33.9363, 'grad_norm': 85.5, 'learning_rate': 6.781561547134276e-05, 'epoch': 1.0}
 16%|█▌        | 787/5000 [3:32:10<17:09:51, 14.67s/it] 16%|█▌        | 788/5000 [3:32:22<16:28:48, 14.09s/it]                                                       {'loss': 51.8105, 'grad_norm': 804.0, 'learning_rate': 6.780755848398529e-05, 'epoch': 1.0}
 16%|█▌        | 788/5000 [3:32:22<16:28:48, 14.09s/it] 16%|█▌        | 789/5000 [3:32:35<15:57:42, 13.65s/it]                                                       {'loss': 30.1549, 'grad_norm': 68.5, 'learning_rate': 6.779948714548712e-05, 'epoch': 1.0}
 16%|█▌        | 789/5000 [3:32:35<15:57:42, 13.65s/it] 16%|█▌        | 790/5000 [3:32:47<15:32:34, 13.29s/it]                                                       {'loss': 33.6095, 'grad_norm': 224.0, 'learning_rate': 6.779140145937895e-05, 'epoch': 1.0}
 16%|█▌        | 790/5000 [3:32:47<15:32:34, 13.29s/it] 16%|█▌        | 791/5000 [3:33:03<16:10:56, 13.84s/it]                                                       {'loss': 29.7467, 'grad_norm': 62.0, 'learning_rate': 6.778330142919772e-05, 'epoch': 1.0}
 16%|█▌        | 791/5000 [3:33:03<16:10:56, 13.84s/it] 16%|█▌        | 792/5000 [3:33:19<17:01:29, 14.56s/it]                                                       {'loss': 30.2266, 'grad_norm': 528.0, 'learning_rate': 6.777518705848667e-05, 'epoch': 1.01}
 16%|█▌        | 792/5000 [3:33:19<17:01:29, 14.56s/it] 16%|█▌        | 793/5000 [3:33:32<16:35:04, 14.19s/it]                                                       {'loss': 30.6464, 'grad_norm': 47.5, 'learning_rate': 6.776705835079528e-05, 'epoch': 1.01}
 16%|█▌        | 793/5000 [3:33:32<16:35:04, 14.19s/it] 16%|█▌        | 794/5000 [3:33:46<16:20:15, 13.98s/it]                                                       {'loss': 33.7933, 'grad_norm': 56.5, 'learning_rate': 6.775891530967933e-05, 'epoch': 1.01}
 16%|█▌        | 794/5000 [3:33:46<16:20:15, 13.98s/it] 16%|█▌        | 795/5000 [3:33:58<15:49:39, 13.55s/it]                                                       {'loss': 33.6037, 'grad_norm': 82.0, 'learning_rate': 6.775075793870089e-05, 'epoch': 1.01}
 16%|█▌        | 795/5000 [3:33:58<15:49:39, 13.55s/it] 16%|█▌        | 796/5000 [3:34:22<19:34:42, 16.77s/it]                                                       {'loss': 37.0733, 'grad_norm': 728.0, 'learning_rate': 6.774258624142823e-05, 'epoch': 1.01}
 16%|█▌        | 796/5000 [3:34:22<19:34:42, 16.77s/it] 16%|█▌        | 797/5000 [3:34:35<17:57:20, 15.38s/it]                                                       {'loss': 33.1981, 'grad_norm': 107.5, 'learning_rate': 6.773440022143595e-05, 'epoch': 1.01}
 16%|█▌        | 797/5000 [3:34:35<17:57:20, 15.38s/it] 16%|█▌        | 798/5000 [3:34:50<17:49:35, 15.27s/it]                                                       {'loss': 33.0205, 'grad_norm': 60.5, 'learning_rate': 6.772619988230489e-05, 'epoch': 1.01}
 16%|█▌        | 798/5000 [3:34:50<17:49:35, 15.27s/it] 16%|█▌        | 799/5000 [3:35:02<16:52:42, 14.46s/it]                                                       {'loss': 50.1185, 'grad_norm': 3008.0, 'learning_rate': 6.771798522762216e-05, 'epoch': 1.01}
 16%|█▌        | 799/5000 [3:35:02<16:52:42, 14.46s/it] 16%|█▌        | 800/5000 [3:35:13<15:28:22, 13.26s/it]                                                       {'loss': 47.3663, 'grad_norm': 490.0, 'learning_rate': 6.770975626098112e-05, 'epoch': 1.02}
 16%|█▌        | 800/5000 [3:35:13<15:28:22, 13.26s/it] 16%|█▌        | 801/5000 [3:35:28<16:15:39, 13.94s/it]                                                       {'loss': 36.0231, 'grad_norm': 780.0, 'learning_rate': 6.770151298598142e-05, 'epoch': 1.02}
 16%|█▌        | 801/5000 [3:35:28<16:15:39, 13.94s/it] 16%|█▌        | 802/5000 [3:35:50<19:06:56, 16.39s/it]                                                       {'loss': 32.4525, 'grad_norm': 181.0, 'learning_rate': 6.769325540622893e-05, 'epoch': 1.02}
 16%|█▌        | 802/5000 [3:35:50<19:06:56, 16.39s/it] 16%|█▌        | 803/5000 [3:36:17<22:38:48, 19.43s/it]                                                       {'loss': 30.0294, 'grad_norm': 100.5, 'learning_rate': 6.768498352533578e-05, 'epoch': 1.02}
 16%|█▌        | 803/5000 [3:36:17<22:38:48, 19.43s/it] 16%|█▌        | 804/5000 [3:36:40<24:00:12, 20.59s/it]                                                       {'loss': 28.6411, 'grad_norm': 45.0, 'learning_rate': 6.767669734692041e-05, 'epoch': 1.02}
 16%|█▌        | 804/5000 [3:36:40<24:00:12, 20.59s/it] 16%|█▌        | 805/5000 [3:37:12<27:48:08, 23.86s/it]                                                       {'loss': 28.4049, 'grad_norm': 46.25, 'learning_rate': 6.766839687460747e-05, 'epoch': 1.02}
 16%|█▌        | 805/5000 [3:37:12<27:48:08, 23.86s/it] 16%|█▌        | 806/5000 [3:37:24<23:49:53, 20.46s/it]                                                       {'loss': 32.8455, 'grad_norm': 57.0, 'learning_rate': 6.766008211202783e-05, 'epoch': 1.02}
 16%|█▌        | 806/5000 [3:37:24<23:49:53, 20.46s/it] 16%|█▌        | 807/5000 [3:37:34<20:13:02, 17.36s/it]                                                       {'loss': 34.5577, 'grad_norm': 516.0, 'learning_rate': 6.76517530628187e-05, 'epoch': 1.02}
 16%|█▌        | 807/5000 [3:37:34<20:13:02, 17.36s/it] 16%|█▌        | 808/5000 [3:37:47<18:35:15, 15.96s/it]                                                       {'loss': 46.5413, 'grad_norm': 1152.0, 'learning_rate': 6.764340973062344e-05, 'epoch': 1.03}
 16%|█▌        | 808/5000 [3:37:47<18:35:15, 15.96s/it] 16%|█▌        | 809/5000 [3:37:59<17:19:58, 14.89s/it]                                                       {'loss': 33.0145, 'grad_norm': 73.0, 'learning_rate': 6.763505211909176e-05, 'epoch': 1.03}
 16%|█▌        | 809/5000 [3:37:59<17:19:58, 14.89s/it] 16%|█▌        | 810/5000 [3:38:17<18:16:22, 15.70s/it]                                                       {'loss': 31.761, 'grad_norm': 95.0, 'learning_rate': 6.762668023187952e-05, 'epoch': 1.03}
 16%|█▌        | 810/5000 [3:38:17<18:16:22, 15.70s/it] 16%|█▌        | 811/5000 [3:38:30<17:16:15, 14.84s/it]                                                       {'loss': 34.1882, 'grad_norm': 62.0, 'learning_rate': 6.761829407264888e-05, 'epoch': 1.03}
 16%|█▌        | 811/5000 [3:38:30<17:16:15, 14.84s/it] 16%|█▌        | 812/5000 [3:38:52<19:51:30, 17.07s/it]                                                       {'loss': 30.356, 'grad_norm': 46.0, 'learning_rate': 6.760989364506825e-05, 'epoch': 1.03}
 16%|█▌        | 812/5000 [3:38:52<19:51:30, 17.07s/it] 16%|█▋        | 813/5000 [3:39:06<18:53:26, 16.24s/it]                                                       {'loss': 33.16, 'grad_norm': 2064.0, 'learning_rate': 6.760147895281223e-05, 'epoch': 1.03}
 16%|█▋        | 813/5000 [3:39:06<18:53:26, 16.24s/it] 16%|█▋        | 814/5000 [3:39:23<19:12:01, 16.51s/it]                                                       {'loss': 27.8987, 'grad_norm': 71.0, 'learning_rate': 6.759304999956173e-05, 'epoch': 1.03}
 16%|█▋        | 814/5000 [3:39:23<19:12:01, 16.51s/it] 16%|█▋        | 815/5000 [3:39:37<18:04:10, 15.54s/it]                                                       {'loss': 27.9382, 'grad_norm': 49.0, 'learning_rate': 6.758460678900383e-05, 'epoch': 1.03}
 16%|█▋        | 815/5000 [3:39:37<18:04:10, 15.54s/it] 16%|█▋        | 816/5000 [3:39:50<17:22:14, 14.95s/it]                                                       {'loss': 30.7025, 'grad_norm': 43.0, 'learning_rate': 6.757614932483187e-05, 'epoch': 1.04}
 16%|█▋        | 816/5000 [3:39:50<17:22:14, 14.95s/it] 16%|█▋        | 817/5000 [3:40:11<19:28:54, 16.77s/it]                                                       {'loss': 33.0535, 'grad_norm': 70.5, 'learning_rate': 6.756767761074547e-05, 'epoch': 1.04}
 16%|█▋        | 817/5000 [3:40:11<19:28:54, 16.77s/it] 16%|█▋        | 818/5000 [3:40:23<17:45:35, 15.29s/it]                                                       {'loss': 29.2686, 'grad_norm': 47.0, 'learning_rate': 6.755919165045043e-05, 'epoch': 1.04}
 16%|█▋        | 818/5000 [3:40:23<17:45:35, 15.29s/it] 16%|█▋        | 819/5000 [3:40:35<16:40:23, 14.36s/it]                                                       {'loss': 32.7659, 'grad_norm': 64.5, 'learning_rate': 6.755069144765876e-05, 'epoch': 1.04}
 16%|█▋        | 819/5000 [3:40:35<16:40:23, 14.36s/it] 16%|█▋        | 820/5000 [3:40:49<16:33:55, 14.27s/it]                                                       {'loss': 29.7998, 'grad_norm': 99.0, 'learning_rate': 6.75421770060888e-05, 'epoch': 1.04}
 16%|█▋        | 820/5000 [3:40:49<16:33:55, 14.27s/it] 16%|█▋        | 821/5000 [3:41:08<18:02:57, 15.55s/it]                                                       {'loss': 28.173, 'grad_norm': 72.5, 'learning_rate': 6.753364832946502e-05, 'epoch': 1.04}
 16%|█▋        | 821/5000 [3:41:08<18:02:57, 15.55s/it] 16%|█▋        | 822/5000 [3:41:31<20:45:40, 17.89s/it]                                                       {'loss': 29.8351, 'grad_norm': 49.0, 'learning_rate': 6.752510542151814e-05, 'epoch': 1.04}
 16%|█▋        | 822/5000 [3:41:31<20:45:40, 17.89s/it] 16%|█▋        | 823/5000 [3:41:45<19:13:10, 16.56s/it]                                                       {'loss': 30.281, 'grad_norm': 49.5, 'learning_rate': 6.751654828598514e-05, 'epoch': 1.05}
 16%|█▋        | 823/5000 [3:41:45<19:13:10, 16.56s/it] 16%|█▋        | 824/5000 [3:42:01<19:06:44, 16.48s/it]                                                       {'loss': 26.9996, 'grad_norm': 69.5, 'learning_rate': 6.750797692660922e-05, 'epoch': 1.05}
 16%|█▋        | 824/5000 [3:42:01<19:06:44, 16.48s/it] 16%|█▋        | 825/5000 [3:42:13<17:24:28, 15.01s/it]                                                       {'loss': 34.1363, 'grad_norm': 61.75, 'learning_rate': 6.749939134713974e-05, 'epoch': 1.05}
 16%|█▋        | 825/5000 [3:42:13<17:24:28, 15.01s/it] 17%|█▋        | 826/5000 [3:42:24<16:14:26, 14.01s/it]                                                       {'loss': 37.2384, 'grad_norm': 292.0, 'learning_rate': 6.749079155133235e-05, 'epoch': 1.05}
 17%|█▋        | 826/5000 [3:42:24<16:14:26, 14.01s/it] 17%|█▋        | 827/5000 [3:42:38<16:07:48, 13.92s/it]                                                       {'loss': 37.4021, 'grad_norm': 1464.0, 'learning_rate': 6.74821775429489e-05, 'epoch': 1.05}
 17%|█▋        | 827/5000 [3:42:38<16:07:48, 13.92s/it] 17%|█▋        | 828/5000 [3:42:50<15:27:41, 13.34s/it]                                                       {'loss': 32.4377, 'grad_norm': 584.0, 'learning_rate': 6.747354932575743e-05, 'epoch': 1.05}
 17%|█▋        | 828/5000 [3:42:50<15:27:41, 13.34s/it] 17%|█▋        | 829/5000 [3:43:00<14:18:20, 12.35s/it]                                                       {'loss': 35.3143, 'grad_norm': 141.0, 'learning_rate': 6.746490690353223e-05, 'epoch': 1.05}
 17%|█▋        | 829/5000 [3:43:00<14:18:20, 12.35s/it] 17%|█▋        | 830/5000 [3:43:16<15:40:49, 13.54s/it]                                                       {'loss': 30.9967, 'grad_norm': 112.0, 'learning_rate': 6.745625028005379e-05, 'epoch': 1.05}
 17%|█▋        | 830/5000 [3:43:16<15:40:49, 13.54s/it] 17%|█▋        | 831/5000 [3:43:30<15:48:14, 13.65s/it]                                                       {'loss': 30.2583, 'grad_norm': 45.75, 'learning_rate': 6.74475794591088e-05, 'epoch': 1.06}
 17%|█▋        | 831/5000 [3:43:30<15:48:14, 13.65s/it] 17%|█▋        | 832/5000 [3:43:42<15:13:48, 13.15s/it]                                                       {'loss': 32.3542, 'grad_norm': 61.0, 'learning_rate': 6.743889444449015e-05, 'epoch': 1.06}
 17%|█▋        | 832/5000 [3:43:42<15:13:48, 13.15s/it] 17%|█▋        | 833/5000 [3:43:59<16:26:01, 14.20s/it]                                                       {'loss': 31.5192, 'grad_norm': 48.0, 'learning_rate': 6.743019523999703e-05, 'epoch': 1.06}
 17%|█▋        | 833/5000 [3:43:59<16:26:01, 14.20s/it] 17%|█▋        | 834/5000 [3:44:12<16:09:06, 13.96s/it]                                                       {'loss': 29.3737, 'grad_norm': 77.5, 'learning_rate': 6.742148184943469e-05, 'epoch': 1.06}
 17%|█▋        | 834/5000 [3:44:12<16:09:06, 13.96s/it] 17%|█▋        | 835/5000 [3:44:23<15:07:30, 13.07s/it]                                                       {'loss': 32.9699, 'grad_norm': 216.0, 'learning_rate': 6.741275427661472e-05, 'epoch': 1.06}
 17%|█▋        | 835/5000 [3:44:23<15:07:30, 13.07s/it] 17%|█▋        | 836/5000 [3:44:37<15:19:16, 13.25s/it]                                                       {'loss': 28.4411, 'grad_norm': 52.25, 'learning_rate': 6.740401252535481e-05, 'epoch': 1.06}
 17%|█▋        | 836/5000 [3:44:37<15:19:16, 13.25s/it] 17%|█▋        | 837/5000 [3:44:50<15:11:56, 13.14s/it]                                                       {'loss': 31.9532, 'grad_norm': 78.0, 'learning_rate': 6.739525659947894e-05, 'epoch': 1.06}
 17%|█▋        | 837/5000 [3:44:50<15:11:56, 13.14s/it] 17%|█▋        | 838/5000 [3:45:08<16:54:31, 14.63s/it]                                                       {'loss': 27.4485, 'grad_norm': 63.5, 'learning_rate': 6.738648650281723e-05, 'epoch': 1.06}
 17%|█▋        | 838/5000 [3:45:08<16:54:31, 14.63s/it] 17%|█▋        | 839/5000 [3:45:30<19:19:41, 16.72s/it]                                                       {'loss': 30.031, 'grad_norm': 178.0, 'learning_rate': 6.737770223920602e-05, 'epoch': 1.07}
 17%|█▋        | 839/5000 [3:45:30<19:19:41, 16.72s/it] 17%|█▋        | 840/5000 [3:45:53<21:38:10, 18.72s/it]                                                       {'loss': 27.9791, 'grad_norm': 65.5, 'learning_rate': 6.736890381248785e-05, 'epoch': 1.07}
 17%|█▋        | 840/5000 [3:45:53<21:38:10, 18.72s/it] 17%|█▋        | 841/5000 [3:46:11<21:19:14, 18.46s/it]                                                       {'loss': 27.6702, 'grad_norm': 169.0, 'learning_rate': 6.736009122651144e-05, 'epoch': 1.07}
 17%|█▋        | 841/5000 [3:46:11<21:19:14, 18.46s/it] 17%|█▋        | 842/5000 [3:46:21<18:32:00, 16.05s/it]                                                       {'loss': 31.8981, 'grad_norm': 151.0, 'learning_rate': 6.735126448513174e-05, 'epoch': 1.07}
 17%|█▋        | 842/5000 [3:46:21<18:32:00, 16.05s/it] 17%|█▋        | 843/5000 [3:46:33<17:03:49, 14.78s/it]                                                       {'loss': 28.4022, 'grad_norm': 46.75, 'learning_rate': 6.734242359220982e-05, 'epoch': 1.07}
 17%|█▋        | 843/5000 [3:46:33<17:03:49, 14.78s/it] 17%|█▋        | 844/5000 [3:46:45<16:09:25, 14.00s/it]                                                       {'loss': 44.9909, 'grad_norm': 792.0, 'learning_rate': 6.733356855161304e-05, 'epoch': 1.07}
 17%|█▋        | 844/5000 [3:46:45<16:09:25, 14.00s/it] 17%|█▋        | 845/5000 [3:46:58<15:42:28, 13.61s/it]                                                       {'loss': 31.0486, 'grad_norm': 85.5, 'learning_rate': 6.732469936721486e-05, 'epoch': 1.07}
 17%|█▋        | 845/5000 [3:46:58<15:42:28, 13.61s/it] 17%|█▋        | 846/5000 [3:47:10<15:19:09, 13.28s/it]                                                       {'loss': 34.8445, 'grad_norm': 138.0, 'learning_rate': 6.731581604289496e-05, 'epoch': 1.07}
 17%|█▋        | 846/5000 [3:47:10<15:19:09, 13.28s/it] 17%|█▋        | 847/5000 [3:47:40<20:50:52, 18.07s/it]                                                       {'loss': 29.5546, 'grad_norm': 177.0, 'learning_rate': 6.730691858253925e-05, 'epoch': 1.08}
 17%|█▋        | 847/5000 [3:47:40<20:50:52, 18.07s/it] 17%|█▋        | 848/5000 [3:47:52<18:53:19, 16.38s/it]                                                       {'loss': 32.783, 'grad_norm': 131.0, 'learning_rate': 6.729800699003974e-05, 'epoch': 1.08}
 17%|█▋        | 848/5000 [3:47:52<18:53:19, 16.38s/it] 17%|█▋        | 849/5000 [3:48:12<20:11:33, 17.51s/it]                                                       {'loss': 29.1784, 'grad_norm': 79.5, 'learning_rate': 6.728908126929466e-05, 'epoch': 1.08}
 17%|█▋        | 849/5000 [3:48:12<20:11:33, 17.51s/it] 17%|█▋        | 850/5000 [3:48:24<18:03:24, 15.66s/it]                                                       {'loss': 28.4187, 'grad_norm': 47.75, 'learning_rate': 6.728014142420846e-05, 'epoch': 1.08}
 17%|█▋        | 850/5000 [3:48:24<18:03:24, 15.66s/it] 17%|█▋        | 851/5000 [3:48:40<18:26:28, 16.00s/it]                                                       {'loss': 31.0978, 'grad_norm': 300.0, 'learning_rate': 6.72711874586917e-05, 'epoch': 1.08}
 17%|█▋        | 851/5000 [3:48:40<18:26:28, 16.00s/it] 17%|█▋        | 852/5000 [3:48:51<16:39:05, 14.45s/it]                                                       {'loss': 31.0067, 'grad_norm': 45.0, 'learning_rate': 6.726221937666116e-05, 'epoch': 1.08}
 17%|█▋        | 852/5000 [3:48:51<16:39:05, 14.45s/it] 17%|█▋        | 853/5000 [3:49:09<17:50:08, 15.48s/it]                                                       {'loss': 28.3802, 'grad_norm': 199.0, 'learning_rate': 6.72532371820398e-05, 'epoch': 1.08}
 17%|█▋        | 853/5000 [3:49:09<17:50:08, 15.48s/it] 17%|█▋        | 854/5000 [3:49:23<17:19:14, 15.04s/it]                                                       {'loss': 40.5619, 'grad_norm': 1360.0, 'learning_rate': 6.72442408787567e-05, 'epoch': 1.08}
 17%|█▋        | 854/5000 [3:49:23<17:19:14, 15.04s/it] 17%|█▋        | 855/5000 [3:49:35<16:18:45, 14.17s/it]                                                       {'loss': 100.4108, 'grad_norm': 2848.0, 'learning_rate': 6.723523047074718e-05, 'epoch': 1.09}
 17%|█▋        | 855/5000 [3:49:35<16:18:45, 14.17s/it] 17%|█▋        | 856/5000 [3:49:49<16:08:22, 14.02s/it]                                                       {'loss': 31.1691, 'grad_norm': 230.0, 'learning_rate': 6.722620596195269e-05, 'epoch': 1.09}
 17%|█▋        | 856/5000 [3:49:49<16:08:22, 14.02s/it] 17%|█▋        | 857/5000 [3:50:00<15:04:57, 13.11s/it]                                                       {'loss': 29.1345, 'grad_norm': 50.0, 'learning_rate': 6.721716735632085e-05, 'epoch': 1.09}
 17%|█▋        | 857/5000 [3:50:00<15:04:57, 13.11s/it] 17%|█▋        | 858/5000 [3:50:19<17:11:25, 14.94s/it]                                                       {'loss': 27.4298, 'grad_norm': 41.0, 'learning_rate': 6.720811465780545e-05, 'epoch': 1.09}
 17%|█▋        | 858/5000 [3:50:19<17:11:25, 14.94s/it] 17%|█▋        | 859/5000 [3:50:34<17:19:45, 15.07s/it]                                                       {'loss': 27.5522, 'grad_norm': 45.0, 'learning_rate': 6.719904787036648e-05, 'epoch': 1.09}
 17%|█▋        | 859/5000 [3:50:34<17:19:45, 15.07s/it] 17%|█▋        | 860/5000 [3:50:50<17:32:20, 15.25s/it]                                                       {'loss': 27.522, 'grad_norm': 116.5, 'learning_rate': 6.718996699797002e-05, 'epoch': 1.09}
 17%|█▋        | 860/5000 [3:50:50<17:32:20, 15.25s/it] 17%|█▋        | 861/5000 [3:51:04<16:59:28, 14.78s/it]                                                       {'loss': 27.1401, 'grad_norm': 38.25, 'learning_rate': 6.718087204458836e-05, 'epoch': 1.09}
 17%|█▋        | 861/5000 [3:51:04<16:59:28, 14.78s/it] 17%|█▋        | 862/5000 [3:51:19<17:02:22, 14.82s/it]                                                       {'loss': 55.679, 'grad_norm': 424.0, 'learning_rate': 6.717176301419997e-05, 'epoch': 1.09}
 17%|█▋        | 862/5000 [3:51:19<17:02:22, 14.82s/it] 17%|█▋        | 863/5000 [3:51:42<19:52:23, 17.29s/it]                                                       {'loss': 47.825, 'grad_norm': 936.0, 'learning_rate': 6.716263991078942e-05, 'epoch': 1.1}
 17%|█▋        | 863/5000 [3:51:42<19:52:23, 17.29s/it] 17%|█▋        | 864/5000 [3:51:53<17:43:17, 15.42s/it]                                                       {'loss': 74.654, 'grad_norm': 2096.0, 'learning_rate': 6.715350273834748e-05, 'epoch': 1.1}
 17%|█▋        | 864/5000 [3:51:53<17:43:17, 15.42s/it] 17%|█▋        | 865/5000 [3:52:05<16:41:45, 14.54s/it]                                                       {'loss': 46.8108, 'grad_norm': 576.0, 'learning_rate': 6.714435150087105e-05, 'epoch': 1.1}
 17%|█▋        | 865/5000 [3:52:05<16:41:45, 14.54s/it] 17%|█▋        | 866/5000 [3:52:18<16:03:35, 13.99s/it]                                                       {'loss': 94.5073, 'grad_norm': 5376.0, 'learning_rate': 6.71351862023632e-05, 'epoch': 1.1}
 17%|█▋        | 866/5000 [3:52:18<16:03:35, 13.99s/it] 17%|█▋        | 867/5000 [3:52:32<15:54:25, 13.86s/it]                                                       {'loss': 52.9446, 'grad_norm': 796.0, 'learning_rate': 6.712600684683313e-05, 'epoch': 1.1}
 17%|█▋        | 867/5000 [3:52:32<15:54:25, 13.86s/it] 17%|█▋        | 868/5000 [3:52:47<16:18:26, 14.21s/it]                                                       {'loss': 69.5213, 'grad_norm': 628.0, 'learning_rate': 6.711681343829623e-05, 'epoch': 1.1}
 17%|█▋        | 868/5000 [3:52:47<16:18:26, 14.21s/it] 17%|█▋        | 869/5000 [3:53:02<16:46:58, 14.63s/it]                                                       {'loss': 54.6853, 'grad_norm': 460.0, 'learning_rate': 6.710760598077398e-05, 'epoch': 1.1}
 17%|█▋        | 869/5000 [3:53:02<16:46:58, 14.63s/it] 17%|█▋        | 870/5000 [3:53:14<15:38:45, 13.64s/it]                                                       {'loss': 47.4723, 'grad_norm': 1360.0, 'learning_rate': 6.709838447829404e-05, 'epoch': 1.1}
 17%|█▋        | 870/5000 [3:53:14<15:38:45, 13.64s/it] 17%|█▋        | 871/5000 [3:53:27<15:24:44, 13.44s/it]                                                       {'loss': 69.4192, 'grad_norm': 3024.0, 'learning_rate': 6.708914893489023e-05, 'epoch': 1.11}
 17%|█▋        | 871/5000 [3:53:27<15:24:44, 13.44s/it] 17%|█▋        | 872/5000 [3:53:41<15:46:43, 13.76s/it]                                                       {'loss': 47.9108, 'grad_norm': 414.0, 'learning_rate': 6.707989935460247e-05, 'epoch': 1.11}
 17%|█▋        | 872/5000 [3:53:41<15:46:43, 13.76s/it] 17%|█▋        | 873/5000 [3:54:02<18:22:08, 16.02s/it]                                                       {'loss': 48.9355, 'grad_norm': 276.0, 'learning_rate': 6.707063574147684e-05, 'epoch': 1.11}
 17%|█▋        | 873/5000 [3:54:02<18:22:08, 16.02s/it] 17%|█▋        | 874/5000 [3:54:18<18:24:08, 16.06s/it]                                                       {'loss': 42.5828, 'grad_norm': 462.0, 'learning_rate': 6.706135809956557e-05, 'epoch': 1.11}
 17%|█▋        | 874/5000 [3:54:18<18:24:08, 16.06s/it] 18%|█▊        | 875/5000 [3:54:38<19:33:50, 17.07s/it]                                                       {'loss': 35.7716, 'grad_norm': 98.0, 'learning_rate': 6.7052066432927e-05, 'epoch': 1.11}
 18%|█▊        | 875/5000 [3:54:38<19:33:50, 17.07s/it] 18%|█▊        | 876/5000 [3:54:51<18:18:40, 15.98s/it]                                                       {'loss': 41.692, 'grad_norm': 318.0, 'learning_rate': 6.704276074562563e-05, 'epoch': 1.11}
 18%|█▊        | 876/5000 [3:54:51<18:18:40, 15.98s/it] 18%|█▊        | 877/5000 [3:55:05<17:32:06, 15.31s/it]                                                       {'loss': 71.088, 'grad_norm': 984.0, 'learning_rate': 6.703344104173211e-05, 'epoch': 1.11}
 18%|█▊        | 877/5000 [3:55:05<17:32:06, 15.31s/it] 18%|█▊        | 878/5000 [3:55:18<16:34:09, 14.47s/it]                                                       {'loss': 62.198, 'grad_norm': 764.0, 'learning_rate': 6.702410732532315e-05, 'epoch': 1.11}
 18%|█▊        | 878/5000 [3:55:18<16:34:09, 14.47s/it] 18%|█▊        | 879/5000 [3:55:29<15:24:19, 13.46s/it]                                                       {'loss': 35.1532, 'grad_norm': 92.0, 'learning_rate': 6.701475960048166e-05, 'epoch': 1.12}
 18%|█▊        | 879/5000 [3:55:29<15:24:19, 13.46s/it] 18%|█▊        | 880/5000 [3:55:52<18:38:29, 16.29s/it]                                                       {'loss': 32.8413, 'grad_norm': 102.0, 'learning_rate': 6.700539787129665e-05, 'epoch': 1.12}
 18%|█▊        | 880/5000 [3:55:52<18:38:29, 16.29s/it] 18%|█▊        | 881/5000 [3:56:14<20:50:15, 18.21s/it]                                                       {'loss': 35.0593, 'grad_norm': 580.0, 'learning_rate': 6.699602214186325e-05, 'epoch': 1.12}
 18%|█▊        | 881/5000 [3:56:14<20:50:15, 18.21s/it] 18%|█▊        | 882/5000 [3:56:26<18:42:53, 16.36s/it]                                                       {'loss': 50.6315, 'grad_norm': 251.0, 'learning_rate': 6.698663241628273e-05, 'epoch': 1.12}
 18%|█▊        | 882/5000 [3:56:26<18:42:53, 16.36s/it] 18%|█▊        | 883/5000 [3:56:42<18:25:56, 16.12s/it]                                                       {'loss': 36.2492, 'grad_norm': 478.0, 'learning_rate': 6.697722869866247e-05, 'epoch': 1.12}
 18%|█▊        | 883/5000 [3:56:42<18:25:56, 16.12s/it] 18%|█▊        | 884/5000 [3:56:54<16:52:44, 14.76s/it]                                                       {'loss': 34.2924, 'grad_norm': 51.25, 'learning_rate': 6.696781099311599e-05, 'epoch': 1.12}
 18%|█▊        | 884/5000 [3:56:54<16:52:44, 14.76s/it] 18%|█▊        | 885/5000 [3:57:09<17:03:59, 14.93s/it]                                                       {'loss': 38.4506, 'grad_norm': 408.0, 'learning_rate': 6.69583793037629e-05, 'epoch': 1.12}
 18%|█▊        | 885/5000 [3:57:09<17:03:59, 14.93s/it] 18%|█▊        | 886/5000 [3:57:22<16:29:39, 14.43s/it]                                                       {'loss': 30.2101, 'grad_norm': 148.0, 'learning_rate': 6.694893363472893e-05, 'epoch': 1.13}
 18%|█▊        | 886/5000 [3:57:22<16:29:39, 14.43s/it] 18%|█▊        | 887/5000 [3:57:40<17:50:13, 15.61s/it]                                                       {'loss': 26.6335, 'grad_norm': 30.625, 'learning_rate': 6.693947399014596e-05, 'epoch': 1.13}
 18%|█▊        | 887/5000 [3:57:40<17:50:13, 15.61s/it] 18%|█▊        | 888/5000 [3:57:51<16:00:44, 14.02s/it]                                                       {'loss': 37.7421, 'grad_norm': 89.0, 'learning_rate': 6.693000037415196e-05, 'epoch': 1.13}
 18%|█▊        | 888/5000 [3:57:51<16:00:44, 14.02s/it] 18%|█▊        | 889/5000 [3:58:03<15:20:03, 13.43s/it]                                                       {'loss': 33.9695, 'grad_norm': 73.0, 'learning_rate': 6.692051279089098e-05, 'epoch': 1.13}
 18%|█▊        | 889/5000 [3:58:03<15:20:03, 13.43s/it] 18%|█▊        | 890/5000 [3:58:28<19:22:35, 16.97s/it]                                                       {'loss': 34.8135, 'grad_norm': 972.0, 'learning_rate': 6.691101124451325e-05, 'epoch': 1.13}
 18%|█▊        | 890/5000 [3:58:28<19:22:35, 16.97s/it] 18%|█▊        | 891/5000 [3:58:43<18:42:37, 16.39s/it]                                                       {'loss': 28.876, 'grad_norm': 44.75, 'learning_rate': 6.690149573917505e-05, 'epoch': 1.13}
 18%|█▊        | 891/5000 [3:58:43<18:42:37, 16.39s/it] 18%|█▊        | 892/5000 [3:58:56<17:23:26, 15.24s/it]                                                       {'loss': 29.5267, 'grad_norm': 169.0, 'learning_rate': 6.689196627903879e-05, 'epoch': 1.13}
 18%|█▊        | 892/5000 [3:58:56<17:23:26, 15.24s/it] 18%|█▊        | 893/5000 [3:59:10<17:11:22, 15.07s/it]                                                       {'loss': 32.5261, 'grad_norm': 193.0, 'learning_rate': 6.688242286827298e-05, 'epoch': 1.13}
 18%|█▊        | 893/5000 [3:59:10<17:11:22, 15.07s/it] 18%|█▊        | 894/5000 [3:59:33<19:39:25, 17.23s/it]                                                       {'loss': 29.8377, 'grad_norm': 175.0, 'learning_rate': 6.687286551105223e-05, 'epoch': 1.14}
 18%|█▊        | 894/5000 [3:59:33<19:39:25, 17.23s/it] 18%|█▊        | 895/5000 [3:59:44<17:35:09, 15.42s/it]                                                       {'loss': 31.7906, 'grad_norm': 193.0, 'learning_rate': 6.686329421155724e-05, 'epoch': 1.14}
 18%|█▊        | 895/5000 [3:59:44<17:35:09, 15.42s/it] 18%|█▊        | 896/5000 [4:00:03<18:46:52, 16.47s/it]                                                       {'loss': 28.2743, 'grad_norm': 66.5, 'learning_rate': 6.685370897397484e-05, 'epoch': 1.14}
 18%|█▊        | 896/5000 [4:00:03<18:46:52, 16.47s/it] 18%|█▊        | 897/5000 [4:00:17<17:55:18, 15.72s/it]                                                       {'loss': 31.463, 'grad_norm': 704.0, 'learning_rate': 6.684410980249793e-05, 'epoch': 1.14}
 18%|█▊        | 897/5000 [4:00:17<17:55:18, 15.72s/it] 18%|█▊        | 898/5000 [4:00:34<18:22:51, 16.13s/it]                                                       {'loss': 32.2295, 'grad_norm': 61.0, 'learning_rate': 6.683449670132553e-05, 'epoch': 1.14}
 18%|█▊        | 898/5000 [4:00:34<18:22:51, 16.13s/it] 18%|█▊        | 899/5000 [4:00:48<17:46:26, 15.60s/it]                                                       {'loss': 28.2578, 'grad_norm': 41.5, 'learning_rate': 6.682486967466271e-05, 'epoch': 1.14}
 18%|█▊        | 899/5000 [4:00:48<17:46:26, 15.60s/it] 18%|█▊        | 900/5000 [4:01:00<16:26:34, 14.44s/it]                                                       {'loss': 26.719, 'grad_norm': 47.75, 'learning_rate': 6.681522872672069e-05, 'epoch': 1.14}
 18%|█▊        | 900/5000 [4:01:00<16:26:34, 14.44s/it] 18%|█▊        | 901/5000 [4:01:10<15:05:07, 13.25s/it]                                                       {'loss': 33.2021, 'grad_norm': 89.0, 'learning_rate': 6.680557386171671e-05, 'epoch': 1.14}
 18%|█▊        | 901/5000 [4:01:10<15:05:07, 13.25s/it] 18%|█▊        | 902/5000 [4:01:22<14:23:43, 12.65s/it]                                                       {'loss': 31.5802, 'grad_norm': 60.5, 'learning_rate': 6.679590508387415e-05, 'epoch': 1.15}
 18%|█▊        | 902/5000 [4:01:22<14:23:43, 12.65s/it] 18%|█▊        | 903/5000 [4:01:38<15:34:12, 13.68s/it]                                                       {'loss': 32.7376, 'grad_norm': 117.5, 'learning_rate': 6.67862223974225e-05, 'epoch': 1.15}
 18%|█▊        | 903/5000 [4:01:38<15:34:12, 13.68s/it] 18%|█▊        | 904/5000 [4:01:50<15:14:24, 13.39s/it]                                                       {'loss': 31.4374, 'grad_norm': 45.5, 'learning_rate': 6.677652580659724e-05, 'epoch': 1.15}
 18%|█▊        | 904/5000 [4:01:50<15:14:24, 13.39s/it] 18%|█▊        | 905/5000 [4:02:04<15:10:11, 13.34s/it]                                                       {'loss': 30.32, 'grad_norm': 57.25, 'learning_rate': 6.676681531564002e-05, 'epoch': 1.15}
 18%|█▊        | 905/5000 [4:02:04<15:10:11, 13.34s/it] 18%|█▊        | 906/5000 [4:02:16<14:57:27, 13.15s/it]                                                       {'loss': 47.5489, 'grad_norm': 672.0, 'learning_rate': 6.675709092879852e-05, 'epoch': 1.15}
 18%|█▊        | 906/5000 [4:02:16<14:57:27, 13.15s/it] 18%|█▊        | 907/5000 [4:02:32<15:54:41, 13.99s/it]                                                       {'loss': 28.4006, 'grad_norm': 46.5, 'learning_rate': 6.674735265032655e-05, 'epoch': 1.15}
 18%|█▊        | 907/5000 [4:02:32<15:54:41, 13.99s/it] 18%|█▊        | 908/5000 [4:02:55<18:45:15, 16.50s/it]                                                       {'loss': 30.0668, 'grad_norm': 50.5, 'learning_rate': 6.673760048448392e-05, 'epoch': 1.15}
 18%|█▊        | 908/5000 [4:02:55<18:45:15, 16.50s/it] 18%|█▊        | 909/5000 [4:03:19<21:25:11, 18.85s/it]                                                       {'loss': 33.8276, 'grad_norm': 49.0, 'learning_rate': 6.67278344355366e-05, 'epoch': 1.15}
 18%|█▊        | 909/5000 [4:03:19<21:25:11, 18.85s/it] 18%|█▊        | 910/5000 [4:03:34<20:16:08, 17.84s/it]                                                       {'loss': 30.1679, 'grad_norm': 1152.0, 'learning_rate': 6.671805450775655e-05, 'epoch': 1.16}
 18%|█▊        | 910/5000 [4:03:34<20:16:08, 17.84s/it] 18%|█▊        | 911/5000 [4:03:55<21:11:21, 18.66s/it]                                                       {'loss': 28.8325, 'grad_norm': 49.5, 'learning_rate': 6.670826070542189e-05, 'epoch': 1.16}
 18%|█▊        | 911/5000 [4:03:55<21:11:21, 18.66s/it] 18%|█▊        | 912/5000 [4:04:07<18:46:22, 16.53s/it]                                                       {'loss': 29.2801, 'grad_norm': 43.25, 'learning_rate': 6.669845303281671e-05, 'epoch': 1.16}
 18%|█▊        | 912/5000 [4:04:07<18:46:22, 16.53s/it] 18%|█▊        | 913/5000 [4:04:20<17:33:50, 15.47s/it]                                                       {'loss': 29.1081, 'grad_norm': 40.75, 'learning_rate': 6.668863149423127e-05, 'epoch': 1.16}
 18%|█▊        | 913/5000 [4:04:20<17:33:50, 15.47s/it] 18%|█▊        | 914/5000 [4:04:31<16:20:08, 14.39s/it]                                                       {'loss': 30.7153, 'grad_norm': 173.0, 'learning_rate': 6.66787960939618e-05, 'epoch': 1.16}
 18%|█▊        | 914/5000 [4:04:31<16:20:08, 14.39s/it] 18%|█▊        | 915/5000 [4:04:50<17:38:40, 15.55s/it]                                                       {'loss': 30.3974, 'grad_norm': 98.0, 'learning_rate': 6.666894683631068e-05, 'epoch': 1.16}
 18%|█▊        | 915/5000 [4:04:50<17:38:40, 15.55s/it] 18%|█▊        | 916/5000 [4:05:05<17:30:44, 15.44s/it]                                                       {'loss': 27.5903, 'grad_norm': 318.0, 'learning_rate': 6.665908372558626e-05, 'epoch': 1.16}
 18%|█▊        | 916/5000 [4:05:05<17:30:44, 15.44s/it] 18%|█▊        | 917/5000 [4:05:17<16:32:00, 14.58s/it]                                                       {'loss': 28.4795, 'grad_norm': 49.75, 'learning_rate': 6.664920676610306e-05, 'epoch': 1.16}
 18%|█▊        | 917/5000 [4:05:17<16:32:00, 14.58s/it] 18%|█▊        | 918/5000 [4:05:29<15:37:30, 13.78s/it]                                                       {'loss': 32.6033, 'grad_norm': 210.0, 'learning_rate': 6.663931596218156e-05, 'epoch': 1.17}
 18%|█▊        | 918/5000 [4:05:29<15:37:30, 13.78s/it] 18%|█▊        | 919/5000 [4:05:44<15:58:20, 14.09s/it]                                                       {'loss': 27.4682, 'grad_norm': 34.5, 'learning_rate': 6.662941131814833e-05, 'epoch': 1.17}
 18%|█▊        | 919/5000 [4:05:44<15:58:20, 14.09s/it] 18%|█▊        | 920/5000 [4:06:03<17:26:40, 15.39s/it]                                                       {'loss': 29.3725, 'grad_norm': 772.0, 'learning_rate': 6.661949283833601e-05, 'epoch': 1.17}
 18%|█▊        | 920/5000 [4:06:03<17:26:40, 15.39s/it] 18%|█▊        | 921/5000 [4:06:15<16:33:15, 14.61s/it]                                                       {'loss': 29.675, 'grad_norm': 66.0, 'learning_rate': 6.660956052708327e-05, 'epoch': 1.17}
 18%|█▊        | 921/5000 [4:06:15<16:33:15, 14.61s/it] 18%|█▊        | 922/5000 [4:06:32<17:06:33, 15.10s/it]                                                       {'loss': 25.9694, 'grad_norm': 33.25, 'learning_rate': 6.659961438873488e-05, 'epoch': 1.17}
 18%|█▊        | 922/5000 [4:06:32<17:06:33, 15.10s/it] 18%|█▊        | 923/5000 [4:06:47<17:06:12, 15.10s/it]                                                       {'loss': 25.3595, 'grad_norm': 29.25, 'learning_rate': 6.658965442764155e-05, 'epoch': 1.17}
 18%|█▊        | 923/5000 [4:06:47<17:06:12, 15.10s/it] 18%|█▊        | 924/5000 [4:06:59<16:09:39, 14.27s/it]                                                       {'loss': 28.2404, 'grad_norm': 41.5, 'learning_rate': 6.657968064816016e-05, 'epoch': 1.17}
 18%|█▊        | 924/5000 [4:06:59<16:09:39, 14.27s/it] 18%|█▊        | 925/5000 [4:07:13<15:59:28, 14.13s/it]                                                       {'loss': 26.5318, 'grad_norm': 37.75, 'learning_rate': 6.656969305465356e-05, 'epoch': 1.17}
 18%|█▊        | 925/5000 [4:07:13<15:59:28, 14.13s/it] 19%|█▊        | 926/5000 [4:07:24<14:57:40, 13.22s/it]                                                       {'loss': 27.3292, 'grad_norm': 67.5, 'learning_rate': 6.655969165149066e-05, 'epoch': 1.18}
 19%|█▊        | 926/5000 [4:07:24<14:57:40, 13.22s/it] 19%|█▊        | 927/5000 [4:07:37<14:52:48, 13.15s/it]                                                       {'loss': 28.2024, 'grad_norm': 62.5, 'learning_rate': 6.654967644304642e-05, 'epoch': 1.18}
 19%|█▊        | 927/5000 [4:07:37<14:52:48, 13.15s/it] 19%|█▊        | 928/5000 [4:07:51<15:05:33, 13.34s/it]                                                       {'loss': 27.4194, 'grad_norm': 58.25, 'learning_rate': 6.653964743370183e-05, 'epoch': 1.18}
 19%|█▊        | 928/5000 [4:07:51<15:05:33, 13.34s/it] 19%|█▊        | 929/5000 [4:08:13<18:06:27, 16.01s/it]                                                       {'loss': 27.3944, 'grad_norm': 48.25, 'learning_rate': 6.652960462784393e-05, 'epoch': 1.18}
 19%|█▊        | 929/5000 [4:08:13<18:06:27, 16.01s/it] 19%|█▊        | 930/5000 [4:08:35<20:15:03, 17.91s/it]                                                       {'loss': 26.4642, 'grad_norm': 47.75, 'learning_rate': 6.651954802986577e-05, 'epoch': 1.18}
 19%|█▊        | 930/5000 [4:08:35<20:15:03, 17.91s/it] 19%|█▊        | 931/5000 [4:08:50<19:07:38, 16.92s/it]                                                       {'loss': 27.7184, 'grad_norm': 35.75, 'learning_rate': 6.650947764416644e-05, 'epoch': 1.18}
 19%|█▊        | 931/5000 [4:08:50<19:07:38, 16.92s/it] 19%|█▊        | 932/5000 [4:09:05<18:32:05, 16.40s/it]                                                       {'loss': 28.1162, 'grad_norm': 78.0, 'learning_rate': 6.649939347515109e-05, 'epoch': 1.18}
 19%|█▊        | 932/5000 [4:09:05<18:32:05, 16.40s/it] 19%|█▊        | 933/5000 [4:09:16<16:45:50, 14.84s/it]                                                       {'loss': 29.8343, 'grad_norm': 68.0, 'learning_rate': 6.648929552723088e-05, 'epoch': 1.18}
 19%|█▊        | 933/5000 [4:09:16<16:45:50, 14.84s/it] 19%|█▊        | 934/5000 [4:09:28<15:44:03, 13.93s/it]                                                       {'loss': 27.7174, 'grad_norm': 30.75, 'learning_rate': 6.647918380482298e-05, 'epoch': 1.19}
 19%|█▊        | 934/5000 [4:09:28<15:44:03, 13.93s/it] 19%|█▊        | 935/5000 [4:09:45<16:36:24, 14.71s/it]                                                       {'loss': 32.9052, 'grad_norm': 64.5, 'learning_rate': 6.64690583123506e-05, 'epoch': 1.19}
 19%|█▊        | 935/5000 [4:09:45<16:36:24, 14.71s/it] 19%|█▊        | 936/5000 [4:09:58<16:10:34, 14.33s/it]                                                       {'loss': 26.915, 'grad_norm': 40.0, 'learning_rate': 6.645891905424298e-05, 'epoch': 1.19}
 19%|█▊        | 936/5000 [4:09:58<16:10:34, 14.33s/it] 19%|█▊        | 937/5000 [4:10:10<15:21:10, 13.60s/it]                                                       {'loss': 27.7909, 'grad_norm': 95.5, 'learning_rate': 6.644876603493538e-05, 'epoch': 1.19}
 19%|█▊        | 937/5000 [4:10:10<15:21:10, 13.60s/it] 19%|█▉        | 938/5000 [4:10:22<14:41:02, 13.01s/it]                                                       {'loss': 27.1261, 'grad_norm': 76.0, 'learning_rate': 6.643859925886908e-05, 'epoch': 1.19}
 19%|█▉        | 938/5000 [4:10:22<14:41:02, 13.01s/it] 19%|█▉        | 939/5000 [4:10:34<14:25:15, 12.78s/it]                                                       {'loss': 29.5296, 'grad_norm': 1312.0, 'learning_rate': 6.642841873049136e-05, 'epoch': 1.19}
 19%|█▉        | 939/5000 [4:10:34<14:25:15, 12.78s/it] 19%|█▉        | 940/5000 [4:10:45<13:41:52, 12.15s/it]                                                       {'loss': 29.783, 'grad_norm': 48.25, 'learning_rate': 6.641822445425552e-05, 'epoch': 1.19}
 19%|█▉        | 940/5000 [4:10:45<13:41:52, 12.15s/it] 19%|█▉        | 941/5000 [4:10:58<14:00:41, 12.43s/it]                                                       {'loss': 26.6684, 'grad_norm': 42.25, 'learning_rate': 6.640801643462091e-05, 'epoch': 1.19}
 19%|█▉        | 941/5000 [4:10:58<14:00:41, 12.43s/it] 19%|█▉        | 942/5000 [4:11:09<13:36:58, 12.08s/it]                                                       {'loss': 30.9226, 'grad_norm': 96.0, 'learning_rate': 6.639779467605285e-05, 'epoch': 1.2}
 19%|█▉        | 942/5000 [4:11:09<13:36:58, 12.08s/it] 19%|█▉        | 943/5000 [4:11:20<13:13:08, 11.73s/it]                                                       {'loss': 31.527, 'grad_norm': 78.0, 'learning_rate': 6.638755918302268e-05, 'epoch': 1.2}
 19%|█▉        | 943/5000 [4:11:20<13:13:08, 11.73s/it] 19%|█▉        | 944/5000 [4:11:37<14:56:25, 13.26s/it]                                                       {'loss': 27.2531, 'grad_norm': 52.0, 'learning_rate': 6.637730996000777e-05, 'epoch': 1.2}
 19%|█▉        | 944/5000 [4:11:37<14:56:25, 13.26s/it] 19%|█▉        | 945/5000 [4:11:52<15:30:08, 13.76s/it]                                                       {'loss': 25.5068, 'grad_norm': 64.0, 'learning_rate': 6.636704701149146e-05, 'epoch': 1.2}
 19%|█▉        | 945/5000 [4:11:52<15:30:08, 13.76s/it] 19%|█▉        | 946/5000 [4:12:05<15:14:40, 13.54s/it]                                                       {'loss': 28.4772, 'grad_norm': 74.0, 'learning_rate': 6.635677034196311e-05, 'epoch': 1.2}
 19%|█▉        | 946/5000 [4:12:05<15:14:40, 13.54s/it] 19%|█▉        | 947/5000 [4:12:35<20:55:17, 18.58s/it]                                                       {'loss': 26.7616, 'grad_norm': 40.25, 'learning_rate': 6.634647995591811e-05, 'epoch': 1.2}
 19%|█▉        | 947/5000 [4:12:35<20:55:17, 18.58s/it] 19%|█▉        | 948/5000 [4:12:49<19:12:53, 17.07s/it]                                                       {'loss': 61.1856, 'grad_norm': 624.0, 'learning_rate': 6.633617585785779e-05, 'epoch': 1.2}
 19%|█▉        | 948/5000 [4:12:49<19:12:53, 17.07s/it] 19%|█▉        | 949/5000 [4:13:14<21:54:29, 19.47s/it]                                                       {'loss': 27.6649, 'grad_norm': 62.25, 'learning_rate': 6.632585805228954e-05, 'epoch': 1.21}
 19%|█▉        | 949/5000 [4:13:14<21:54:29, 19.47s/it] 19%|█▉        | 950/5000 [4:13:34<22:09:34, 19.70s/it]                                                       {'loss': 29.0563, 'grad_norm': 60.25, 'learning_rate': 6.631552654372672e-05, 'epoch': 1.21}
 19%|█▉        | 950/5000 [4:13:34<22:09:34, 19.70s/it] 19%|█▉        | 951/5000 [4:13:46<19:35:41, 17.42s/it]                                                       {'loss': 31.4463, 'grad_norm': 73.5, 'learning_rate': 6.630518133668865e-05, 'epoch': 1.21}
 19%|█▉        | 951/5000 [4:13:46<19:35:41, 17.42s/it] 19%|█▉        | 952/5000 [4:13:58<17:40:47, 15.72s/it]                                                       {'loss': 29.7015, 'grad_norm': 43.75, 'learning_rate': 6.629482243570072e-05, 'epoch': 1.21}
 19%|█▉        | 952/5000 [4:13:58<17:40:47, 15.72s/it] 19%|█▉        | 953/5000 [4:14:15<18:07:43, 16.13s/it]                                                       {'loss': 30.296, 'grad_norm': 43.25, 'learning_rate': 6.628444984529422e-05, 'epoch': 1.21}
 19%|█▉        | 953/5000 [4:14:15<18:07:43, 16.13s/it] 19%|█▉        | 954/5000 [4:14:26<16:37:06, 14.79s/it]                                                       {'loss': 26.8691, 'grad_norm': 71.0, 'learning_rate': 6.627406357000651e-05, 'epoch': 1.21}
 19%|█▉        | 954/5000 [4:14:26<16:37:06, 14.79s/it] 19%|█▉        | 955/5000 [4:14:46<18:05:39, 16.10s/it]                                                       {'loss': 27.979, 'grad_norm': 35.5, 'learning_rate': 6.626366361438088e-05, 'epoch': 1.21}
 19%|█▉        | 955/5000 [4:14:46<18:05:39, 16.10s/it] 19%|█▉        | 956/5000 [4:15:01<17:43:12, 15.77s/it]                                                       {'loss': 25.215, 'grad_norm': 36.25, 'learning_rate': 6.625324998296664e-05, 'epoch': 1.21}
 19%|█▉        | 956/5000 [4:15:01<17:43:12, 15.77s/it] 19%|█▉        | 957/5000 [4:15:12<16:08:02, 14.37s/it]                                                       {'loss': 29.122, 'grad_norm': 52.5, 'learning_rate': 6.624282268031904e-05, 'epoch': 1.22}
 19%|█▉        | 957/5000 [4:15:12<16:08:02, 14.37s/it] 19%|█▉        | 958/5000 [4:15:25<15:37:54, 13.92s/it]                                                       {'loss': 29.8029, 'grad_norm': 140.0, 'learning_rate': 6.623238171099936e-05, 'epoch': 1.22}
 19%|█▉        | 958/5000 [4:15:25<15:37:54, 13.92s/it] 19%|█▉        | 959/5000 [4:15:42<16:58:05, 15.12s/it]                                                       {'loss': 74.3246, 'grad_norm': 732.0, 'learning_rate': 6.622192707957483e-05, 'epoch': 1.22}
 19%|█▉        | 959/5000 [4:15:42<16:58:05, 15.12s/it] 19%|█▉        | 960/5000 [4:15:53<15:34:48, 13.88s/it]                                                       {'loss': 29.5529, 'grad_norm': 79.5, 'learning_rate': 6.621145879061865e-05, 'epoch': 1.22}
 19%|█▉        | 960/5000 [4:15:53<15:34:48, 13.88s/it] 19%|█▉        | 961/5000 [4:16:08<15:44:44, 14.03s/it]                                                       {'loss': 29.2686, 'grad_norm': 49.75, 'learning_rate': 6.620097684871e-05, 'epoch': 1.22}
 19%|█▉        | 961/5000 [4:16:08<15:44:44, 14.03s/it] 19%|█▉        | 962/5000 [4:16:32<19:15:24, 17.17s/it]                                                       {'loss': 30.2446, 'grad_norm': 246.0, 'learning_rate': 6.619048125843408e-05, 'epoch': 1.22}
 19%|█▉        | 962/5000 [4:16:32<19:15:24, 17.17s/it] 19%|█▉        | 963/5000 [4:16:59<22:31:11, 20.08s/it]                                                       {'loss': 28.5269, 'grad_norm': 183.0, 'learning_rate': 6.617997202438197e-05, 'epoch': 1.22}
 19%|█▉        | 963/5000 [4:16:59<22:31:11, 20.08s/it] 19%|█▉        | 964/5000 [4:17:13<20:16:19, 18.08s/it]                                                       {'loss': 26.1555, 'grad_norm': 151.0, 'learning_rate': 6.61694491511508e-05, 'epoch': 1.22}
 19%|█▉        | 964/5000 [4:17:13<20:16:19, 18.08s/it] 19%|█▉        | 965/5000 [4:17:32<20:37:15, 18.40s/it]                                                       {'loss': 27.2751, 'grad_norm': 100.0, 'learning_rate': 6.615891264334359e-05, 'epoch': 1.23}
 19%|█▉        | 965/5000 [4:17:32<20:37:15, 18.40s/it] 19%|█▉        | 966/5000 [4:17:43<18:18:33, 16.34s/it]                                                       {'loss': 28.6145, 'grad_norm': 872.0, 'learning_rate': 6.614836250556942e-05, 'epoch': 1.23}
 19%|█▉        | 966/5000 [4:17:43<18:18:33, 16.34s/it] 19%|█▉        | 967/5000 [4:17:57<17:30:10, 15.62s/it]                                                       {'loss': 26.4596, 'grad_norm': 98.5, 'learning_rate': 6.613779874244323e-05, 'epoch': 1.23}
 19%|█▉        | 967/5000 [4:17:57<17:30:10, 15.62s/it] 19%|█▉        | 968/5000 [4:18:12<17:19:36, 15.47s/it]                                                       {'loss': 29.9669, 'grad_norm': 65.0, 'learning_rate': 6.6127221358586e-05, 'epoch': 1.23}
 19%|█▉        | 968/5000 [4:18:12<17:19:36, 15.47s/it] 19%|█▉        | 969/5000 [4:18:26<16:35:55, 14.82s/it]                                                       {'loss': 28.0431, 'grad_norm': 56.25, 'learning_rate': 6.611663035862462e-05, 'epoch': 1.23}
 19%|█▉        | 969/5000 [4:18:26<16:35:55, 14.82s/it] 19%|█▉        | 970/5000 [4:18:39<15:55:43, 14.23s/it]                                                       {'loss': 33.2289, 'grad_norm': 540.0, 'learning_rate': 6.610602574719197e-05, 'epoch': 1.23}
 19%|█▉        | 970/5000 [4:18:39<15:55:43, 14.23s/it] 19%|█▉        | 971/5000 [4:18:54<16:17:00, 14.55s/it]                                                       {'loss': 32.001, 'grad_norm': 900.0, 'learning_rate': 6.609540752892686e-05, 'epoch': 1.23}
 19%|█▉        | 971/5000 [4:18:54<16:17:00, 14.55s/it] 19%|█▉        | 972/5000 [4:19:15<18:28:37, 16.51s/it]                                                       {'loss': 24.2673, 'grad_norm': 29.5, 'learning_rate': 6.608477570847406e-05, 'epoch': 1.23}
 19%|█▉        | 972/5000 [4:19:15<18:28:37, 16.51s/it] 19%|█▉        | 973/5000 [4:19:32<18:38:43, 16.67s/it]                                                       {'loss': 29.3751, 'grad_norm': 41.75, 'learning_rate': 6.607413029048427e-05, 'epoch': 1.24}
 19%|█▉        | 973/5000 [4:19:32<18:38:43, 16.67s/it] 19%|█▉        | 974/5000 [4:19:52<19:42:20, 17.62s/it]                                                       {'loss': 28.1359, 'grad_norm': 28.375, 'learning_rate': 6.606347127961419e-05, 'epoch': 1.24}
 19%|█▉        | 974/5000 [4:19:52<19:42:20, 17.62s/it] 20%|█▉        | 975/5000 [4:20:03<17:34:35, 15.72s/it]                                                       {'loss': 32.2401, 'grad_norm': 54.25, 'learning_rate': 6.60527986805264e-05, 'epoch': 1.24}
 20%|█▉        | 975/5000 [4:20:03<17:34:35, 15.72s/it] 20%|█▉        | 976/5000 [4:20:24<19:23:55, 17.35s/it]                                                       {'loss': 130.5735, 'grad_norm': 22912.0, 'learning_rate': 6.604211249788949e-05, 'epoch': 1.24}
 20%|█▉        | 976/5000 [4:20:24<19:23:55, 17.35s/it] 20%|█▉        | 977/5000 [4:20:52<22:59:05, 20.57s/it]                                                       {'loss': 38.1962, 'grad_norm': 2112.0, 'learning_rate': 6.603141273637794e-05, 'epoch': 1.24}
 20%|█▉        | 977/5000 [4:20:52<22:59:05, 20.57s/it] 20%|█▉        | 978/5000 [4:21:07<21:07:17, 18.91s/it]                                                       {'loss': 28.3152, 'grad_norm': 198.0, 'learning_rate': 6.60206994006722e-05, 'epoch': 1.24}
 20%|█▉        | 978/5000 [4:21:07<21:07:17, 18.91s/it] 20%|█▉        | 979/5000 [4:21:26<20:59:29, 18.79s/it]                                                       {'loss': 26.748, 'grad_norm': 52.5, 'learning_rate': 6.600997249545864e-05, 'epoch': 1.24}
 20%|█▉        | 979/5000 [4:21:26<20:59:29, 18.79s/it] 20%|█▉        | 980/5000 [4:21:50<22:40:14, 20.30s/it]                                                       {'loss': 27.0743, 'grad_norm': 65.5, 'learning_rate': 6.599923202542957e-05, 'epoch': 1.24}
 20%|█▉        | 980/5000 [4:21:50<22:40:14, 20.30s/it] 20%|█▉        | 981/5000 [4:22:02<19:57:51, 17.88s/it]                                                       {'loss': 30.9273, 'grad_norm': 172.0, 'learning_rate': 6.598847799528324e-05, 'epoch': 1.25}
 20%|█▉        | 981/5000 [4:22:02<19:57:51, 17.88s/it] 20%|█▉        | 982/5000 [4:22:19<19:37:33, 17.58s/it]                                                       {'loss': 32.8721, 'grad_norm': 100.5, 'learning_rate': 6.597771040972383e-05, 'epoch': 1.25}
 20%|█▉        | 982/5000 [4:22:19<19:37:33, 17.58s/it] 20%|█▉        | 983/5000 [4:22:33<18:19:46, 16.43s/it]                                                       {'loss': 39.9581, 'grad_norm': 1168.0, 'learning_rate': 6.596692927346144e-05, 'epoch': 1.25}
 20%|█▉        | 983/5000 [4:22:33<18:19:46, 16.43s/it] 20%|█▉        | 984/5000 [4:22:46<17:11:15, 15.41s/it]                                                       {'loss': 33.6243, 'grad_norm': 125.0, 'learning_rate': 6.595613459121212e-05, 'epoch': 1.25}
 20%|█▉        | 984/5000 [4:22:46<17:11:15, 15.41s/it] 20%|█▉        | 985/5000 [4:22:59<16:23:28, 14.70s/it]                                                       {'loss': 29.148, 'grad_norm': 151.0, 'learning_rate': 6.594532636769782e-05, 'epoch': 1.25}
 20%|█▉        | 985/5000 [4:22:59<16:23:28, 14.70s/it] 20%|█▉        | 986/5000 [4:23:13<16:24:16, 14.71s/it]                                                       {'loss': 27.2747, 'grad_norm': 214.0, 'learning_rate': 6.593450460764642e-05, 'epoch': 1.25}
 20%|█▉        | 986/5000 [4:23:13<16:24:16, 14.71s/it] 20%|█▉        | 987/5000 [4:23:28<16:24:34, 14.72s/it]                                                       {'loss': 27.4281, 'grad_norm': 100.0, 'learning_rate': 6.592366931579174e-05, 'epoch': 1.25}
 20%|█▉        | 987/5000 [4:23:28<16:24:34, 14.72s/it] 20%|█▉        | 988/5000 [4:23:46<17:25:40, 15.64s/it]                                                       {'loss': 33.1604, 'grad_norm': 149.0, 'learning_rate': 6.591282049687349e-05, 'epoch': 1.25}
 20%|█▉        | 988/5000 [4:23:46<17:25:40, 15.64s/it] 20%|█▉        | 989/5000 [4:23:59<16:41:23, 14.98s/it]                                                       {'loss': 32.3513, 'grad_norm': 114.0, 'learning_rate': 6.590195815563732e-05, 'epoch': 1.26}
 20%|█▉        | 989/5000 [4:23:59<16:41:23, 14.98s/it] 20%|█▉        | 990/5000 [4:24:13<16:09:53, 14.51s/it]                                                       {'loss': 32.0339, 'grad_norm': 102.0, 'learning_rate': 6.589108229683479e-05, 'epoch': 1.26}
 20%|█▉        | 990/5000 [4:24:13<16:09:53, 14.51s/it] 20%|█▉        | 991/5000 [4:24:26<15:49:43, 14.21s/it]                                                       {'loss': 40.3025, 'grad_norm': 260.0, 'learning_rate': 6.588019292522336e-05, 'epoch': 1.26}
 20%|█▉        | 991/5000 [4:24:26<15:49:43, 14.21s/it] 20%|█▉        | 992/5000 [4:24:49<18:40:26, 16.77s/it]                                                       {'loss': 26.9715, 'grad_norm': 45.0, 'learning_rate': 6.586929004556642e-05, 'epoch': 1.26}
 20%|█▉        | 992/5000 [4:24:49<18:40:26, 16.77s/it] 20%|█▉        | 993/5000 [4:25:13<21:00:45, 18.88s/it]                                                       {'loss': 29.85, 'grad_norm': 55.75, 'learning_rate': 6.585837366263326e-05, 'epoch': 1.26}
 20%|█▉        | 993/5000 [4:25:13<21:00:45, 18.88s/it] 20%|█▉        | 994/5000 [4:25:30<20:26:04, 18.36s/it]                                                       {'loss': 27.2101, 'grad_norm': 40.75, 'learning_rate': 6.584744378119905e-05, 'epoch': 1.26}
 20%|█▉        | 994/5000 [4:25:30<20:26:04, 18.36s/it] 20%|█▉        | 995/5000 [4:25:53<22:00:04, 19.78s/it]                                                       {'loss': 31.3339, 'grad_norm': 70.5, 'learning_rate': 6.583650040604494e-05, 'epoch': 1.26}
 20%|█▉        | 995/5000 [4:25:53<22:00:04, 19.78s/it] 20%|█▉        | 996/5000 [4:26:04<19:04:56, 17.16s/it]                                                       {'loss': 27.1742, 'grad_norm': 42.75, 'learning_rate': 6.582554354195792e-05, 'epoch': 1.26}
 20%|█▉        | 996/5000 [4:26:04<19:04:56, 17.16s/it] 20%|█▉        | 997/5000 [4:26:16<17:25:03, 15.66s/it]                                                       {'loss': 28.0325, 'grad_norm': 36.5, 'learning_rate': 6.581457319373086e-05, 'epoch': 1.27}
 20%|█▉        | 997/5000 [4:26:16<17:25:03, 15.66s/it] 20%|█▉        | 998/5000 [4:26:31<17:01:13, 15.31s/it]                                                       {'loss': 24.3898, 'grad_norm': 24.75, 'learning_rate': 6.58035893661626e-05, 'epoch': 1.27}
 20%|█▉        | 998/5000 [4:26:31<17:01:13, 15.31s/it] 20%|█▉        | 999/5000 [4:26:44<16:23:00, 14.74s/it]                                                       {'loss': 26.3937, 'grad_norm': 19.75, 'learning_rate': 6.579259206405781e-05, 'epoch': 1.27}
 20%|█▉        | 999/5000 [4:26:44<16:23:00, 14.74s/it] 20%|██        | 1000/5000 [4:27:01<17:07:18, 15.41s/it]                                                        {'loss': 27.5478, 'grad_norm': 26.0, 'learning_rate': 6.578158129222711e-05, 'epoch': 1.27}
 20%|██        | 1000/5000 [4:27:01<17:07:18, 15.41s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:09,  4.30s/it][A
  3%|▎         | 3/88 [00:15<07:49,  5.53s/it][A
  5%|▍         | 4/88 [00:20<07:10,  5.12s/it][A
  6%|▌         | 5/88 [00:24<06:29,  4.69s/it][A
  7%|▋         | 6/88 [00:29<06:41,  4.89s/it][A
  8%|▊         | 7/88 [00:33<06:08,  4.55s/it][A
  9%|▉         | 8/88 [00:37<05:44,  4.30s/it][A
 10%|█         | 9/88 [00:40<05:24,  4.11s/it][A
 11%|█▏        | 10/88 [00:43<04:48,  3.70s/it][A
 12%|█▎        | 11/88 [00:46<04:21,  3.39s/it][A
 14%|█▎        | 12/88 [00:48<03:50,  3.04s/it][A
 15%|█▍        | 13/88 [00:50<03:32,  2.84s/it][A
 16%|█▌        | 14/88 [00:56<04:26,  3.60s/it][A
 17%|█▋        | 15/88 [01:01<05:00,  4.12s/it][A
 18%|█▊        | 16/88 [01:04<04:36,  3.84s/it][A
 19%|█▉        | 17/88 [01:09<05:01,  4.25s/it][A
 20%|██        | 18/88 [01:12<04:30,  3.86s/it][A
 22%|██▏       | 19/88 [01:16<04:31,  3.94s/it][A
 23%|██▎       | 20/88 [01:20<04:23,  3.87s/it][A
 24%|██▍       | 21/88 [01:23<04:02,  3.62s/it][A
 25%|██▌       | 22/88 [01:27<03:59,  3.63s/it][A
 26%|██▌       | 23/88 [01:29<03:32,  3.26s/it][A
 27%|██▋       | 24/88 [01:38<05:07,  4.81s/it][A
 28%|██▊       | 25/88 [01:41<04:32,  4.32s/it][A
 30%|██▉       | 26/88 [01:47<05:04,  4.91s/it][A
 31%|███       | 27/88 [01:51<04:45,  4.69s/it][A
 32%|███▏      | 28/88 [02:00<05:48,  5.81s/it][A
 33%|███▎      | 29/88 [02:06<05:54,  6.01s/it][A
 34%|███▍      | 30/88 [02:11<05:33,  5.74s/it][A
 35%|███▌      | 31/88 [02:15<04:56,  5.20s/it][A
 36%|███▋      | 32/88 [02:24<05:51,  6.28s/it][A
 38%|███▊      | 33/88 [02:29<05:16,  5.76s/it][A
 39%|███▊      | 34/88 [02:37<05:56,  6.59s/it][A
 40%|███▉      | 35/88 [02:40<04:48,  5.45s/it][A
 41%|████      | 36/88 [02:45<04:36,  5.31s/it][A
 42%|████▏     | 37/88 [02:48<04:02,  4.75s/it][A
 43%|████▎     | 38/88 [02:52<03:46,  4.53s/it][A
 44%|████▍     | 39/88 [02:55<03:18,  4.04s/it][A
 45%|████▌     | 40/88 [03:02<03:48,  4.77s/it][A
 47%|████▋     | 41/88 [03:11<04:48,  6.13s/it][A
 48%|████▊     | 42/88 [03:14<04:03,  5.30s/it][A
 49%|████▉     | 43/88 [03:23<04:40,  6.24s/it][A
 50%|█████     | 44/88 [03:26<03:53,  5.32s/it][A
 51%|█████     | 45/88 [03:30<03:35,  5.00s/it][A
 52%|█████▏    | 46/88 [03:34<03:14,  4.63s/it][A
 53%|█████▎    | 47/88 [03:36<02:40,  3.92s/it][A
 55%|█████▍    | 48/88 [03:38<02:14,  3.37s/it][A
 56%|█████▌    | 49/88 [03:43<02:28,  3.81s/it][A
 57%|█████▋    | 50/88 [03:47<02:26,  3.86s/it][A
 58%|█████▊    | 51/88 [03:51<02:23,  3.88s/it][A
 59%|█████▉    | 52/88 [03:57<02:38,  4.40s/it][A
 60%|██████    | 53/88 [04:02<02:38,  4.52s/it][A
 61%|██████▏   | 54/88 [04:11<03:24,  6.01s/it][A
 62%|██████▎   | 55/88 [04:16<03:08,  5.72s/it][A
 64%|██████▎   | 56/88 [04:18<02:30,  4.71s/it][A
 65%|██████▍   | 57/88 [04:22<02:16,  4.40s/it][A
 66%|██████▌   | 58/88 [04:31<02:49,  5.66s/it][A
 67%|██████▋   | 59/88 [04:36<02:44,  5.67s/it][A
 68%|██████▊   | 60/88 [04:40<02:17,  4.93s/it][A
 69%|██████▉   | 61/88 [04:43<02:03,  4.59s/it][A
 70%|███████   | 62/88 [04:46<01:45,  4.06s/it][A
 72%|███████▏  | 63/88 [04:52<01:51,  4.45s/it][A
 73%|███████▎  | 64/88 [04:55<01:42,  4.28s/it][A
 74%|███████▍  | 65/88 [05:00<01:38,  4.26s/it][A
 75%|███████▌  | 66/88 [05:03<01:29,  4.07s/it][A
 76%|███████▌  | 67/88 [05:06<01:17,  3.70s/it][A
 77%|███████▋  | 68/88 [05:11<01:21,  4.08s/it][A
 78%|███████▊  | 69/88 [05:17<01:29,  4.70s/it][A
 80%|███████▉  | 70/88 [05:21<01:21,  4.54s/it][A
 81%|████████  | 71/88 [05:25<01:12,  4.28s/it][A
 82%|████████▏ | 72/88 [05:29<01:05,  4.11s/it][A
 83%|████████▎ | 73/88 [05:32<00:55,  3.73s/it][A
 84%|████████▍ | 74/88 [05:34<00:47,  3.38s/it][A
 85%|████████▌ | 75/88 [05:39<00:48,  3.76s/it][A
 86%|████████▋ | 76/88 [05:42<00:44,  3.72s/it][A
 88%|████████▊ | 77/88 [05:50<00:52,  4.79s/it][A
 89%|████████▊ | 78/88 [05:53<00:44,  4.46s/it][A
 90%|████████▉ | 79/88 [05:58<00:40,  4.48s/it][A
 91%|█████████ | 80/88 [06:01<00:31,  3.97s/it][A
 92%|█████████▏| 81/88 [06:06<00:29,  4.23s/it][A
 93%|█████████▎| 82/88 [06:09<00:24,  4.08s/it][A
 94%|█████████▍| 83/88 [06:13<00:19,  3.93s/it][A
 95%|█████████▌| 84/88 [06:17<00:15,  3.84s/it][A
 97%|█████████▋| 85/88 [06:19<00:10,  3.53s/it][A
 98%|█████████▊| 86/88 [06:22<00:06,  3.26s/it][A
 99%|█████████▉| 87/88 [06:26<00:03,  3.45s/it][A
100%|██████████| 88/88 [06:30<00:00,  3.52s/it][A                                                        
                                               [A{'eval_loss': 27.388214111328125, 'eval_runtime': 393.8294, 'eval_samples_per_second': 7.11, 'eval_steps_per_second': 0.223, 'epoch': 1.27}
 20%|██        | 1000/5000 [4:33:35<17:07:18, 15.41s/it]
100%|██████████| 88/88 [06:30<00:00,  3.52s/it][A
                                               [A2024-06-13 14:09:12,852 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-13 14:09:23,119 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 20%|██        | 1001/5000 [4:34:03<152:40:12, 137.44s/it]                                                          {'loss': 25.7103, 'grad_norm': 104.5, 'learning_rate': 6.577055705548697e-05, 'epoch': 1.27}
 20%|██        | 1001/5000 [4:34:03<152:40:12, 137.44s/it] 20%|██        | 1002/5000 [4:34:20<112:26:28, 101.25s/it]                                                          {'loss': 27.1258, 'grad_norm': 69.5, 'learning_rate': 6.575951935865979e-05, 'epoch': 1.27}
 20%|██        | 1002/5000 [4:34:20<112:26:28, 101.25s/it] 20%|██        | 1003/5000 [4:34:40<85:20:13, 76.86s/it]                                                          {'loss': 27.6597, 'grad_norm': 57.5, 'learning_rate': 6.57484682065738e-05, 'epoch': 1.27}
 20%|██        | 1003/5000 [4:34:40<85:20:13, 76.86s/it] 20%|██        | 1004/5000 [4:34:52<63:34:13, 57.27s/it]                                                        {'loss': 27.9598, 'grad_norm': 33.25, 'learning_rate': 6.573740360406316e-05, 'epoch': 1.27}
 20%|██        | 1004/5000 [4:34:52<63:34:13, 57.27s/it] 20%|██        | 1005/5000 [4:35:07<49:30:11, 44.61s/it]                                                        {'loss': 29.1765, 'grad_norm': 55.0, 'learning_rate': 6.572632555596791e-05, 'epoch': 1.28}
 20%|██        | 1005/5000 [4:35:07<49:30:11, 44.61s/it] 20%|██        | 1006/5000 [4:35:18<38:32:05, 34.73s/it]                                                        {'loss': 40.5356, 'grad_norm': 274.0, 'learning_rate': 6.571523406713396e-05, 'epoch': 1.28}
 20%|██        | 1006/5000 [4:35:18<38:32:05, 34.73s/it] 20%|██        | 1007/5000 [4:35:36<32:52:08, 29.63s/it]                                                        {'loss': 28.019, 'grad_norm': 65.0, 'learning_rate': 6.57041291424131e-05, 'epoch': 1.28}
 20%|██        | 1007/5000 [4:35:36<32:52:08, 29.63s/it] 20%|██        | 1008/5000 [4:35:49<27:18:57, 24.63s/it]                                                        {'loss': 26.2775, 'grad_norm': 38.25, 'learning_rate': 6.569301078666304e-05, 'epoch': 1.28}
 20%|██        | 1008/5000 [4:35:49<27:18:57, 24.63s/it] 20%|██        | 1009/5000 [4:36:02<23:17:28, 21.01s/it]                                                        {'loss': 27.2219, 'grad_norm': 42.25, 'learning_rate': 6.568187900474726e-05, 'epoch': 1.28}
 20%|██        | 1009/5000 [4:36:02<23:17:28, 21.01s/it] 20%|██        | 1010/5000 [4:36:17<21:23:25, 19.30s/it]                                                        {'loss': 25.4893, 'grad_norm': 27.0, 'learning_rate': 6.567073380153521e-05, 'epoch': 1.28}
 20%|██        | 1010/5000 [4:36:17<21:23:25, 19.30s/it] 20%|██        | 1011/5000 [4:36:40<22:41:47, 20.48s/it]                                                        {'loss': 31.8388, 'grad_norm': 154.0, 'learning_rate': 6.56595751819022e-05, 'epoch': 1.28}
 20%|██        | 1011/5000 [4:36:40<22:41:47, 20.48s/it] 20%|██        | 1012/5000 [4:37:02<23:08:29, 20.89s/it]                                                        {'loss': 30.2668, 'grad_norm': 54.75, 'learning_rate': 6.564840315072937e-05, 'epoch': 1.29}
 20%|██        | 1012/5000 [4:37:02<23:08:29, 20.89s/it] 20%|██        | 1013/5000 [4:37:17<21:12:55, 19.16s/it]                                                        {'loss': 34.6881, 'grad_norm': 3472.0, 'learning_rate': 6.563721771290376e-05, 'epoch': 1.29}
 20%|██        | 1013/5000 [4:37:17<21:12:55, 19.16s/it] 20%|██        | 1014/5000 [4:37:33<19:59:39, 18.06s/it]                                                        {'loss': 23.6551, 'grad_norm': 30.0, 'learning_rate': 6.562601887331823e-05, 'epoch': 1.29}
 20%|██        | 1014/5000 [4:37:33<19:59:39, 18.06s/it] 20%|██        | 1015/5000 [4:37:45<17:57:24, 16.22s/it]                                                        {'loss': 28.1765, 'grad_norm': 59.5, 'learning_rate': 6.561480663687157e-05, 'epoch': 1.29}
 20%|██        | 1015/5000 [4:37:45<17:57:24, 16.22s/it] 20%|██        | 1016/5000 [4:37:58<16:53:00, 15.26s/it]                                                        {'loss': 27.9664, 'grad_norm': 40.0, 'learning_rate': 6.560358100846836e-05, 'epoch': 1.29}
 20%|██        | 1016/5000 [4:37:58<16:53:00, 15.26s/it] 20%|██        | 1017/5000 [4:38:12<16:29:36, 14.91s/it]                                                        {'loss': 29.5687, 'grad_norm': 165.0, 'learning_rate': 6.55923419930191e-05, 'epoch': 1.29}
 20%|██        | 1017/5000 [4:38:12<16:29:36, 14.91s/it] 20%|██        | 1018/5000 [4:38:34<18:48:09, 17.00s/it]                                                        {'loss': 28.3639, 'grad_norm': 80.0, 'learning_rate': 6.55810895954401e-05, 'epoch': 1.29}
 20%|██        | 1018/5000 [4:38:34<18:48:09, 17.00s/it] 20%|██        | 1019/5000 [4:38:55<20:24:22, 18.45s/it]                                                        {'loss': 27.5804, 'grad_norm': 215.0, 'learning_rate': 6.556982382065354e-05, 'epoch': 1.29}
 20%|██        | 1019/5000 [4:38:55<20:24:22, 18.45s/it] 20%|██        | 1020/5000 [4:39:09<18:55:06, 17.11s/it]                                                        {'loss': 27.0578, 'grad_norm': 43.0, 'learning_rate': 6.555854467358745e-05, 'epoch': 1.3}
 20%|██        | 1020/5000 [4:39:09<18:55:06, 17.11s/it] 20%|██        | 1021/5000 [4:39:20<16:55:19, 15.31s/it]                                                        {'loss': 27.6345, 'grad_norm': 35.0, 'learning_rate': 6.554725215917573e-05, 'epoch': 1.3}
 20%|██        | 1021/5000 [4:39:21<16:55:19, 15.31s/it] 20%|██        | 1022/5000 [4:39:32<15:39:45, 14.17s/it]                                                        {'loss': 27.5793, 'grad_norm': 45.75, 'learning_rate': 6.55359462823581e-05, 'epoch': 1.3}
 20%|██        | 1022/5000 [4:39:32<15:39:45, 14.17s/it] 20%|██        | 1023/5000 [4:39:48<16:23:28, 14.84s/it]                                                        {'loss': 26.2052, 'grad_norm': 278.0, 'learning_rate': 6.552462704808013e-05, 'epoch': 1.3}
 20%|██        | 1023/5000 [4:39:48<16:23:28, 14.84s/it] 20%|██        | 1024/5000 [4:40:00<15:25:50, 13.97s/it]                                                        {'loss': 27.6573, 'grad_norm': 63.5, 'learning_rate': 6.551329446129325e-05, 'epoch': 1.3}
 20%|██        | 1024/5000 [4:40:00<15:25:50, 13.97s/it] 20%|██        | 1025/5000 [4:40:16<15:59:18, 14.48s/it]                                                        {'loss': 26.5591, 'grad_norm': 36.0, 'learning_rate': 6.550194852695469e-05, 'epoch': 1.3}
 20%|██        | 1025/5000 [4:40:16<15:59:18, 14.48s/it] 21%|██        | 1026/5000 [4:40:46<21:09:57, 19.17s/it]                                                        {'loss': 24.2228, 'grad_norm': 27.625, 'learning_rate': 6.54905892500276e-05, 'epoch': 1.3}
 21%|██        | 1026/5000 [4:40:46<21:09:57, 19.17s/it] 21%|██        | 1027/5000 [4:40:58<18:39:53, 16.91s/it]                                                        {'loss': 28.3757, 'grad_norm': 71.5, 'learning_rate': 6.547921663548085e-05, 'epoch': 1.3}
 21%|██        | 1027/5000 [4:40:58<18:39:53, 16.91s/it] 21%|██        | 1028/5000 [4:41:12<17:47:16, 16.12s/it]                                                        {'loss': 25.1437, 'grad_norm': 33.0, 'learning_rate': 6.546783068828925e-05, 'epoch': 1.31}
 21%|██        | 1028/5000 [4:41:12<17:47:16, 16.12s/it] 21%|██        | 1029/5000 [4:41:25<16:42:46, 15.15s/it]                                                        {'loss': 25.6193, 'grad_norm': 36.0, 'learning_rate': 6.54564314134334e-05, 'epoch': 1.31}
 21%|██        | 1029/5000 [4:41:25<16:42:46, 15.15s/it] 21%|██        | 1030/5000 [4:41:37<15:31:03, 14.07s/it]                                                        {'loss': 27.8598, 'grad_norm': 82.0, 'learning_rate': 6.54450188158997e-05, 'epoch': 1.31}
 21%|██        | 1030/5000 [4:41:37<15:31:03, 14.07s/it] 21%|██        | 1031/5000 [4:41:52<15:56:30, 14.46s/it]                                                        {'loss': 23.4427, 'grad_norm': 19.5, 'learning_rate': 6.543359290068044e-05, 'epoch': 1.31}
 21%|██        | 1031/5000 [4:41:52<15:56:30, 14.46s/it] 21%|██        | 1032/5000 [4:42:03<14:57:03, 13.56s/it]                                                        {'loss': 28.4202, 'grad_norm': 48.0, 'learning_rate': 6.542215367277368e-05, 'epoch': 1.31}
 21%|██        | 1032/5000 [4:42:03<14:57:03, 13.56s/it] 21%|██        | 1033/5000 [4:42:17<14:49:16, 13.45s/it]                                                        {'loss': 33.93, 'grad_norm': 61.25, 'learning_rate': 6.541070113718335e-05, 'epoch': 1.31}
 21%|██        | 1033/5000 [4:42:17<14:49:16, 13.45s/it] 21%|██        | 1034/5000 [4:42:33<15:51:40, 14.40s/it]                                                        {'loss': 34.0854, 'grad_norm': 158.0, 'learning_rate': 6.539923529891914e-05, 'epoch': 1.31}
 21%|██        | 1034/5000 [4:42:33<15:51:40, 14.40s/it] 21%|██        | 1035/5000 [4:42:46<15:24:31, 13.99s/it]                                                        {'loss': 26.3369, 'grad_norm': 37.75, 'learning_rate': 6.538775616299665e-05, 'epoch': 1.31}
 21%|██        | 1035/5000 [4:42:46<15:24:31, 13.99s/it] 21%|██        | 1036/5000 [4:43:09<18:10:22, 16.50s/it]                                                        {'loss': 29.9372, 'grad_norm': 1488.0, 'learning_rate': 6.537626373443721e-05, 'epoch': 1.32}
 21%|██        | 1036/5000 [4:43:09<18:10:22, 16.50s/it] 21%|██        | 1037/5000 [4:43:25<17:59:20, 16.34s/it]                                                        {'loss': 26.4691, 'grad_norm': 113.5, 'learning_rate': 6.536475801826801e-05, 'epoch': 1.32}
 21%|██        | 1037/5000 [4:43:25<17:59:20, 16.34s/it] 21%|██        | 1038/5000 [4:43:47<19:56:48, 18.12s/it]                                                        {'loss': 26.6348, 'grad_norm': 51.75, 'learning_rate': 6.535323901952203e-05, 'epoch': 1.32}
 21%|██        | 1038/5000 [4:43:47<19:56:48, 18.12s/it] 21%|██        | 1039/5000 [4:44:04<19:36:38, 17.82s/it]                                                        {'loss': 31.0882, 'grad_norm': 308.0, 'learning_rate': 6.534170674323808e-05, 'epoch': 1.32}
 21%|██        | 1039/5000 [4:44:04<19:36:38, 17.82s/it] 21%|██        | 1040/5000 [4:44:16<17:44:57, 16.14s/it]                                                        {'loss': 27.7525, 'grad_norm': 72.5, 'learning_rate': 6.533016119446076e-05, 'epoch': 1.32}
 21%|██        | 1040/5000 [4:44:16<17:44:57, 16.14s/it] 21%|██        | 1041/5000 [4:44:41<20:31:04, 18.66s/it]                                                        {'loss': 34.1725, 'grad_norm': 164.0, 'learning_rate': 6.53186023782405e-05, 'epoch': 1.32}
 21%|██        | 1041/5000 [4:44:41<20:31:04, 18.66s/it] 21%|██        | 1042/5000 [4:45:04<22:10:49, 20.17s/it]                                                        {'loss': 25.654, 'grad_norm': 41.75, 'learning_rate': 6.53070302996335e-05, 'epoch': 1.32}
 21%|██        | 1042/5000 [4:45:04<22:10:49, 20.17s/it] 21%|██        | 1043/5000 [4:45:26<22:38:48, 20.60s/it]                                                        {'loss': 25.8617, 'grad_norm': 49.25, 'learning_rate': 6.529544496370181e-05, 'epoch': 1.32}
 21%|██        | 1043/5000 [4:45:26<22:38:48, 20.60s/it] 21%|██        | 1044/5000 [4:45:37<19:32:43, 17.79s/it]                                                        {'loss': 28.5675, 'grad_norm': 51.25, 'learning_rate': 6.528384637551323e-05, 'epoch': 1.33}
 21%|██        | 1044/5000 [4:45:37<19:32:43, 17.79s/it] 21%|██        | 1045/5000 [4:45:49<17:43:33, 16.13s/it]                                                        {'loss': 30.0424, 'grad_norm': 76.0, 'learning_rate': 6.527223454014136e-05, 'epoch': 1.33}
 21%|██        | 1045/5000 [4:45:49<17:43:33, 16.13s/it] 21%|██        | 1046/5000 [4:46:11<19:35:20, 17.84s/it]                                                        {'loss': 28.3661, 'grad_norm': 35.5, 'learning_rate': 6.526060946266565e-05, 'epoch': 1.33}
 21%|██        | 1046/5000 [4:46:11<19:35:20, 17.84s/it] 21%|██        | 1047/5000 [4:46:23<17:41:25, 16.11s/it]                                                        {'loss': 26.5286, 'grad_norm': 82.0, 'learning_rate': 6.524897114817126e-05, 'epoch': 1.33}
 21%|██        | 1047/5000 [4:46:23<17:41:25, 16.11s/it] 21%|██        | 1048/5000 [4:46:40<17:49:02, 16.23s/it]                                                        {'loss': 26.5099, 'grad_norm': 52.5, 'learning_rate': 6.523731960174923e-05, 'epoch': 1.33}
 21%|██        | 1048/5000 [4:46:40<17:49:02, 16.23s/it] 21%|██        | 1049/5000 [4:46:57<18:02:43, 16.44s/it]                                                        {'loss': 25.4834, 'grad_norm': 44.25, 'learning_rate': 6.522565482849632e-05, 'epoch': 1.33}
 21%|██        | 1049/5000 [4:46:57<18:02:43, 16.44s/it] 21%|██        | 1050/5000 [4:47:08<16:26:20, 14.98s/it]                                                        {'loss': 45.9764, 'grad_norm': 732.0, 'learning_rate': 6.521397683351509e-05, 'epoch': 1.33}
 21%|██        | 1050/5000 [4:47:08<16:26:20, 14.98s/it] 21%|██        | 1051/5000 [4:47:23<16:14:42, 14.81s/it]                                                        {'loss': 27.7907, 'grad_norm': 71.5, 'learning_rate': 6.520228562191389e-05, 'epoch': 1.33}
 21%|██        | 1051/5000 [4:47:23<16:14:42, 14.81s/it] 21%|██        | 1052/5000 [4:47:34<14:58:28, 13.65s/it]                                                        {'loss': 114.245, 'grad_norm': 3664.0, 'learning_rate': 6.519058119880686e-05, 'epoch': 1.34}
 21%|██        | 1052/5000 [4:47:34<14:58:28, 13.65s/it] 21%|██        | 1053/5000 [4:47:51<16:01:49, 14.62s/it]                                                        {'loss': 30.0815, 'grad_norm': 109.5, 'learning_rate': 6.517886356931394e-05, 'epoch': 1.34}
 21%|██        | 1053/5000 [4:47:51<16:01:49, 14.62s/it] 21%|██        | 1054/5000 [4:48:04<15:41:22, 14.31s/it]                                                        {'loss': 28.6698, 'grad_norm': 227.0, 'learning_rate': 6.516713273856078e-05, 'epoch': 1.34}
 21%|██        | 1054/5000 [4:48:04<15:41:22, 14.31s/it] 21%|██        | 1055/5000 [4:48:20<16:19:47, 14.90s/it]                                                        {'loss': 26.7437, 'grad_norm': 40.0, 'learning_rate': 6.515538871167885e-05, 'epoch': 1.34}
 21%|██        | 1055/5000 [4:48:20<16:19:47, 14.90s/it] 21%|██        | 1056/5000 [4:48:36<16:38:18, 15.19s/it]                                                        {'loss': 30.1344, 'grad_norm': 59.5, 'learning_rate': 6.51436314938054e-05, 'epoch': 1.34}
 21%|██        | 1056/5000 [4:48:36<16:38:18, 15.19s/it] 21%|██        | 1057/5000 [4:48:51<16:18:03, 14.88s/it]                                                        {'loss': 23.8051, 'grad_norm': 26.125, 'learning_rate': 6.513186109008343e-05, 'epoch': 1.34}
 21%|██        | 1057/5000 [4:48:51<16:18:03, 14.88s/it] 21%|██        | 1058/5000 [4:49:02<15:19:53, 14.00s/it]                                                        {'loss': 27.5466, 'grad_norm': 109.5, 'learning_rate': 6.512007750566172e-05, 'epoch': 1.34}
 21%|██        | 1058/5000 [4:49:02<15:19:53, 14.00s/it] 21%|██        | 1059/5000 [4:49:17<15:28:46, 14.14s/it]                                                        {'loss': 28.4633, 'grad_norm': 129.0, 'learning_rate': 6.51082807456948e-05, 'epoch': 1.34}
 21%|██        | 1059/5000 [4:49:17<15:28:46, 14.14s/it] 21%|██        | 1060/5000 [4:49:31<15:17:50, 13.98s/it]                                                        {'loss': 27.8422, 'grad_norm': 44.75, 'learning_rate': 6.509647081534299e-05, 'epoch': 1.35}
 21%|██        | 1060/5000 [4:49:31<15:17:50, 13.98s/it] 21%|██        | 1061/5000 [4:49:46<15:54:07, 14.53s/it]                                                        {'loss': 26.4443, 'grad_norm': 24.625, 'learning_rate': 6.508464771977234e-05, 'epoch': 1.35}
 21%|██        | 1061/5000 [4:49:46<15:54:07, 14.53s/it] 21%|██        | 1062/5000 [4:50:11<19:11:32, 17.55s/it]                                                        {'loss': 28.6526, 'grad_norm': 50.25, 'learning_rate': 6.507281146415466e-05, 'epoch': 1.35}
 21%|██        | 1062/5000 [4:50:11<19:11:32, 17.55s/it] 21%|██▏       | 1063/5000 [4:50:28<18:53:43, 17.28s/it]                                                        {'loss': 26.7902, 'grad_norm': 43.75, 'learning_rate': 6.506096205366757e-05, 'epoch': 1.35}
 21%|██▏       | 1063/5000 [4:50:28<18:53:43, 17.28s/it] 21%|██▏       | 1064/5000 [4:50:43<18:19:42, 16.76s/it]                                                        {'loss': 24.9303, 'grad_norm': 137.0, 'learning_rate': 6.504909949349438e-05, 'epoch': 1.35}
 21%|██▏       | 1064/5000 [4:50:43<18:19:42, 16.76s/it] 21%|██▏       | 1065/5000 [4:50:57<17:19:10, 15.85s/it]                                                        {'loss': 28.6836, 'grad_norm': 95.5, 'learning_rate': 6.503722378882418e-05, 'epoch': 1.35}
 21%|██▏       | 1065/5000 [4:50:57<17:19:10, 15.85s/it] 21%|██▏       | 1066/5000 [4:51:14<17:44:50, 16.24s/it]                                                        {'loss': 27.5511, 'grad_norm': 36.0, 'learning_rate': 6.50253349448518e-05, 'epoch': 1.35}
 21%|██▏       | 1066/5000 [4:51:14<17:44:50, 16.24s/it] 21%|██▏       | 1067/5000 [4:51:25<16:08:10, 14.77s/it]                                                        {'loss': 28.0551, 'grad_norm': 38.25, 'learning_rate': 6.501343296677782e-05, 'epoch': 1.35}
 21%|██▏       | 1067/5000 [4:51:25<16:08:10, 14.77s/it] 21%|██▏       | 1068/5000 [4:51:39<15:54:48, 14.57s/it]                                                        {'loss': 26.0542, 'grad_norm': 70.5, 'learning_rate': 6.50015178598086e-05, 'epoch': 1.36}
 21%|██▏       | 1068/5000 [4:51:39<15:54:48, 14.57s/it] 21%|██▏       | 1069/5000 [4:51:54<15:47:48, 14.47s/it]                                                        {'loss': 25.792, 'grad_norm': 36.75, 'learning_rate': 6.498958962915618e-05, 'epoch': 1.36}
 21%|██▏       | 1069/5000 [4:51:54<15:47:48, 14.47s/it] 21%|██▏       | 1070/5000 [4:52:07<15:26:36, 14.15s/it]                                                        {'loss': 30.0767, 'grad_norm': 181.0, 'learning_rate': 6.49776482800384e-05, 'epoch': 1.36}
 21%|██▏       | 1070/5000 [4:52:07<15:26:36, 14.15s/it] 21%|██▏       | 1071/5000 [4:52:24<16:15:02, 14.89s/it]                                                        {'loss': 24.7165, 'grad_norm': 44.25, 'learning_rate': 6.496569381767878e-05, 'epoch': 1.36}
 21%|██▏       | 1071/5000 [4:52:24<16:15:02, 14.89s/it] 21%|██▏       | 1072/5000 [4:52:40<16:35:14, 15.20s/it]                                                        {'loss': 22.66, 'grad_norm': 22.5, 'learning_rate': 6.495372624730664e-05, 'epoch': 1.36}
 21%|██▏       | 1072/5000 [4:52:40<16:35:14, 15.20s/it] 21%|██▏       | 1073/5000 [4:53:05<19:49:55, 18.18s/it]                                                        {'loss': 28.161, 'grad_norm': 40.5, 'learning_rate': 6.494174557415697e-05, 'epoch': 1.36}
 21%|██▏       | 1073/5000 [4:53:05<19:49:55, 18.18s/it] 21%|██▏       | 1074/5000 [4:53:18<18:05:41, 16.59s/it]                                                        {'loss': 27.986, 'grad_norm': 170.0, 'learning_rate': 6.492975180347055e-05, 'epoch': 1.36}
 21%|██▏       | 1074/5000 [4:53:18<18:05:41, 16.59s/it] 22%|██▏       | 1075/5000 [4:53:31<17:03:46, 15.65s/it]                                                        {'loss': 25.1243, 'grad_norm': 144.0, 'learning_rate': 6.491774494049386e-05, 'epoch': 1.37}
 22%|██▏       | 1075/5000 [4:53:31<17:03:46, 15.65s/it] 22%|██▏       | 1076/5000 [4:53:55<19:46:56, 18.15s/it]                                                        {'loss': 25.0007, 'grad_norm': 82.5, 'learning_rate': 6.49057249904791e-05, 'epoch': 1.37}
 22%|██▏       | 1076/5000 [4:53:55<19:46:56, 18.15s/it] 22%|██▏       | 1077/5000 [4:54:09<18:30:58, 16.99s/it]                                                        {'loss': 24.321, 'grad_norm': 58.5, 'learning_rate': 6.48936919586842e-05, 'epoch': 1.37}
 22%|██▏       | 1077/5000 [4:54:09<18:30:58, 16.99s/it] 22%|██▏       | 1078/5000 [4:54:22<17:10:20, 15.76s/it]                                                        {'loss': 23.6196, 'grad_norm': 18.125, 'learning_rate': 6.488164585037283e-05, 'epoch': 1.37}
 22%|██▏       | 1078/5000 [4:54:22<17:10:20, 15.76s/it] 22%|██▏       | 1079/5000 [4:54:39<17:27:49, 16.03s/it]                                                        {'loss': 23.8962, 'grad_norm': 27.5, 'learning_rate': 6.486958667081438e-05, 'epoch': 1.37}
 22%|██▏       | 1079/5000 [4:54:39<17:27:49, 16.03s/it] 22%|██▏       | 1080/5000 [4:54:50<15:49:06, 14.53s/it]                                                        {'loss': 27.753, 'grad_norm': 66.5, 'learning_rate': 6.48575144252839e-05, 'epoch': 1.37}
 22%|██▏       | 1080/5000 [4:54:50<15:49:06, 14.53s/it] 22%|██▏       | 1081/5000 [4:55:03<15:20:59, 14.10s/it]                                                        {'loss': 26.1239, 'grad_norm': 47.5, 'learning_rate': 6.484542911906224e-05, 'epoch': 1.37}
 22%|██▏       | 1081/5000 [4:55:03<15:20:59, 14.10s/it] 22%|██▏       | 1082/5000 [4:55:17<15:09:13, 13.92s/it]                                                        {'loss': 25.0285, 'grad_norm': 63.5, 'learning_rate': 6.483333075743592e-05, 'epoch': 1.37}
 22%|██▏       | 1082/5000 [4:55:17<15:09:13, 13.92s/it] 22%|██▏       | 1083/5000 [4:55:28<14:14:29, 13.09s/it]                                                        {'loss': 27.3942, 'grad_norm': 46.5, 'learning_rate': 6.482121934569716e-05, 'epoch': 1.38}
 22%|██▏       | 1083/5000 [4:55:28<14:14:29, 13.09s/it] 22%|██▏       | 1084/5000 [4:55:52<17:53:49, 16.45s/it]                                                        {'loss': 24.3863, 'grad_norm': 34.0, 'learning_rate': 6.48090948891439e-05, 'epoch': 1.38}
 22%|██▏       | 1084/5000 [4:55:52<17:53:49, 16.45s/it] 22%|██▏       | 1085/5000 [4:56:08<17:42:46, 16.29s/it]                                                        {'loss': 28.1616, 'grad_norm': 29.75, 'learning_rate': 6.47969573930798e-05, 'epoch': 1.38}
 22%|██▏       | 1085/5000 [4:56:08<17:42:46, 16.29s/it] 22%|██▏       | 1086/5000 [4:56:31<20:00:56, 18.41s/it]                                                        {'loss': 25.6058, 'grad_norm': 231.0, 'learning_rate': 6.478480686281421e-05, 'epoch': 1.38}
 22%|██▏       | 1086/5000 [4:56:31<20:00:56, 18.41s/it] 22%|██▏       | 1087/5000 [4:56:53<21:08:57, 19.46s/it]                                                        {'loss': 27.5382, 'grad_norm': 144.0, 'learning_rate': 6.477264330366218e-05, 'epoch': 1.38}
 22%|██▏       | 1087/5000 [4:56:53<21:08:57, 19.46s/it] 22%|██▏       | 1088/5000 [4:57:05<18:42:58, 17.22s/it]                                                        {'loss': 24.7127, 'grad_norm': 30.875, 'learning_rate': 6.476046672094448e-05, 'epoch': 1.38}
 22%|██▏       | 1088/5000 [4:57:05<18:42:58, 17.22s/it] 22%|██▏       | 1089/5000 [4:57:20<18:03:31, 16.62s/it]                                                        {'loss': 29.363, 'grad_norm': 1360.0, 'learning_rate': 6.474827711998756e-05, 'epoch': 1.38}
 22%|██▏       | 1089/5000 [4:57:20<18:03:31, 16.62s/it] 22%|██▏       | 1090/5000 [4:57:33<16:39:42, 15.34s/it]                                                        {'loss': 41.122, 'grad_norm': 110.0, 'learning_rate': 6.473607450612353e-05, 'epoch': 1.38}
 22%|██▏       | 1090/5000 [4:57:33<16:39:42, 15.34s/it] 22%|██▏       | 1091/5000 [4:57:50<17:07:37, 15.77s/it]                                                        {'loss': 24.9538, 'grad_norm': 96.0, 'learning_rate': 6.472385888469027e-05, 'epoch': 1.39}
 22%|██▏       | 1091/5000 [4:57:50<17:07:37, 15.77s/it] 22%|██▏       | 1092/5000 [4:58:01<15:51:54, 14.61s/it]                                                        {'loss': 36.6768, 'grad_norm': 195.0, 'learning_rate': 6.471163026103128e-05, 'epoch': 1.39}
 22%|██▏       | 1092/5000 [4:58:01<15:51:54, 14.61s/it] 22%|██▏       | 1093/5000 [4:58:14<15:04:55, 13.90s/it]                                                        {'loss': 27.2411, 'grad_norm': 70.5, 'learning_rate': 6.46993886404958e-05, 'epoch': 1.39}
 22%|██▏       | 1093/5000 [4:58:14<15:04:55, 13.90s/it] 22%|██▏       | 1094/5000 [4:58:27<14:47:18, 13.63s/it]                                                        {'loss': 31.159, 'grad_norm': 136.0, 'learning_rate': 6.468713402843872e-05, 'epoch': 1.39}
 22%|██▏       | 1094/5000 [4:58:27<14:47:18, 13.63s/it] 22%|██▏       | 1095/5000 [4:58:41<14:51:42, 13.70s/it]                                                        {'loss': 29.113, 'grad_norm': 109.0, 'learning_rate': 6.46748664302206e-05, 'epoch': 1.39}
 22%|██▏       | 1095/5000 [4:58:41<14:51:42, 13.70s/it] 22%|██▏       | 1096/5000 [4:58:54<14:37:57, 13.49s/it]                                                        {'loss': 25.1385, 'grad_norm': 54.5, 'learning_rate': 6.466258585120774e-05, 'epoch': 1.39}
 22%|██▏       | 1096/5000 [4:58:54<14:37:57, 13.49s/it] 22%|██▏       | 1097/5000 [4:59:07<14:29:58, 13.37s/it]                                                        {'loss': 28.4628, 'grad_norm': 81.0, 'learning_rate': 6.465029229677207e-05, 'epoch': 1.39}
 22%|██▏       | 1097/5000 [4:59:07<14:29:58, 13.37s/it] 22%|██▏       | 1098/5000 [4:59:20<14:27:04, 13.33s/it]                                                        {'loss': 31.51, 'grad_norm': 420.0, 'learning_rate': 6.463798577229121e-05, 'epoch': 1.39}
 22%|██▏       | 1098/5000 [4:59:20<14:27:04, 13.33s/it] 22%|██▏       | 1099/5000 [4:59:43<17:31:35, 16.17s/it]                                                        {'loss': 29.6373, 'grad_norm': 328.0, 'learning_rate': 6.462566628314844e-05, 'epoch': 1.4}
 22%|██▏       | 1099/5000 [4:59:43<17:31:35, 16.17s/it] 22%|██▏       | 1100/5000 [4:59:56<16:41:35, 15.41s/it]                                                        {'loss': 24.2938, 'grad_norm': 336.0, 'learning_rate': 6.461333383473272e-05, 'epoch': 1.4}
 22%|██▏       | 1100/5000 [4:59:56<16:41:35, 15.41s/it] 22%|██▏       | 1101/5000 [5:00:10<16:06:17, 14.87s/it]                                                        {'loss': 26.8871, 'grad_norm': 61.0, 'learning_rate': 6.460098843243872e-05, 'epoch': 1.4}
 22%|██▏       | 1101/5000 [5:00:10<16:06:17, 14.87s/it] 22%|██▏       | 1102/5000 [5:00:27<16:56:24, 15.64s/it]                                                        {'loss': 25.623, 'grad_norm': 29.875, 'learning_rate': 6.45886300816667e-05, 'epoch': 1.4}
 22%|██▏       | 1102/5000 [5:00:27<16:56:24, 15.64s/it] 22%|██▏       | 1103/5000 [5:00:42<16:27:40, 15.21s/it]                                                        {'loss': 25.3474, 'grad_norm': 26.125, 'learning_rate': 6.457625878782263e-05, 'epoch': 1.4}
 22%|██▏       | 1103/5000 [5:00:42<16:27:40, 15.21s/it] 22%|██▏       | 1104/5000 [5:01:03<18:20:24, 16.95s/it]                                                        {'loss': 24.2421, 'grad_norm': 65.5, 'learning_rate': 6.456387455631815e-05, 'epoch': 1.4}
 22%|██▏       | 1104/5000 [5:01:03<18:20:24, 16.95s/it] 22%|██▏       | 1105/5000 [5:01:22<19:02:55, 17.61s/it]                                                        {'loss': 24.0403, 'grad_norm': 49.5, 'learning_rate': 6.455147739257053e-05, 'epoch': 1.4}
 22%|██▏       | 1105/5000 [5:01:22<19:02:55, 17.61s/it] 22%|██▏       | 1106/5000 [5:01:33<17:01:22, 15.74s/it]                                                        {'loss': 26.498, 'grad_norm': 178.0, 'learning_rate': 6.45390673020027e-05, 'epoch': 1.4}
 22%|██▏       | 1106/5000 [5:01:33<17:01:22, 15.74s/it] 22%|██▏       | 1107/5000 [5:01:53<18:23:45, 17.01s/it]                                                        {'loss': 24.5381, 'grad_norm': 26.5, 'learning_rate': 6.452664429004327e-05, 'epoch': 1.41}
 22%|██▏       | 1107/5000 [5:01:53<18:23:45, 17.01s/it] 22%|██▏       | 1108/5000 [5:02:18<20:54:38, 19.34s/it]                                                        {'loss': 23.128, 'grad_norm': 22.75, 'learning_rate': 6.45142083621265e-05, 'epoch': 1.41}
 22%|██▏       | 1108/5000 [5:02:18<20:54:38, 19.34s/it] 22%|██▏       | 1109/5000 [5:02:32<19:04:16, 17.65s/it]                                                        {'loss': 27.8574, 'grad_norm': 121.0, 'learning_rate': 6.450175952369226e-05, 'epoch': 1.41}
 22%|██▏       | 1109/5000 [5:02:32<19:04:16, 17.65s/it] 22%|██▏       | 1110/5000 [5:02:44<17:30:33, 16.20s/it]                                                        {'loss': 28.0851, 'grad_norm': 33.0, 'learning_rate': 6.44892977801861e-05, 'epoch': 1.41}
 22%|██▏       | 1110/5000 [5:02:44<17:30:33, 16.20s/it] 22%|██▏       | 1111/5000 [5:02:58<16:33:54, 15.33s/it]                                                        {'loss': 26.8525, 'grad_norm': 52.25, 'learning_rate': 6.447682313705924e-05, 'epoch': 1.41}
 22%|██▏       | 1111/5000 [5:02:58<16:33:54, 15.33s/it] 22%|██▏       | 1112/5000 [5:03:13<16:39:36, 15.43s/it]                                                        {'loss': 25.2469, 'grad_norm': 85.5, 'learning_rate': 6.446433559976849e-05, 'epoch': 1.41}
 22%|██▏       | 1112/5000 [5:03:13<16:39:36, 15.43s/it] 22%|██▏       | 1113/5000 [5:03:31<17:22:30, 16.09s/it]                                                        {'loss': 26.1212, 'grad_norm': 62.75, 'learning_rate': 6.44518351737763e-05, 'epoch': 1.41}
 22%|██▏       | 1113/5000 [5:03:31<17:22:30, 16.09s/it] 22%|██▏       | 1114/5000 [5:03:47<17:15:12, 15.98s/it]                                                        {'loss': 27.1494, 'grad_norm': 50.75, 'learning_rate': 6.443932186455081e-05, 'epoch': 1.41}
 22%|██▏       | 1114/5000 [5:03:47<17:15:12, 15.98s/it] 22%|██▏       | 1115/5000 [5:04:00<16:26:28, 15.24s/it]                                                        {'loss': 27.6007, 'grad_norm': 62.25, 'learning_rate': 6.442679567756577e-05, 'epoch': 1.42}
 22%|██▏       | 1115/5000 [5:04:00<16:26:28, 15.24s/it] 22%|██▏       | 1116/5000 [5:04:12<15:25:25, 14.30s/it]                                                        {'loss': 29.5654, 'grad_norm': 49.0, 'learning_rate': 6.441425661830055e-05, 'epoch': 1.42}
 22%|██▏       | 1116/5000 [5:04:12<15:25:25, 14.30s/it] 22%|██▏       | 1117/5000 [5:04:36<18:28:32, 17.13s/it]                                                        {'loss': 23.51, 'grad_norm': 41.25, 'learning_rate': 6.440170469224016e-05, 'epoch': 1.42}
 22%|██▏       | 1117/5000 [5:04:36<18:28:32, 17.13s/it] 22%|██▏       | 1118/5000 [5:04:50<17:17:57, 16.04s/it]                                                        {'loss': 28.3477, 'grad_norm': 45.25, 'learning_rate': 6.438913990487525e-05, 'epoch': 1.42}
 22%|██▏       | 1118/5000 [5:04:50<17:17:57, 16.04s/it] 22%|██▏       | 1119/5000 [5:05:03<16:26:11, 15.25s/it]                                                        {'loss': 26.291, 'grad_norm': 31.0, 'learning_rate': 6.437656226170206e-05, 'epoch': 1.42}
 22%|██▏       | 1119/5000 [5:05:03<16:26:11, 15.25s/it] 22%|██▏       | 1120/5000 [5:05:17<15:58:16, 14.82s/it]                                                        {'loss': 56.6446, 'grad_norm': 6048.0, 'learning_rate': 6.43639717682225e-05, 'epoch': 1.42}
 22%|██▏       | 1120/5000 [5:05:17<15:58:16, 14.82s/it] 22%|██▏       | 1121/5000 [5:05:31<15:49:10, 14.68s/it]                                                        {'loss': 24.9149, 'grad_norm': 50.25, 'learning_rate': 6.435136842994407e-05, 'epoch': 1.42}
 22%|██▏       | 1121/5000 [5:05:31<15:49:10, 14.68s/it] 22%|██▏       | 1122/5000 [5:05:47<16:14:50, 15.08s/it]                                                        {'loss': 23.5578, 'grad_norm': 22.75, 'learning_rate': 6.433875225237991e-05, 'epoch': 1.42}
 22%|██▏       | 1122/5000 [5:05:47<16:14:50, 15.08s/it] 22%|██▏       | 1123/5000 [5:06:02<16:13:53, 15.07s/it]                                                        {'loss': 26.1258, 'grad_norm': 278.0, 'learning_rate': 6.432612324104875e-05, 'epoch': 1.43}
 22%|██▏       | 1123/5000 [5:06:02<16:13:53, 15.07s/it] 22%|██▏       | 1124/5000 [5:06:17<16:02:36, 14.90s/it]                                                        {'loss': 25.894, 'grad_norm': 45.25, 'learning_rate': 6.431348140147496e-05, 'epoch': 1.43}
 22%|██▏       | 1124/5000 [5:06:17<16:02:36, 14.90s/it] 22%|██▎       | 1125/5000 [5:06:30<15:35:53, 14.49s/it]                                                        {'loss': 23.9821, 'grad_norm': 35.75, 'learning_rate': 6.430082673918849e-05, 'epoch': 1.43}
 22%|██▎       | 1125/5000 [5:06:30<15:35:53, 14.49s/it] 23%|██▎       | 1126/5000 [5:06:45<15:42:54, 14.60s/it]                                                        {'loss': 27.5836, 'grad_norm': 49.0, 'learning_rate': 6.428815925972495e-05, 'epoch': 1.43}
 23%|██▎       | 1126/5000 [5:06:45<15:42:54, 14.60s/it] 23%|██▎       | 1127/5000 [5:07:06<17:47:29, 16.54s/it]                                                        {'loss': 23.8253, 'grad_norm': 22.375, 'learning_rate': 6.427547896862551e-05, 'epoch': 1.43}
 23%|██▎       | 1127/5000 [5:07:06<17:47:29, 16.54s/it] 23%|██▎       | 1128/5000 [5:07:18<16:09:21, 15.02s/it]                                                        {'loss': 26.6136, 'grad_norm': 27.125, 'learning_rate': 6.426278587143695e-05, 'epoch': 1.43}
 23%|██▎       | 1128/5000 [5:07:18<16:09:21, 15.02s/it] 23%|██▎       | 1129/5000 [5:07:31<15:29:30, 14.41s/it]                                                        {'loss': 24.3505, 'grad_norm': 29.625, 'learning_rate': 6.425007997371167e-05, 'epoch': 1.43}
 23%|██▎       | 1129/5000 [5:07:31<15:29:30, 14.41s/it] 23%|██▎       | 1130/5000 [5:07:53<18:00:29, 16.75s/it]                                                        {'loss': 24.1073, 'grad_norm': 24.25, 'learning_rate': 6.423736128100768e-05, 'epoch': 1.43}
 23%|██▎       | 1130/5000 [5:07:53<18:00:29, 16.75s/it] 23%|██▎       | 1131/5000 [5:08:16<20:05:45, 18.70s/it]                                                        {'loss': 25.9269, 'grad_norm': 27.0, 'learning_rate': 6.422462979888854e-05, 'epoch': 1.44}
 23%|██▎       | 1131/5000 [5:08:16<20:05:45, 18.70s/it] 23%|██▎       | 1132/5000 [5:08:35<20:07:09, 18.73s/it]                                                        {'loss': 25.0822, 'grad_norm': 27.25, 'learning_rate': 6.421188553292345e-05, 'epoch': 1.44}
 23%|██▎       | 1132/5000 [5:08:35<20:07:09, 18.73s/it] 23%|██▎       | 1133/5000 [5:08:53<19:57:05, 18.57s/it]                                                        {'loss': 22.9921, 'grad_norm': 45.25, 'learning_rate': 6.419912848868716e-05, 'epoch': 1.44}
 23%|██▎       | 1133/5000 [5:08:53<19:57:05, 18.57s/it] 23%|██▎       | 1134/5000 [5:09:04<17:28:05, 16.27s/it]                                                        {'loss': 29.2143, 'grad_norm': 216.0, 'learning_rate': 6.418635867176008e-05, 'epoch': 1.44}
 23%|██▎       | 1134/5000 [5:09:04<17:28:05, 16.27s/it] 23%|██▎       | 1135/5000 [5:09:21<17:49:01, 16.60s/it]                                                        {'loss': 24.7697, 'grad_norm': 60.5, 'learning_rate': 6.417357608772812e-05, 'epoch': 1.44}
 23%|██▎       | 1135/5000 [5:09:21<17:49:01, 16.60s/it] 23%|██▎       | 1136/5000 [5:09:36<17:15:00, 16.07s/it]                                                        {'loss': 24.121, 'grad_norm': 63.5, 'learning_rate': 6.416078074218284e-05, 'epoch': 1.44}
 23%|██▎       | 1136/5000 [5:09:36<17:15:00, 16.07s/it] 23%|██▎       | 1137/5000 [5:09:51<16:48:32, 15.66s/it]                                                        {'loss': 25.4179, 'grad_norm': 118.5, 'learning_rate': 6.414797264072136e-05, 'epoch': 1.44}
 23%|██▎       | 1137/5000 [5:09:51<16:48:32, 15.66s/it] 23%|██▎       | 1138/5000 [5:10:13<18:56:17, 17.65s/it]                                                        {'loss': 26.1165, 'grad_norm': 44.75, 'learning_rate': 6.413515178894634e-05, 'epoch': 1.45}
 23%|██▎       | 1138/5000 [5:10:13<18:56:17, 17.65s/it] 23%|██▎       | 1139/5000 [5:10:30<18:31:01, 17.27s/it]                                                        {'loss': 23.363, 'grad_norm': 21.125, 'learning_rate': 6.412231819246612e-05, 'epoch': 1.45}
 23%|██▎       | 1139/5000 [5:10:30<18:31:01, 17.27s/it] 23%|██▎       | 1140/5000 [5:10:43<17:12:12, 16.04s/it]                                                        {'loss': 52.5606, 'grad_norm': 1592.0, 'learning_rate': 6.41094718568945e-05, 'epoch': 1.45}
 23%|██▎       | 1140/5000 [5:10:43<17:12:12, 16.04s/it] 23%|██▎       | 1141/5000 [5:11:05<19:13:22, 17.93s/it]                                                        {'loss': 23.5953, 'grad_norm': 55.5, 'learning_rate': 6.409661278785093e-05, 'epoch': 1.45}
 23%|██▎       | 1141/5000 [5:11:05<19:13:22, 17.93s/it] 23%|██▎       | 1142/5000 [5:11:19<18:00:40, 16.81s/it]                                                        {'loss': 29.184, 'grad_norm': 216.0, 'learning_rate': 6.408374099096038e-05, 'epoch': 1.45}
 23%|██▎       | 1142/5000 [5:11:19<18:00:40, 16.81s/it] 23%|██▎       | 1143/5000 [5:11:37<18:10:15, 16.96s/it]                                                        {'loss': 30.4029, 'grad_norm': 41.25, 'learning_rate': 6.407085647185344e-05, 'epoch': 1.45}
 23%|██▎       | 1143/5000 [5:11:37<18:10:15, 16.96s/it] 23%|██▎       | 1144/5000 [5:11:57<19:14:37, 17.97s/it]                                                        {'loss': 24.1813, 'grad_norm': 31.25, 'learning_rate': 6.405795923616622e-05, 'epoch': 1.45}
 23%|██▎       | 1144/5000 [5:11:57<19:14:37, 17.97s/it] 23%|██▎       | 1145/5000 [5:12:10<17:42:39, 16.54s/it]                                                        {'loss': 27.8318, 'grad_norm': 600.0, 'learning_rate': 6.40450492895404e-05, 'epoch': 1.45}
 23%|██▎       | 1145/5000 [5:12:10<17:42:39, 16.54s/it] 23%|██▎       | 1146/5000 [5:12:25<17:11:23, 16.06s/it]                                                        {'loss': 24.5571, 'grad_norm': 49.0, 'learning_rate': 6.403212663762325e-05, 'epoch': 1.46}
 23%|██▎       | 1146/5000 [5:12:25<17:11:23, 16.06s/it] 23%|██▎       | 1147/5000 [5:12:38<16:20:11, 15.26s/it]                                                        {'loss': 23.0124, 'grad_norm': 70.5, 'learning_rate': 6.401919128606756e-05, 'epoch': 1.46}
 23%|██▎       | 1147/5000 [5:12:38<16:20:11, 15.26s/it] 23%|██▎       | 1148/5000 [5:12:51<15:26:05, 14.42s/it]                                                        {'loss': 29.7277, 'grad_norm': 71.0, 'learning_rate': 6.40062432405317e-05, 'epoch': 1.46}
 23%|██▎       | 1148/5000 [5:12:51<15:26:05, 14.42s/it] 23%|██▎       | 1149/5000 [5:13:02<14:28:23, 13.53s/it]                                                        {'loss': 26.6196, 'grad_norm': 62.0, 'learning_rate': 6.399328250667958e-05, 'epoch': 1.46}
 23%|██▎       | 1149/5000 [5:13:02<14:28:23, 13.53s/it] 23%|██▎       | 1150/5000 [5:13:16<14:31:21, 13.58s/it]                                                        {'loss': 25.8629, 'grad_norm': 192.0, 'learning_rate': 6.398030909018069e-05, 'epoch': 1.46}
 23%|██▎       | 1150/5000 [5:13:16<14:31:21, 13.58s/it] 23%|██▎       | 1151/5000 [5:13:27<13:48:02, 12.91s/it]                                                        {'loss': 30.1408, 'grad_norm': 161.0, 'learning_rate': 6.396732299671002e-05, 'epoch': 1.46}
 23%|██▎       | 1151/5000 [5:13:27<13:48:02, 12.91s/it] 23%|██▎       | 1152/5000 [5:13:41<13:57:50, 13.06s/it]                                                        {'loss': 26.5458, 'grad_norm': 64.0, 'learning_rate': 6.395432423194813e-05, 'epoch': 1.46}
 23%|██▎       | 1152/5000 [5:13:41<13:57:50, 13.06s/it] 23%|██▎       | 1153/5000 [5:13:53<13:34:25, 12.70s/it]                                                        {'loss': 28.0387, 'grad_norm': 83.0, 'learning_rate': 6.394131280158111e-05, 'epoch': 1.46}
 23%|██▎       | 1153/5000 [5:13:53<13:34:25, 12.70s/it] 23%|██▎       | 1154/5000 [5:14:15<16:37:13, 15.56s/it]                                                        {'loss': 26.3877, 'grad_norm': 91.5, 'learning_rate': 6.392828871130062e-05, 'epoch': 1.47}
 23%|██▎       | 1154/5000 [5:14:15<16:37:13, 15.56s/it] 23%|██▎       | 1155/5000 [5:14:29<16:16:20, 15.24s/it]                                                        {'loss': 26.1493, 'grad_norm': 52.25, 'learning_rate': 6.391525196680384e-05, 'epoch': 1.47}
 23%|██▎       | 1155/5000 [5:14:29<16:16:20, 15.24s/it] 23%|██▎       | 1156/5000 [5:14:44<16:03:59, 15.05s/it]                                                        {'loss': 22.8231, 'grad_norm': 30.625, 'learning_rate': 6.390220257379347e-05, 'epoch': 1.47}
 23%|██▎       | 1156/5000 [5:14:44<16:03:59, 15.05s/it] 23%|██▎       | 1157/5000 [5:14:58<15:43:47, 14.74s/it]                                                        {'loss': 23.475, 'grad_norm': 26.25, 'learning_rate': 6.388914053797778e-05, 'epoch': 1.47}
 23%|██▎       | 1157/5000 [5:14:58<15:43:47, 14.74s/it] 23%|██▎       | 1158/5000 [5:15:12<15:26:42, 14.47s/it]                                                        {'loss': 23.4545, 'grad_norm': 56.5, 'learning_rate': 6.387606586507053e-05, 'epoch': 1.47}
 23%|██▎       | 1158/5000 [5:15:12<15:26:42, 14.47s/it] 23%|██▎       | 1159/5000 [5:15:25<15:07:11, 14.17s/it]                                                        {'loss': 24.9986, 'grad_norm': 25.625, 'learning_rate': 6.386297856079103e-05, 'epoch': 1.47}
 23%|██▎       | 1159/5000 [5:15:25<15:07:11, 14.17s/it] 23%|██▎       | 1160/5000 [5:15:38<14:41:47, 13.78s/it]                                                        {'loss': 24.8333, 'grad_norm': 46.75, 'learning_rate': 6.38498786308641e-05, 'epoch': 1.47}
 23%|██▎       | 1160/5000 [5:15:38<14:41:47, 13.78s/it] 23%|██▎       | 1161/5000 [5:15:53<15:01:38, 14.09s/it]                                                        {'loss': 25.0691, 'grad_norm': 34.75, 'learning_rate': 6.383676608102012e-05, 'epoch': 1.47}
 23%|██▎       | 1161/5000 [5:15:53<15:01:38, 14.09s/it] 23%|██▎       | 1162/5000 [5:16:16<17:54:04, 16.79s/it]                                                        {'loss': 22.6333, 'grad_norm': 51.5, 'learning_rate': 6.382364091699495e-05, 'epoch': 1.48}
 23%|██▎       | 1162/5000 [5:16:16<17:54:04, 16.79s/it] 23%|██▎       | 1163/5000 [5:16:34<18:11:49, 17.07s/it]                                                        {'loss': 27.7587, 'grad_norm': 119.0, 'learning_rate': 6.381050314452999e-05, 'epoch': 1.48}
 23%|██▎       | 1163/5000 [5:16:34<18:11:49, 17.07s/it] 23%|██▎       | 1164/5000 [5:16:57<20:14:40, 19.00s/it]                                                        {'loss': 24.6179, 'grad_norm': 33.5, 'learning_rate': 6.379735276937214e-05, 'epoch': 1.48}
 23%|██▎       | 1164/5000 [5:16:57<20:14:40, 19.00s/it] 23%|██▎       | 1165/5000 [5:17:12<18:49:51, 17.68s/it]                                                        {'loss': 28.4895, 'grad_norm': 74.5, 'learning_rate': 6.378418979727383e-05, 'epoch': 1.48}
 23%|██▎       | 1165/5000 [5:17:12<18:49:51, 17.68s/it] 23%|██▎       | 1166/5000 [5:17:29<18:40:33, 17.54s/it]                                                        {'loss': 21.7891, 'grad_norm': 88.5, 'learning_rate': 6.377101423399297e-05, 'epoch': 1.48}
 23%|██▎       | 1166/5000 [5:17:29<18:40:33, 17.54s/it] 23%|██▎       | 1167/5000 [5:17:54<20:54:58, 19.64s/it]                                                        {'loss': 22.851, 'grad_norm': 36.25, 'learning_rate': 6.375782608529302e-05, 'epoch': 1.48}
 23%|██▎       | 1167/5000 [5:17:54<20:54:58, 19.64s/it] 23%|██▎       | 1168/5000 [5:18:07<18:54:28, 17.76s/it]                                                        {'loss': 23.856, 'grad_norm': 30.125, 'learning_rate': 6.374462535694292e-05, 'epoch': 1.48}
 23%|██▎       | 1168/5000 [5:18:07<18:54:28, 17.76s/it] 23%|██▎       | 1169/5000 [5:18:21<17:48:59, 16.74s/it]                                                        {'loss': 25.4181, 'grad_norm': 35.5, 'learning_rate': 6.373141205471712e-05, 'epoch': 1.48}
 23%|██▎       | 1169/5000 [5:18:21<17:48:59, 16.74s/it] 23%|██▎       | 1170/5000 [5:18:42<19:00:51, 17.87s/it]                                                        {'loss': 28.1313, 'grad_norm': 432.0, 'learning_rate': 6.371818618439555e-05, 'epoch': 1.49}
 23%|██▎       | 1170/5000 [5:18:42<19:00:51, 17.87s/it] 23%|██▎       | 1171/5000 [5:18:55<17:23:21, 16.35s/it]                                                        {'loss': 26.435, 'grad_norm': 37.25, 'learning_rate': 6.370494775176368e-05, 'epoch': 1.49}
 23%|██▎       | 1171/5000 [5:18:55<17:23:21, 16.35s/it] 23%|██▎       | 1172/5000 [5:19:11<17:25:00, 16.38s/it]                                                        {'loss': 23.6601, 'grad_norm': 43.5, 'learning_rate': 6.369169676261242e-05, 'epoch': 1.49}
 23%|██▎       | 1172/5000 [5:19:11<17:25:00, 16.38s/it] 23%|██▎       | 1173/5000 [5:19:42<22:06:41, 20.80s/it]                                                        {'loss': 25.7295, 'grad_norm': 438.0, 'learning_rate': 6.367843322273825e-05, 'epoch': 1.49}
 23%|██▎       | 1173/5000 [5:19:42<22:06:41, 20.80s/it] 23%|██▎       | 1174/5000 [5:19:56<19:47:21, 18.62s/it]                                                        {'loss': 26.1366, 'grad_norm': 90.5, 'learning_rate': 6.366515713794303e-05, 'epoch': 1.49}
 23%|██▎       | 1174/5000 [5:19:56<19:47:21, 18.62s/it] 24%|██▎       | 1175/5000 [5:20:21<21:44:11, 20.46s/it]                                                        {'loss': 24.8429, 'grad_norm': 85.5, 'learning_rate': 6.365186851403423e-05, 'epoch': 1.49}
 24%|██▎       | 1175/5000 [5:20:21<21:44:11, 20.46s/it] 24%|██▎       | 1176/5000 [5:20:35<19:42:26, 18.55s/it]                                                        {'loss': 24.3056, 'grad_norm': 26.0, 'learning_rate': 6.363856735682472e-05, 'epoch': 1.49}
 24%|██▎       | 1176/5000 [5:20:35<19:42:26, 18.55s/it] 24%|██▎       | 1177/5000 [5:20:51<18:50:56, 17.75s/it]                                                        {'loss': 22.2553, 'grad_norm': 18.375, 'learning_rate': 6.362525367213286e-05, 'epoch': 1.49}
 24%|██▎       | 1177/5000 [5:20:51<18:50:56, 17.75s/it] 24%|██▎       | 1178/5000 [5:21:05<17:49:34, 16.79s/it]                                                        {'loss': 22.3241, 'grad_norm': 80.0, 'learning_rate': 6.361192746578253e-05, 'epoch': 1.5}
 24%|██▎       | 1178/5000 [5:21:05<17:49:34, 16.79s/it] 24%|██▎       | 1179/5000 [5:21:20<17:07:36, 16.14s/it]                                                        {'loss': 26.3936, 'grad_norm': 67.0, 'learning_rate': 6.359858874360306e-05, 'epoch': 1.5}
 24%|██▎       | 1179/5000 [5:21:20<17:07:36, 16.14s/it] 24%|██▎       | 1180/5000 [5:21:32<15:51:38, 14.95s/it]                                                        {'loss': 26.3346, 'grad_norm': 92.0, 'learning_rate': 6.358523751142927e-05, 'epoch': 1.5}
 24%|██▎       | 1180/5000 [5:21:32<15:51:38, 14.95s/it] 24%|██▎       | 1181/5000 [5:21:44<14:59:43, 14.14s/it]                                                        {'loss': 30.9644, 'grad_norm': 284.0, 'learning_rate': 6.357187377510143e-05, 'epoch': 1.5}
 24%|██▎       | 1181/5000 [5:21:44<14:59:43, 14.14s/it] 24%|██▎       | 1182/5000 [5:21:58<14:54:42, 14.06s/it]                                                        {'loss': 24.513, 'grad_norm': 33.5, 'learning_rate': 6.35584975404653e-05, 'epoch': 1.5}
 24%|██▎       | 1182/5000 [5:21:58<14:54:42, 14.06s/it] 24%|██▎       | 1183/5000 [5:22:25<18:52:26, 17.80s/it]                                                        {'loss': 26.5501, 'grad_norm': 84.0, 'learning_rate': 6.354510881337209e-05, 'epoch': 1.5}
 24%|██▎       | 1183/5000 [5:22:25<18:52:26, 17.80s/it] 24%|██▎       | 1184/5000 [5:22:37<17:07:30, 16.16s/it]                                                        {'loss': 24.8892, 'grad_norm': 29.75, 'learning_rate': 6.35317075996785e-05, 'epoch': 1.5}
 24%|██▎       | 1184/5000 [5:22:37<17:07:30, 16.16s/it] 24%|██▎       | 1185/5000 [5:22:53<17:16:22, 16.30s/it]                                                        {'loss': 23.3827, 'grad_norm': 25.0, 'learning_rate': 6.351829390524665e-05, 'epoch': 1.5}
 24%|██▎       | 1185/5000 [5:22:53<17:16:22, 16.30s/it] 24%|██▎       | 1186/5000 [5:23:09<17:06:30, 16.15s/it]                                                        {'loss': 23.638, 'grad_norm': 44.75, 'learning_rate': 6.350486773594417e-05, 'epoch': 1.51}
 24%|██▎       | 1186/5000 [5:23:09<17:06:30, 16.15s/it] 24%|██▎       | 1187/5000 [5:23:24<16:37:11, 15.69s/it]                                                        {'loss': 22.6375, 'grad_norm': 30.0, 'learning_rate': 6.349142909764411e-05, 'epoch': 1.51}
 24%|██▎       | 1187/5000 [5:23:24<16:37:11, 15.69s/it] 24%|██▍       | 1188/5000 [5:23:38<15:57:50, 15.08s/it]                                                        {'loss': 23.1252, 'grad_norm': 50.0, 'learning_rate': 6.3477977996225e-05, 'epoch': 1.51}
 24%|██▍       | 1188/5000 [5:23:38<15:57:50, 15.08s/it] 24%|██▍       | 1189/5000 [5:23:51<15:35:44, 14.73s/it]                                                        {'loss': 23.2079, 'grad_norm': 25.0, 'learning_rate': 6.346451443757079e-05, 'epoch': 1.51}
 24%|██▍       | 1189/5000 [5:23:51<15:35:44, 14.73s/it] 24%|██▍       | 1190/5000 [5:24:16<18:39:55, 17.64s/it]                                                        {'loss': 26.3488, 'grad_norm': 54.75, 'learning_rate': 6.345103842757093e-05, 'epoch': 1.51}
 24%|██▍       | 1190/5000 [5:24:16<18:39:55, 17.64s/it] 24%|██▍       | 1191/5000 [5:24:29<17:06:39, 16.17s/it]                                                        {'loss': 28.5745, 'grad_norm': 217.0, 'learning_rate': 6.343754997212024e-05, 'epoch': 1.51}
 24%|██▍       | 1191/5000 [5:24:29<17:06:39, 16.17s/it] 24%|██▍       | 1192/5000 [5:24:40<15:33:58, 14.72s/it]                                                        {'loss': 28.2884, 'grad_norm': 127.5, 'learning_rate': 6.342404907711905e-05, 'epoch': 1.51}
 24%|██▍       | 1192/5000 [5:24:40<15:33:58, 14.72s/it] 24%|██▍       | 1193/5000 [5:24:51<14:27:54, 13.68s/it]                                                        {'loss': 25.2446, 'grad_norm': 30.125, 'learning_rate': 6.341053574847311e-05, 'epoch': 1.51}
 24%|██▍       | 1193/5000 [5:24:51<14:27:54, 13.68s/it] 24%|██▍       | 1194/5000 [5:25:15<17:39:50, 16.71s/it]                                                        {'loss': 24.4632, 'grad_norm': 38.75, 'learning_rate': 6.339700999209362e-05, 'epoch': 1.52}
 24%|██▍       | 1194/5000 [5:25:15<17:39:50, 16.71s/it] 24%|██▍       | 1195/5000 [5:25:40<20:21:41, 19.26s/it]                                                        {'loss': 28.97, 'grad_norm': 143.0, 'learning_rate': 6.33834718138972e-05, 'epoch': 1.52}
 24%|██▍       | 1195/5000 [5:25:40<20:21:41, 19.26s/it] 24%|██▍       | 1196/5000 [5:25:56<19:15:44, 18.23s/it]                                                        {'loss': 23.3776, 'grad_norm': 24.875, 'learning_rate': 6.336992121980589e-05, 'epoch': 1.52}
 24%|██▍       | 1196/5000 [5:25:56<19:15:44, 18.23s/it] 24%|██▍       | 1197/5000 [5:26:08<17:22:10, 16.44s/it]                                                        {'loss': 25.5509, 'grad_norm': 33.5, 'learning_rate': 6.335635821574721e-05, 'epoch': 1.52}
 24%|██▍       | 1197/5000 [5:26:08<17:22:10, 16.44s/it] 24%|██▍       | 1198/5000 [5:26:29<18:52:00, 17.86s/it]                                                        {'loss': 25.4557, 'grad_norm': 103.0, 'learning_rate': 6.334278280765406e-05, 'epoch': 1.52}
 24%|██▍       | 1198/5000 [5:26:29<18:52:00, 17.86s/it] 24%|██▍       | 1199/5000 [5:26:43<17:27:48, 16.54s/it]                                                        {'loss': 24.4564, 'grad_norm': 52.25, 'learning_rate': 6.332919500146478e-05, 'epoch': 1.52}
 24%|██▍       | 1199/5000 [5:26:43<17:27:48, 16.54s/it] 24%|██▍       | 1200/5000 [5:26:54<15:47:53, 14.97s/it]                                                        {'loss': 23.9101, 'grad_norm': 29.375, 'learning_rate': 6.331559480312315e-05, 'epoch': 1.52}
 24%|██▍       | 1200/5000 [5:26:54<15:47:53, 14.97s/it] 24%|██▍       | 1201/5000 [5:27:07<15:11:40, 14.40s/it]                                                        {'loss': 25.8297, 'grad_norm': 44.0, 'learning_rate': 6.330198221857836e-05, 'epoch': 1.53}
 24%|██▍       | 1201/5000 [5:27:07<15:11:40, 14.40s/it] 24%|██▍       | 1202/5000 [5:27:26<16:24:16, 15.55s/it]                                                        {'loss': 26.2879, 'grad_norm': 28.875, 'learning_rate': 6.328835725378501e-05, 'epoch': 1.53}
 24%|██▍       | 1202/5000 [5:27:26<16:24:16, 15.55s/it] 24%|██▍       | 1203/5000 [5:27:44<17:23:16, 16.49s/it]                                                        {'loss': 25.3994, 'grad_norm': 153.0, 'learning_rate': 6.327471991470313e-05, 'epoch': 1.53}
 24%|██▍       | 1203/5000 [5:27:44<17:23:16, 16.49s/it] 24%|██▍       | 1204/5000 [5:28:02<17:56:24, 17.01s/it]                                                        {'loss': 24.437, 'grad_norm': 60.0, 'learning_rate': 6.326107020729815e-05, 'epoch': 1.53}
 24%|██▍       | 1204/5000 [5:28:02<17:56:24, 17.01s/it] 24%|██▍       | 1205/5000 [5:28:17<17:05:52, 16.22s/it]                                                        {'loss': 28.1624, 'grad_norm': 194.0, 'learning_rate': 6.324740813754091e-05, 'epoch': 1.53}
 24%|██▍       | 1205/5000 [5:28:17<17:05:52, 16.22s/it] 24%|██▍       | 1206/5000 [5:28:28<15:23:54, 14.61s/it]                                                        {'loss': 27.018, 'grad_norm': 50.0, 'learning_rate': 6.323373371140767e-05, 'epoch': 1.53}
 24%|██▍       | 1206/5000 [5:28:28<15:23:54, 14.61s/it] 24%|██▍       | 1207/5000 [5:28:39<14:26:59, 13.71s/it]                                                        {'loss': 25.5554, 'grad_norm': 55.25, 'learning_rate': 6.322004693488009e-05, 'epoch': 1.53}
 24%|██▍       | 1207/5000 [5:28:39<14:26:59, 13.71s/it] 24%|██▍       | 1208/5000 [5:28:56<15:15:24, 14.48s/it]                                                        {'loss': 27.1119, 'grad_norm': 64.0, 'learning_rate': 6.320634781394522e-05, 'epoch': 1.53}
 24%|██▍       | 1208/5000 [5:28:56<15:15:24, 14.48s/it] 24%|██▍       | 1209/5000 [5:29:12<15:52:28, 15.07s/it]                                                        {'loss': 23.5702, 'grad_norm': 30.25, 'learning_rate': 6.319263635459554e-05, 'epoch': 1.54}
 24%|██▍       | 1209/5000 [5:29:12<15:52:28, 15.07s/it] 24%|██▍       | 1210/5000 [5:29:31<17:01:48, 16.18s/it]                                                        {'loss': 26.0012, 'grad_norm': 25.625, 'learning_rate': 6.31789125628289e-05, 'epoch': 1.54}
 24%|██▍       | 1210/5000 [5:29:31<17:01:48, 16.18s/it] 24%|██▍       | 1211/5000 [5:29:54<19:17:35, 18.33s/it]                                                        {'loss': 22.4197, 'grad_norm': 16.75, 'learning_rate': 6.316517644464855e-05, 'epoch': 1.54}
 24%|██▍       | 1211/5000 [5:29:54<19:17:35, 18.33s/it] 24%|██▍       | 1212/5000 [5:30:14<19:37:08, 18.65s/it]                                                        {'loss': 23.1181, 'grad_norm': 36.75, 'learning_rate': 6.315142800606313e-05, 'epoch': 1.54}
 24%|██▍       | 1212/5000 [5:30:14<19:37:08, 18.65s/it] 24%|██▍       | 1213/5000 [5:30:25<17:28:44, 16.62s/it]                                                        {'loss': 24.5242, 'grad_norm': 21.875, 'learning_rate': 6.313766725308669e-05, 'epoch': 1.54}
 24%|██▍       | 1213/5000 [5:30:25<17:28:44, 16.62s/it] 24%|██▍       | 1214/5000 [5:30:40<16:56:48, 16.11s/it]                                                        {'loss': 23.6601, 'grad_norm': 42.25, 'learning_rate': 6.312389419173864e-05, 'epoch': 1.54}
 24%|██▍       | 1214/5000 [5:30:40<16:56:48, 16.11s/it] 24%|██▍       | 1215/5000 [5:31:03<18:55:40, 18.00s/it]                                                        {'loss': 23.0609, 'grad_norm': 30.25, 'learning_rate': 6.311010882804377e-05, 'epoch': 1.54}
 24%|██▍       | 1215/5000 [5:31:03<18:55:40, 18.00s/it] 24%|██▍       | 1216/5000 [5:31:19<18:19:32, 17.43s/it]                                                        {'loss': 23.9173, 'grad_norm': 21.875, 'learning_rate': 6.309631116803228e-05, 'epoch': 1.54}
 24%|██▍       | 1216/5000 [5:31:19<18:19:32, 17.43s/it] 24%|██▍       | 1217/5000 [5:31:37<18:35:33, 17.69s/it]                                                        {'loss': 26.5678, 'grad_norm': 95.0, 'learning_rate': 6.308250121773976e-05, 'epoch': 1.55}
 24%|██▍       | 1217/5000 [5:31:37<18:35:33, 17.69s/it] 24%|██▍       | 1218/5000 [5:31:56<18:50:22, 17.93s/it]                                                        {'loss': 25.057, 'grad_norm': 34.5, 'learning_rate': 6.306867898320712e-05, 'epoch': 1.55}
 24%|██▍       | 1218/5000 [5:31:56<18:50:22, 17.93s/it] 24%|██▍       | 1219/5000 [5:32:13<18:33:47, 17.67s/it]                                                        {'loss': 23.7203, 'grad_norm': 27.25, 'learning_rate': 6.305484447048068e-05, 'epoch': 1.55}
 24%|██▍       | 1219/5000 [5:32:13<18:33:47, 17.67s/it] 24%|██▍       | 1220/5000 [5:32:27<17:20:15, 16.51s/it]                                                        {'loss': 21.7032, 'grad_norm': 24.125, 'learning_rate': 6.304099768561212e-05, 'epoch': 1.55}
 24%|██▍       | 1220/5000 [5:32:27<17:20:15, 16.51s/it] 24%|██▍       | 1221/5000 [5:32:49<19:14:27, 18.33s/it]                                                        {'loss': 26.279, 'grad_norm': 80.5, 'learning_rate': 6.302713863465851e-05, 'epoch': 1.55}
 24%|██▍       | 1221/5000 [5:32:49<19:14:27, 18.33s/it] 24%|██▍       | 1222/5000 [5:33:03<17:53:40, 17.05s/it]                                                        {'loss': 23.4515, 'grad_norm': 27.5, 'learning_rate': 6.301326732368228e-05, 'epoch': 1.55}
 24%|██▍       | 1222/5000 [5:33:03<17:53:40, 17.05s/it] 24%|██▍       | 1223/5000 [5:33:15<16:20:11, 15.57s/it]                                                        {'loss': 24.5735, 'grad_norm': 101.0, 'learning_rate': 6.299938375875119e-05, 'epoch': 1.55}
 24%|██▍       | 1223/5000 [5:33:15<16:20:11, 15.57s/it] 24%|██▍       | 1224/5000 [5:33:32<16:37:33, 15.85s/it]                                                        {'loss': 25.5503, 'grad_norm': 28.625, 'learning_rate': 6.298548794593837e-05, 'epoch': 1.55}
 24%|██▍       | 1224/5000 [5:33:32<16:37:33, 15.85s/it] 24%|██▍       | 1225/5000 [5:33:55<18:54:17, 18.03s/it]                                                        {'loss': 25.182, 'grad_norm': 37.25, 'learning_rate': 6.297157989132236e-05, 'epoch': 1.56}
 24%|██▍       | 1225/5000 [5:33:55<18:54:17, 18.03s/it] 25%|██▍       | 1226/5000 [5:34:09<17:44:13, 16.92s/it]                                                        {'loss': 21.4402, 'grad_norm': 22.5, 'learning_rate': 6.295765960098699e-05, 'epoch': 1.56}
 25%|██▍       | 1226/5000 [5:34:09<17:44:13, 16.92s/it] 25%|██▍       | 1227/5000 [5:34:23<16:41:06, 15.92s/it]                                                        {'loss': 29.5414, 'grad_norm': 544.0, 'learning_rate': 6.294372708102147e-05, 'epoch': 1.56}
 25%|██▍       | 1227/5000 [5:34:23<16:41:06, 15.92s/it] 25%|██▍       | 1228/5000 [5:34:46<18:50:39, 17.98s/it]                                                        {'loss': 25.5638, 'grad_norm': 68.5, 'learning_rate': 6.292978233752036e-05, 'epoch': 1.56}
 25%|██▍       | 1228/5000 [5:34:46<18:50:39, 17.98s/it] 25%|██▍       | 1229/5000 [5:35:05<19:16:18, 18.40s/it]                                                        {'loss': 25.7442, 'grad_norm': 42.25, 'learning_rate': 6.291582537658357e-05, 'epoch': 1.56}
 25%|██▍       | 1229/5000 [5:35:05<19:16:18, 18.40s/it] 25%|██▍       | 1230/5000 [5:35:20<18:20:27, 17.51s/it]                                                        {'loss': 23.318, 'grad_norm': 64.0, 'learning_rate': 6.290185620431635e-05, 'epoch': 1.56}
 25%|██▍       | 1230/5000 [5:35:20<18:20:27, 17.51s/it] 25%|██▍       | 1231/5000 [5:35:39<18:33:51, 17.73s/it]                                                        {'loss': 22.1029, 'grad_norm': 31.75, 'learning_rate': 6.288787482682929e-05, 'epoch': 1.56}
 25%|██▍       | 1231/5000 [5:35:39<18:33:51, 17.73s/it] 25%|██▍       | 1232/5000 [5:35:50<16:40:58, 15.94s/it]                                                        {'loss': 30.4076, 'grad_norm': 106.5, 'learning_rate': 6.28738812502383e-05, 'epoch': 1.56}
 25%|██▍       | 1232/5000 [5:35:50<16:40:58, 15.94s/it] 25%|██▍       | 1233/5000 [5:36:05<16:24:27, 15.68s/it]                                                        {'loss': 21.8823, 'grad_norm': 21.25, 'learning_rate': 6.285987548066465e-05, 'epoch': 1.57}
 25%|██▍       | 1233/5000 [5:36:05<16:24:27, 15.68s/it] 25%|██▍       | 1234/5000 [5:36:20<16:00:27, 15.30s/it]                                                        {'loss': 25.4398, 'grad_norm': 52.75, 'learning_rate': 6.284585752423497e-05, 'epoch': 1.57}
 25%|██▍       | 1234/5000 [5:36:20<16:00:27, 15.30s/it] 25%|██▍       | 1235/5000 [5:36:35<15:58:39, 15.28s/it]                                                        {'loss': 21.8773, 'grad_norm': 24.0, 'learning_rate': 6.283182738708117e-05, 'epoch': 1.57}
 25%|██▍       | 1235/5000 [5:36:35<15:58:39, 15.28s/it] 25%|██▍       | 1236/5000 [5:36:49<15:39:40, 14.98s/it]                                                        {'loss': 23.8388, 'grad_norm': 5760.0, 'learning_rate': 6.281778507534049e-05, 'epoch': 1.57}
 25%|██▍       | 1236/5000 [5:36:49<15:39:40, 14.98s/it] 25%|██▍       | 1237/5000 [5:37:05<15:53:48, 15.21s/it]                                                        {'loss': 25.453, 'grad_norm': 42.0, 'learning_rate': 6.280373059515554e-05, 'epoch': 1.57}
 25%|██▍       | 1237/5000 [5:37:05<15:53:48, 15.21s/it] 25%|██▍       | 1238/5000 [5:37:19<15:23:41, 14.73s/it]                                                        {'loss': 25.4677, 'grad_norm': 73.5, 'learning_rate': 6.278966395267424e-05, 'epoch': 1.57}
 25%|██▍       | 1238/5000 [5:37:19<15:23:41, 14.73s/it] 25%|██▍       | 1239/5000 [5:37:36<16:07:59, 15.44s/it]                                                        {'loss': 39.9247, 'grad_norm': 580.0, 'learning_rate': 6.277558515404978e-05, 'epoch': 1.57}
 25%|██▍       | 1239/5000 [5:37:36<16:07:59, 15.44s/it] 25%|██▍       | 1240/5000 [5:37:47<14:49:47, 14.20s/it]                                                        {'loss': 29.3049, 'grad_norm': 167.0, 'learning_rate': 6.276149420544072e-05, 'epoch': 1.57}
 25%|██▍       | 1240/5000 [5:37:47<14:49:47, 14.20s/it] 25%|██▍       | 1241/5000 [5:38:02<14:58:18, 14.34s/it]                                                        {'loss': 23.8868, 'grad_norm': 30.375, 'learning_rate': 6.274739111301092e-05, 'epoch': 1.58}
 25%|██▍       | 1241/5000 [5:38:02<14:58:18, 14.34s/it] 25%|██▍       | 1242/5000 [5:38:22<16:54:02, 16.19s/it]                                                        {'loss': 25.0095, 'grad_norm': 36.0, 'learning_rate': 6.273327588292958e-05, 'epoch': 1.58}
 25%|██▍       | 1242/5000 [5:38:22<16:54:02, 16.19s/it] 25%|██▍       | 1243/5000 [5:38:37<16:32:22, 15.85s/it]                                                        {'loss': 22.6341, 'grad_norm': 36.25, 'learning_rate': 6.271914852137113e-05, 'epoch': 1.58}
 25%|██▍       | 1243/5000 [5:38:37<16:32:22, 15.85s/it] 25%|██▍       | 1244/5000 [5:38:50<15:25:30, 14.78s/it]                                                        {'loss': 25.1619, 'grad_norm': 122.0, 'learning_rate': 6.270500903451538e-05, 'epoch': 1.58}
 25%|██▍       | 1244/5000 [5:38:50<15:25:30, 14.78s/it] 25%|██▍       | 1245/5000 [5:39:04<15:14:45, 14.62s/it]                                                        {'loss': 22.3659, 'grad_norm': 28.25, 'learning_rate': 6.269085742854743e-05, 'epoch': 1.58}
 25%|██▍       | 1245/5000 [5:39:04<15:14:45, 14.62s/it] 25%|██▍       | 1246/5000 [5:39:21<16:06:48, 15.45s/it]                                                        {'loss': 26.1392, 'grad_norm': 28.75, 'learning_rate': 6.267669370965768e-05, 'epoch': 1.58}
 25%|██▍       | 1246/5000 [5:39:21<16:06:48, 15.45s/it] 25%|██▍       | 1247/5000 [5:39:37<16:13:29, 15.56s/it]                                                        {'loss': 24.9426, 'grad_norm': 21.375, 'learning_rate': 6.26625178840418e-05, 'epoch': 1.58}
 25%|██▍       | 1247/5000 [5:39:37<16:13:29, 15.56s/it] 25%|██▍       | 1248/5000 [5:40:02<19:02:16, 18.27s/it]                                                        {'loss': 23.3102, 'grad_norm': 31.125, 'learning_rate': 6.264832995790078e-05, 'epoch': 1.58}
 25%|██▍       | 1248/5000 [5:40:02<19:02:16, 18.27s/it] 25%|██▍       | 1249/5000 [5:40:18<18:15:38, 17.53s/it]                                                        {'loss': 22.3488, 'grad_norm': 49.75, 'learning_rate': 6.26341299374409e-05, 'epoch': 1.59}
 25%|██▍       | 1249/5000 [5:40:18<18:15:38, 17.53s/it] 25%|██▌       | 1250/5000 [5:40:31<16:54:52, 16.24s/it]                                                        {'loss': 21.9679, 'grad_norm': 57.5, 'learning_rate': 6.261991782887377e-05, 'epoch': 1.59}
 25%|██▌       | 1250/5000 [5:40:31<16:54:52, 16.24s/it] 25%|██▌       | 1251/5000 [5:40:49<17:24:38, 16.72s/it]                                                        {'loss': 24.8336, 'grad_norm': 108.5, 'learning_rate': 6.260569363841621e-05, 'epoch': 1.59}
 25%|██▌       | 1251/5000 [5:40:49<17:24:38, 16.72s/it] 25%|██▌       | 1252/5000 [5:41:03<16:37:02, 15.96s/it]                                                        {'loss': 24.1113, 'grad_norm': 70.5, 'learning_rate': 6.259145737229038e-05, 'epoch': 1.59}
 25%|██▌       | 1252/5000 [5:41:03<16:37:02, 15.96s/it] 25%|██▌       | 1253/5000 [5:41:18<16:27:10, 15.81s/it]                                                        {'loss': 22.8597, 'grad_norm': 27.875, 'learning_rate': 6.257720903672367e-05, 'epoch': 1.59}
 25%|██▌       | 1253/5000 [5:41:18<16:27:10, 15.81s/it] 25%|██▌       | 1254/5000 [5:41:31<15:30:20, 14.90s/it]                                                        {'loss': 26.4963, 'grad_norm': 47.25, 'learning_rate': 6.256294863794884e-05, 'epoch': 1.59}
 25%|██▌       | 1254/5000 [5:41:31<15:30:20, 14.90s/it] 25%|██▌       | 1255/5000 [5:41:48<16:13:44, 15.60s/it]                                                        {'loss': 23.0272, 'grad_norm': 108.5, 'learning_rate': 6.254867618220383e-05, 'epoch': 1.59}
 25%|██▌       | 1255/5000 [5:41:48<16:13:44, 15.60s/it] 25%|██▌       | 1256/5000 [5:42:00<15:09:05, 14.57s/it]                                                        {'loss': 25.4705, 'grad_norm': 268.0, 'learning_rate': 6.253439167573194e-05, 'epoch': 1.59}
 25%|██▌       | 1256/5000 [5:42:00<15:09:05, 14.57s/it] 25%|██▌       | 1257/5000 [5:42:14<14:47:37, 14.23s/it]                                                        {'loss': 27.5005, 'grad_norm': 156.0, 'learning_rate': 6.252009512478166e-05, 'epoch': 1.6}
 25%|██▌       | 1257/5000 [5:42:14<14:47:37, 14.23s/it] 25%|██▌       | 1258/5000 [5:42:28<14:40:43, 14.12s/it]                                                        {'loss': 22.6853, 'grad_norm': 30.75, 'learning_rate': 6.25057865356068e-05, 'epoch': 1.6}
 25%|██▌       | 1258/5000 [5:42:28<14:40:43, 14.12s/it] 25%|██▌       | 1259/5000 [5:42:42<14:37:34, 14.07s/it]                                                        {'loss': 23.3225, 'grad_norm': 28.125, 'learning_rate': 6.249146591446643e-05, 'epoch': 1.6}
 25%|██▌       | 1259/5000 [5:42:42<14:37:34, 14.07s/it] 25%|██▌       | 1260/5000 [5:42:57<15:05:44, 14.53s/it]                                                        {'loss': 27.4002, 'grad_norm': 233.0, 'learning_rate': 6.247713326762486e-05, 'epoch': 1.6}
 25%|██▌       | 1260/5000 [5:42:57<15:05:44, 14.53s/it] 25%|██▌       | 1261/5000 [5:43:13<15:23:59, 14.83s/it]                                                        {'loss': 21.3174, 'grad_norm': 40.25, 'learning_rate': 6.246278860135169e-05, 'epoch': 1.6}
 25%|██▌       | 1261/5000 [5:43:13<15:23:59, 14.83s/it] 25%|██▌       | 1262/5000 [5:43:31<16:35:00, 15.97s/it]                                                        {'loss': 21.592, 'grad_norm': 37.5, 'learning_rate': 6.244843192192178e-05, 'epoch': 1.6}
 25%|██▌       | 1262/5000 [5:43:31<16:35:00, 15.97s/it] 25%|██▌       | 1263/5000 [5:43:45<15:56:52, 15.36s/it]                                                        {'loss': 47.0131, 'grad_norm': 3120.0, 'learning_rate': 6.243406323561518e-05, 'epoch': 1.6}
 25%|██▌       | 1263/5000 [5:43:45<15:56:52, 15.36s/it] 25%|██▌       | 1264/5000 [5:44:01<16:06:10, 15.52s/it]                                                        {'loss': 74.4378, 'grad_norm': 3472.0, 'learning_rate': 6.24196825487173e-05, 'epoch': 1.61}
 25%|██▌       | 1264/5000 [5:44:01<16:06:10, 15.52s/it] 25%|██▌       | 1265/5000 [5:44:16<15:45:57, 15.20s/it]                                                        {'loss': 65.7985, 'grad_norm': 4672.0, 'learning_rate': 6.240528986751871e-05, 'epoch': 1.61}
 25%|██▌       | 1265/5000 [5:44:16<15:45:57, 15.20s/it] 25%|██▌       | 1266/5000 [5:44:29<15:10:28, 14.63s/it]                                                        {'loss': 32.6972, 'grad_norm': 412.0, 'learning_rate': 6.239088519831525e-05, 'epoch': 1.61}
 25%|██▌       | 1266/5000 [5:44:29<15:10:28, 14.63s/it] 25%|██▌       | 1267/5000 [5:44:43<14:53:17, 14.36s/it]                                                        {'loss': 32.7223, 'grad_norm': 194.0, 'learning_rate': 6.237646854740801e-05, 'epoch': 1.61}
 25%|██▌       | 1267/5000 [5:44:43<14:53:17, 14.36s/it] 25%|██▌       | 1268/5000 [5:44:59<15:28:54, 14.93s/it]                                                        {'loss': 27.5031, 'grad_norm': 88.0, 'learning_rate': 6.236203992110337e-05, 'epoch': 1.61}
 25%|██▌       | 1268/5000 [5:44:59<15:28:54, 14.93s/it] 25%|██▌       | 1269/5000 [5:45:12<14:58:24, 14.45s/it]                                                        {'loss': 26.8097, 'grad_norm': 67.0, 'learning_rate': 6.234759932571284e-05, 'epoch': 1.61}
 25%|██▌       | 1269/5000 [5:45:12<14:58:24, 14.45s/it] 25%|██▌       | 1270/5000 [5:45:27<15:10:38, 14.65s/it]                                                        {'loss': 24.1307, 'grad_norm': 49.0, 'learning_rate': 6.233314676755326e-05, 'epoch': 1.61}
 25%|██▌       | 1270/5000 [5:45:27<15:10:38, 14.65s/it] 25%|██▌       | 1271/5000 [5:45:43<15:22:48, 14.85s/it]                                                        {'loss': 30.3829, 'grad_norm': 206.0, 'learning_rate': 6.231868225294666e-05, 'epoch': 1.61}
 25%|██▌       | 1271/5000 [5:45:43<15:22:48, 14.85s/it] 25%|██▌       | 1272/5000 [5:45:56<14:52:03, 14.36s/it]                                                        {'loss': 28.8664, 'grad_norm': 972.0, 'learning_rate': 6.230420578822033e-05, 'epoch': 1.62}
 25%|██▌       | 1272/5000 [5:45:56<14:52:03, 14.36s/it] 25%|██▌       | 1273/5000 [5:46:13<15:41:53, 15.16s/it]                                                        {'loss': 22.2841, 'grad_norm': 47.25, 'learning_rate': 6.228971737970674e-05, 'epoch': 1.62}
 25%|██▌       | 1273/5000 [5:46:13<15:41:53, 15.16s/it] 25%|██▌       | 1274/5000 [5:46:27<15:12:42, 14.70s/it]                                                        {'loss': 21.9803, 'grad_norm': 74.5, 'learning_rate': 6.227521703374362e-05, 'epoch': 1.62}
 25%|██▌       | 1274/5000 [5:46:27<15:12:42, 14.70s/it] 26%|██▌       | 1275/5000 [5:46:38<14:06:42, 13.64s/it]                                                        {'loss': 28.0932, 'grad_norm': 52.25, 'learning_rate': 6.226070475667393e-05, 'epoch': 1.62}
 26%|██▌       | 1275/5000 [5:46:38<14:06:42, 13.64s/it] 26%|██▌       | 1276/5000 [5:46:52<14:16:17, 13.80s/it]                                                        {'loss': 26.309, 'grad_norm': 36.5, 'learning_rate': 6.224618055484581e-05, 'epoch': 1.62}
 26%|██▌       | 1276/5000 [5:46:52<14:16:17, 13.80s/it] 26%|██▌       | 1277/5000 [5:47:08<14:57:31, 14.46s/it]                                                        {'loss': 24.1327, 'grad_norm': 46.75, 'learning_rate': 6.223164443461266e-05, 'epoch': 1.62}
 26%|██▌       | 1277/5000 [5:47:08<14:57:31, 14.46s/it] 26%|██▌       | 1278/5000 [5:47:30<17:23:14, 16.82s/it]                                                        {'loss': 26.2257, 'grad_norm': 125.5, 'learning_rate': 6.221709640233306e-05, 'epoch': 1.62}
 26%|██▌       | 1278/5000 [5:47:30<17:23:14, 16.82s/it] 26%|██▌       | 1279/5000 [5:47:45<16:37:29, 16.08s/it]                                                        {'loss': 28.829, 'grad_norm': 83.0, 'learning_rate': 6.220253646437081e-05, 'epoch': 1.62}
 26%|██▌       | 1279/5000 [5:47:45<16:37:29, 16.08s/it] 26%|██▌       | 1280/5000 [5:48:04<17:34:29, 17.01s/it]                                                        {'loss': 23.8613, 'grad_norm': 32.5, 'learning_rate': 6.218796462709492e-05, 'epoch': 1.63}
 26%|██▌       | 1280/5000 [5:48:04<17:34:29, 17.01s/it] 26%|██▌       | 1281/5000 [5:48:17<16:16:17, 15.75s/it]                                                        {'loss': 23.6239, 'grad_norm': 19.625, 'learning_rate': 6.217338089687963e-05, 'epoch': 1.63}
 26%|██▌       | 1281/5000 [5:48:17<16:16:17, 15.75s/it] 26%|██▌       | 1282/5000 [5:48:30<15:39:49, 15.17s/it]                                                        {'loss': 24.2721, 'grad_norm': 22.0, 'learning_rate': 6.215878528010434e-05, 'epoch': 1.63}
 26%|██▌       | 1282/5000 [5:48:30<15:39:49, 15.17s/it] 26%|██▌       | 1283/5000 [5:48:46<15:53:42, 15.39s/it]                                                        {'loss': 22.3337, 'grad_norm': 21.625, 'learning_rate': 6.214417778315368e-05, 'epoch': 1.63}
 26%|██▌       | 1283/5000 [5:48:46<15:53:42, 15.39s/it] 26%|██▌       | 1284/5000 [5:49:03<16:19:59, 15.82s/it]                                                        {'loss': 22.8634, 'grad_norm': 26.625, 'learning_rate': 6.212955841241745e-05, 'epoch': 1.63}
 26%|██▌       | 1284/5000 [5:49:03<16:19:59, 15.82s/it] 26%|██▌       | 1285/5000 [5:49:19<16:28:01, 15.96s/it]                                                        {'loss': 23.9607, 'grad_norm': 23.0, 'learning_rate': 6.211492717429067e-05, 'epoch': 1.63}
 26%|██▌       | 1285/5000 [5:49:19<16:28:01, 15.96s/it] 26%|██▌       | 1286/5000 [5:49:37<16:47:33, 16.28s/it]                                                        {'loss': 23.13, 'grad_norm': 59.25, 'learning_rate': 6.210028407517353e-05, 'epoch': 1.63}
 26%|██▌       | 1286/5000 [5:49:37<16:47:33, 16.28s/it] 26%|██▌       | 1287/5000 [5:49:53<16:43:57, 16.22s/it]                                                        {'loss': 21.0621, 'grad_norm': 43.25, 'learning_rate': 6.208562912147143e-05, 'epoch': 1.63}
 26%|██▌       | 1287/5000 [5:49:53<16:43:57, 16.22s/it] 26%|██▌       | 1288/5000 [5:50:08<16:30:23, 16.01s/it]                                                        {'loss': 23.5094, 'grad_norm': 34.25, 'learning_rate': 6.207096231959495e-05, 'epoch': 1.64}
 26%|██▌       | 1288/5000 [5:50:08<16:30:23, 16.01s/it] 26%|██▌       | 1289/5000 [5:50:31<18:30:39, 17.96s/it]                                                        {'loss': 25.8239, 'grad_norm': 350.0, 'learning_rate': 6.205628367595984e-05, 'epoch': 1.64}
 26%|██▌       | 1289/5000 [5:50:31<18:30:39, 17.96s/it] 26%|██▌       | 1290/5000 [5:50:43<16:38:34, 16.15s/it]                                                        {'loss': 25.0478, 'grad_norm': 130.0, 'learning_rate': 6.204159319698703e-05, 'epoch': 1.64}
 26%|██▌       | 1290/5000 [5:50:43<16:38:34, 16.15s/it] 26%|██▌       | 1291/5000 [5:50:55<15:36:09, 15.14s/it]                                                        {'loss': 22.9858, 'grad_norm': 31.25, 'learning_rate': 6.202689088910262e-05, 'epoch': 1.64}
 26%|██▌       | 1291/5000 [5:50:55<15:36:09, 15.14s/it] 26%|██▌       | 1292/5000 [5:51:08<14:49:57, 14.40s/it]                                                        {'loss': 23.5314, 'grad_norm': 33.0, 'learning_rate': 6.201217675873795e-05, 'epoch': 1.64}
 26%|██▌       | 1292/5000 [5:51:08<14:49:57, 14.40s/it] 26%|██▌       | 1293/5000 [5:51:25<15:29:29, 15.04s/it]                                                        {'loss': 20.6002, 'grad_norm': 21.125, 'learning_rate': 6.199745081232944e-05, 'epoch': 1.64}
 26%|██▌       | 1293/5000 [5:51:25<15:29:29, 15.04s/it] 26%|██▌       | 1294/5000 [5:51:43<16:38:26, 16.16s/it]                                                        {'loss': 24.5056, 'grad_norm': 25.75, 'learning_rate': 6.198271305631873e-05, 'epoch': 1.64}
 26%|██▌       | 1294/5000 [5:51:43<16:38:26, 16.16s/it] 26%|██▌       | 1295/5000 [5:51:56<15:42:10, 15.26s/it]                                                        {'loss': 23.5972, 'grad_norm': 24.0, 'learning_rate': 6.196796349715262e-05, 'epoch': 1.64}
 26%|██▌       | 1295/5000 [5:51:56<15:42:10, 15.26s/it] 26%|██▌       | 1296/5000 [5:52:09<14:48:12, 14.39s/it]                                                        {'loss': 25.3771, 'grad_norm': 92.0, 'learning_rate': 6.195320214128305e-05, 'epoch': 1.65}
 26%|██▌       | 1296/5000 [5:52:09<14:48:12, 14.39s/it] 26%|██▌       | 1297/5000 [5:52:25<15:13:32, 14.80s/it]                                                        {'loss': 23.0362, 'grad_norm': 31.25, 'learning_rate': 6.193842899516716e-05, 'epoch': 1.65}
 26%|██▌       | 1297/5000 [5:52:25<15:13:32, 14.80s/it] 26%|██▌       | 1298/5000 [5:52:48<17:53:36, 17.40s/it]                                                        {'loss': 22.891, 'grad_norm': 72.0, 'learning_rate': 6.192364406526722e-05, 'epoch': 1.65}
 26%|██▌       | 1298/5000 [5:52:48<17:53:36, 17.40s/it] 26%|██▌       | 1299/5000 [5:53:04<17:26:08, 16.96s/it]                                                        {'loss': 21.1864, 'grad_norm': 16.375, 'learning_rate': 6.190884735805066e-05, 'epoch': 1.65}
 26%|██▌       | 1299/5000 [5:53:04<17:26:08, 16.96s/it] 26%|██▌       | 1300/5000 [5:53:22<17:35:55, 17.12s/it]                                                        {'loss': 22.3071, 'grad_norm': 18.625, 'learning_rate': 6.189403887999006e-05, 'epoch': 1.65}
 26%|██▌       | 1300/5000 [5:53:22<17:35:55, 17.12s/it] 26%|██▌       | 1301/5000 [5:53:38<17:30:44, 17.04s/it]                                                        {'loss': 22.8364, 'grad_norm': 21.875, 'learning_rate': 6.187921863756315e-05, 'epoch': 1.65}
 26%|██▌       | 1301/5000 [5:53:38<17:30:44, 17.04s/it] 26%|██▌       | 1302/5000 [5:53:56<17:42:14, 17.23s/it]                                                        {'loss': 21.3178, 'grad_norm': 22.125, 'learning_rate': 6.186438663725282e-05, 'epoch': 1.65}
 26%|██▌       | 1302/5000 [5:53:56<17:42:14, 17.23s/it] 26%|██▌       | 1303/5000 [5:54:21<20:02:59, 19.52s/it]                                                        {'loss': 23.6466, 'grad_norm': 20.625, 'learning_rate': 6.184954288554706e-05, 'epoch': 1.65}
 26%|██▌       | 1303/5000 [5:54:21<20:02:59, 19.52s/it] 26%|██▌       | 1304/5000 [5:54:35<18:24:20, 17.93s/it]                                                        {'loss': 20.9265, 'grad_norm': 15.8125, 'learning_rate': 6.183468738893907e-05, 'epoch': 1.66}
 26%|██▌       | 1304/5000 [5:54:35<18:24:20, 17.93s/it] 26%|██▌       | 1305/5000 [5:54:55<19:06:15, 18.61s/it]                                                        {'loss': 25.0762, 'grad_norm': 24.875, 'learning_rate': 6.181982015392711e-05, 'epoch': 1.66}
 26%|██▌       | 1305/5000 [5:54:55<19:06:15, 18.61s/it] 26%|██▌       | 1306/5000 [5:55:18<20:23:45, 19.88s/it]                                                        {'loss': 25.7569, 'grad_norm': 138.0, 'learning_rate': 6.180494118701466e-05, 'epoch': 1.66}
 26%|██▌       | 1306/5000 [5:55:18<20:23:45, 19.88s/it] 26%|██▌       | 1307/5000 [5:55:41<21:26:27, 20.90s/it]                                                        {'loss': 24.573, 'grad_norm': 45.75, 'learning_rate': 6.179005049471025e-05, 'epoch': 1.66}
 26%|██▌       | 1307/5000 [5:55:41<21:26:27, 20.90s/it] 26%|██▌       | 1308/5000 [5:55:56<19:27:02, 18.97s/it]                                                        {'loss': 23.0417, 'grad_norm': 61.25, 'learning_rate': 6.177514808352757e-05, 'epoch': 1.66}
 26%|██▌       | 1308/5000 [5:55:56<19:27:02, 18.97s/it] 26%|██▌       | 1309/5000 [5:56:10<17:52:53, 17.44s/it]                                                        {'loss': 23.1968, 'grad_norm': 30.125, 'learning_rate': 6.176023395998548e-05, 'epoch': 1.66}
 26%|██▌       | 1309/5000 [5:56:10<17:52:53, 17.44s/it] 26%|██▌       | 1310/5000 [5:56:35<20:10:46, 19.69s/it]                                                        {'loss': 21.6053, 'grad_norm': 26.625, 'learning_rate': 6.174530813060789e-05, 'epoch': 1.66}
 26%|██▌       | 1310/5000 [5:56:35<20:10:46, 19.69s/it] 26%|██▌       | 1311/5000 [5:56:59<21:38:25, 21.12s/it]                                                        {'loss': 21.6878, 'grad_norm': 18.5, 'learning_rate': 6.173037060192387e-05, 'epoch': 1.66}
 26%|██▌       | 1311/5000 [5:56:59<21:38:25, 21.12s/it] 26%|██▌       | 1312/5000 [5:57:14<19:41:39, 19.22s/it]                                                        {'loss': 23.6937, 'grad_norm': 224.0, 'learning_rate': 6.171542138046762e-05, 'epoch': 1.67}
 26%|██▌       | 1312/5000 [5:57:14<19:41:39, 19.22s/it] 26%|██▋       | 1313/5000 [5:57:26<17:32:49, 17.13s/it]                                                        {'loss': 24.0039, 'grad_norm': 84.5, 'learning_rate': 6.17004604727784e-05, 'epoch': 1.67}
 26%|██▋       | 1313/5000 [5:57:26<17:32:49, 17.13s/it] 26%|██▋       | 1314/5000 [5:57:43<17:30:49, 17.11s/it]                                                        {'loss': 26.6863, 'grad_norm': 201.0, 'learning_rate': 6.168548788540068e-05, 'epoch': 1.67}
 26%|██▋       | 1314/5000 [5:57:43<17:30:49, 17.11s/it] 26%|██▋       | 1315/5000 [5:58:00<17:29:54, 17.09s/it]                                                        {'loss': 21.7895, 'grad_norm': 796.0, 'learning_rate': 6.167050362488391e-05, 'epoch': 1.67}
 26%|██▋       | 1315/5000 [5:58:00<17:29:54, 17.09s/it] 26%|██▋       | 1316/5000 [5:58:17<17:18:25, 16.91s/it]                                                        {'loss': 24.0085, 'grad_norm': 38.5, 'learning_rate': 6.165550769778277e-05, 'epoch': 1.67}
 26%|██▋       | 1316/5000 [5:58:17<17:18:25, 16.91s/it] 26%|██▋       | 1317/5000 [5:58:30<16:12:56, 15.85s/it]                                                        {'loss': 22.4902, 'grad_norm': 34.75, 'learning_rate': 6.164050011065696e-05, 'epoch': 1.67}
 26%|██▋       | 1317/5000 [5:58:30<16:12:56, 15.85s/it] 26%|██▋       | 1318/5000 [5:58:44<15:36:16, 15.26s/it]                                                        {'loss': 21.3227, 'grad_norm': 21.375, 'learning_rate': 6.162548087007131e-05, 'epoch': 1.67}
 26%|██▋       | 1318/5000 [5:58:44<15:36:16, 15.26s/it] 26%|██▋       | 1319/5000 [5:58:56<14:35:32, 14.27s/it]                                                        {'loss': 26.5798, 'grad_norm': 32.0, 'learning_rate': 6.161044998259576e-05, 'epoch': 1.67}
 26%|██▋       | 1319/5000 [5:58:56<14:35:32, 14.27s/it] 26%|██▋       | 1320/5000 [5:59:10<14:20:45, 14.03s/it]                                                        {'loss': 23.456, 'grad_norm': 18.0, 'learning_rate': 6.15954074548053e-05, 'epoch': 1.68}
 26%|██▋       | 1320/5000 [5:59:10<14:20:45, 14.03s/it] 26%|██▋       | 1321/5000 [5:59:26<15:04:28, 14.75s/it]                                                        {'loss': 21.4341, 'grad_norm': 16.125, 'learning_rate': 6.158035329328006e-05, 'epoch': 1.68}
 26%|██▋       | 1321/5000 [5:59:26<15:04:28, 14.75s/it] 26%|██▋       | 1322/5000 [5:59:40<14:58:40, 14.66s/it]                                                        {'loss': 22.7137, 'grad_norm': 53.25, 'learning_rate': 6.156528750460526e-05, 'epoch': 1.68}
 26%|██▋       | 1322/5000 [5:59:40<14:58:40, 14.66s/it] 26%|██▋       | 1323/5000 [5:59:58<15:53:08, 15.55s/it]                                                        {'loss': 23.5171, 'grad_norm': 22.375, 'learning_rate': 6.155021009537119e-05, 'epoch': 1.68}
 26%|██▋       | 1323/5000 [5:59:58<15:53:08, 15.55s/it] 26%|██▋       | 1324/5000 [6:00:12<15:26:33, 15.12s/it]                                                        {'loss': 23.7974, 'grad_norm': 94.0, 'learning_rate': 6.153512107217317e-05, 'epoch': 1.68}
 26%|██▋       | 1324/5000 [6:00:12<15:26:33, 15.12s/it] 26%|██▋       | 1325/5000 [6:00:30<16:08:21, 15.81s/it]                                                        {'loss': 22.4129, 'grad_norm': 36.25, 'learning_rate': 6.152002044161171e-05, 'epoch': 1.68}
 26%|██▋       | 1325/5000 [6:00:30<16:08:21, 15.81s/it] 27%|██▋       | 1326/5000 [6:00:47<16:38:04, 16.30s/it]                                                        {'loss': 22.5639, 'grad_norm': 72.0, 'learning_rate': 6.150490821029232e-05, 'epoch': 1.68}
 27%|██▋       | 1326/5000 [6:00:47<16:38:04, 16.30s/it] 27%|██▋       | 1327/5000 [6:01:00<15:36:25, 15.30s/it]                                                        {'loss': 23.8553, 'grad_norm': 26.5, 'learning_rate': 6.148978438482557e-05, 'epoch': 1.69}
 27%|██▋       | 1327/5000 [6:01:00<15:36:25, 15.30s/it] 27%|██▋       | 1328/5000 [6:01:14<15:21:24, 15.06s/it]                                                        {'loss': 24.9813, 'grad_norm': 138.0, 'learning_rate': 6.147464897182718e-05, 'epoch': 1.69}
 27%|██▋       | 1328/5000 [6:01:14<15:21:24, 15.06s/it] 27%|██▋       | 1329/5000 [6:01:38<17:54:15, 17.56s/it]                                                        {'loss': 23.0124, 'grad_norm': 19.75, 'learning_rate': 6.145950197791787e-05, 'epoch': 1.69}
 27%|██▋       | 1329/5000 [6:01:38<17:54:15, 17.56s/it] 27%|██▋       | 1330/5000 [6:01:54<17:28:59, 17.15s/it]                                                        {'loss': 22.7815, 'grad_norm': 15.75, 'learning_rate': 6.144434340972346e-05, 'epoch': 1.69}
 27%|██▋       | 1330/5000 [6:01:54<17:28:59, 17.15s/it] 27%|██▋       | 1331/5000 [6:02:10<17:10:11, 16.85s/it]                                                        {'loss': 22.9447, 'grad_norm': 15.625, 'learning_rate': 6.142917327387482e-05, 'epoch': 1.69}
 27%|██▋       | 1331/5000 [6:02:10<17:10:11, 16.85s/it] 27%|██▋       | 1332/5000 [6:02:35<19:34:15, 19.21s/it]                                                        {'loss': 22.6799, 'grad_norm': 25.875, 'learning_rate': 6.141399157700787e-05, 'epoch': 1.69}
 27%|██▋       | 1332/5000 [6:02:35<19:34:15, 19.21s/it] 27%|██▋       | 1333/5000 [6:02:51<18:45:00, 18.41s/it]                                                        {'loss': 20.276, 'grad_norm': 76.5, 'learning_rate': 6.139879832576362e-05, 'epoch': 1.69}
 27%|██▋       | 1333/5000 [6:02:51<18:45:00, 18.41s/it] 27%|██▋       | 1334/5000 [6:03:06<17:42:13, 17.39s/it]                                                        {'loss': 23.3378, 'grad_norm': 30.0, 'learning_rate': 6.13835935267881e-05, 'epoch': 1.69}
 27%|██▋       | 1334/5000 [6:03:06<17:42:13, 17.39s/it] 27%|██▋       | 1335/5000 [6:03:29<19:23:26, 19.05s/it]                                                        {'loss': 22.4659, 'grad_norm': 22.375, 'learning_rate': 6.136837718673244e-05, 'epoch': 1.7}
 27%|██▋       | 1335/5000 [6:03:29<19:23:26, 19.05s/it] 27%|██▋       | 1336/5000 [6:03:44<18:06:51, 17.80s/it]                                                        {'loss': 19.6555, 'grad_norm': 18.125, 'learning_rate': 6.135314931225273e-05, 'epoch': 1.7}
 27%|██▋       | 1336/5000 [6:03:44<18:06:51, 17.80s/it] 27%|██▋       | 1337/5000 [6:04:00<17:33:52, 17.26s/it]                                                        {'loss': 20.0921, 'grad_norm': 17.25, 'learning_rate': 6.13379099100102e-05, 'epoch': 1.7}
 27%|██▋       | 1337/5000 [6:04:00<17:33:52, 17.26s/it] 27%|██▋       | 1338/5000 [6:04:25<19:48:22, 19.47s/it]                                                        {'loss': 23.3629, 'grad_norm': 74.0, 'learning_rate': 6.132265898667105e-05, 'epoch': 1.7}
 27%|██▋       | 1338/5000 [6:04:25<19:48:22, 19.47s/it] 27%|██▋       | 1339/5000 [6:04:39<18:04:32, 17.77s/it]                                                        {'loss': 22.0551, 'grad_norm': 34.75, 'learning_rate': 6.130739654890657e-05, 'epoch': 1.7}
 27%|██▋       | 1339/5000 [6:04:39<18:04:32, 17.77s/it] 27%|██▋       | 1340/5000 [6:04:54<17:17:49, 17.01s/it]                                                        {'loss': 22.8037, 'grad_norm': 43.0, 'learning_rate': 6.129212260339309e-05, 'epoch': 1.7}
 27%|██▋       | 1340/5000 [6:04:54<17:17:49, 17.01s/it] 27%|██▋       | 1341/5000 [6:05:11<17:11:40, 16.92s/it]                                                        {'loss': 21.0023, 'grad_norm': 19.875, 'learning_rate': 6.127683715681192e-05, 'epoch': 1.7}
 27%|██▋       | 1341/5000 [6:05:11<17:11:40, 16.92s/it] 27%|██▋       | 1342/5000 [6:05:25<16:33:32, 16.30s/it]                                                        {'loss': 23.5778, 'grad_norm': 34.0, 'learning_rate': 6.126154021584944e-05, 'epoch': 1.7}
 27%|██▋       | 1342/5000 [6:05:25<16:33:32, 16.30s/it] 27%|██▋       | 1343/5000 [6:05:42<16:32:00, 16.28s/it]                                                        {'loss': 20.9095, 'grad_norm': 24.625, 'learning_rate': 6.124623178719706e-05, 'epoch': 1.71}
 27%|██▋       | 1343/5000 [6:05:42<16:32:00, 16.28s/it] 27%|██▋       | 1344/5000 [6:05:56<15:48:14, 15.56s/it]                                                        {'loss': 20.9806, 'grad_norm': 21.875, 'learning_rate': 6.12309118775512e-05, 'epoch': 1.71}
 27%|██▋       | 1344/5000 [6:05:56<15:48:14, 15.56s/it] 27%|██▋       | 1345/5000 [6:06:11<15:51:17, 15.62s/it]                                                        {'loss': 24.4743, 'grad_norm': 116.5, 'learning_rate': 6.121558049361332e-05, 'epoch': 1.71}
 27%|██▋       | 1345/5000 [6:06:11<15:51:17, 15.62s/it] 27%|██▋       | 1346/5000 [6:06:30<16:43:10, 16.47s/it]                                                        {'loss': 22.2238, 'grad_norm': 33.5, 'learning_rate': 6.120023764208987e-05, 'epoch': 1.71}
 27%|██▋       | 1346/5000 [6:06:30<16:43:10, 16.47s/it] 27%|██▋       | 1347/5000 [6:06:46<16:42:00, 16.46s/it]                                                        {'loss': 20.861, 'grad_norm': 17.125, 'learning_rate': 6.118488332969235e-05, 'epoch': 1.71}
 27%|██▋       | 1347/5000 [6:06:46<16:42:00, 16.46s/it] 27%|██▋       | 1348/5000 [6:07:05<17:22:01, 17.12s/it]                                                        {'loss': 20.8311, 'grad_norm': 26.375, 'learning_rate': 6.116951756313727e-05, 'epoch': 1.71}
 27%|██▋       | 1348/5000 [6:07:05<17:22:01, 17.12s/it] 27%|██▋       | 1349/5000 [6:07:28<19:11:36, 18.93s/it]                                                        {'loss': 33.3313, 'grad_norm': 620.0, 'learning_rate': 6.11541403491461e-05, 'epoch': 1.71}
 27%|██▋       | 1349/5000 [6:07:28<19:11:36, 18.93s/it] 27%|██▋       | 1350/5000 [6:07:50<20:07:14, 19.84s/it]                                                        {'loss': 23.3017, 'grad_norm': 36.25, 'learning_rate': 6.113875169444539e-05, 'epoch': 1.71}
 27%|██▋       | 1350/5000 [6:07:50<20:07:14, 19.84s/it] 27%|██▋       | 1351/5000 [6:08:07<19:16:22, 19.01s/it]                                                        {'loss': 22.1447, 'grad_norm': 185.0, 'learning_rate': 6.112335160576663e-05, 'epoch': 1.72}
 27%|██▋       | 1351/5000 [6:08:07<19:16:22, 19.01s/it] 27%|██▋       | 1352/5000 [6:08:23<18:16:19, 18.03s/it]                                                        {'loss': 24.7094, 'grad_norm': 78.0, 'learning_rate': 6.110794008984638e-05, 'epoch': 1.72}
 27%|██▋       | 1352/5000 [6:08:23<18:16:19, 18.03s/it] 27%|██▋       | 1353/5000 [6:08:36<16:47:35, 16.58s/it]                                                        {'loss': 30.4336, 'grad_norm': 161.0, 'learning_rate': 6.109251715342611e-05, 'epoch': 1.72}
 27%|██▋       | 1353/5000 [6:08:36<16:47:35, 16.58s/it] 27%|██▋       | 1354/5000 [6:09:02<19:33:17, 19.31s/it]                                                        {'loss': 21.3502, 'grad_norm': 35.75, 'learning_rate': 6.10770828032524e-05, 'epoch': 1.72}
 27%|██▋       | 1354/5000 [6:09:02<19:33:17, 19.31s/it] 27%|██▋       | 1355/5000 [6:09:15<17:47:46, 17.58s/it]                                                        {'loss': 23.4017, 'grad_norm': 25.125, 'learning_rate': 6.106163704607671e-05, 'epoch': 1.72}
 27%|██▋       | 1355/5000 [6:09:15<17:47:46, 17.58s/it] 27%|██▋       | 1356/5000 [6:09:32<17:26:44, 17.23s/it]                                                        {'loss': 21.4664, 'grad_norm': 221.0, 'learning_rate': 6.104617988865555e-05, 'epoch': 1.72}
 27%|██▋       | 1356/5000 [6:09:32<17:26:44, 17.23s/it] 27%|██▋       | 1357/5000 [6:09:52<18:29:13, 18.27s/it]                                                        {'loss': 22.2463, 'grad_norm': 23.5, 'learning_rate': 6.103071133775042e-05, 'epoch': 1.72}
 27%|██▋       | 1357/5000 [6:09:52<18:29:13, 18.27s/it] 27%|██▋       | 1358/5000 [6:10:18<20:51:47, 20.62s/it]                                                        {'loss': 21.1753, 'grad_norm': 31.25, 'learning_rate': 6.1015231400127776e-05, 'epoch': 1.72}
 27%|██▋       | 1358/5000 [6:10:18<20:51:47, 20.62s/it] 27%|██▋       | 1359/5000 [6:10:35<19:34:44, 19.36s/it]                                                        {'loss': 36.552, 'grad_norm': 624.0, 'learning_rate': 6.0999740082559076e-05, 'epoch': 1.73}
 27%|██▋       | 1359/5000 [6:10:35<19:34:44, 19.36s/it] 27%|██▋       | 1360/5000 [6:10:50<18:15:04, 18.05s/it]                                                        {'loss': 24.087, 'grad_norm': 135.0, 'learning_rate': 6.0984237391820744e-05, 'epoch': 1.73}
 27%|██▋       | 1360/5000 [6:10:50<18:15:04, 18.05s/it] 27%|██▋       | 1361/5000 [6:11:01<16:17:06, 16.11s/it]                                                        {'loss': 25.8199, 'grad_norm': 92.5, 'learning_rate': 6.096872333469419e-05, 'epoch': 1.73}
 27%|██▋       | 1361/5000 [6:11:01<16:17:06, 16.11s/it] 27%|██▋       | 1362/5000 [6:11:18<16:24:28, 16.24s/it]                                                        {'loss': 21.3024, 'grad_norm': 29.125, 'learning_rate': 6.095319791796577e-05, 'epoch': 1.73}
 27%|██▋       | 1362/5000 [6:11:18<16:24:28, 16.24s/it] 27%|██▋       | 1363/5000 [6:11:32<15:45:58, 15.61s/it]                                                        {'loss': 22.3429, 'grad_norm': 81.0, 'learning_rate': 6.093766114842686e-05, 'epoch': 1.73}
 27%|██▋       | 1363/5000 [6:11:32<15:45:58, 15.61s/it] 27%|██▋       | 1364/5000 [6:11:58<18:47:21, 18.60s/it]                                                        {'loss': 20.0765, 'grad_norm': 23.0, 'learning_rate': 6.092211303287374e-05, 'epoch': 1.73}
 27%|██▋       | 1364/5000 [6:11:58<18:47:21, 18.60s/it] 27%|██▋       | 1365/5000 [6:12:14<18:05:40, 17.92s/it]                                                        {'loss': 22.8746, 'grad_norm': 36.25, 'learning_rate': 6.09065535781077e-05, 'epoch': 1.73}
 27%|██▋       | 1365/5000 [6:12:14<18:05:40, 17.92s/it] 27%|██▋       | 1366/5000 [6:12:30<17:33:21, 17.39s/it]                                                        {'loss': 24.1485, 'grad_norm': 102.0, 'learning_rate': 6.089098279093498e-05, 'epoch': 1.73}
 27%|██▋       | 1366/5000 [6:12:30<17:33:21, 17.39s/it] 27%|██▋       | 1367/5000 [6:12:47<17:25:56, 17.27s/it]                                                        {'loss': 22.0408, 'grad_norm': 24.25, 'learning_rate': 6.087540067816676e-05, 'epoch': 1.74}
 27%|██▋       | 1367/5000 [6:12:47<17:25:56, 17.27s/it] 27%|██▋       | 1368/5000 [6:13:08<18:25:35, 18.26s/it]                                                        {'loss': 22.4255, 'grad_norm': 16.875, 'learning_rate': 6.085980724661919e-05, 'epoch': 1.74}
 27%|██▋       | 1368/5000 [6:13:08<18:25:35, 18.26s/it] 27%|██▋       | 1369/5000 [6:13:22<17:16:22, 17.13s/it]                                                        {'loss': 20.3372, 'grad_norm': 22.375, 'learning_rate': 6.084420250311337e-05, 'epoch': 1.74}
 27%|██▋       | 1369/5000 [6:13:22<17:16:22, 17.13s/it] 27%|██▋       | 1370/5000 [6:13:42<17:57:01, 17.80s/it]                                                        {'loss': 22.1719, 'grad_norm': 26.0, 'learning_rate': 6.0828586454475335e-05, 'epoch': 1.74}
 27%|██▋       | 1370/5000 [6:13:42<17:57:01, 17.80s/it] 27%|██▋       | 1371/5000 [6:13:58<17:38:18, 17.50s/it]                                                        {'loss': 29.4969, 'grad_norm': 516.0, 'learning_rate': 6.081295910753608e-05, 'epoch': 1.74}
 27%|██▋       | 1371/5000 [6:13:58<17:38:18, 17.50s/it] 27%|██▋       | 1372/5000 [6:14:13<16:50:04, 16.70s/it]                                                        {'loss': 24.2778, 'grad_norm': 131.0, 'learning_rate': 6.079732046913154e-05, 'epoch': 1.74}
 27%|██▋       | 1372/5000 [6:14:13<16:50:04, 16.70s/it] 27%|██▋       | 1373/5000 [6:14:31<17:07:34, 17.00s/it]                                                        {'loss': 26.9026, 'grad_norm': 364.0, 'learning_rate': 6.078167054610259e-05, 'epoch': 1.74}
 27%|██▋       | 1373/5000 [6:14:31<17:07:34, 17.00s/it] 27%|██▋       | 1374/5000 [6:14:46<16:40:33, 16.56s/it]                                                        {'loss': 21.8112, 'grad_norm': 36.25, 'learning_rate': 6.076600934529503e-05, 'epoch': 1.74}
 27%|██▋       | 1374/5000 [6:14:46<16:40:33, 16.56s/it] 28%|██▊       | 1375/5000 [6:15:03<16:39:02, 16.54s/it]                                                        {'loss': 23.0926, 'grad_norm': 46.0, 'learning_rate': 6.0750336873559605e-05, 'epoch': 1.75}
 28%|██▊       | 1375/5000 [6:15:03<16:39:02, 16.54s/it] 28%|██▊       | 1376/5000 [6:15:18<16:17:31, 16.18s/it]                                                        {'loss': 20.9749, 'grad_norm': 26.75, 'learning_rate': 6.073465313775198e-05, 'epoch': 1.75}
 28%|██▊       | 1376/5000 [6:15:18<16:17:31, 16.18s/it] 28%|██▊       | 1377/5000 [6:15:42<18:36:05, 18.48s/it]                                                        {'loss': 21.4959, 'grad_norm': 15.9375, 'learning_rate': 6.071895814473277e-05, 'epoch': 1.75}
 28%|██▊       | 1377/5000 [6:15:42<18:36:05, 18.48s/it] 28%|██▊       | 1378/5000 [6:15:58<17:42:16, 17.60s/it]                                                        {'loss': 21.4534, 'grad_norm': 77.0, 'learning_rate': 6.070325190136748e-05, 'epoch': 1.75}
 28%|██▊       | 1378/5000 [6:15:58<17:42:16, 17.60s/it] 28%|██▊       | 1379/5000 [6:16:12<16:39:18, 16.56s/it]                                                        {'loss': 27.1575, 'grad_norm': 780.0, 'learning_rate': 6.068753441452657e-05, 'epoch': 1.75}
 28%|██▊       | 1379/5000 [6:16:12<16:39:18, 16.56s/it] 28%|██▊       | 1380/5000 [6:16:30<17:10:14, 17.08s/it]                                                        {'loss': 22.3506, 'grad_norm': 86.5, 'learning_rate': 6.067180569108539e-05, 'epoch': 1.75}
 28%|██▊       | 1380/5000 [6:16:30<17:10:14, 17.08s/it] 28%|██▊       | 1381/5000 [6:16:44<16:17:26, 16.21s/it]                                                        {'loss': 26.2723, 'grad_norm': 52.25, 'learning_rate': 6.065606573792422e-05, 'epoch': 1.75}
 28%|██▊       | 1381/5000 [6:16:44<16:17:26, 16.21s/it] 28%|██▊       | 1382/5000 [6:16:59<15:52:09, 15.79s/it]                                                        {'loss': 20.5338, 'grad_norm': 24.5, 'learning_rate': 6.0640314561928265e-05, 'epoch': 1.75}
 28%|██▊       | 1382/5000 [6:16:59<15:52:09, 15.79s/it] 28%|██▊       | 1383/5000 [6:17:15<15:57:00, 15.88s/it]                                                        {'loss': 21.0586, 'grad_norm': 22.625, 'learning_rate': 6.0624552169987604e-05, 'epoch': 1.76}
 28%|██▊       | 1383/5000 [6:17:15<15:57:00, 15.88s/it] 28%|██▊       | 1384/5000 [6:17:38<18:10:24, 18.09s/it]                                                        {'loss': 22.4956, 'grad_norm': 43.0, 'learning_rate': 6.060877856899726e-05, 'epoch': 1.76}
 28%|██▊       | 1384/5000 [6:17:38<18:10:24, 18.09s/it] 28%|██▊       | 1385/5000 [6:17:54<17:24:32, 17.34s/it]                                                        {'loss': 23.2683, 'grad_norm': 86.0, 'learning_rate': 6.0592993765857126e-05, 'epoch': 1.76}
 28%|██▊       | 1385/5000 [6:17:54<17:24:32, 17.34s/it] 28%|██▊       | 1386/5000 [6:18:09<16:43:24, 16.66s/it]                                                        {'loss': 21.1754, 'grad_norm': 29.125, 'learning_rate': 6.057719776747202e-05, 'epoch': 1.76}
 28%|██▊       | 1386/5000 [6:18:09<16:43:24, 16.66s/it] 28%|██▊       | 1387/5000 [6:18:24<16:17:16, 16.23s/it]                                                        {'loss': 38.1424, 'grad_norm': 1712.0, 'learning_rate': 6.056139058075165e-05, 'epoch': 1.76}
 28%|██▊       | 1387/5000 [6:18:24<16:17:16, 16.23s/it] 28%|██▊       | 1388/5000 [6:18:46<17:46:47, 17.72s/it]                                                        {'loss': 23.3786, 'grad_norm': 368.0, 'learning_rate': 6.054557221261062e-05, 'epoch': 1.76}
 28%|██▊       | 1388/5000 [6:18:46<17:46:47, 17.72s/it] 28%|██▊       | 1389/5000 [6:19:11<20:11:48, 20.14s/it]                                                        {'loss': 25.3478, 'grad_norm': 39.0, 'learning_rate': 6.0529742669968407e-05, 'epoch': 1.76}
 28%|██▊       | 1389/5000 [6:19:11<20:11:48, 20.14s/it] 28%|██▊       | 1390/5000 [6:19:27<18:57:45, 18.91s/it]                                                        {'loss': 23.8252, 'grad_norm': 36.5, 'learning_rate': 6.0513901959749396e-05, 'epoch': 1.77}
 28%|██▊       | 1390/5000 [6:19:27<18:57:45, 18.91s/it] 28%|██▊       | 1391/5000 [6:19:44<18:19:17, 18.28s/it]                                                        {'loss': 24.9383, 'grad_norm': 188.0, 'learning_rate': 6.049805008888286e-05, 'epoch': 1.77}
 28%|██▊       | 1391/5000 [6:19:44<18:19:17, 18.28s/it] 28%|██▊       | 1392/5000 [6:20:03<18:20:59, 18.31s/it]                                                        {'loss': 23.278, 'grad_norm': 33.5, 'learning_rate': 6.0482187064302946e-05, 'epoch': 1.77}
 28%|██▊       | 1392/5000 [6:20:03<18:20:59, 18.31s/it] 28%|██▊       | 1393/5000 [6:20:18<17:31:29, 17.49s/it]                                                        {'loss': 23.1587, 'grad_norm': 30.0, 'learning_rate': 6.046631289294867e-05, 'epoch': 1.77}
 28%|██▊       | 1393/5000 [6:20:18<17:31:29, 17.49s/it] 28%|██▊       | 1394/5000 [6:20:31<16:04:22, 16.05s/it]                                                        {'loss': 27.1006, 'grad_norm': 75.0, 'learning_rate': 6.045042758176394e-05, 'epoch': 1.77}
 28%|██▊       | 1394/5000 [6:20:31<16:04:22, 16.05s/it] 28%|██▊       | 1395/5000 [6:20:45<15:24:25, 15.39s/it]                                                        {'loss': 21.9762, 'grad_norm': 29.125, 'learning_rate': 6.043453113769752e-05, 'epoch': 1.77}
 28%|██▊       | 1395/5000 [6:20:45<15:24:25, 15.39s/it] 28%|██▊       | 1396/5000 [6:20:59<14:57:43, 14.95s/it]                                                        {'loss': 21.8894, 'grad_norm': 26.125, 'learning_rate': 6.041862356770308e-05, 'epoch': 1.77}
 28%|██▊       | 1396/5000 [6:20:59<14:57:43, 14.95s/it] 28%|██▊       | 1397/5000 [6:21:25<18:24:29, 18.39s/it]                                                        {'loss': 22.2619, 'grad_norm': 31.0, 'learning_rate': 6.040270487873911e-05, 'epoch': 1.77}
 28%|██▊       | 1397/5000 [6:21:25<18:24:29, 18.39s/it] 28%|██▊       | 1398/5000 [6:21:40<17:14:29, 17.23s/it]                                                        {'loss': 22.0174, 'grad_norm': 23.125, 'learning_rate': 6.0386775077768996e-05, 'epoch': 1.78}
 28%|██▊       | 1398/5000 [6:21:40<17:14:29, 17.23s/it] 28%|██▊       | 1399/5000 [6:21:54<16:24:09, 16.40s/it]                                                        {'loss': 21.1868, 'grad_norm': 22.5, 'learning_rate': 6.0370834171760956e-05, 'epoch': 1.78}
 28%|██▊       | 1399/5000 [6:21:54<16:24:09, 16.40s/it] 28%|██▊       | 1400/5000 [6:22:11<16:38:00, 16.63s/it]                                                        {'loss': 20.5469, 'grad_norm': 21.0, 'learning_rate': 6.035488216768811e-05, 'epoch': 1.78}
 28%|██▊       | 1400/5000 [6:22:11<16:38:00, 16.63s/it] 28%|██▊       | 1401/5000 [6:22:27<16:25:52, 16.44s/it]                                                        {'loss': 23.5326, 'grad_norm': 38.0, 'learning_rate': 6.0338919072528387e-05, 'epoch': 1.78}
 28%|██▊       | 1401/5000 [6:22:27<16:25:52, 16.44s/it] 28%|██▊       | 1402/5000 [6:22:39<15:03:04, 15.06s/it]                                                        {'loss': 23.2714, 'grad_norm': 24.375, 'learning_rate': 6.0322944893264593e-05, 'epoch': 1.78}
 28%|██▊       | 1402/5000 [6:22:39<15:03:04, 15.06s/it] 28%|██▊       | 1403/5000 [6:23:01<17:16:24, 17.29s/it]                                                        {'loss': 22.9995, 'grad_norm': 32.5, 'learning_rate': 6.030695963688439e-05, 'epoch': 1.78}
 28%|██▊       | 1403/5000 [6:23:01<17:16:24, 17.29s/it] 28%|██▊       | 1404/5000 [6:23:20<17:44:39, 17.76s/it]                                                        {'loss': 22.8358, 'grad_norm': 29.875, 'learning_rate': 6.029096331038024e-05, 'epoch': 1.78}
 28%|██▊       | 1404/5000 [6:23:20<17:44:39, 17.76s/it] 28%|██▊       | 1405/5000 [6:23:36<17:09:30, 17.18s/it]                                                        {'loss': 22.4844, 'grad_norm': 30.5, 'learning_rate': 6.027495592074952e-05, 'epoch': 1.78}
 28%|██▊       | 1405/5000 [6:23:36<17:09:30, 17.18s/it] 28%|██▊       | 1406/5000 [6:23:50<16:03:29, 16.09s/it]                                                        {'loss': 28.3479, 'grad_norm': 150.0, 'learning_rate': 6.025893747499437e-05, 'epoch': 1.79}
 28%|██▊       | 1406/5000 [6:23:50<16:03:29, 16.09s/it] 28%|██▊       | 1407/5000 [6:24:09<17:01:59, 17.07s/it]                                                        {'loss': 20.1906, 'grad_norm': 109.0, 'learning_rate': 6.0242907980121825e-05, 'epoch': 1.79}
 28%|██▊       | 1407/5000 [6:24:09<17:01:59, 17.07s/it] 28%|██▊       | 1408/5000 [6:24:23<16:11:36, 16.23s/it]                                                        {'loss': 23.211, 'grad_norm': 124.0, 'learning_rate': 6.0226867443143725e-05, 'epoch': 1.79}
 28%|██▊       | 1408/5000 [6:24:23<16:11:36, 16.23s/it] 28%|██▊       | 1409/5000 [6:24:35<14:55:35, 14.96s/it]                                                        {'loss': 23.9545, 'grad_norm': 332.0, 'learning_rate': 6.021081587107674e-05, 'epoch': 1.79}
 28%|██▊       | 1409/5000 [6:24:35<14:55:35, 14.96s/it] 28%|██▊       | 1410/5000 [6:24:51<14:59:56, 15.04s/it]                                                        {'loss': 21.5929, 'grad_norm': 32.75, 'learning_rate': 6.0194753270942373e-05, 'epoch': 1.79}
 28%|██▊       | 1410/5000 [6:24:51<14:59:56, 15.04s/it] 28%|██▊       | 1411/5000 [6:25:05<14:40:57, 14.73s/it]                                                        {'loss': 20.8389, 'grad_norm': 29.375, 'learning_rate': 6.0178679649766966e-05, 'epoch': 1.79}
 28%|██▊       | 1411/5000 [6:25:05<14:40:57, 14.73s/it] 28%|██▊       | 1412/5000 [6:25:18<14:25:08, 14.47s/it]                                                        {'loss': 46.8852, 'grad_norm': 1984.0, 'learning_rate': 6.0162595014581644e-05, 'epoch': 1.79}
 28%|██▊       | 1412/5000 [6:25:18<14:25:08, 14.47s/it] 28%|██▊       | 1413/5000 [6:25:41<16:51:02, 16.91s/it]                                                        {'loss': 22.4333, 'grad_norm': 47.0, 'learning_rate': 6.0146499372422404e-05, 'epoch': 1.79}
 28%|██▊       | 1413/5000 [6:25:41<16:51:02, 16.91s/it] 28%|██▊       | 1414/5000 [6:26:01<17:37:36, 17.70s/it]                                                        {'loss': 34.2149, 'grad_norm': 684.0, 'learning_rate': 6.0130392730329996e-05, 'epoch': 1.8}
 28%|██▊       | 1414/5000 [6:26:01<17:37:36, 17.70s/it] 28%|██▊       | 1415/5000 [6:26:14<16:22:40, 16.45s/it]                                                        {'loss': 25.0502, 'grad_norm': 194.0, 'learning_rate': 6.0114275095350024e-05, 'epoch': 1.8}
 28%|██▊       | 1415/5000 [6:26:14<16:22:40, 16.45s/it] 28%|██▊       | 1416/5000 [6:26:27<15:21:31, 15.43s/it]                                                        {'loss': 25.1016, 'grad_norm': 102.5, 'learning_rate': 6.009814647453289e-05, 'epoch': 1.8}
 28%|██▊       | 1416/5000 [6:26:27<15:21:31, 15.43s/it] 28%|██▊       | 1417/5000 [6:26:41<14:49:13, 14.89s/it]                                                        {'loss': 23.055, 'grad_norm': 29.125, 'learning_rate': 6.00820068749338e-05, 'epoch': 1.8}
 28%|██▊       | 1417/5000 [6:26:41<14:49:13, 14.89s/it] 28%|██▊       | 1418/5000 [6:27:00<16:01:40, 16.11s/it]                                                        {'loss': 20.8298, 'grad_norm': 61.75, 'learning_rate': 6.0065856303612777e-05, 'epoch': 1.8}
 28%|██▊       | 1418/5000 [6:27:00<16:01:40, 16.11s/it] 28%|██▊       | 1419/5000 [6:27:12<14:57:53, 15.04s/it]                                                        {'loss': 23.0569, 'grad_norm': 32.75, 'learning_rate': 6.00496947676346e-05, 'epoch': 1.8}
 28%|██▊       | 1419/5000 [6:27:12<14:57:53, 15.04s/it] 28%|██▊       | 1420/5000 [6:27:27<14:48:32, 14.89s/it]                                                        {'loss': 21.7477, 'grad_norm': 30.75, 'learning_rate': 6.003352227406889e-05, 'epoch': 1.8}
 28%|██▊       | 1420/5000 [6:27:27<14:48:32, 14.89s/it] 28%|██▊       | 1421/5000 [6:27:43<15:16:45, 15.37s/it]                                                        {'loss': 20.0554, 'grad_norm': 18.0, 'learning_rate': 6.0017338829990044e-05, 'epoch': 1.8}
 28%|██▊       | 1421/5000 [6:27:43<15:16:45, 15.37s/it] 28%|██▊       | 1422/5000 [6:28:07<17:41:01, 17.79s/it]                                                        {'loss': 20.8466, 'grad_norm': 16.625, 'learning_rate': 6.0001144442477256e-05, 'epoch': 1.81}
 28%|██▊       | 1422/5000 [6:28:07<17:41:01, 17.79s/it] 28%|██▊       | 1423/5000 [6:28:27<18:21:42, 18.48s/it]                                                        {'loss': 21.0642, 'grad_norm': 30.0, 'learning_rate': 5.99849391186145e-05, 'epoch': 1.81}
 28%|██▊       | 1423/5000 [6:28:27<18:21:42, 18.48s/it] 28%|██▊       | 1424/5000 [6:28:43<17:37:15, 17.74s/it]                                                        {'loss': 20.3789, 'grad_norm': 13.375, 'learning_rate': 5.9968722865490535e-05, 'epoch': 1.81}
 28%|██▊       | 1424/5000 [6:28:43<17:37:15, 17.74s/it] 28%|██▊       | 1425/5000 [6:29:00<17:25:49, 17.55s/it]                                                        {'loss': 19.0664, 'grad_norm': 18.0, 'learning_rate': 5.9952495690198894e-05, 'epoch': 1.81}
 28%|██▊       | 1425/5000 [6:29:00<17:25:49, 17.55s/it] 29%|██▊       | 1426/5000 [6:29:15<16:47:32, 16.91s/it]                                                        {'loss': 24.4925, 'grad_norm': 179.0, 'learning_rate': 5.9936257599837894e-05, 'epoch': 1.81}
 29%|██▊       | 1426/5000 [6:29:15<16:47:32, 16.91s/it] 29%|██▊       | 1427/5000 [6:29:31<16:18:06, 16.43s/it]                                                        {'loss': 22.053, 'grad_norm': 33.25, 'learning_rate': 5.992000860151064e-05, 'epoch': 1.81}
 29%|██▊       | 1427/5000 [6:29:31<16:18:06, 16.43s/it] 29%|██▊       | 1428/5000 [6:29:48<16:41:38, 16.82s/it]                                                        {'loss': 20.1055, 'grad_norm': 21.875, 'learning_rate': 5.990374870232499e-05, 'epoch': 1.81}
 29%|██▊       | 1428/5000 [6:29:48<16:41:38, 16.82s/it] 29%|██▊       | 1429/5000 [6:30:05<16:44:50, 16.88s/it]                                                        {'loss': 20.8042, 'grad_norm': 22.375, 'learning_rate': 5.988747790939356e-05, 'epoch': 1.81}
 29%|██▊       | 1429/5000 [6:30:05<16:44:50, 16.88s/it] 29%|██▊       | 1430/5000 [6:30:22<16:38:28, 16.78s/it]                                                        {'loss': 21.3944, 'grad_norm': 46.75, 'learning_rate': 5.987119622983377e-05, 'epoch': 1.82}
 29%|██▊       | 1430/5000 [6:30:22<16:38:28, 16.78s/it] 29%|██▊       | 1431/5000 [6:30:35<15:33:47, 15.70s/it]                                                        {'loss': 23.0581, 'grad_norm': 37.0, 'learning_rate': 5.985490367076777e-05, 'epoch': 1.82}
 29%|██▊       | 1431/5000 [6:30:35<15:33:47, 15.70s/it] 29%|██▊       | 1432/5000 [6:30:50<15:16:13, 15.41s/it]                                                        {'loss': 21.707, 'grad_norm': 92.0, 'learning_rate': 5.9838600239322485e-05, 'epoch': 1.82}
 29%|██▊       | 1432/5000 [6:30:50<15:16:13, 15.41s/it] 29%|██▊       | 1433/5000 [6:31:14<17:46:32, 17.94s/it]                                                        {'loss': 22.6917, 'grad_norm': 23.25, 'learning_rate': 5.982228594262959e-05, 'epoch': 1.82}
 29%|██▊       | 1433/5000 [6:31:14<17:46:32, 17.94s/it] 29%|██▊       | 1434/5000 [6:31:32<17:48:11, 17.97s/it]                                                        {'loss': 20.2518, 'grad_norm': 26.5, 'learning_rate': 5.9805960787825494e-05, 'epoch': 1.82}
 29%|██▊       | 1434/5000 [6:31:32<17:48:11, 17.97s/it] 29%|██▊       | 1435/5000 [6:31:50<17:44:51, 17.92s/it]                                                        {'loss': 22.6527, 'grad_norm': 30.375, 'learning_rate': 5.97896247820514e-05, 'epoch': 1.82}
 29%|██▊       | 1435/5000 [6:31:50<17:44:51, 17.92s/it] 29%|██▊       | 1436/5000 [6:32:03<16:32:43, 16.71s/it]                                                        {'loss': 23.4795, 'grad_norm': 83.0, 'learning_rate': 5.9773277932453214e-05, 'epoch': 1.82}
 29%|██▊       | 1436/5000 [6:32:03<16:32:43, 16.71s/it] 29%|██▊       | 1437/5000 [6:32:18<15:49:19, 15.99s/it]                                                        {'loss': 20.8728, 'grad_norm': 15.8125, 'learning_rate': 5.9756920246181614e-05, 'epoch': 1.82}
 29%|██▊       | 1437/5000 [6:32:18<15:49:19, 15.99s/it] 29%|██▉       | 1438/5000 [6:32:41<17:51:36, 18.05s/it]                                                        {'loss': 23.3112, 'grad_norm': 26.375, 'learning_rate': 5.9740551730392e-05, 'epoch': 1.83}
 29%|██▉       | 1438/5000 [6:32:41<17:51:36, 18.05s/it] 29%|██▉       | 1439/5000 [6:32:56<17:07:33, 17.31s/it]                                                        {'loss': 22.8533, 'grad_norm': 27.0, 'learning_rate': 5.972417239224452e-05, 'epoch': 1.83}
 29%|██▉       | 1439/5000 [6:32:56<17:07:33, 17.31s/it] 29%|██▉       | 1440/5000 [6:33:14<17:10:04, 17.36s/it]                                                        {'loss': 23.119, 'grad_norm': 38.25, 'learning_rate': 5.970778223890406e-05, 'epoch': 1.83}
 29%|██▉       | 1440/5000 [6:33:14<17:10:04, 17.36s/it] 29%|██▉       | 1441/5000 [6:33:31<17:14:50, 17.45s/it]                                                        {'loss': 21.0097, 'grad_norm': 44.5, 'learning_rate': 5.9691381277540215e-05, 'epoch': 1.83}
 29%|██▉       | 1441/5000 [6:33:31<17:14:50, 17.45s/it] 29%|██▉       | 1442/5000 [6:33:44<15:43:44, 15.91s/it]                                                        {'loss': 23.7103, 'grad_norm': 39.0, 'learning_rate': 5.967496951532733e-05, 'epoch': 1.83}
 29%|██▉       | 1442/5000 [6:33:44<15:43:44, 15.91s/it] 29%|██▉       | 1443/5000 [6:33:56<14:43:55, 14.91s/it]                                                        {'loss': 22.9359, 'grad_norm': 33.5, 'learning_rate': 5.965854695944448e-05, 'epoch': 1.83}
 29%|██▉       | 1443/5000 [6:33:56<14:43:55, 14.91s/it] 29%|██▉       | 1444/5000 [6:34:10<14:26:46, 14.62s/it]                                                        {'loss': 23.8813, 'grad_norm': 31.5, 'learning_rate': 5.964211361707543e-05, 'epoch': 1.83}
 29%|██▉       | 1444/5000 [6:34:10<14:26:46, 14.62s/it] 29%|██▉       | 1445/5000 [6:34:27<15:05:29, 15.28s/it]                                                        {'loss': 21.0109, 'grad_norm': 42.25, 'learning_rate': 5.962566949540869e-05, 'epoch': 1.83}
 29%|██▉       | 1445/5000 [6:34:27<15:05:29, 15.28s/it] 29%|██▉       | 1446/5000 [6:34:39<14:11:19, 14.37s/it]                                                        {'loss': 20.7732, 'grad_norm': 24.375, 'learning_rate': 5.960921460163747e-05, 'epoch': 1.84}
 29%|██▉       | 1446/5000 [6:34:39<14:11:19, 14.37s/it] 29%|██▉       | 1447/5000 [6:35:03<16:58:24, 17.20s/it]                                                        {'loss': 21.9091, 'grad_norm': 35.25, 'learning_rate': 5.959274894295972e-05, 'epoch': 1.84}
 29%|██▉       | 1447/5000 [6:35:03<16:58:24, 17.20s/it] 29%|██▉       | 1448/5000 [6:35:18<16:24:51, 16.64s/it]                                                        {'loss': 22.6775, 'grad_norm': 494.0, 'learning_rate': 5.957627252657805e-05, 'epoch': 1.84}
 29%|██▉       | 1448/5000 [6:35:18<16:24:51, 16.64s/it] 29%|██▉       | 1449/5000 [6:35:34<15:58:34, 16.20s/it]                                                        {'loss': 32.7551, 'grad_norm': 218.0, 'learning_rate': 5.9559785359699837e-05, 'epoch': 1.84}
 29%|██▉       | 1449/5000 [6:35:34<15:58:34, 16.20s/it] 29%|██▉       | 1450/5000 [6:35:47<15:03:12, 15.27s/it]                                                        {'loss': 24.141, 'grad_norm': 97.0, 'learning_rate': 5.954328744953709e-05, 'epoch': 1.84}
 29%|██▉       | 1450/5000 [6:35:47<15:03:12, 15.27s/it] 29%|██▉       | 1451/5000 [6:36:11<17:51:14, 18.11s/it]                                                        {'loss': 22.3603, 'grad_norm': 107.5, 'learning_rate': 5.95267788033066e-05, 'epoch': 1.84}
 29%|██▉       | 1451/5000 [6:36:11<17:51:14, 18.11s/it] 29%|██▉       | 1452/5000 [6:36:26<16:55:49, 17.18s/it]                                                        {'loss': 40.0698, 'grad_norm': 142.0, 'learning_rate': 5.951025942822977e-05, 'epoch': 1.84}
 29%|██▉       | 1452/5000 [6:36:26<16:55:49, 17.18s/it] 29%|██▉       | 1453/5000 [6:36:44<16:57:16, 17.21s/it]                                                        {'loss': 20.3695, 'grad_norm': 50.25, 'learning_rate': 5.949372933153275e-05, 'epoch': 1.85}
 29%|██▉       | 1453/5000 [6:36:44<16:57:16, 17.21s/it] 29%|██▉       | 1454/5000 [6:37:07<18:49:39, 19.11s/it]                                                        {'loss': 56.7889, 'grad_norm': 2784.0, 'learning_rate': 5.947718852044637e-05, 'epoch': 1.85}
 29%|██▉       | 1454/5000 [6:37:07<18:49:39, 19.11s/it] 29%|██▉       | 1455/5000 [6:37:21<17:14:04, 17.50s/it]                                                        {'loss': 23.6616, 'grad_norm': 48.25, 'learning_rate': 5.946063700220614e-05, 'epoch': 1.85}
 29%|██▉       | 1455/5000 [6:37:21<17:14:04, 17.50s/it] 29%|██▉       | 1456/5000 [6:37:33<15:42:12, 15.95s/it]                                                        {'loss': 26.9075, 'grad_norm': 512.0, 'learning_rate': 5.944407478405226e-05, 'epoch': 1.85}
 29%|██▉       | 1456/5000 [6:37:33<15:42:12, 15.95s/it] 29%|██▉       | 1457/5000 [6:37:46<14:40:31, 14.91s/it]                                                        {'loss': 22.7262, 'grad_norm': 38.0, 'learning_rate': 5.94275018732296e-05, 'epoch': 1.85}
 29%|██▉       | 1457/5000 [6:37:46<14:40:31, 14.91s/it] 29%|██▉       | 1458/5000 [6:38:02<15:00:20, 15.25s/it]                                                        {'loss': 25.1295, 'grad_norm': 72.5, 'learning_rate': 5.9410918276987715e-05, 'epoch': 1.85}
 29%|██▉       | 1458/5000 [6:38:02<15:00:20, 15.25s/it] 29%|██▉       | 1459/5000 [6:38:17<14:55:57, 15.18s/it]                                                        {'loss': 21.5697, 'grad_norm': 26.0, 'learning_rate': 5.9394324002580835e-05, 'epoch': 1.85}
 29%|██▉       | 1459/5000 [6:38:17<14:55:57, 15.18s/it] 29%|██▉       | 1460/5000 [6:38:29<14:04:21, 14.31s/it]                                                        {'loss': 24.7409, 'grad_norm': 318.0, 'learning_rate': 5.9377719057267875e-05, 'epoch': 1.85}
 29%|██▉       | 1460/5000 [6:38:29<14:04:21, 14.31s/it] 29%|██▉       | 1461/5000 [6:38:58<18:15:31, 18.57s/it]                                                        {'loss': 21.3767, 'grad_norm': 23.0, 'learning_rate': 5.936110344831237e-05, 'epoch': 1.86}
 29%|██▉       | 1461/5000 [6:38:58<18:15:31, 18.57s/it] 29%|██▉       | 1462/5000 [6:39:12<16:55:01, 17.21s/it]                                                        {'loss': 29.3695, 'grad_norm': 153.0, 'learning_rate': 5.9344477182982574e-05, 'epoch': 1.86}
 29%|██▉       | 1462/5000 [6:39:12<16:55:01, 17.21s/it] 29%|██▉       | 1463/5000 [6:39:27<16:27:19, 16.75s/it]                                                        {'loss': 21.9332, 'grad_norm': 39.75, 'learning_rate': 5.9327840268551383e-05, 'epoch': 1.86}
 29%|██▉       | 1463/5000 [6:39:27<16:27:19, 16.75s/it] 29%|██▉       | 1464/5000 [6:39:43<15:58:55, 16.27s/it]                                                        {'loss': 25.2098, 'grad_norm': 253.0, 'learning_rate': 5.931119271229634e-05, 'epoch': 1.86}
 29%|██▉       | 1464/5000 [6:39:43<15:58:55, 16.27s/it] 29%|██▉       | 1465/5000 [6:39:57<15:24:01, 15.68s/it]                                                        {'loss': 20.9405, 'grad_norm': 45.0, 'learning_rate': 5.9294534521499645e-05, 'epoch': 1.86}
 29%|██▉       | 1465/5000 [6:39:57<15:24:01, 15.68s/it] 29%|██▉       | 1466/5000 [6:40:09<14:15:03, 14.52s/it]                                                        {'loss': 23.9847, 'grad_norm': 32.75, 'learning_rate': 5.9277865703448165e-05, 'epoch': 1.86}
 29%|██▉       | 1466/5000 [6:40:09<14:15:03, 14.52s/it] 29%|██▉       | 1467/5000 [6:40:25<14:52:13, 15.15s/it]                                                        {'loss': 20.9097, 'grad_norm': 25.0, 'learning_rate': 5.9261186265433416e-05, 'epoch': 1.86}
 29%|██▉       | 1467/5000 [6:40:25<14:52:13, 15.15s/it] 29%|██▉       | 1468/5000 [6:40:49<17:23:33, 17.73s/it]                                                        {'loss': 23.8094, 'grad_norm': 149.0, 'learning_rate': 5.9244496214751544e-05, 'epoch': 1.86}
 29%|██▉       | 1468/5000 [6:40:49<17:23:33, 17.73s/it] 29%|██▉       | 1469/5000 [6:41:04<16:34:28, 16.90s/it]                                                        {'loss': 21.0756, 'grad_norm': 25.25, 'learning_rate': 5.922779555870333e-05, 'epoch': 1.87}
 29%|██▉       | 1469/5000 [6:41:04<16:34:28, 16.90s/it] 29%|██▉       | 1470/5000 [6:41:20<16:10:53, 16.50s/it]                                                        {'loss': 24.9753, 'grad_norm': 118.5, 'learning_rate': 5.921108430459422e-05, 'epoch': 1.87}
 29%|██▉       | 1470/5000 [6:41:20<16:10:53, 16.50s/it] 29%|██▉       | 1471/5000 [6:41:43<18:10:48, 18.55s/it]                                                        {'loss': 20.7196, 'grad_norm': 34.0, 'learning_rate': 5.919436245973429e-05, 'epoch': 1.87}
 29%|██▉       | 1471/5000 [6:41:43<18:10:48, 18.55s/it] 29%|██▉       | 1472/5000 [6:41:58<17:13:28, 17.58s/it]                                                        {'loss': 22.9408, 'grad_norm': 43.5, 'learning_rate': 5.9177630031438234e-05, 'epoch': 1.87}
 29%|██▉       | 1472/5000 [6:41:58<17:13:28, 17.58s/it] 29%|██▉       | 1473/5000 [6:42:17<17:27:59, 17.83s/it]                                                        {'loss': 20.2159, 'grad_norm': 15.125, 'learning_rate': 5.9160887027025395e-05, 'epoch': 1.87}
 29%|██▉       | 1473/5000 [6:42:17<17:27:59, 17.83s/it] 29%|██▉       | 1474/5000 [6:42:31<16:19:36, 16.67s/it]                                                        {'loss': 23.4784, 'grad_norm': 44.0, 'learning_rate': 5.914413345381972e-05, 'epoch': 1.87}
 29%|██▉       | 1474/5000 [6:42:31<16:19:36, 16.67s/it] 30%|██▉       | 1475/5000 [6:42:48<16:32:56, 16.90s/it]                                                        {'loss': 19.5365, 'grad_norm': 16.625, 'learning_rate': 5.91273693191498e-05, 'epoch': 1.87}
 30%|██▉       | 1475/5000 [6:42:48<16:32:56, 16.90s/it] 30%|██▉       | 1476/5000 [6:43:01<15:29:50, 15.83s/it]                                                        {'loss': 21.6685, 'grad_norm': 22.0, 'learning_rate': 5.911059463034884e-05, 'epoch': 1.87}
 30%|██▉       | 1476/5000 [6:43:01<15:29:50, 15.83s/it] 30%|██▉       | 1477/5000 [6:43:18<15:40:31, 16.02s/it]                                                        {'loss': 20.4267, 'grad_norm': 17.25, 'learning_rate': 5.909380939475463e-05, 'epoch': 1.88}
 30%|██▉       | 1477/5000 [6:43:18<15:40:31, 16.02s/it] 30%|██▉       | 1478/5000 [6:43:30<14:26:45, 14.77s/it]                                                        {'loss': 23.4243, 'grad_norm': 41.0, 'learning_rate': 5.9077013619709646e-05, 'epoch': 1.88}
 30%|██▉       | 1478/5000 [6:43:30<14:26:45, 14.77s/it] 30%|██▉       | 1479/5000 [6:43:42<13:47:46, 14.11s/it]                                                        {'loss': 25.1967, 'grad_norm': 46.75, 'learning_rate': 5.9060207312560904e-05, 'epoch': 1.88}
 30%|██▉       | 1479/5000 [6:43:42<13:47:46, 14.11s/it] 30%|██▉       | 1480/5000 [6:44:01<15:08:41, 15.49s/it]                                                        {'loss': 21.1869, 'grad_norm': 45.5, 'learning_rate': 5.904339048066005e-05, 'epoch': 1.88}
 30%|██▉       | 1480/5000 [6:44:01<15:08:41, 15.49s/it] 30%|██▉       | 1481/5000 [6:44:21<16:23:38, 16.77s/it]                                                        {'loss': 21.4993, 'grad_norm': 24.0, 'learning_rate': 5.902656313136335e-05, 'epoch': 1.88}
 30%|██▉       | 1481/5000 [6:44:21<16:23:38, 16.77s/it] 30%|██▉       | 1482/5000 [6:44:36<15:59:56, 16.37s/it]                                                        {'loss': 21.6138, 'grad_norm': 45.0, 'learning_rate': 5.900972527203166e-05, 'epoch': 1.88}
 30%|██▉       | 1482/5000 [6:44:36<15:59:56, 16.37s/it] 30%|██▉       | 1483/5000 [6:44:48<14:43:03, 15.06s/it]                                                        {'loss': 23.735, 'grad_norm': 75.5, 'learning_rate': 5.899287691003042e-05, 'epoch': 1.88}
 30%|██▉       | 1483/5000 [6:44:48<14:43:03, 15.06s/it] 30%|██▉       | 1484/5000 [6:45:11<17:07:11, 17.53s/it]                                                        {'loss': 21.6175, 'grad_norm': 21.0, 'learning_rate': 5.897601805272969e-05, 'epoch': 1.88}
 30%|██▉       | 1484/5000 [6:45:11<17:07:11, 17.53s/it] 30%|██▉       | 1485/5000 [6:45:37<19:32:05, 20.01s/it]                                                        {'loss': 21.6992, 'grad_norm': 18.875, 'learning_rate': 5.89591487075041e-05, 'epoch': 1.89}
 30%|██▉       | 1485/5000 [6:45:37<19:32:05, 20.01s/it] 30%|██▉       | 1486/5000 [6:45:56<19:04:20, 19.54s/it]                                                        {'loss': 21.3464, 'grad_norm': 21.125, 'learning_rate': 5.894226888173288e-05, 'epoch': 1.89}
 30%|██▉       | 1486/5000 [6:45:56<19:04:20, 19.54s/it] 30%|██▉       | 1487/5000 [6:46:30<23:29:58, 24.08s/it]                                                        {'loss': 22.3068, 'grad_norm': 28.875, 'learning_rate': 5.892537858279981e-05, 'epoch': 1.89}
 30%|██▉       | 1487/5000 [6:46:30<23:29:58, 24.08s/it] 30%|██▉       | 1488/5000 [6:46:50<22:13:31, 22.78s/it]                                                        {'loss': 27.2843, 'grad_norm': 378.0, 'learning_rate': 5.890847781809331e-05, 'epoch': 1.89}
 30%|██▉       | 1488/5000 [6:46:50<22:13:31, 22.78s/it] 30%|██▉       | 1489/5000 [6:47:07<20:29:44, 21.02s/it]                                                        {'loss': 42.8771, 'grad_norm': 3872.0, 'learning_rate': 5.889156659500636e-05, 'epoch': 1.89}
 30%|██▉       | 1489/5000 [6:47:07<20:29:44, 21.02s/it] 30%|██▉       | 1490/5000 [6:47:19<17:57:06, 18.41s/it]                                                        {'loss': 21.5095, 'grad_norm': 50.75, 'learning_rate': 5.887464492093648e-05, 'epoch': 1.89}
 30%|██▉       | 1490/5000 [6:47:19<17:57:06, 18.41s/it] 30%|██▉       | 1491/5000 [6:47:44<19:49:31, 20.34s/it]                                                        {'loss': 20.9494, 'grad_norm': 19.0, 'learning_rate': 5.885771280328578e-05, 'epoch': 1.89}
 30%|██▉       | 1491/5000 [6:47:44<19:49:31, 20.34s/it] 30%|██▉       | 1492/5000 [6:47:58<17:49:43, 18.30s/it]                                                        {'loss': 22.4602, 'grad_norm': 36.5, 'learning_rate': 5.8840770249460966e-05, 'epoch': 1.89}
 30%|██▉       | 1492/5000 [6:47:58<17:49:43, 18.30s/it] 30%|██▉       | 1493/5000 [6:48:23<19:51:10, 20.38s/it]                                                        {'loss': 21.1962, 'grad_norm': 24.375, 'learning_rate': 5.8823817266873265e-05, 'epoch': 1.9}
 30%|██▉       | 1493/5000 [6:48:23<19:51:10, 20.38s/it] 30%|██▉       | 1494/5000 [6:48:37<17:53:45, 18.38s/it]                                                        {'loss': 22.6948, 'grad_norm': 38.75, 'learning_rate': 5.880685386293851e-05, 'epoch': 1.9}
 30%|██▉       | 1494/5000 [6:48:37<17:53:45, 18.38s/it] 30%|██▉       | 1495/5000 [6:48:53<17:17:29, 17.76s/it]                                                        {'loss': 20.5018, 'grad_norm': 18.125, 'learning_rate': 5.878988004507706e-05, 'epoch': 1.9}
 30%|██▉       | 1495/5000 [6:48:53<17:17:29, 17.76s/it] 30%|██▉       | 1496/5000 [6:49:06<16:02:23, 16.48s/it]                                                        {'loss': 20.9229, 'grad_norm': 278.0, 'learning_rate': 5.877289582071382e-05, 'epoch': 1.9}
 30%|██▉       | 1496/5000 [6:49:06<16:02:23, 16.48s/it] 30%|██▉       | 1497/5000 [6:49:24<16:13:08, 16.67s/it]                                                        {'loss': 20.3759, 'grad_norm': 30.875, 'learning_rate': 5.87559011972783e-05, 'epoch': 1.9}
 30%|██▉       | 1497/5000 [6:49:24<16:13:08, 16.67s/it] 30%|██▉       | 1498/5000 [6:49:48<18:31:20, 19.04s/it]                                                        {'loss': 20.5264, 'grad_norm': 17.125, 'learning_rate': 5.87388961822045e-05, 'epoch': 1.9}
 30%|██▉       | 1498/5000 [6:49:48<18:31:20, 19.04s/it] 30%|██▉       | 1499/5000 [6:50:02<16:58:21, 17.45s/it]                                                        {'loss': 26.2186, 'grad_norm': 56.5, 'learning_rate': 5.872188078293101e-05, 'epoch': 1.9}
 30%|██▉       | 1499/5000 [6:50:02<16:58:21, 17.45s/it] 30%|███       | 1500/5000 [6:50:17<16:23:16, 16.86s/it]                                                        {'loss': 31.9094, 'grad_norm': 110.0, 'learning_rate': 5.870485500690094e-05, 'epoch': 1.9}
 30%|███       | 1500/5000 [6:50:17<16:23:16, 16.86s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:18,  4.40s/it][A
  3%|▎         | 3/88 [00:16<08:05,  5.72s/it][A
  5%|▍         | 4/88 [00:20<07:20,  5.25s/it][A
  6%|▌         | 5/88 [00:24<06:40,  4.83s/it][A
  7%|▋         | 6/88 [00:30<06:54,  5.05s/it][A
  8%|▊         | 7/88 [00:34<06:19,  4.69s/it][A
  9%|▉         | 8/88 [00:38<05:54,  4.44s/it][A
 10%|█         | 9/88 [00:41<05:34,  4.23s/it][A
 11%|█▏        | 10/88 [00:44<04:58,  3.82s/it][A
 12%|█▎        | 11/88 [00:47<04:31,  3.52s/it][A
 14%|█▎        | 12/88 [00:49<03:58,  3.14s/it][A
 15%|█▍        | 13/88 [00:52<03:44,  2.99s/it][A
 16%|█▌        | 14/88 [00:58<04:38,  3.76s/it][A
 17%|█▋        | 15/88 [01:03<05:11,  4.27s/it][A
 18%|█▊        | 16/88 [01:06<04:46,  3.98s/it][A
 19%|█▉        | 17/88 [01:12<05:10,  4.38s/it][A
 20%|██        | 18/88 [01:15<04:39,  3.99s/it][A
 22%|██▏       | 19/88 [01:19<04:40,  4.06s/it][A
 23%|██▎       | 20/88 [01:23<04:30,  3.98s/it][A
 24%|██▍       | 21/88 [01:26<04:13,  3.78s/it][A
 25%|██▌       | 22/88 [01:30<04:09,  3.79s/it][A
 26%|██▌       | 23/88 [01:32<03:42,  3.42s/it][A
 27%|██▋       | 24/88 [01:41<05:19,  4.98s/it][A
 28%|██▊       | 25/88 [01:44<04:43,  4.50s/it][A
 30%|██▉       | 26/88 [01:51<05:14,  5.07s/it][A
 31%|███       | 27/88 [01:56<05:12,  5.12s/it][A
 32%|███▏      | 28/88 [02:05<06:12,  6.21s/it][A
 33%|███▎      | 29/88 [02:11<06:08,  6.25s/it][A
 34%|███▍      | 30/88 [02:17<05:51,  6.05s/it][A
 35%|███▌      | 31/88 [02:21<05:11,  5.46s/it][A
 36%|███▋      | 32/88 [02:30<06:04,  6.52s/it][A
 38%|███▊      | 33/88 [02:34<05:27,  5.95s/it][A
 39%|███▊      | 34/88 [02:43<06:07,  6.81s/it][A
 40%|███▉      | 35/88 [02:46<04:58,  5.64s/it][A
 41%|████      | 36/88 [02:51<04:45,  5.49s/it][A
 42%|████▏     | 37/88 [02:55<04:11,  4.93s/it][A
 43%|████▎     | 38/88 [02:59<03:54,  4.69s/it][A
 44%|████▍     | 39/88 [03:02<03:25,  4.20s/it][A
 45%|████▌     | 40/88 [03:09<03:56,  4.92s/it][A
 47%|████▋     | 41/88 [03:18<04:58,  6.36s/it][A
 48%|████▊     | 42/88 [03:22<04:12,  5.49s/it][A
 49%|████▉     | 43/88 [03:31<04:49,  6.43s/it][A
 50%|█████     | 44/88 [03:34<04:01,  5.48s/it][A
 51%|█████     | 45/88 [03:38<03:42,  5.17s/it][A
 52%|█████▏    | 46/88 [03:42<03:20,  4.78s/it][A
 53%|█████▎    | 47/88 [03:45<02:47,  4.09s/it][A
 55%|█████▍    | 48/88 [03:47<02:22,  3.56s/it][A
 56%|█████▌    | 49/88 [03:52<02:35,  3.98s/it][A
 57%|█████▋    | 50/88 [03:56<02:32,  4.01s/it][A
 58%|█████▊    | 51/88 [04:00<02:29,  4.04s/it][A
 59%|█████▉    | 52/88 [04:06<02:43,  4.55s/it][A
 60%|██████    | 53/88 [04:11<02:42,  4.65s/it][A
 61%|██████▏   | 54/88 [04:20<03:28,  6.12s/it][A
 62%|██████▎   | 55/88 [04:25<03:09,  5.75s/it][A
 64%|██████▎   | 56/88 [04:28<02:32,  4.77s/it][A
 65%|██████▍   | 57/88 [04:31<02:19,  4.49s/it][A
 66%|██████▌   | 58/88 [04:40<02:53,  5.78s/it][A
 67%|██████▋   | 59/88 [04:46<02:46,  5.75s/it][A
 68%|██████▊   | 60/88 [04:49<02:20,  5.02s/it][A
 69%|██████▉   | 61/88 [04:53<02:07,  4.71s/it][A
 70%|███████   | 62/88 [04:56<01:49,  4.23s/it][A
 72%|███████▏  | 63/88 [05:02<01:55,  4.61s/it][A
 73%|███████▎  | 64/88 [05:06<01:46,  4.42s/it][A
 74%|███████▍  | 65/88 [05:10<01:40,  4.39s/it][A
 75%|███████▌  | 66/88 [05:14<01:32,  4.18s/it][A
 76%|███████▌  | 67/88 [05:17<01:20,  3.83s/it][A
 77%|███████▋  | 68/88 [05:22<01:23,  4.20s/it][A
 78%|███████▊  | 69/88 [05:28<01:31,  4.83s/it][A
 80%|███████▉  | 70/88 [05:32<01:23,  4.65s/it][A
 81%|████████  | 71/88 [05:36<01:14,  4.38s/it][A
 82%|████████▏ | 72/88 [05:40<01:07,  4.23s/it][A
 83%|████████▎ | 73/88 [05:43<00:57,  3.86s/it][A
 84%|████████▍ | 74/88 [05:46<00:49,  3.53s/it][A
 85%|████████▌ | 75/88 [05:51<00:50,  3.89s/it][A
 86%|████████▋ | 76/88 [05:54<00:46,  3.85s/it][A
 88%|████████▊ | 77/88 [06:01<00:53,  4.82s/it][A
 89%|████████▊ | 78/88 [06:05<00:45,  4.56s/it][A
 90%|████████▉ | 79/88 [06:10<00:41,  4.60s/it][A
 91%|█████████ | 80/88 [06:13<00:32,  4.09s/it][A
 92%|█████████▏| 81/88 [06:18<00:30,  4.35s/it][A
 93%|█████████▎| 82/88 [06:22<00:25,  4.20s/it][A
 94%|█████████▍| 83/88 [06:26<00:20,  4.07s/it][A
 95%|█████████▌| 84/88 [06:29<00:15,  3.98s/it][A
 97%|█████████▋| 85/88 [06:32<00:10,  3.65s/it][A
 98%|█████████▊| 86/88 [06:35<00:06,  3.39s/it][A
 99%|█████████▉| 87/88 [06:39<00:03,  3.62s/it][A
100%|██████████| 88/88 [06:43<00:00,  3.68s/it][A                                                        
                                               [A{'eval_loss': 21.289512634277344, 'eval_runtime': 407.3351, 'eval_samples_per_second': 6.874, 'eval_steps_per_second': 0.216, 'epoch': 1.9}
 30%|███       | 1500/5000 [6:57:05<16:23:16, 16.86s/it]
100%|██████████| 88/88 [06:43<00:00,  3.68s/it][A
                                               [A2024-06-13 16:32:42,543 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-13 16:32:53,064 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 30%|███       | 1501/5000 [6:57:36<139:20:08, 143.36s/it]                                                          {'loss': 20.85, 'grad_norm': 161.0, 'learning_rate': 5.868781886156192e-05, 'epoch': 1.91}
 30%|███       | 1501/5000 [6:57:36<139:20:08, 143.36s/it] 30%|███       | 1502/5000 [6:57:50<101:44:55, 104.72s/it]                                                          {'loss': 19.9229, 'grad_norm': 46.5, 'learning_rate': 5.8670772354366165e-05, 'epoch': 1.91}
 30%|███       | 1502/5000 [6:57:50<101:44:55, 104.72s/it] 30%|███       | 1503/5000 [6:58:05<75:30:32, 77.73s/it]                                                          {'loss': 25.3045, 'grad_norm': 48.25, 'learning_rate': 5.86537154927704e-05, 'epoch': 1.91}
 30%|███       | 1503/5000 [6:58:05<75:30:32, 77.73s/it] 30%|███       | 1504/5000 [6:58:27<59:09:34, 60.92s/it]                                                        {'loss': 20.1442, 'grad_norm': 24.375, 'learning_rate': 5.863664828423585e-05, 'epoch': 1.91}
 30%|███       | 1504/5000 [6:58:27<59:09:34, 60.92s/it] 30%|███       | 1505/5000 [6:58:47<47:23:58, 48.82s/it]                                                        {'loss': 20.4808, 'grad_norm': 23.75, 'learning_rate': 5.8619570736228314e-05, 'epoch': 1.91}
 30%|███       | 1505/5000 [6:58:47<47:23:58, 48.82s/it] 30%|███       | 1506/5000 [6:59:05<38:14:53, 39.41s/it]                                                        {'loss': 20.2003, 'grad_norm': 16.875, 'learning_rate': 5.860248285621808e-05, 'epoch': 1.91}
 30%|███       | 1506/5000 [6:59:05<38:14:53, 39.41s/it] 30%|███       | 1507/5000 [6:59:22<31:36:35, 32.58s/it]                                                        {'loss': 21.6397, 'grad_norm': 32.5, 'learning_rate': 5.858538465167997e-05, 'epoch': 1.91}
 30%|███       | 1507/5000 [6:59:22<31:36:35, 32.58s/it] 30%|███       | 1508/5000 [6:59:41<27:39:35, 28.52s/it]                                                        {'loss': 21.2324, 'grad_norm': 29.75, 'learning_rate': 5.8568276130093324e-05, 'epoch': 1.91}
 30%|███       | 1508/5000 [6:59:41<27:39:35, 28.52s/it] 30%|███       | 1509/5000 [6:59:59<24:46:39, 25.55s/it]                                                        {'loss': 20.6856, 'grad_norm': 22.75, 'learning_rate': 5.855115729894198e-05, 'epoch': 1.92}
 30%|███       | 1509/5000 [6:59:59<24:46:39, 25.55s/it] 30%|███       | 1510/5000 [7:00:14<21:35:29, 22.27s/it]                                                        {'loss': 21.1557, 'grad_norm': 25.125, 'learning_rate': 5.853402816571431e-05, 'epoch': 1.92}
 30%|███       | 1510/5000 [7:00:14<21:35:29, 22.27s/it] 30%|███       | 1511/5000 [7:00:29<19:39:09, 20.28s/it]                                                        {'loss': 22.7611, 'grad_norm': 33.25, 'learning_rate': 5.851688873790316e-05, 'epoch': 1.92}
 30%|███       | 1511/5000 [7:00:29<19:39:09, 20.28s/it] 30%|███       | 1512/5000 [7:00:44<17:55:06, 18.49s/it]                                                        {'loss': 25.7786, 'grad_norm': 99.0, 'learning_rate': 5.849973902300593e-05, 'epoch': 1.92}
 30%|███       | 1512/5000 [7:00:44<17:55:06, 18.49s/it] 30%|███       | 1513/5000 [7:00:56<16:07:39, 16.65s/it]                                                        {'loss': 21.9325, 'grad_norm': 45.75, 'learning_rate': 5.8482579028524455e-05, 'epoch': 1.92}
 30%|███       | 1513/5000 [7:00:56<16:07:39, 16.65s/it] 30%|███       | 1514/5000 [7:01:19<17:51:43, 18.45s/it]                                                        {'loss': 19.9146, 'grad_norm': 28.125, 'learning_rate': 5.846540876196512e-05, 'epoch': 1.92}
 30%|███       | 1514/5000 [7:01:19<17:51:43, 18.45s/it] 30%|███       | 1515/5000 [7:01:37<17:53:31, 18.48s/it]                                                        {'loss': 21.0918, 'grad_norm': 36.25, 'learning_rate': 5.844822823083877e-05, 'epoch': 1.92}
 30%|███       | 1515/5000 [7:01:37<17:53:31, 18.48s/it] 30%|███       | 1516/5000 [7:01:53<17:02:55, 17.62s/it]                                                        {'loss': 20.1543, 'grad_norm': 22.75, 'learning_rate': 5.843103744266076e-05, 'epoch': 1.93}
 30%|███       | 1516/5000 [7:01:53<17:02:55, 17.62s/it] 30%|███       | 1517/5000 [7:02:09<16:32:13, 17.09s/it]                                                        {'loss': 21.0652, 'grad_norm': 23.5, 'learning_rate': 5.841383640495092e-05, 'epoch': 1.93}
 30%|███       | 1517/5000 [7:02:09<16:32:13, 17.09s/it] 30%|███       | 1518/5000 [7:02:28<16:59:22, 17.57s/it]                                                        {'loss': 20.9692, 'grad_norm': 15.75, 'learning_rate': 5.839662512523358e-05, 'epoch': 1.93}
 30%|███       | 1518/5000 [7:02:28<16:59:22, 17.57s/it] 30%|███       | 1519/5000 [7:02:40<15:35:39, 16.13s/it]                                                        {'loss': 20.8698, 'grad_norm': 25.125, 'learning_rate': 5.837940361103753e-05, 'epoch': 1.93}
 30%|███       | 1519/5000 [7:02:40<15:35:39, 16.13s/it] 30%|███       | 1520/5000 [7:02:56<15:36:28, 16.15s/it]                                                        {'loss': 21.4509, 'grad_norm': 103.5, 'learning_rate': 5.8362171869896036e-05, 'epoch': 1.93}
 30%|███       | 1520/5000 [7:02:56<15:36:28, 16.15s/it] 30%|███       | 1521/5000 [7:03:10<14:42:59, 15.23s/it]                                                        {'loss': 22.7557, 'grad_norm': 43.75, 'learning_rate': 5.834492990934686e-05, 'epoch': 1.93}
 30%|███       | 1521/5000 [7:03:10<14:42:59, 15.23s/it] 30%|███       | 1522/5000 [7:03:26<14:56:27, 15.47s/it]                                                        {'loss': 21.17, 'grad_norm': 28.375, 'learning_rate': 5.832767773693223e-05, 'epoch': 1.93}
 30%|███       | 1522/5000 [7:03:26<14:56:27, 15.47s/it] 30%|███       | 1523/5000 [7:03:49<17:22:13, 17.98s/it]                                                        {'loss': 21.0939, 'grad_norm': 27.125, 'learning_rate': 5.83104153601988e-05, 'epoch': 1.93}
 30%|███       | 1523/5000 [7:03:49<17:22:13, 17.98s/it] 30%|███       | 1524/5000 [7:04:05<16:34:57, 17.17s/it]                                                        {'loss': 21.2804, 'grad_norm': 25.375, 'learning_rate': 5.829314278669775e-05, 'epoch': 1.94}
 30%|███       | 1524/5000 [7:04:05<16:34:57, 17.17s/it] 30%|███       | 1525/5000 [7:04:20<16:07:49, 16.71s/it]                                                        {'loss': 21.1768, 'grad_norm': 17.5, 'learning_rate': 5.827586002398468e-05, 'epoch': 1.94}
 30%|███       | 1525/5000 [7:04:20<16:07:49, 16.71s/it] 31%|███       | 1526/5000 [7:04:34<15:14:28, 15.79s/it]                                                        {'loss': 20.7586, 'grad_norm': 67.5, 'learning_rate': 5.8258567079619655e-05, 'epoch': 1.94}
 31%|███       | 1526/5000 [7:04:34<15:14:28, 15.79s/it] 31%|███       | 1527/5000 [7:04:57<17:22:10, 18.00s/it]                                                        {'loss': 22.9683, 'grad_norm': 696.0, 'learning_rate': 5.8241263961167215e-05, 'epoch': 1.94}
 31%|███       | 1527/5000 [7:04:57<17:22:10, 18.00s/it] 31%|███       | 1528/5000 [7:05:22<19:13:35, 19.94s/it]                                                        {'loss': 20.2837, 'grad_norm': 29.25, 'learning_rate': 5.82239506761963e-05, 'epoch': 1.94}
 31%|███       | 1528/5000 [7:05:22<19:13:35, 19.94s/it] 31%|███       | 1529/5000 [7:05:37<17:47:08, 18.45s/it]                                                        {'loss': 21.1958, 'grad_norm': 32.25, 'learning_rate': 5.820662723228036e-05, 'epoch': 1.94}
 31%|███       | 1529/5000 [7:05:37<17:47:08, 18.45s/it] 31%|███       | 1530/5000 [7:05:51<16:35:14, 17.21s/it]                                                        {'loss': 141.4576, 'grad_norm': 75776.0, 'learning_rate': 5.818929363699723e-05, 'epoch': 1.94}
 31%|███       | 1530/5000 [7:05:51<16:35:14, 17.21s/it] 31%|███       | 1531/5000 [7:06:07<16:12:12, 16.82s/it]                                                        {'loss': 23.9292, 'grad_norm': 36.25, 'learning_rate': 5.817194989792924e-05, 'epoch': 1.94}
 31%|███       | 1531/5000 [7:06:07<16:12:12, 16.82s/it] 31%|███       | 1532/5000 [7:06:22<15:35:43, 16.19s/it]                                                        {'loss': 20.7515, 'grad_norm': 41.75, 'learning_rate': 5.81545960226631e-05, 'epoch': 1.95}
 31%|███       | 1532/5000 [7:06:22<15:35:43, 16.19s/it] 31%|███       | 1533/5000 [7:06:39<15:52:32, 16.48s/it]                                                        {'loss': 20.814, 'grad_norm': 27.25, 'learning_rate': 5.813723201879002e-05, 'epoch': 1.95}
 31%|███       | 1533/5000 [7:06:39<15:52:32, 16.48s/it] 31%|███       | 1534/5000 [7:07:05<18:40:28, 19.40s/it]                                                        {'loss': 21.2449, 'grad_norm': 17.625, 'learning_rate': 5.811985789390558e-05, 'epoch': 1.95}
 31%|███       | 1534/5000 [7:07:05<18:40:28, 19.40s/it] 31%|███       | 1535/5000 [7:07:20<17:18:37, 17.98s/it]                                                        {'loss': 21.3356, 'grad_norm': 19.0, 'learning_rate': 5.810247365560983e-05, 'epoch': 1.95}
 31%|███       | 1535/5000 [7:07:20<17:18:37, 17.98s/it] 31%|███       | 1536/5000 [7:07:44<19:10:10, 19.92s/it]                                                        {'loss': 21.5731, 'grad_norm': 199.0, 'learning_rate': 5.80850793115072e-05, 'epoch': 1.95}
 31%|███       | 1536/5000 [7:07:44<19:10:10, 19.92s/it] 31%|███       | 1537/5000 [7:08:05<19:30:00, 20.27s/it]                                                        {'loss': 21.3342, 'grad_norm': 43.0, 'learning_rate': 5.8067674869206584e-05, 'epoch': 1.95}
 31%|███       | 1537/5000 [7:08:05<19:30:00, 20.27s/it] 31%|███       | 1538/5000 [7:08:18<17:16:09, 17.96s/it]                                                        {'loss': 21.8382, 'grad_norm': 23.125, 'learning_rate': 5.805026033632129e-05, 'epoch': 1.95}
 31%|███       | 1538/5000 [7:08:18<17:16:09, 17.96s/it] 31%|███       | 1539/5000 [7:08:33<16:37:07, 17.29s/it]                                                        {'loss': 19.9662, 'grad_norm': 22.125, 'learning_rate': 5.8032835720469e-05, 'epoch': 1.95}
 31%|███       | 1539/5000 [7:08:33<16:37:07, 17.29s/it] 31%|███       | 1540/5000 [7:08:55<17:54:33, 18.63s/it]                                                        {'loss': 23.4114, 'grad_norm': 352.0, 'learning_rate': 5.8015401029271856e-05, 'epoch': 1.96}
 31%|███       | 1540/5000 [7:08:55<17:54:33, 18.63s/it] 31%|███       | 1541/5000 [7:09:10<16:52:26, 17.56s/it]                                                        {'loss': 21.9429, 'grad_norm': 66.5, 'learning_rate': 5.7997956270356365e-05, 'epoch': 1.96}
 31%|███       | 1541/5000 [7:09:10<16:52:26, 17.56s/it] 31%|███       | 1542/5000 [7:09:24<15:40:13, 16.31s/it]                                                        {'loss': 23.1036, 'grad_norm': 98.5, 'learning_rate': 5.798050145135347e-05, 'epoch': 1.96}
 31%|███       | 1542/5000 [7:09:24<15:40:13, 16.31s/it] 31%|███       | 1543/5000 [7:09:36<14:32:23, 15.14s/it]                                                        {'loss': 22.1624, 'grad_norm': 34.25, 'learning_rate': 5.7963036579898494e-05, 'epoch': 1.96}
 31%|███       | 1543/5000 [7:09:36<14:32:23, 15.14s/it] 31%|███       | 1544/5000 [7:09:49<13:48:37, 14.39s/it]                                                        {'loss': 22.6681, 'grad_norm': 21.875, 'learning_rate': 5.794556166363118e-05, 'epoch': 1.96}
 31%|███       | 1544/5000 [7:09:49<13:48:37, 14.39s/it] 31%|███       | 1545/5000 [7:10:02<13:33:24, 14.13s/it]                                                        {'loss': 20.2, 'grad_norm': 42.0, 'learning_rate': 5.792807671019562e-05, 'epoch': 1.96}
 31%|███       | 1545/5000 [7:10:02<13:33:24, 14.13s/it] 31%|███       | 1546/5000 [7:10:19<14:11:49, 14.80s/it]                                                        {'loss': 21.5397, 'grad_norm': 25.875, 'learning_rate': 5.791058172724036e-05, 'epoch': 1.96}
 31%|███       | 1546/5000 [7:10:19<14:11:49, 14.80s/it] 31%|███       | 1547/5000 [7:10:41<16:29:20, 17.19s/it]                                                        {'loss': 23.1864, 'grad_norm': 34.75, 'learning_rate': 5.7893076722418284e-05, 'epoch': 1.96}
 31%|███       | 1547/5000 [7:10:41<16:29:20, 17.19s/it] 31%|███       | 1548/5000 [7:10:56<15:53:43, 16.58s/it]                                                        {'loss': 26.7554, 'grad_norm': 384.0, 'learning_rate': 5.787556170338667e-05, 'epoch': 1.97}
 31%|███       | 1548/5000 [7:10:56<15:53:43, 16.58s/it] 31%|███       | 1549/5000 [7:11:11<15:16:35, 15.94s/it]                                                        {'loss': 21.8192, 'grad_norm': 96.0, 'learning_rate': 5.7858036677807195e-05, 'epoch': 1.97}
 31%|███       | 1549/5000 [7:11:11<15:16:35, 15.94s/it] 31%|███       | 1550/5000 [7:11:27<15:11:48, 15.86s/it]                                                        {'loss': 23.7579, 'grad_norm': 78.0, 'learning_rate': 5.784050165334589e-05, 'epoch': 1.97}
 31%|███       | 1550/5000 [7:11:27<15:11:48, 15.86s/it] 31%|███       | 1551/5000 [7:11:43<15:19:57, 16.00s/it]                                                        {'loss': 23.5841, 'grad_norm': 55.5, 'learning_rate': 5.7822956637673194e-05, 'epoch': 1.97}
 31%|███       | 1551/5000 [7:11:43<15:19:57, 16.00s/it] 31%|███       | 1552/5000 [7:11:58<14:57:04, 15.61s/it]                                                        {'loss': 21.6509, 'grad_norm': 75.5, 'learning_rate': 5.7805401638463854e-05, 'epoch': 1.97}
 31%|███       | 1552/5000 [7:11:58<14:57:04, 15.61s/it] 31%|███       | 1553/5000 [7:12:14<15:17:50, 15.98s/it]                                                        {'loss': 21.3239, 'grad_norm': 27.625, 'learning_rate': 5.778783666339705e-05, 'epoch': 1.97}
 31%|███       | 1553/5000 [7:12:14<15:17:50, 15.98s/it] 31%|███       | 1554/5000 [7:12:27<14:14:04, 14.87s/it]                                                        {'loss': 24.9918, 'grad_norm': 60.25, 'learning_rate': 5.7770261720156294e-05, 'epoch': 1.97}
 31%|███       | 1554/5000 [7:12:27<14:14:04, 14.87s/it] 31%|███       | 1555/5000 [7:12:40<13:41:53, 14.31s/it]                                                        {'loss': 20.5396, 'grad_norm': 33.5, 'learning_rate': 5.775267681642945e-05, 'epoch': 1.97}
 31%|███       | 1555/5000 [7:12:40<13:41:53, 14.31s/it] 31%|███       | 1556/5000 [7:12:53<13:16:49, 13.88s/it]                                                        {'loss': 73.4674, 'grad_norm': 728.0, 'learning_rate': 5.773508195990878e-05, 'epoch': 1.98}
 31%|███       | 1556/5000 [7:12:53<13:16:49, 13.88s/it] 31%|███       | 1557/5000 [7:13:08<13:34:06, 14.19s/it]                                                        {'loss': 21.1993, 'grad_norm': 23.375, 'learning_rate': 5.7717477158290855e-05, 'epoch': 1.98}
 31%|███       | 1557/5000 [7:13:08<13:34:06, 14.19s/it] 31%|███       | 1558/5000 [7:13:20<12:56:14, 13.53s/it]                                                        {'loss': 48.1754, 'grad_norm': 1144.0, 'learning_rate': 5.76998624192766e-05, 'epoch': 1.98}
 31%|███       | 1558/5000 [7:13:20<12:56:14, 13.53s/it] 31%|███       | 1559/5000 [7:13:35<13:31:08, 14.14s/it]                                                        {'loss': 35.5915, 'grad_norm': 948.0, 'learning_rate': 5.7682237750571334e-05, 'epoch': 1.98}
 31%|███       | 1559/5000 [7:13:35<13:31:08, 14.14s/it] 31%|███       | 1560/5000 [7:13:54<14:56:54, 15.64s/it]                                                        {'loss': 19.7141, 'grad_norm': 33.0, 'learning_rate': 5.766460315988465e-05, 'epoch': 1.98}
 31%|███       | 1560/5000 [7:13:54<14:56:54, 15.64s/it] 31%|███       | 1561/5000 [7:14:06<13:48:10, 14.45s/it]                                                        {'loss': 25.3335, 'grad_norm': 1504.0, 'learning_rate': 5.764695865493055e-05, 'epoch': 1.98}
 31%|███       | 1561/5000 [7:14:06<13:48:10, 14.45s/it] 31%|███       | 1562/5000 [7:14:26<15:26:24, 16.17s/it]                                                        {'loss': 23.7215, 'grad_norm': 184.0, 'learning_rate': 5.762930424342732e-05, 'epoch': 1.98}
 31%|███       | 1562/5000 [7:14:26<15:26:24, 16.17s/it] 31%|███▏      | 1563/5000 [7:14:42<15:28:25, 16.21s/it]                                                        {'loss': 26.4848, 'grad_norm': 1352.0, 'learning_rate': 5.761163993309759e-05, 'epoch': 1.98}
 31%|███▏      | 1563/5000 [7:14:42<15:28:25, 16.21s/it] 31%|███▏      | 1564/5000 [7:14:54<14:17:14, 14.97s/it]                                                        {'loss': 22.5069, 'grad_norm': 49.25, 'learning_rate': 5.7593965731668356e-05, 'epoch': 1.99}
 31%|███▏      | 1564/5000 [7:14:54<14:17:14, 14.97s/it] 31%|███▏      | 1565/5000 [7:15:09<14:10:47, 14.86s/it]                                                        {'loss': 22.2175, 'grad_norm': 31.375, 'learning_rate': 5.757628164687089e-05, 'epoch': 1.99}
 31%|███▏      | 1565/5000 [7:15:09<14:10:47, 14.86s/it] 31%|███▏      | 1566/5000 [7:15:25<14:28:02, 15.17s/it]                                                        {'loss': 21.2417, 'grad_norm': 25.625, 'learning_rate': 5.7558587686440826e-05, 'epoch': 1.99}
 31%|███▏      | 1566/5000 [7:15:25<14:28:02, 15.17s/it] 31%|███▏      | 1567/5000 [7:15:40<14:24:58, 15.12s/it]                                                        {'loss': 20.8151, 'grad_norm': 25.25, 'learning_rate': 5.7540883858118105e-05, 'epoch': 1.99}
 31%|███▏      | 1567/5000 [7:15:40<14:24:58, 15.12s/it] 31%|███▏      | 1568/5000 [7:16:12<19:10:07, 20.11s/it]                                                        {'loss': 20.4227, 'grad_norm': 98.5, 'learning_rate': 5.752317016964696e-05, 'epoch': 1.99}
 31%|███▏      | 1568/5000 [7:16:12<19:10:07, 20.11s/it] 31%|███▏      | 1569/5000 [7:16:34<19:43:52, 20.70s/it]                                                        {'loss': 20.2556, 'grad_norm': 21.25, 'learning_rate': 5.7505446628775995e-05, 'epoch': 1.99}
 31%|███▏      | 1569/5000 [7:16:34<19:43:52, 20.70s/it] 31%|███▏      | 1570/5000 [7:16:47<17:31:49, 18.40s/it]                                                        {'loss': 25.4094, 'grad_norm': 78.5, 'learning_rate': 5.7487713243258075e-05, 'epoch': 1.99}
 31%|███▏      | 1570/5000 [7:16:47<17:31:49, 18.40s/it] 31%|███▏      | 1571/5000 [7:17:04<17:07:28, 17.98s/it]                                                        {'loss': 28.6026, 'grad_norm': 117.5, 'learning_rate': 5.746997002085037e-05, 'epoch': 1.99}
 31%|███▏      | 1571/5000 [7:17:04<17:07:28, 17.98s/it] 31%|███▏      | 1572/5000 [7:17:20<16:39:26, 17.49s/it]                                                        {'loss': 21.9947, 'grad_norm': 36.25, 'learning_rate': 5.745221696931438e-05, 'epoch': 2.0}
 31%|███▏      | 1572/5000 [7:17:20<16:39:26, 17.49s/it] 31%|███▏      | 1573/5000 [7:17:32<15:02:57, 15.81s/it]                                                        {'loss': 20.6896, 'grad_norm': 23.25, 'learning_rate': 5.7434454096415905e-05, 'epoch': 2.0}
 31%|███▏      | 1573/5000 [7:17:32<15:02:57, 15.81s/it] 31%|███▏      | 1574/5000 [7:17:46<14:38:12, 15.38s/it]                                                        {'loss': 20.9889, 'grad_norm': 33.25, 'learning_rate': 5.741668140992501e-05, 'epoch': 2.0}
 31%|███▏      | 1574/5000 [7:17:46<14:38:12, 15.38s/it] 32%|███▏      | 1575/5000 [7:17:59<13:54:01, 14.61s/it]                                                        {'loss': 20.0876, 'grad_norm': 18.875, 'learning_rate': 5.739889891761608e-05, 'epoch': 2.0}
 32%|███▏      | 1575/5000 [7:17:59<13:54:01, 14.61s/it] 32%|███▏      | 1576/5000 [7:18:24<16:42:01, 17.56s/it]                                                        {'loss': 20.6961, 'grad_norm': 42.25, 'learning_rate': 5.738110662726777e-05, 'epoch': 2.0}
 32%|███▏      | 1576/5000 [7:18:24<16:42:01, 17.56s/it] 32%|███▏      | 1577/5000 [7:18:38<15:43:01, 16.53s/it]                                                        {'loss': 41.7046, 'grad_norm': 1056.0, 'learning_rate': 5.7363304546663054e-05, 'epoch': 2.0}
 32%|███▏      | 1577/5000 [7:18:38<15:43:01, 16.53s/it] 32%|███▏      | 1578/5000 [7:18:51<14:49:17, 15.59s/it]                                                        {'loss': 20.8326, 'grad_norm': 28.375, 'learning_rate': 5.734549268358915e-05, 'epoch': 2.0}
 32%|███▏      | 1578/5000 [7:18:51<14:49:17, 15.59s/it] 32%|███▏      | 1579/5000 [7:19:09<15:26:32, 16.25s/it]                                                        {'loss': 20.0188, 'grad_norm': 25.75, 'learning_rate': 5.7327671045837585e-05, 'epoch': 2.01}
 32%|███▏      | 1579/5000 [7:19:09<15:26:32, 16.25s/it] 32%|███▏      | 1580/5000 [7:19:25<15:27:42, 16.28s/it]                                                        {'loss': 22.6204, 'grad_norm': 134.0, 'learning_rate': 5.7309839641204136e-05, 'epoch': 2.01}
 32%|███▏      | 1580/5000 [7:19:25<15:27:42, 16.28s/it] 32%|███▏      | 1581/5000 [7:19:40<14:51:28, 15.64s/it]                                                        {'loss': 21.6432, 'grad_norm': 27.0, 'learning_rate': 5.729199847748887e-05, 'epoch': 2.01}
 32%|███▏      | 1581/5000 [7:19:40<14:51:28, 15.64s/it] 32%|███▏      | 1582/5000 [7:20:05<17:45:53, 18.71s/it]                                                        {'loss': 20.7608, 'grad_norm': 19.75, 'learning_rate': 5.7274147562496125e-05, 'epoch': 2.01}
 32%|███▏      | 1582/5000 [7:20:05<17:45:53, 18.71s/it] 32%|███▏      | 1583/5000 [7:20:24<17:40:15, 18.62s/it]                                                        {'loss': 21.3726, 'grad_norm': 23.75, 'learning_rate': 5.725628690403448e-05, 'epoch': 2.01}
 32%|███▏      | 1583/5000 [7:20:24<17:40:15, 18.62s/it] 32%|███▏      | 1584/5000 [7:20:38<16:28:40, 17.37s/it]                                                        {'loss': 21.6084, 'grad_norm': 59.0, 'learning_rate': 5.723841650991682e-05, 'epoch': 2.01}
 32%|███▏      | 1584/5000 [7:20:38<16:28:40, 17.37s/it] 32%|███▏      | 1585/5000 [7:21:02<18:19:33, 19.32s/it]                                                        {'loss': 20.3949, 'grad_norm': 19.0, 'learning_rate': 5.722053638796023e-05, 'epoch': 2.01}
 32%|███▏      | 1585/5000 [7:21:02<18:19:33, 19.32s/it] 32%|███▏      | 1586/5000 [7:21:24<18:57:58, 20.00s/it]                                                        {'loss': 21.8273, 'grad_norm': 45.5, 'learning_rate': 5.7202646545986114e-05, 'epoch': 2.01}
 32%|███▏      | 1586/5000 [7:21:24<18:57:58, 20.00s/it] 32%|███▏      | 1587/5000 [7:21:38<17:21:26, 18.31s/it]                                                        {'loss': 20.6242, 'grad_norm': 20.25, 'learning_rate': 5.718474699182008e-05, 'epoch': 2.02}
 32%|███▏      | 1587/5000 [7:21:38<17:21:26, 18.31s/it] 32%|███▏      | 1588/5000 [7:21:56<17:07:27, 18.07s/it]                                                        {'loss': 20.6112, 'grad_norm': 49.25, 'learning_rate': 5.7166837733292e-05, 'epoch': 2.02}
 32%|███▏      | 1588/5000 [7:21:56<17:07:27, 18.07s/it] 32%|███▏      | 1589/5000 [7:22:09<15:43:11, 16.59s/it]                                                        {'loss': 21.0896, 'grad_norm': 63.5, 'learning_rate': 5.7148918778236e-05, 'epoch': 2.02}
 32%|███▏      | 1589/5000 [7:22:09<15:43:11, 16.59s/it] 32%|███▏      | 1590/5000 [7:22:30<17:07:07, 18.07s/it]                                                        {'loss': 21.3751, 'grad_norm': 53.5, 'learning_rate': 5.713099013449044e-05, 'epoch': 2.02}
 32%|███▏      | 1590/5000 [7:22:30<17:07:07, 18.07s/it] 32%|███▏      | 1591/5000 [7:22:45<16:05:18, 16.99s/it]                                                        {'loss': 22.077, 'grad_norm': 23.0, 'learning_rate': 5.711305180989791e-05, 'epoch': 2.02}
 32%|███▏      | 1591/5000 [7:22:45<16:05:18, 16.99s/it] 32%|███▏      | 1592/5000 [7:23:00<15:33:03, 16.43s/it]                                                        {'loss': 21.9048, 'grad_norm': 22.5, 'learning_rate': 5.709510381230524e-05, 'epoch': 2.02}
 32%|███▏      | 1592/5000 [7:23:00<15:33:03, 16.43s/it] 32%|███▏      | 1593/5000 [7:23:20<16:30:08, 17.44s/it]                                                        {'loss': 23.349, 'grad_norm': 20.75, 'learning_rate': 5.707714614956351e-05, 'epoch': 2.02}
 32%|███▏      | 1593/5000 [7:23:20<16:30:08, 17.44s/it] 32%|███▏      | 1594/5000 [7:23:36<16:17:29, 17.22s/it]                                                        {'loss': 22.197, 'grad_norm': 436.0, 'learning_rate': 5.7059178829528e-05, 'epoch': 2.02}
 32%|███▏      | 1594/5000 [7:23:36<16:17:29, 17.22s/it] 32%|███▏      | 1595/5000 [7:23:50<15:19:37, 16.20s/it]                                                        {'loss': 21.1522, 'grad_norm': 228.0, 'learning_rate': 5.7041201860058235e-05, 'epoch': 2.03}
 32%|███▏      | 1595/5000 [7:23:50<15:19:37, 16.20s/it] 32%|███▏      | 1596/5000 [7:24:04<14:34:09, 15.41s/it]                                                        {'loss': 20.3294, 'grad_norm': 29.5, 'learning_rate': 5.702321524901793e-05, 'epoch': 2.03}
 32%|███▏      | 1596/5000 [7:24:04<14:34:09, 15.41s/it] 32%|███▏      | 1597/5000 [7:24:25<16:17:24, 17.23s/it]                                                        {'loss': 22.4097, 'grad_norm': 380.0, 'learning_rate': 5.700521900427506e-05, 'epoch': 2.03}
 32%|███▏      | 1597/5000 [7:24:25<16:17:24, 17.23s/it] 32%|███▏      | 1598/5000 [7:24:51<18:39:46, 19.75s/it]                                                        {'loss': 22.6216, 'grad_norm': 24.25, 'learning_rate': 5.6987213133701804e-05, 'epoch': 2.03}
 32%|███▏      | 1598/5000 [7:24:51<18:39:46, 19.75s/it] 32%|███▏      | 1599/5000 [7:25:08<17:58:13, 19.02s/it]                                                        {'loss': 20.7353, 'grad_norm': 23.625, 'learning_rate': 5.696919764517451e-05, 'epoch': 2.03}
 32%|███▏      | 1599/5000 [7:25:08<17:58:13, 19.02s/it] 32%|███▏      | 1600/5000 [7:25:27<17:51:28, 18.91s/it]                                                        {'loss': 21.5022, 'grad_norm': 26.0, 'learning_rate': 5.6951172546573794e-05, 'epoch': 2.03}
 32%|███▏      | 1600/5000 [7:25:27<17:51:28, 18.91s/it] 32%|███▏      | 1601/5000 [7:25:39<16:01:07, 16.97s/it]                                                        {'loss': 23.683, 'grad_norm': 27.875, 'learning_rate': 5.693313784578443e-05, 'epoch': 2.03}
 32%|███▏      | 1601/5000 [7:25:39<16:01:07, 16.97s/it] 32%|███▏      | 1602/5000 [7:25:56<16:00:59, 16.97s/it]                                                        {'loss': 20.2504, 'grad_norm': 34.0, 'learning_rate': 5.6915093550695415e-05, 'epoch': 2.03}
 32%|███▏      | 1602/5000 [7:25:56<16:00:59, 16.97s/it] 32%|███▏      | 1603/5000 [7:26:18<17:15:31, 18.29s/it]                                                        {'loss': 20.9789, 'grad_norm': 24.375, 'learning_rate': 5.689703966919992e-05, 'epoch': 2.04}
 32%|███▏      | 1603/5000 [7:26:18<17:15:31, 18.29s/it] 32%|███▏      | 1604/5000 [7:26:31<16:00:01, 16.96s/it]                                                        {'loss': 21.4291, 'grad_norm': 164.0, 'learning_rate': 5.687897620919535e-05, 'epoch': 2.04}
 32%|███▏      | 1604/5000 [7:26:31<16:00:01, 16.96s/it] 32%|███▏      | 1605/5000 [7:26:46<15:21:58, 16.29s/it]                                                        {'loss': 21.7683, 'grad_norm': 24.5, 'learning_rate': 5.6860903178583274e-05, 'epoch': 2.04}
 32%|███▏      | 1605/5000 [7:26:46<15:21:58, 16.29s/it] 32%|███▏      | 1606/5000 [7:27:12<18:02:02, 19.13s/it]                                                        {'loss': 19.4204, 'grad_norm': 13.75, 'learning_rate': 5.684282058526944e-05, 'epoch': 2.04}
 32%|███▏      | 1606/5000 [7:27:12<18:02:02, 19.13s/it] 32%|███▏      | 1607/5000 [7:27:24<16:06:11, 17.09s/it]                                                        {'loss': 22.0515, 'grad_norm': 26.75, 'learning_rate': 5.6824728437163795e-05, 'epoch': 2.04}
 32%|███▏      | 1607/5000 [7:27:24<16:06:11, 17.09s/it] 32%|███▏      | 1608/5000 [7:27:39<15:19:38, 16.27s/it]                                                        {'loss': 20.2448, 'grad_norm': 15.4375, 'learning_rate': 5.680662674218044e-05, 'epoch': 2.04}
 32%|███▏      | 1608/5000 [7:27:39<15:19:38, 16.27s/it] 32%|███▏      | 1609/5000 [7:28:00<16:43:30, 17.76s/it]                                                        {'loss': 29.0413, 'grad_norm': 95.5, 'learning_rate': 5.678851550823769e-05, 'epoch': 2.04}
 32%|███▏      | 1609/5000 [7:28:00<16:43:30, 17.76s/it] 32%|███▏      | 1610/5000 [7:28:16<16:20:10, 17.35s/it]                                                        {'loss': 19.7853, 'grad_norm': 15.4375, 'learning_rate': 5.677039474325802e-05, 'epoch': 2.04}
 32%|███▏      | 1610/5000 [7:28:16<16:20:10, 17.35s/it] 32%|███▏      | 1611/5000 [7:28:31<15:34:46, 16.55s/it]                                                        {'loss': 20.0179, 'grad_norm': 16.875, 'learning_rate': 5.675226445516806e-05, 'epoch': 2.05}
 32%|███▏      | 1611/5000 [7:28:31<15:34:46, 16.55s/it] 32%|███▏      | 1612/5000 [7:28:44<14:40:42, 15.60s/it]                                                        {'loss': 21.4405, 'grad_norm': 88.5, 'learning_rate': 5.673412465189861e-05, 'epoch': 2.05}
 32%|███▏      | 1612/5000 [7:28:44<14:40:42, 15.60s/it] 32%|███▏      | 1613/5000 [7:29:00<14:43:12, 15.65s/it]                                                        {'loss': 22.1843, 'grad_norm': 46.25, 'learning_rate': 5.671597534138464e-05, 'epoch': 2.05}
 32%|███▏      | 1613/5000 [7:29:00<14:43:12, 15.65s/it] 32%|███▏      | 1614/5000 [7:29:17<15:09:02, 16.11s/it]                                                        {'loss': 22.1728, 'grad_norm': 28.125, 'learning_rate': 5.6697816531565265e-05, 'epoch': 2.05}
 32%|███▏      | 1614/5000 [7:29:17<15:09:02, 16.11s/it] 32%|███▏      | 1615/5000 [7:29:35<15:31:38, 16.51s/it]                                                        {'loss': 18.5752, 'grad_norm': 78.5, 'learning_rate': 5.667964823038378e-05, 'epoch': 2.05}
 32%|███▏      | 1615/5000 [7:29:35<15:31:38, 16.51s/it] 32%|███▏      | 1616/5000 [7:29:50<15:07:22, 16.09s/it]                                                        {'loss': 19.598, 'grad_norm': 30.625, 'learning_rate': 5.666147044578761e-05, 'epoch': 2.05}
 32%|███▏      | 1616/5000 [7:29:50<15:07:22, 16.09s/it] 32%|███▏      | 1617/5000 [7:30:04<14:40:09, 15.61s/it]                                                        {'loss': 20.9326, 'grad_norm': 18.875, 'learning_rate': 5.6643283185728346e-05, 'epoch': 2.05}
 32%|███▏      | 1617/5000 [7:30:04<14:40:09, 15.61s/it] 32%|███▏      | 1618/5000 [7:30:19<14:19:31, 15.25s/it]                                                        {'loss': 19.5279, 'grad_norm': 15.6875, 'learning_rate': 5.66250864581617e-05, 'epoch': 2.05}
 32%|███▏      | 1618/5000 [7:30:19<14:19:31, 15.25s/it] 32%|███▏      | 1619/5000 [7:30:33<14:01:52, 14.94s/it]                                                        {'loss': 20.3269, 'grad_norm': 19.5, 'learning_rate': 5.660688027104755e-05, 'epoch': 2.06}
 32%|███▏      | 1619/5000 [7:30:33<14:01:52, 14.94s/it] 32%|███▏      | 1620/5000 [7:30:59<17:06:23, 18.22s/it]                                                        {'loss': 19.8055, 'grad_norm': 14.375, 'learning_rate': 5.658866463234988e-05, 'epoch': 2.06}
 32%|███▏      | 1620/5000 [7:30:59<17:06:23, 18.22s/it] 32%|███▏      | 1621/5000 [7:31:13<15:54:01, 16.94s/it]                                                        {'loss': 22.6013, 'grad_norm': 44.0, 'learning_rate': 5.657043955003686e-05, 'epoch': 2.06}
 32%|███▏      | 1621/5000 [7:31:13<15:54:01, 16.94s/it] 32%|███▏      | 1622/5000 [7:31:25<14:36:21, 15.57s/it]                                                        {'loss': 22.6503, 'grad_norm': 34.25, 'learning_rate': 5.655220503208073e-05, 'epoch': 2.06}
 32%|███▏      | 1622/5000 [7:31:25<14:36:21, 15.57s/it] 32%|███▏      | 1623/5000 [7:31:38<13:47:43, 14.71s/it]                                                        {'loss': 20.7413, 'grad_norm': 24.625, 'learning_rate': 5.6533961086457904e-05, 'epoch': 2.06}
 32%|███▏      | 1623/5000 [7:31:38<13:47:43, 14.71s/it] 32%|███▏      | 1624/5000 [7:31:51<13:22:27, 14.26s/it]                                                        {'loss': 23.7513, 'grad_norm': 42.25, 'learning_rate': 5.651570772114891e-05, 'epoch': 2.06}
 32%|███▏      | 1624/5000 [7:31:51<13:22:27, 14.26s/it] 32%|███▎      | 1625/5000 [7:32:09<14:29:55, 15.47s/it]                                                        {'loss': 21.4403, 'grad_norm': 168.0, 'learning_rate': 5.6497444944138376e-05, 'epoch': 2.06}
 32%|███▎      | 1625/5000 [7:32:09<14:29:55, 15.47s/it] 33%|███▎      | 1626/5000 [7:32:21<13:23:12, 14.28s/it]                                                        {'loss': 24.4084, 'grad_norm': 33.5, 'learning_rate': 5.6479172763415064e-05, 'epoch': 2.06}
 33%|███▎      | 1626/5000 [7:32:21<13:23:12, 14.28s/it] 33%|███▎      | 1627/5000 [7:32:34<13:13:04, 14.11s/it]                                                        {'loss': 21.4288, 'grad_norm': 74.0, 'learning_rate': 5.646089118697185e-05, 'epoch': 2.07}
 33%|███▎      | 1627/5000 [7:32:35<13:13:04, 14.11s/it] 33%|███▎      | 1628/5000 [7:32:49<13:13:25, 14.12s/it]                                                        {'loss': 22.2302, 'grad_norm': 31.125, 'learning_rate': 5.644260022280571e-05, 'epoch': 2.07}
 33%|███▎      | 1628/5000 [7:32:49<13:13:25, 14.12s/it] 33%|███▎      | 1629/5000 [7:33:01<12:35:12, 13.44s/it]                                                        {'loss': 23.3941, 'grad_norm': 59.25, 'learning_rate': 5.642429987891775e-05, 'epoch': 2.07}
 33%|███▎      | 1629/5000 [7:33:01<12:35:12, 13.44s/it] 33%|███▎      | 1630/5000 [7:33:16<13:04:22, 13.97s/it]                                                        {'loss': 19.91, 'grad_norm': 23.375, 'learning_rate': 5.6405990163313144e-05, 'epoch': 2.07}
 33%|███▎      | 1630/5000 [7:33:16<13:04:22, 13.97s/it] 33%|███▎      | 1631/5000 [7:33:31<13:28:00, 14.39s/it]                                                        {'loss': 21.554, 'grad_norm': 78.5, 'learning_rate': 5.638767108400119e-05, 'epoch': 2.07}
 33%|███▎      | 1631/5000 [7:33:31<13:28:00, 14.39s/it] 33%|███▎      | 1632/5000 [7:33:55<16:10:13, 17.28s/it]                                                        {'loss': 20.6567, 'grad_norm': 23.25, 'learning_rate': 5.6369342648995286e-05, 'epoch': 2.07}
 33%|███▎      | 1632/5000 [7:33:55<16:10:13, 17.28s/it] 33%|███▎      | 1633/5000 [7:34:09<15:16:41, 16.34s/it]                                                        {'loss': 21.7651, 'grad_norm': 36.0, 'learning_rate': 5.635100486631289e-05, 'epoch': 2.07}
 33%|███▎      | 1633/5000 [7:34:09<15:16:41, 16.34s/it] 33%|███▎      | 1634/5000 [7:34:22<14:24:19, 15.41s/it]                                                        {'loss': 21.6575, 'grad_norm': 30.5, 'learning_rate': 5.633265774397559e-05, 'epoch': 2.07}
 33%|███▎      | 1634/5000 [7:34:22<14:24:19, 15.41s/it] 33%|███▎      | 1635/5000 [7:34:46<16:36:11, 17.76s/it]                                                        {'loss': 22.107, 'grad_norm': 34.5, 'learning_rate': 5.631430129000904e-05, 'epoch': 2.08}
 33%|███▎      | 1635/5000 [7:34:46<16:36:11, 17.76s/it] 33%|███▎      | 1636/5000 [7:35:02<16:13:14, 17.36s/it]                                                        {'loss': 20.8677, 'grad_norm': 26.25, 'learning_rate': 5.629593551244296e-05, 'epoch': 2.08}
 33%|███▎      | 1636/5000 [7:35:02<16:13:14, 17.36s/it] 33%|███▎      | 1637/5000 [7:35:14<14:45:58, 15.81s/it]                                                        {'loss': 22.6708, 'grad_norm': 63.0, 'learning_rate': 5.627756041931119e-05, 'epoch': 2.08}
 33%|███▎      | 1637/5000 [7:35:14<14:45:58, 15.81s/it] 33%|███▎      | 1638/5000 [7:35:28<14:15:02, 15.26s/it]                                                        {'loss': 28.9258, 'grad_norm': 72.5, 'learning_rate': 5.625917601865159e-05, 'epoch': 2.08}
 33%|███▎      | 1638/5000 [7:35:28<14:15:02, 15.26s/it] 33%|███▎      | 1639/5000 [7:35:53<16:51:36, 18.06s/it]                                                        {'loss': 20.4818, 'grad_norm': 44.0, 'learning_rate': 5.624078231850615e-05, 'epoch': 2.08}
 33%|███▎      | 1639/5000 [7:35:53<16:51:36, 18.06s/it] 33%|███▎      | 1640/5000 [7:36:05<15:13:04, 16.30s/it]                                                        {'loss': 21.1431, 'grad_norm': 40.5, 'learning_rate': 5.622237932692088e-05, 'epoch': 2.08}
 33%|███▎      | 1640/5000 [7:36:05<15:13:04, 16.30s/it] 33%|███▎      | 1641/5000 [7:36:19<14:39:45, 15.71s/it]                                                        {'loss': 21.1693, 'grad_norm': 25.5, 'learning_rate': 5.620396705194588e-05, 'epoch': 2.08}
 33%|███▎      | 1641/5000 [7:36:19<14:39:45, 15.71s/it] 33%|███▎      | 1642/5000 [7:36:43<16:53:00, 18.10s/it]                                                        {'loss': 20.9777, 'grad_norm': 33.75, 'learning_rate': 5.618554550163531e-05, 'epoch': 2.09}
 33%|███▎      | 1642/5000 [7:36:43<16:53:00, 18.10s/it] 33%|███▎      | 1643/5000 [7:36:58<16:06:58, 17.28s/it]                                                        {'loss': 51.2226, 'grad_norm': 161.0, 'learning_rate': 5.616711468404737e-05, 'epoch': 2.09}
 33%|███▎      | 1643/5000 [7:36:59<16:06:58, 17.28s/it] 33%|███▎      | 1644/5000 [7:37:19<17:08:39, 18.39s/it]                                                        {'loss': 19.7657, 'grad_norm': 47.25, 'learning_rate': 5.614867460724433e-05, 'epoch': 2.09}
 33%|███▎      | 1644/5000 [7:37:19<17:08:39, 18.39s/it] 33%|███▎      | 1645/5000 [7:37:35<16:27:32, 17.66s/it]                                                        {'loss': 21.6691, 'grad_norm': 30.625, 'learning_rate': 5.613022527929252e-05, 'epoch': 2.09}
 33%|███▎      | 1645/5000 [7:37:35<16:27:32, 17.66s/it] 33%|███▎      | 1646/5000 [7:37:53<16:26:54, 17.65s/it]                                                        {'loss': 21.3967, 'grad_norm': 21.625, 'learning_rate': 5.6111766708262287e-05, 'epoch': 2.09}
 33%|███▎      | 1646/5000 [7:37:53<16:26:54, 17.65s/it] 33%|███▎      | 1647/5000 [7:38:09<15:57:13, 17.13s/it]                                                        {'loss': 25.7005, 'grad_norm': 100.5, 'learning_rate': 5.609329890222804e-05, 'epoch': 2.09}
 33%|███▎      | 1647/5000 [7:38:09<15:57:13, 17.13s/it] 33%|███▎      | 1648/5000 [7:38:23<15:06:39, 16.23s/it]                                                        {'loss': 21.9095, 'grad_norm': 26.75, 'learning_rate': 5.607482186926824e-05, 'epoch': 2.09}
 33%|███▎      | 1648/5000 [7:38:23<15:06:39, 16.23s/it] 33%|███▎      | 1649/5000 [7:38:40<15:16:10, 16.40s/it]                                                        {'loss': 19.0021, 'grad_norm': 14.6875, 'learning_rate': 5.6056335617465344e-05, 'epoch': 2.09}
 33%|███▎      | 1649/5000 [7:38:40<15:16:10, 16.40s/it] 33%|███▎      | 1650/5000 [7:38:56<15:17:03, 16.42s/it]                                                        {'loss': 18.8725, 'grad_norm': 51.5, 'learning_rate': 5.603784015490587e-05, 'epoch': 2.1}
 33%|███▎      | 1650/5000 [7:38:56<15:17:03, 16.42s/it] 33%|███▎      | 1651/5000 [7:39:10<14:21:34, 15.44s/it]                                                        {'loss': 21.9414, 'grad_norm': 22.25, 'learning_rate': 5.601933548968038e-05, 'epoch': 2.1}
 33%|███▎      | 1651/5000 [7:39:10<14:21:34, 15.44s/it] 33%|███▎      | 1652/5000 [7:39:26<14:32:36, 15.64s/it]                                                        {'loss': 20.8636, 'grad_norm': 23.375, 'learning_rate': 5.600082162988343e-05, 'epoch': 2.1}
 33%|███▎      | 1652/5000 [7:39:26<14:32:36, 15.64s/it] 33%|███▎      | 1653/5000 [7:39:40<14:14:23, 15.32s/it]                                                        {'loss': 20.7199, 'grad_norm': 19.625, 'learning_rate': 5.598229858361363e-05, 'epoch': 2.1}
 33%|███▎      | 1653/5000 [7:39:40<14:14:23, 15.32s/it] 33%|███▎      | 1654/5000 [7:39:57<14:36:19, 15.71s/it]                                                        {'loss': 20.8165, 'grad_norm': 43.0, 'learning_rate': 5.5963766358973554e-05, 'epoch': 2.1}
 33%|███▎      | 1654/5000 [7:39:57<14:36:19, 15.71s/it] 33%|███▎      | 1655/5000 [7:40:09<13:36:42, 14.65s/it]                                                        {'loss': 22.3993, 'grad_norm': 96.5, 'learning_rate': 5.5945224964069854e-05, 'epoch': 2.1}
 33%|███▎      | 1655/5000 [7:40:09<13:36:42, 14.65s/it] 33%|███▎      | 1656/5000 [7:40:24<13:36:48, 14.66s/it]                                                        {'loss': 21.2029, 'grad_norm': 31.875, 'learning_rate': 5.592667440701316e-05, 'epoch': 2.1}
 33%|███▎      | 1656/5000 [7:40:24<13:36:48, 14.66s/it] 33%|███▎      | 1657/5000 [7:40:40<14:08:27, 15.23s/it]                                                        {'loss': 21.5189, 'grad_norm': 41.25, 'learning_rate': 5.590811469591811e-05, 'epoch': 2.1}
 33%|███▎      | 1657/5000 [7:40:40<14:08:27, 15.23s/it] 33%|███▎      | 1658/5000 [7:40:54<13:49:32, 14.89s/it]                                                        {'loss': 21.4456, 'grad_norm': 26.0, 'learning_rate': 5.588954583890336e-05, 'epoch': 2.11}
 33%|███▎      | 1658/5000 [7:40:54<13:49:32, 14.89s/it] 33%|███▎      | 1659/5000 [7:41:09<13:37:05, 14.67s/it]                                                        {'loss': 20.9493, 'grad_norm': 23.375, 'learning_rate': 5.5870967844091565e-05, 'epoch': 2.11}
 33%|███▎      | 1659/5000 [7:41:09<13:37:05, 14.67s/it] 33%|███▎      | 1660/5000 [7:41:23<13:36:07, 14.66s/it]                                                        {'loss': 19.5138, 'grad_norm': 28.25, 'learning_rate': 5.585238071960935e-05, 'epoch': 2.11}
 33%|███▎      | 1660/5000 [7:41:23<13:36:07, 14.66s/it] 33%|███▎      | 1661/5000 [7:41:40<14:10:59, 15.29s/it]                                                        {'loss': 19.9137, 'grad_norm': 17.25, 'learning_rate': 5.5833784473587366e-05, 'epoch': 2.11}
 33%|███▎      | 1661/5000 [7:41:40<14:10:59, 15.29s/it] 33%|███▎      | 1662/5000 [7:41:53<13:37:43, 14.70s/it]                                                        {'loss': 21.3819, 'grad_norm': 66.5, 'learning_rate': 5.5815179114160244e-05, 'epoch': 2.11}
 33%|███▎      | 1662/5000 [7:41:53<13:37:43, 14.70s/it] 33%|███▎      | 1663/5000 [7:42:17<16:04:46, 17.35s/it]                                                        {'loss': 18.915, 'grad_norm': 29.5, 'learning_rate': 5.57965646494666e-05, 'epoch': 2.11}
 33%|███▎      | 1663/5000 [7:42:17<16:04:46, 17.35s/it] 33%|███▎      | 1664/5000 [7:42:42<18:24:00, 19.86s/it]                                                        {'loss': 19.1246, 'grad_norm': 13.625, 'learning_rate': 5.577794108764904e-05, 'epoch': 2.11}
 33%|███▎      | 1664/5000 [7:42:42<18:24:00, 19.86s/it] 33%|███▎      | 1665/5000 [7:42:56<16:37:01, 17.94s/it]                                                        {'loss': 20.3528, 'grad_norm': 14.1875, 'learning_rate': 5.575930843685414e-05, 'epoch': 2.11}
 33%|███▎      | 1665/5000 [7:42:56<16:37:01, 17.94s/it] 33%|███▎      | 1666/5000 [7:43:10<15:30:13, 16.74s/it]                                                        {'loss': 19.3294, 'grad_norm': 50.75, 'learning_rate': 5.5740666705232436e-05, 'epoch': 2.12}
 33%|███▎      | 1666/5000 [7:43:10<15:30:13, 16.74s/it] 33%|███▎      | 1667/5000 [7:43:28<15:45:57, 17.03s/it]                                                        {'loss': 18.1836, 'grad_norm': 14.6875, 'learning_rate': 5.5722015900938484e-05, 'epoch': 2.12}
 33%|███▎      | 1667/5000 [7:43:28<15:45:57, 17.03s/it] 33%|███▎      | 1668/5000 [7:43:56<19:00:44, 20.54s/it]                                                        {'loss': 21.1852, 'grad_norm': 19.375, 'learning_rate': 5.570335603213075e-05, 'epoch': 2.12}
 33%|███▎      | 1668/5000 [7:43:56<19:00:44, 20.54s/it] 33%|███▎      | 1669/5000 [7:44:11<17:16:20, 18.67s/it]                                                        {'loss': 19.9528, 'grad_norm': 19.875, 'learning_rate': 5.568468710697172e-05, 'epoch': 2.12}
 33%|███▎      | 1669/5000 [7:44:11<17:16:20, 18.67s/it] 33%|███▎      | 1670/5000 [7:44:25<16:01:34, 17.33s/it]                                                        {'loss': 20.9332, 'grad_norm': 43.75, 'learning_rate': 5.566600913362779e-05, 'epoch': 2.12}
 33%|███▎      | 1670/5000 [7:44:25<16:01:34, 17.33s/it] 33%|███▎      | 1671/5000 [7:44:37<14:31:18, 15.70s/it]                                                        {'loss': 23.0278, 'grad_norm': 31.875, 'learning_rate': 5.564732212026937e-05, 'epoch': 2.12}
 33%|███▎      | 1671/5000 [7:44:37<14:31:18, 15.70s/it] 33%|███▎      | 1672/5000 [7:44:52<14:26:03, 15.61s/it]                                                        {'loss': 20.57, 'grad_norm': 21.25, 'learning_rate': 5.5628626075070784e-05, 'epoch': 2.12}
 33%|███▎      | 1672/5000 [7:44:52<14:26:03, 15.61s/it] 33%|███▎      | 1673/5000 [7:45:05<13:46:47, 14.91s/it]                                                        {'loss': 21.143, 'grad_norm': 20.125, 'learning_rate': 5.56099210062103e-05, 'epoch': 2.12}
 33%|███▎      | 1673/5000 [7:45:05<13:46:47, 14.91s/it] 33%|███▎      | 1674/5000 [7:45:28<15:51:01, 17.16s/it]                                                        {'loss': 18.5697, 'grad_norm': 17.375, 'learning_rate': 5.559120692187016e-05, 'epoch': 2.13}
 33%|███▎      | 1674/5000 [7:45:28<15:51:01, 17.16s/it] 34%|███▎      | 1675/5000 [7:45:45<15:48:25, 17.11s/it]                                                        {'loss': 19.6083, 'grad_norm': 21.25, 'learning_rate': 5.557248383023655e-05, 'epoch': 2.13}
 34%|███▎      | 1675/5000 [7:45:45<15:48:25, 17.11s/it] 34%|███▎      | 1676/5000 [7:45:59<14:59:20, 16.23s/it]                                                        {'loss': 20.9181, 'grad_norm': 38.0, 'learning_rate': 5.555375173949958e-05, 'epoch': 2.13}
 34%|███▎      | 1676/5000 [7:45:59<14:59:20, 16.23s/it] 34%|███▎      | 1677/5000 [7:46:15<15:00:18, 16.26s/it]                                                        {'loss': 20.6035, 'grad_norm': 17.75, 'learning_rate': 5.553501065785332e-05, 'epoch': 2.13}
 34%|███▎      | 1677/5000 [7:46:15<15:00:18, 16.26s/it] 34%|███▎      | 1678/5000 [7:46:34<15:43:09, 17.03s/it]                                                        {'loss': 21.6499, 'grad_norm': 72.0, 'learning_rate': 5.5516260593495725e-05, 'epoch': 2.13}
 34%|███▎      | 1678/5000 [7:46:34<15:43:09, 17.03s/it] 34%|███▎      | 1679/5000 [7:46:52<16:04:19, 17.42s/it]                                                        {'loss': 20.3537, 'grad_norm': 22.75, 'learning_rate': 5.5497501554628715e-05, 'epoch': 2.13}
 34%|███▎      | 1679/5000 [7:46:52<16:04:19, 17.42s/it] 34%|███▎      | 1680/5000 [7:47:05<14:42:31, 15.95s/it]                                                        {'loss': 21.0941, 'grad_norm': 24.375, 'learning_rate': 5.5478733549458155e-05, 'epoch': 2.13}
 34%|███▎      | 1680/5000 [7:47:05<14:42:31, 15.95s/it] 34%|███▎      | 1681/5000 [7:47:29<16:51:07, 18.28s/it]                                                        {'loss': 21.2526, 'grad_norm': 20.125, 'learning_rate': 5.545995658619379e-05, 'epoch': 2.13}
 34%|███▎      | 1681/5000 [7:47:29<16:51:07, 18.28s/it] 34%|███▎      | 1682/5000 [7:47:52<18:22:00, 19.93s/it]                                                        {'loss': 18.3564, 'grad_norm': 21.625, 'learning_rate': 5.5441170673049295e-05, 'epoch': 2.14}
 34%|███▎      | 1682/5000 [7:47:52<18:22:00, 19.93s/it] 34%|███▎      | 1683/5000 [7:48:21<20:49:28, 22.60s/it]                                                        {'loss': 20.3819, 'grad_norm': 13.4375, 'learning_rate': 5.542237581824229e-05, 'epoch': 2.14}
 34%|███▎      | 1683/5000 [7:48:21<20:49:28, 22.60s/it] 34%|███▎      | 1684/5000 [7:48:44<20:58:16, 22.77s/it]                                                        {'loss': 21.9606, 'grad_norm': 134.0, 'learning_rate': 5.540357202999426e-05, 'epoch': 2.14}
 34%|███▎      | 1684/5000 [7:48:44<20:58:16, 22.77s/it] 34%|███▎      | 1685/5000 [7:49:01<19:21:04, 21.01s/it]                                                        {'loss': 20.5852, 'grad_norm': 54.5, 'learning_rate': 5.5384759316530635e-05, 'epoch': 2.14}
 34%|███▎      | 1685/5000 [7:49:01<19:21:04, 21.01s/it] 34%|███▎      | 1686/5000 [7:49:22<19:16:33, 20.94s/it]                                                        {'loss': 18.5102, 'grad_norm': 27.125, 'learning_rate': 5.5365937686080735e-05, 'epoch': 2.14}
 34%|███▎      | 1686/5000 [7:49:22<19:16:33, 20.94s/it] 34%|███▎      | 1687/5000 [7:49:36<17:26:38, 18.96s/it]                                                        {'loss': 23.3899, 'grad_norm': 188.0, 'learning_rate': 5.5347107146877766e-05, 'epoch': 2.14}
 34%|███▎      | 1687/5000 [7:49:36<17:26:38, 18.96s/it] 34%|███▍      | 1688/5000 [7:49:56<17:35:09, 19.12s/it]                                                        {'loss': 20.2961, 'grad_norm': 36.25, 'learning_rate': 5.5328267707158866e-05, 'epoch': 2.14}
 34%|███▍      | 1688/5000 [7:49:56<17:35:09, 19.12s/it] 34%|███▍      | 1689/5000 [7:50:08<15:40:12, 17.04s/it]                                                        {'loss': 22.2464, 'grad_norm': 36.25, 'learning_rate': 5.530941937516503e-05, 'epoch': 2.14}
 34%|███▍      | 1689/5000 [7:50:08<15:40:12, 17.04s/it] 34%|███▍      | 1690/5000 [7:50:21<14:29:45, 15.77s/it]                                                        {'loss': 23.526, 'grad_norm': 32.0, 'learning_rate': 5.529056215914116e-05, 'epoch': 2.15}
 34%|███▍      | 1690/5000 [7:50:21<14:29:45, 15.77s/it] 34%|███▍      | 1691/5000 [7:50:40<15:16:49, 16.62s/it]                                                        {'loss': 20.9225, 'grad_norm': 47.0, 'learning_rate': 5.527169606733606e-05, 'epoch': 2.15}
 34%|███▍      | 1691/5000 [7:50:40<15:16:49, 16.62s/it] 34%|███▍      | 1692/5000 [7:50:55<14:59:45, 16.32s/it]                                                        {'loss': 20.1932, 'grad_norm': 22.75, 'learning_rate': 5.525282110800237e-05, 'epoch': 2.15}
 34%|███▍      | 1692/5000 [7:50:55<14:59:45, 16.32s/it] 34%|███▍      | 1693/5000 [7:51:10<14:27:27, 15.74s/it]                                                        {'loss': 21.01, 'grad_norm': 25.5, 'learning_rate': 5.5233937289396675e-05, 'epoch': 2.15}
 34%|███▍      | 1693/5000 [7:51:10<14:27:27, 15.74s/it] 34%|███▍      | 1694/5000 [7:51:26<14:44:43, 16.06s/it]                                                        {'loss': 19.5463, 'grad_norm': 18.875, 'learning_rate': 5.521504461977936e-05, 'epoch': 2.15}
 34%|███▍      | 1694/5000 [7:51:26<14:44:43, 16.06s/it] 34%|███▍      | 1695/5000 [7:51:39<13:44:27, 14.97s/it]                                                        {'loss': 22.195, 'grad_norm': 65.0, 'learning_rate': 5.5196143107414744e-05, 'epoch': 2.15}
 34%|███▍      | 1695/5000 [7:51:39<13:44:27, 14.97s/it] 34%|███▍      | 1696/5000 [7:51:53<13:27:51, 14.67s/it]                                                        {'loss': 22.0255, 'grad_norm': 58.5, 'learning_rate': 5.517723276057101e-05, 'epoch': 2.15}
 34%|███▍      | 1696/5000 [7:51:53<13:27:51, 14.67s/it] 34%|███▍      | 1697/5000 [7:52:10<14:04:54, 15.35s/it]                                                        {'loss': 20.2228, 'grad_norm': 28.75, 'learning_rate': 5.515831358752013e-05, 'epoch': 2.15}
 34%|███▍      | 1697/5000 [7:52:10<14:04:54, 15.35s/it] 34%|███▍      | 1698/5000 [7:52:24<13:45:40, 15.00s/it]                                                        {'loss': 28.0968, 'grad_norm': 520.0, 'learning_rate': 5.513938559653804e-05, 'epoch': 2.16}
 34%|███▍      | 1698/5000 [7:52:24<13:45:40, 15.00s/it] 34%|███▍      | 1699/5000 [7:52:41<14:17:18, 15.58s/it]                                                        {'loss': 19.8115, 'grad_norm': 21.375, 'learning_rate': 5.5120448795904466e-05, 'epoch': 2.16}
 34%|███▍      | 1699/5000 [7:52:41<14:17:18, 15.58s/it] 34%|███▍      | 1700/5000 [7:52:54<13:44:28, 14.99s/it]                                                        {'loss': 24.3448, 'grad_norm': 178.0, 'learning_rate': 5.510150319390302e-05, 'epoch': 2.16}
 34%|███▍      | 1700/5000 [7:52:54<13:44:28, 14.99s/it] 34%|███▍      | 1701/5000 [7:53:11<14:06:15, 15.39s/it]                                                        {'loss': 19.2956, 'grad_norm': 76.5, 'learning_rate': 5.5082548798821144e-05, 'epoch': 2.16}
 34%|███▍      | 1701/5000 [7:53:11<14:06:15, 15.39s/it] 34%|███▍      | 1702/5000 [7:53:26<14:03:38, 15.35s/it]                                                        {'loss': 19.8163, 'grad_norm': 20.25, 'learning_rate': 5.506358561895014e-05, 'epoch': 2.16}
 34%|███▍      | 1702/5000 [7:53:26<14:03:38, 15.35s/it] 34%|███▍      | 1703/5000 [7:53:40<13:46:50, 15.05s/it]                                                        {'loss': 18.3365, 'grad_norm': 20.5, 'learning_rate': 5.504461366258514e-05, 'epoch': 2.16}
 34%|███▍      | 1703/5000 [7:53:40<13:46:50, 15.05s/it] 34%|███▍      | 1704/5000 [7:54:05<16:17:03, 17.79s/it]                                                        {'loss': 18.9064, 'grad_norm': 21.125, 'learning_rate': 5.5025632938025134e-05, 'epoch': 2.16}
 34%|███▍      | 1704/5000 [7:54:05<16:17:03, 17.79s/it] 34%|███▍      | 1705/5000 [7:54:29<18:02:19, 19.71s/it]                                                        {'loss': 20.2704, 'grad_norm': 15.9375, 'learning_rate': 5.500664345357291e-05, 'epoch': 2.17}
 34%|███▍      | 1705/5000 [7:54:29<18:02:19, 19.71s/it] 34%|███▍      | 1706/5000 [7:54:44<16:45:16, 18.31s/it]                                                        {'loss': 19.8241, 'grad_norm': 17.0, 'learning_rate': 5.498764521753514e-05, 'epoch': 2.17}
 34%|███▍      | 1706/5000 [7:54:44<16:45:16, 18.31s/it] 34%|███▍      | 1707/5000 [7:54:58<15:35:07, 17.04s/it]                                                        {'loss': 20.1873, 'grad_norm': 13.875, 'learning_rate': 5.4968638238222295e-05, 'epoch': 2.17}
 34%|███▍      | 1707/5000 [7:54:58<15:35:07, 17.04s/it] 34%|███▍      | 1708/5000 [7:55:10<14:21:59, 15.71s/it]                                                        {'loss': 21.0855, 'grad_norm': 15.625, 'learning_rate': 5.494962252394866e-05, 'epoch': 2.17}
 34%|███▍      | 1708/5000 [7:55:10<14:21:59, 15.71s/it] 34%|███▍      | 1709/5000 [7:55:25<14:09:00, 15.48s/it]                                                        {'loss': 19.1179, 'grad_norm': 12.125, 'learning_rate': 5.493059808303236e-05, 'epoch': 2.17}
 34%|███▍      | 1709/5000 [7:55:25<14:09:00, 15.48s/it] 34%|███▍      | 1710/5000 [7:55:41<14:18:00, 15.65s/it]                                                        {'loss': 19.4163, 'grad_norm': 20.25, 'learning_rate': 5.491156492379534e-05, 'epoch': 2.17}
 34%|███▍      | 1710/5000 [7:55:41<14:18:00, 15.65s/it] 34%|███▍      | 1711/5000 [7:56:01<15:16:42, 16.72s/it]                                                        {'loss': 19.212, 'grad_norm': 37.75, 'learning_rate': 5.489252305456334e-05, 'epoch': 2.17}
 34%|███▍      | 1711/5000 [7:56:01<15:16:42, 16.72s/it] 34%|███▍      | 1712/5000 [7:56:17<15:11:59, 16.64s/it]                                                        {'loss': 20.1559, 'grad_norm': 18.375, 'learning_rate': 5.4873472483665936e-05, 'epoch': 2.17}
 34%|███▍      | 1712/5000 [7:56:17<15:11:59, 16.64s/it] 34%|███▍      | 1713/5000 [7:56:41<17:16:39, 18.92s/it]                                                        {'loss': 20.4437, 'grad_norm': 23.625, 'learning_rate': 5.485441321943648e-05, 'epoch': 2.18}
 34%|███▍      | 1713/5000 [7:56:41<17:16:39, 18.92s/it] 34%|███▍      | 1714/5000 [7:56:56<15:59:45, 17.52s/it]                                                        {'loss': 18.8218, 'grad_norm': 22.125, 'learning_rate': 5.483534527021213e-05, 'epoch': 2.18}
 34%|███▍      | 1714/5000 [7:56:56<15:59:45, 17.52s/it] 34%|███▍      | 1715/5000 [7:57:13<15:52:14, 17.39s/it]                                                        {'loss': 22.8493, 'grad_norm': 780.0, 'learning_rate': 5.481626864433389e-05, 'epoch': 2.18}
 34%|███▍      | 1715/5000 [7:57:13<15:52:14, 17.39s/it] 34%|███▍      | 1716/5000 [7:57:27<14:53:55, 16.33s/it]                                                        {'loss': 20.3683, 'grad_norm': 73.5, 'learning_rate': 5.479718335014649e-05, 'epoch': 2.18}
 34%|███▍      | 1716/5000 [7:57:27<14:53:55, 16.33s/it] 34%|███▍      | 1717/5000 [7:57:43<15:01:33, 16.48s/it]                                                        {'loss': 20.6134, 'grad_norm': 28.625, 'learning_rate': 5.477808939599851e-05, 'epoch': 2.18}
 34%|███▍      | 1717/5000 [7:57:43<15:01:33, 16.48s/it] 34%|███▍      | 1718/5000 [7:57:57<14:09:18, 15.53s/it]                                                        {'loss': 22.5487, 'grad_norm': 98.0, 'learning_rate': 5.475898679024227e-05, 'epoch': 2.18}
 34%|███▍      | 1718/5000 [7:57:57<14:09:18, 15.53s/it] 34%|███▍      | 1719/5000 [7:58:17<15:22:42, 16.87s/it]                                                        {'loss': 19.631, 'grad_norm': 38.0, 'learning_rate': 5.473987554123393e-05, 'epoch': 2.18}
 34%|███▍      | 1719/5000 [7:58:17<15:22:42, 16.87s/it] 34%|███▍      | 1720/5000 [7:58:29<14:03:41, 15.43s/it]                                                        {'loss': 21.824, 'grad_norm': 46.0, 'learning_rate': 5.4720755657333376e-05, 'epoch': 2.18}
 34%|███▍      | 1720/5000 [7:58:29<14:03:41, 15.43s/it] 34%|███▍      | 1721/5000 [7:58:43<13:44:19, 15.08s/it]                                                        {'loss': 21.7252, 'grad_norm': 294.0, 'learning_rate': 5.4701627146904304e-05, 'epoch': 2.19}
 34%|███▍      | 1721/5000 [7:58:43<13:44:19, 15.08s/it] 34%|███▍      | 1722/5000 [7:59:08<16:31:57, 18.16s/it]                                                        {'loss': 19.0118, 'grad_norm': 16.25, 'learning_rate': 5.468249001831417e-05, 'epoch': 2.19}
 34%|███▍      | 1722/5000 [7:59:08<16:31:57, 18.16s/it] 34%|███▍      | 1723/5000 [7:59:23<15:30:47, 17.04s/it]                                                        {'loss': 20.5873, 'grad_norm': 35.75, 'learning_rate': 5.466334427993421e-05, 'epoch': 2.19}
 34%|███▍      | 1723/5000 [7:59:23<15:30:47, 17.04s/it] 34%|███▍      | 1724/5000 [7:59:38<14:52:40, 16.35s/it]                                                        {'loss': 25.1468, 'grad_norm': 41.75, 'learning_rate': 5.4644189940139416e-05, 'epoch': 2.19}
 34%|███▍      | 1724/5000 [7:59:38<14:52:40, 16.35s/it] 34%|███▍      | 1725/5000 [7:59:52<14:23:42, 15.82s/it]                                                        {'loss': 20.4974, 'grad_norm': 17.375, 'learning_rate': 5.4625027007308546e-05, 'epoch': 2.19}
 34%|███▍      | 1725/5000 [7:59:52<14:23:42, 15.82s/it] 35%|███▍      | 1726/5000 [8:00:08<14:26:51, 15.89s/it]                                                        {'loss': 20.7824, 'grad_norm': 44.5, 'learning_rate': 5.460585548982412e-05, 'epoch': 2.19}
 35%|███▍      | 1726/5000 [8:00:08<14:26:51, 15.89s/it] 35%|███▍      | 1727/5000 [8:00:30<16:10:30, 17.79s/it]                                                        {'loss': 18.5471, 'grad_norm': 15.0, 'learning_rate': 5.45866753960724e-05, 'epoch': 2.19}
 35%|███▍      | 1727/5000 [8:00:30<16:10:30, 17.79s/it] 35%|███▍      | 1728/5000 [8:00:53<17:21:09, 19.09s/it]                                                        {'loss': 19.8142, 'grad_norm': 18.375, 'learning_rate': 5.456748673444344e-05, 'epoch': 2.19}
 35%|███▍      | 1728/5000 [8:00:53<17:21:09, 19.09s/it] 35%|███▍      | 1729/5000 [8:01:10<16:46:13, 18.46s/it]                                                        {'loss': 19.3256, 'grad_norm': 24.875, 'learning_rate': 5.4548289513330986e-05, 'epoch': 2.2}
 35%|███▍      | 1729/5000 [8:01:10<16:46:13, 18.46s/it] 35%|███▍      | 1730/5000 [8:01:28<16:41:13, 18.37s/it]                                                        {'loss': 19.2357, 'grad_norm': 17.75, 'learning_rate': 5.4529083741132565e-05, 'epoch': 2.2}
 35%|███▍      | 1730/5000 [8:01:28<16:41:13, 18.37s/it] 35%|███▍      | 1731/5000 [8:01:43<15:58:44, 17.60s/it]                                                        {'loss': 20.5796, 'grad_norm': 36.5, 'learning_rate': 5.4509869426249435e-05, 'epoch': 2.2}
 35%|███▍      | 1731/5000 [8:01:43<15:58:44, 17.60s/it] 35%|███▍      | 1732/5000 [8:02:00<15:42:47, 17.31s/it]                                                        {'loss': 18.0265, 'grad_norm': 25.875, 'learning_rate': 5.449064657708658e-05, 'epoch': 2.2}
 35%|███▍      | 1732/5000 [8:02:00<15:42:47, 17.31s/it] 35%|███▍      | 1733/5000 [8:02:23<17:09:06, 18.90s/it]                                                        {'loss': 20.278, 'grad_norm': 18.25, 'learning_rate': 5.447141520205274e-05, 'epoch': 2.2}
 35%|███▍      | 1733/5000 [8:02:23<17:09:06, 18.90s/it] 35%|███▍      | 1734/5000 [8:02:39<16:31:18, 18.21s/it]                                                        {'loss': 19.7644, 'grad_norm': 14.4375, 'learning_rate': 5.445217530956036e-05, 'epoch': 2.2}
 35%|███▍      | 1734/5000 [8:02:39<16:31:18, 18.21s/it] 35%|███▍      | 1735/5000 [8:02:52<14:54:26, 16.44s/it]                                                        {'loss': 24.5128, 'grad_norm': 30.25, 'learning_rate': 5.4432926908025624e-05, 'epoch': 2.2}
 35%|███▍      | 1735/5000 [8:02:52<14:54:26, 16.44s/it] 35%|███▍      | 1736/5000 [8:03:05<13:57:53, 15.40s/it]                                                        {'loss': 21.8484, 'grad_norm': 25.375, 'learning_rate': 5.441367000586845e-05, 'epoch': 2.2}
 35%|███▍      | 1736/5000 [8:03:05<13:57:53, 15.40s/it] 35%|███▍      | 1737/5000 [8:03:21<14:05:52, 15.55s/it]                                                        {'loss': 20.904, 'grad_norm': 20.0, 'learning_rate': 5.439440461151246e-05, 'epoch': 2.21}
 35%|███▍      | 1737/5000 [8:03:21<14:05:52, 15.55s/it] 35%|███▍      | 1738/5000 [8:03:39<14:58:30, 16.53s/it]                                                        {'loss': 21.7274, 'grad_norm': 23.875, 'learning_rate': 5.4375130733384974e-05, 'epoch': 2.21}
 35%|███▍      | 1738/5000 [8:03:39<14:58:30, 16.53s/it] 35%|███▍      | 1739/5000 [8:03:53<14:17:49, 15.78s/it]                                                        {'loss': 18.8672, 'grad_norm': 15.3125, 'learning_rate': 5.4355848379917055e-05, 'epoch': 2.21}
 35%|███▍      | 1739/5000 [8:03:53<14:17:49, 15.78s/it] 35%|███▍      | 1740/5000 [8:04:09<14:21:45, 15.86s/it]                                                        {'loss': 20.1142, 'grad_norm': 86.5, 'learning_rate': 5.433655755954346e-05, 'epoch': 2.21}
 35%|███▍      | 1740/5000 [8:04:09<14:21:45, 15.86s/it] 35%|███▍      | 1741/5000 [8:04:24<14:02:30, 15.51s/it]                                                        {'loss': 20.3213, 'grad_norm': 15.25, 'learning_rate': 5.431725828070265e-05, 'epoch': 2.21}
 35%|███▍      | 1741/5000 [8:04:24<14:02:30, 15.51s/it] 35%|███▍      | 1742/5000 [8:04:38<13:42:34, 15.15s/it]                                                        {'loss': 19.9871, 'grad_norm': 18.125, 'learning_rate': 5.429795055183676e-05, 'epoch': 2.21}
 35%|███▍      | 1742/5000 [8:04:38<13:42:34, 15.15s/it] 35%|███▍      | 1743/5000 [8:04:53<13:26:21, 14.85s/it]                                                        {'loss': 18.9154, 'grad_norm': 25.5, 'learning_rate': 5.427863438139168e-05, 'epoch': 2.21}
 35%|███▍      | 1743/5000 [8:04:53<13:26:21, 14.85s/it] 35%|███▍      | 1744/5000 [8:05:10<14:02:25, 15.52s/it]                                                        {'loss': 19.6739, 'grad_norm': 14.75, 'learning_rate': 5.425930977781695e-05, 'epoch': 2.21}
 35%|███▍      | 1744/5000 [8:05:10<14:02:25, 15.52s/it] 35%|███▍      | 1745/5000 [8:05:25<13:51:47, 15.33s/it]                                                        {'loss': 20.6073, 'grad_norm': 20.125, 'learning_rate': 5.423997674956581e-05, 'epoch': 2.22}
 35%|███▍      | 1745/5000 [8:05:25<13:51:47, 15.33s/it] 35%|███▍      | 1746/5000 [8:05:42<14:32:10, 16.08s/it]                                                        {'loss': 21.0674, 'grad_norm': 39.25, 'learning_rate': 5.4220635305095165e-05, 'epoch': 2.22}
 35%|███▍      | 1746/5000 [8:05:42<14:32:10, 16.08s/it] 35%|███▍      | 1747/5000 [8:06:05<16:21:28, 18.10s/it]                                                        {'loss': 20.273, 'grad_norm': 16.5, 'learning_rate': 5.420128545286564e-05, 'epoch': 2.22}
 35%|███▍      | 1747/5000 [8:06:05<16:21:28, 18.10s/it] 35%|███▍      | 1748/5000 [8:06:21<15:47:15, 17.48s/it]                                                        {'loss': 20.6607, 'grad_norm': 109.0, 'learning_rate': 5.418192720134151e-05, 'epoch': 2.22}
 35%|███▍      | 1748/5000 [8:06:21<15:47:15, 17.48s/it] 35%|███▍      | 1749/5000 [8:06:37<15:13:43, 16.86s/it]                                                        {'loss': 20.7221, 'grad_norm': 15.8125, 'learning_rate': 5.4162560558990726e-05, 'epoch': 2.22}
 35%|███▍      | 1749/5000 [8:06:37<15:13:43, 16.86s/it] 35%|███▌      | 1750/5000 [8:06:51<14:35:03, 16.16s/it]                                                        {'loss': 21.2207, 'grad_norm': 30.125, 'learning_rate': 5.414318553428494e-05, 'epoch': 2.22}
 35%|███▌      | 1750/5000 [8:06:51<14:35:03, 16.16s/it] 35%|███▌      | 1751/5000 [8:07:04<13:48:31, 15.30s/it]                                                        {'loss': 21.1006, 'grad_norm': 19.25, 'learning_rate': 5.412380213569941e-05, 'epoch': 2.22}
 35%|███▌      | 1751/5000 [8:07:04<13:48:31, 15.30s/it] 35%|███▌      | 1752/5000 [8:07:17<13:09:13, 14.58s/it]                                                        {'loss': 21.4215, 'grad_norm': 19.125, 'learning_rate': 5.410441037171313e-05, 'epoch': 2.22}
 35%|███▌      | 1752/5000 [8:07:17<13:09:13, 14.58s/it] 35%|███▌      | 1753/5000 [8:07:35<14:02:05, 15.56s/it]                                                        {'loss': 19.0615, 'grad_norm': 15.4375, 'learning_rate': 5.408501025080868e-05, 'epoch': 2.23}
 35%|███▌      | 1753/5000 [8:07:35<14:02:05, 15.56s/it] 35%|███▌      | 1754/5000 [8:07:55<15:09:27, 16.81s/it]                                                        {'loss': 19.1954, 'grad_norm': 19.125, 'learning_rate': 5.4065601781472355e-05, 'epoch': 2.23}
 35%|███▌      | 1754/5000 [8:07:55<15:09:27, 16.81s/it] 35%|███▌      | 1755/5000 [8:08:08<14:00:34, 15.54s/it]                                                        {'loss': 21.7143, 'grad_norm': 30.25, 'learning_rate': 5.404618497219408e-05, 'epoch': 2.23}
 35%|███▌      | 1755/5000 [8:08:08<14:00:34, 15.54s/it] 35%|███▌      | 1756/5000 [8:08:26<14:51:49, 16.49s/it]                                                        {'loss': 18.9438, 'grad_norm': 20.25, 'learning_rate': 5.402675983146741e-05, 'epoch': 2.23}
 35%|███▌      | 1756/5000 [8:08:26<14:51:49, 16.49s/it] 35%|███▌      | 1757/5000 [8:08:40<14:01:30, 15.57s/it]                                                        {'loss': 20.1464, 'grad_norm': 83.0, 'learning_rate': 5.400732636778957e-05, 'epoch': 2.23}
 35%|███▌      | 1757/5000 [8:08:40<14:01:30, 15.57s/it] 35%|███▌      | 1758/5000 [8:08:55<14:03:05, 15.60s/it]                                                        {'loss': 20.8901, 'grad_norm': 62.5, 'learning_rate': 5.398788458966143e-05, 'epoch': 2.23}
 35%|███▌      | 1758/5000 [8:08:55<14:03:05, 15.60s/it] 35%|███▌      | 1759/5000 [8:09:12<14:14:54, 15.83s/it]                                                        {'loss': 19.4734, 'grad_norm': 16.0, 'learning_rate': 5.396843450558747e-05, 'epoch': 2.23}
 35%|███▌      | 1759/5000 [8:09:12<14:14:54, 15.83s/it] 35%|███▌      | 1760/5000 [8:09:28<14:28:50, 16.09s/it]                                                        {'loss': 19.7024, 'grad_norm': 16.625, 'learning_rate': 5.394897612407582e-05, 'epoch': 2.23}
 35%|███▌      | 1760/5000 [8:09:28<14:28:50, 16.09s/it] 35%|███▌      | 1761/5000 [8:10:02<19:13:11, 21.36s/it]                                                        {'loss': 22.5351, 'grad_norm': 82.0, 'learning_rate': 5.392950945363824e-05, 'epoch': 2.24}
 35%|███▌      | 1761/5000 [8:10:02<19:13:11, 21.36s/it] 35%|███▌      | 1762/5000 [8:10:17<17:35:54, 19.57s/it]                                                        {'loss': 20.478, 'grad_norm': 152.0, 'learning_rate': 5.391003450279012e-05, 'epoch': 2.24}
 35%|███▌      | 1762/5000 [8:10:17<17:35:54, 19.57s/it] 35%|███▌      | 1763/5000 [8:10:31<16:04:18, 17.87s/it]                                                        {'loss': 23.7437, 'grad_norm': 228.0, 'learning_rate': 5.389055128005045e-05, 'epoch': 2.24}
 35%|███▌      | 1763/5000 [8:10:31<16:04:18, 17.87s/it] 35%|███▌      | 1764/5000 [8:10:46<15:08:59, 16.85s/it]                                                        {'loss': 20.543, 'grad_norm': 21.375, 'learning_rate': 5.387105979394186e-05, 'epoch': 2.24}
 35%|███▌      | 1764/5000 [8:10:46<15:08:59, 16.85s/it] 35%|███▌      | 1765/5000 [8:11:00<14:21:26, 15.98s/it]                                                        {'loss': 21.2465, 'grad_norm': 24.375, 'learning_rate': 5.385156005299058e-05, 'epoch': 2.24}
 35%|███▌      | 1765/5000 [8:11:00<14:21:26, 15.98s/it] 35%|███▌      | 1766/5000 [8:11:25<16:54:45, 18.83s/it]                                                        {'loss': 19.2123, 'grad_norm': 32.5, 'learning_rate': 5.3832052065726475e-05, 'epoch': 2.24}
 35%|███▌      | 1766/5000 [8:11:25<16:54:45, 18.83s/it] 35%|███▌      | 1767/5000 [8:11:39<15:33:59, 17.33s/it]                                                        {'loss': 21.6167, 'grad_norm': 45.5, 'learning_rate': 5.381253584068299e-05, 'epoch': 2.24}
 35%|███▌      | 1767/5000 [8:11:39<15:33:59, 17.33s/it] 35%|███▌      | 1768/5000 [8:11:53<14:45:43, 16.44s/it]                                                        {'loss': 22.4067, 'grad_norm': 39.5, 'learning_rate': 5.3793011386397194e-05, 'epoch': 2.25}
 35%|███▌      | 1768/5000 [8:11:53<14:45:43, 16.44s/it] 35%|███▌      | 1769/5000 [8:12:12<15:19:56, 17.08s/it]                                                        {'loss': 19.0966, 'grad_norm': 17.625, 'learning_rate': 5.3773478711409734e-05, 'epoch': 2.25}
 35%|███▌      | 1769/5000 [8:12:12<15:19:56, 17.08s/it] 35%|███▌      | 1770/5000 [8:12:36<17:07:49, 19.09s/it]                                                        {'loss': 19.9299, 'grad_norm': 31.625, 'learning_rate': 5.375393782426487e-05, 'epoch': 2.25}
 35%|███▌      | 1770/5000 [8:12:36<17:07:49, 19.09s/it] 35%|███▌      | 1771/5000 [8:12:49<15:36:24, 17.40s/it]                                                        {'loss': 20.4949, 'grad_norm': 46.5, 'learning_rate': 5.373438873351047e-05, 'epoch': 2.25}
 35%|███▌      | 1771/5000 [8:12:49<15:36:24, 17.40s/it] 35%|███▌      | 1772/5000 [8:13:13<17:23:19, 19.39s/it]                                                        {'loss': 20.0375, 'grad_norm': 18.125, 'learning_rate': 5.371483144769794e-05, 'epoch': 2.25}
 35%|███▌      | 1772/5000 [8:13:13<17:23:19, 19.39s/it] 35%|███▌      | 1773/5000 [8:13:28<16:04:05, 17.93s/it]                                                        {'loss': 20.8815, 'grad_norm': 23.375, 'learning_rate': 5.369526597538232e-05, 'epoch': 2.25}
 35%|███▌      | 1773/5000 [8:13:28<16:04:05, 17.93s/it] 35%|███▌      | 1774/5000 [8:13:47<16:17:30, 18.18s/it]                                                        {'loss': 18.8919, 'grad_norm': 13.25, 'learning_rate': 5.367569232512221e-05, 'epoch': 2.25}
 35%|███▌      | 1774/5000 [8:13:47<16:17:30, 18.18s/it] 36%|███▌      | 1775/5000 [8:14:00<14:53:25, 16.62s/it]                                                        {'loss': 22.3066, 'grad_norm': 118.5, 'learning_rate': 5.3656110505479776e-05, 'epoch': 2.25}
 36%|███▌      | 1775/5000 [8:14:00<14:53:25, 16.62s/it] 36%|███▌      | 1776/5000 [8:14:17<15:04:15, 16.83s/it]                                                        {'loss': 19.299, 'grad_norm': 17.25, 'learning_rate': 5.363652052502079e-05, 'epoch': 2.26}
 36%|███▌      | 1776/5000 [8:14:17<15:04:15, 16.83s/it] 36%|███▌      | 1777/5000 [8:14:33<14:51:32, 16.60s/it]                                                        {'loss': 18.4859, 'grad_norm': 11.5625, 'learning_rate': 5.361692239231456e-05, 'epoch': 2.26}
 36%|███▌      | 1777/5000 [8:14:33<14:51:32, 16.60s/it] 36%|███▌      | 1778/5000 [8:14:47<14:14:15, 15.91s/it]                                                        {'loss': 21.9179, 'grad_norm': 24.875, 'learning_rate': 5.359731611593399e-05, 'epoch': 2.26}
 36%|███▌      | 1778/5000 [8:14:47<14:14:15, 15.91s/it] 36%|███▌      | 1779/5000 [8:15:11<16:21:36, 18.29s/it]                                                        {'loss': 26.7896, 'grad_norm': 66.0, 'learning_rate': 5.357770170445552e-05, 'epoch': 2.26}
 36%|███▌      | 1779/5000 [8:15:11<16:21:36, 18.29s/it] 36%|███▌      | 1780/5000 [8:15:28<15:52:35, 17.75s/it]                                                        {'loss': 19.4819, 'grad_norm': 17.25, 'learning_rate': 5.3558079166459166e-05, 'epoch': 2.26}
 36%|███▌      | 1780/5000 [8:15:28<15:52:35, 17.75s/it] 36%|███▌      | 1781/5000 [8:15:41<14:36:05, 16.33s/it]                                                        {'loss': 21.3319, 'grad_norm': 18.625, 'learning_rate': 5.353844851052848e-05, 'epoch': 2.26}
 36%|███▌      | 1781/5000 [8:15:41<14:36:05, 16.33s/it] 36%|███▌      | 1782/5000 [8:15:56<14:14:32, 15.93s/it]                                                        {'loss': 19.8991, 'grad_norm': 26.875, 'learning_rate': 5.351880974525059e-05, 'epoch': 2.26}
 36%|███▌      | 1782/5000 [8:15:56<14:14:32, 15.93s/it] 36%|███▌      | 1783/5000 [8:16:10<13:55:34, 15.58s/it]                                                        {'loss': 20.6225, 'grad_norm': 23.125, 'learning_rate': 5.3499162879216175e-05, 'epoch': 2.26}
 36%|███▌      | 1783/5000 [8:16:10<13:55:34, 15.58s/it] 36%|███▌      | 1784/5000 [8:16:27<14:09:55, 15.86s/it]                                                        {'loss': 18.6297, 'grad_norm': 19.75, 'learning_rate': 5.34795079210194e-05, 'epoch': 2.27}
 36%|███▌      | 1784/5000 [8:16:27<14:09:55, 15.86s/it] 36%|███▌      | 1785/5000 [8:16:53<16:51:24, 18.88s/it]                                                        {'loss': 18.6139, 'grad_norm': 12.9375, 'learning_rate': 5.345984487925805e-05, 'epoch': 2.27}
 36%|███▌      | 1785/5000 [8:16:53<16:51:24, 18.88s/it] 36%|███▌      | 1786/5000 [8:17:09<16:05:42, 18.03s/it]                                                        {'loss': 19.9019, 'grad_norm': 45.5, 'learning_rate': 5.344017376253339e-05, 'epoch': 2.27}
 36%|███▌      | 1786/5000 [8:17:09<16:05:42, 18.03s/it] 36%|███▌      | 1787/5000 [8:17:25<15:37:49, 17.51s/it]                                                        {'loss': 19.9214, 'grad_norm': 41.0, 'learning_rate': 5.3420494579450236e-05, 'epoch': 2.27}
 36%|███▌      | 1787/5000 [8:17:25<15:37:49, 17.51s/it] 36%|███▌      | 1788/5000 [8:17:50<17:36:36, 19.74s/it]                                                        {'loss': 20.5837, 'grad_norm': 186.0, 'learning_rate': 5.340080733861693e-05, 'epoch': 2.27}
 36%|███▌      | 1788/5000 [8:17:50<17:36:36, 19.74s/it] 36%|███▌      | 1789/5000 [8:18:10<17:34:16, 19.70s/it]                                                        {'loss': 20.4157, 'grad_norm': 81.0, 'learning_rate': 5.338111204864535e-05, 'epoch': 2.27}
 36%|███▌      | 1789/5000 [8:18:10<17:34:16, 19.70s/it] 36%|███▌      | 1790/5000 [8:18:24<16:00:24, 17.95s/it]                                                        {'loss': 24.6471, 'grad_norm': 138.0, 'learning_rate': 5.3361408718150876e-05, 'epoch': 2.27}
 36%|███▌      | 1790/5000 [8:18:24<16:00:24, 17.95s/it] 36%|███▌      | 1791/5000 [8:18:41<15:47:00, 17.71s/it]                                                        {'loss': 19.0941, 'grad_norm': 25.125, 'learning_rate': 5.334169735575241e-05, 'epoch': 2.27}
 36%|███▌      | 1791/5000 [8:18:41<15:47:00, 17.71s/it] 36%|███▌      | 1792/5000 [8:18:55<14:49:36, 16.64s/it]                                                        {'loss': 20.4002, 'grad_norm': 25.5, 'learning_rate': 5.332197797007237e-05, 'epoch': 2.28}
 36%|███▌      | 1792/5000 [8:18:55<14:49:36, 16.64s/it] 36%|███▌      | 1793/5000 [8:19:14<15:36:19, 17.52s/it]                                                        {'loss': 18.3595, 'grad_norm': 14.5625, 'learning_rate': 5.330225056973671e-05, 'epoch': 2.28}
 36%|███▌      | 1793/5000 [8:19:14<15:36:19, 17.52s/it] 36%|███▌      | 1794/5000 [8:19:31<15:22:20, 17.26s/it]                                                        {'loss': 19.6089, 'grad_norm': 14.6875, 'learning_rate': 5.328251516337483e-05, 'epoch': 2.28}
 36%|███▌      | 1794/5000 [8:19:31<15:22:20, 17.26s/it] 36%|███▌      | 1795/5000 [8:19:46<14:47:06, 16.61s/it]                                                        {'loss': 21.0745, 'grad_norm': 45.25, 'learning_rate': 5.32627717596197e-05, 'epoch': 2.28}
 36%|███▌      | 1795/5000 [8:19:46<14:47:06, 16.61s/it] 36%|███▌      | 1796/5000 [8:20:05<15:17:34, 17.18s/it]                                                        {'loss': 19.8212, 'grad_norm': 25.25, 'learning_rate': 5.324302036710774e-05, 'epoch': 2.28}
 36%|███▌      | 1796/5000 [8:20:05<15:17:34, 17.18s/it] 36%|███▌      | 1797/5000 [8:20:21<15:07:03, 16.99s/it]                                                        {'loss': 22.8743, 'grad_norm': 47.5, 'learning_rate': 5.322326099447887e-05, 'epoch': 2.28}
 36%|███▌      | 1797/5000 [8:20:21<15:07:03, 16.99s/it] 36%|███▌      | 1798/5000 [8:20:36<14:31:37, 16.33s/it]                                                        {'loss': 22.5367, 'grad_norm': 129.0, 'learning_rate': 5.3203493650376536e-05, 'epoch': 2.28}
 36%|███▌      | 1798/5000 [8:20:36<14:31:37, 16.33s/it] 36%|███▌      | 1799/5000 [8:21:02<17:04:04, 19.20s/it]                                                        {'loss': 19.374, 'grad_norm': 18.125, 'learning_rate': 5.318371834344763e-05, 'epoch': 2.28}
 36%|███▌      | 1799/5000 [8:21:02<17:04:04, 19.20s/it] 36%|███▌      | 1800/5000 [8:21:20<16:44:03, 18.83s/it]                                                        {'loss': 19.0446, 'grad_norm': 15.75, 'learning_rate': 5.316393508234253e-05, 'epoch': 2.29}
 36%|███▌      | 1800/5000 [8:21:20<16:44:03, 18.83s/it] 36%|███▌      | 1801/5000 [8:21:38<16:30:18, 18.57s/it]                                                        {'loss': 19.4532, 'grad_norm': 27.625, 'learning_rate': 5.314414387571513e-05, 'epoch': 2.29}
 36%|███▌      | 1801/5000 [8:21:38<16:30:18, 18.57s/it] 36%|███▌      | 1802/5000 [8:21:56<16:21:11, 18.41s/it]                                                        {'loss': 20.8666, 'grad_norm': 65.0, 'learning_rate': 5.3124344732222764e-05, 'epoch': 2.29}
 36%|███▌      | 1802/5000 [8:21:56<16:21:11, 18.41s/it] 36%|███▌      | 1803/5000 [8:22:21<18:05:45, 20.38s/it]                                                        {'loss': 19.9136, 'grad_norm': 26.375, 'learning_rate': 5.3104537660526254e-05, 'epoch': 2.29}
 36%|███▌      | 1803/5000 [8:22:21<18:05:45, 20.38s/it] 36%|███▌      | 1804/5000 [8:22:35<16:27:09, 18.53s/it]                                                        {'loss': 20.1023, 'grad_norm': 18.0, 'learning_rate': 5.308472266928989e-05, 'epoch': 2.29}
 36%|███▌      | 1804/5000 [8:22:35<16:27:09, 18.53s/it] 36%|███▌      | 1805/5000 [8:22:51<15:50:21, 17.85s/it]                                                        {'loss': 19.7804, 'grad_norm': 18.875, 'learning_rate': 5.30648997671814e-05, 'epoch': 2.29}
 36%|███▌      | 1805/5000 [8:22:51<15:50:21, 17.85s/it] 36%|███▌      | 1806/5000 [8:23:04<14:35:08, 16.44s/it]                                                        {'loss': 21.3635, 'grad_norm': 28.875, 'learning_rate': 5.304506896287204e-05, 'epoch': 2.29}
 36%|███▌      | 1806/5000 [8:23:04<14:35:08, 16.44s/it] 36%|███▌      | 1807/5000 [8:23:21<14:42:42, 16.59s/it]                                                        {'loss': 31.0742, 'grad_norm': 81.0, 'learning_rate': 5.302523026503643e-05, 'epoch': 2.29}
 36%|███▌      | 1807/5000 [8:23:21<14:42:42, 16.59s/it] 36%|███▌      | 1808/5000 [8:23:37<14:22:31, 16.21s/it]                                                        {'loss': 21.1971, 'grad_norm': 39.5, 'learning_rate': 5.300538368235272e-05, 'epoch': 2.3}
 36%|███▌      | 1808/5000 [8:23:37<14:22:31, 16.21s/it] 36%|███▌      | 1809/5000 [8:24:03<17:02:38, 19.23s/it]                                                        {'loss': 19.6674, 'grad_norm': 18.125, 'learning_rate': 5.298552922350247e-05, 'epoch': 2.3}
 36%|███▌      | 1809/5000 [8:24:03<17:02:38, 19.23s/it] 36%|███▌      | 1810/5000 [8:24:26<18:07:53, 20.46s/it]                                                        {'loss': 22.3293, 'grad_norm': 200.0, 'learning_rate': 5.296566689717069e-05, 'epoch': 2.3}
 36%|███▌      | 1810/5000 [8:24:26<18:07:53, 20.46s/it] 36%|███▌      | 1811/5000 [8:24:51<19:14:54, 21.73s/it]                                                        {'loss': 21.0647, 'grad_norm': 328.0, 'learning_rate': 5.294579671204584e-05, 'epoch': 2.3}
 36%|███▌      | 1811/5000 [8:24:51<19:14:54, 21.73s/it] 36%|███▌      | 1812/5000 [8:25:07<17:42:31, 20.00s/it]                                                        {'loss': 19.0944, 'grad_norm': 14.375, 'learning_rate': 5.292591867681983e-05, 'epoch': 2.3}
 36%|███▌      | 1812/5000 [8:25:07<17:42:31, 20.00s/it] 36%|███▋      | 1813/5000 [8:25:22<16:18:29, 18.42s/it]                                                        {'loss': 23.5036, 'grad_norm': 47.25, 'learning_rate': 5.290603280018796e-05, 'epoch': 2.3}
 36%|███▋      | 1813/5000 [8:25:22<16:18:29, 18.42s/it] 36%|███▋      | 1814/5000 [8:25:34<14:45:40, 16.68s/it]                                                        {'loss': 22.3407, 'grad_norm': 34.5, 'learning_rate': 5.2886139090849e-05, 'epoch': 2.3}
 36%|███▋      | 1814/5000 [8:25:34<14:45:40, 16.68s/it] 36%|███▋      | 1815/5000 [8:25:51<14:47:44, 16.72s/it]                                                        {'loss': 19.0473, 'grad_norm': 37.5, 'learning_rate': 5.286623755750514e-05, 'epoch': 2.3}
 36%|███▋      | 1815/5000 [8:25:51<14:47:44, 16.72s/it] 36%|███▋      | 1816/5000 [8:26:15<16:34:20, 18.74s/it]                                                        {'loss': 29.7124, 'grad_norm': 564.0, 'learning_rate': 5.284632820886198e-05, 'epoch': 2.31}
 36%|███▋      | 1816/5000 [8:26:15<16:34:20, 18.74s/it] 36%|███▋      | 1817/5000 [8:26:41<18:40:38, 21.12s/it]                                                        {'loss': 18.2186, 'grad_norm': 30.5, 'learning_rate': 5.282641105362855e-05, 'epoch': 2.31}
 36%|███▋      | 1817/5000 [8:26:41<18:40:38, 21.12s/it] 36%|███▋      | 1818/5000 [8:26:55<16:49:24, 19.03s/it]                                                        {'loss': 20.3781, 'grad_norm': 23.625, 'learning_rate': 5.280648610051729e-05, 'epoch': 2.31}
 36%|███▋      | 1818/5000 [8:26:55<16:49:24, 19.03s/it] 36%|███▋      | 1819/5000 [8:27:21<18:36:43, 21.06s/it]                                                        {'loss': 20.356, 'grad_norm': 31.375, 'learning_rate': 5.2786553358244045e-05, 'epoch': 2.31}
 36%|███▋      | 1819/5000 [8:27:21<18:36:43, 21.06s/it] 36%|███▋      | 1820/5000 [8:27:38<17:27:38, 19.77s/it]                                                        {'loss': 21.1758, 'grad_norm': 19.875, 'learning_rate': 5.276661283552809e-05, 'epoch': 2.31}
 36%|███▋      | 1820/5000 [8:27:38<17:27:38, 19.77s/it] 36%|███▋      | 1821/5000 [8:28:00<18:08:27, 20.54s/it]                                                        {'loss': 20.9918, 'grad_norm': 55.25, 'learning_rate': 5.274666454109206e-05, 'epoch': 2.31}
 36%|███▋      | 1821/5000 [8:28:00<18:08:27, 20.54s/it] 36%|███▋      | 1822/5000 [8:28:14<16:23:33, 18.57s/it]                                                        {'loss': 19.9928, 'grad_norm': 23.75, 'learning_rate': 5.272670848366205e-05, 'epoch': 2.31}
 36%|███▋      | 1822/5000 [8:28:14<16:23:33, 18.57s/it] 36%|███▋      | 1823/5000 [8:28:26<14:37:48, 16.58s/it]                                                        {'loss': 21.1873, 'grad_norm': 17.0, 'learning_rate': 5.270674467196748e-05, 'epoch': 2.31}
 36%|███▋      | 1823/5000 [8:28:26<14:37:48, 16.58s/it] 36%|███▋      | 1824/5000 [8:28:42<14:18:50, 16.22s/it]                                                        {'loss': 18.7699, 'grad_norm': 17.625, 'learning_rate': 5.268677311474123e-05, 'epoch': 2.32}
 36%|███▋      | 1824/5000 [8:28:42<14:18:50, 16.22s/it] 36%|███▋      | 1825/5000 [8:28:59<14:36:48, 16.57s/it]                                                        {'loss': 19.7559, 'grad_norm': 21.0, 'learning_rate': 5.266679382071953e-05, 'epoch': 2.32}
 36%|███▋      | 1825/5000 [8:28:59<14:36:48, 16.57s/it] 37%|███▋      | 1826/5000 [8:29:19<15:26:34, 17.52s/it]                                                        {'loss': 18.6447, 'grad_norm': 14.25, 'learning_rate': 5.264680679864199e-05, 'epoch': 2.32}
 37%|███▋      | 1826/5000 [8:29:19<15:26:34, 17.52s/it] 37%|███▋      | 1827/5000 [8:29:33<14:28:55, 16.43s/it]                                                        {'loss': 26.0996, 'grad_norm': 608.0, 'learning_rate': 5.262681205725163e-05, 'epoch': 2.32}
 37%|███▋      | 1827/5000 [8:29:33<14:28:55, 16.43s/it] 37%|███▋      | 1828/5000 [8:29:57<16:30:29, 18.74s/it]                                                        {'loss': 22.4064, 'grad_norm': 354.0, 'learning_rate': 5.26068096052948e-05, 'epoch': 2.32}
 37%|███▋      | 1828/5000 [8:29:57<16:30:29, 18.74s/it] 37%|███▋      | 1829/5000 [8:30:13<15:56:18, 18.09s/it]                                                        {'loss': 20.3943, 'grad_norm': 55.75, 'learning_rate': 5.25867994515213e-05, 'epoch': 2.32}
 37%|███▋      | 1829/5000 [8:30:13<15:56:18, 18.09s/it] 37%|███▋      | 1830/5000 [8:30:30<15:34:19, 17.68s/it]                                                        {'loss': 21.2593, 'grad_norm': 26.5, 'learning_rate': 5.2566781604684205e-05, 'epoch': 2.32}
 37%|███▋      | 1830/5000 [8:30:30<15:34:19, 17.68s/it] 37%|███▋      | 1831/5000 [8:30:44<14:33:15, 16.53s/it]                                                        {'loss': 19.7759, 'grad_norm': 16.625, 'learning_rate': 5.2546756073540033e-05, 'epoch': 2.33}
 37%|███▋      | 1831/5000 [8:30:44<14:33:15, 16.53s/it] 37%|███▋      | 1832/5000 [8:31:02<14:50:58, 16.87s/it]                                                        {'loss': 19.1483, 'grad_norm': 28.375, 'learning_rate': 5.252672286684862e-05, 'epoch': 2.33}
 37%|███▋      | 1832/5000 [8:31:02<14:50:58, 16.87s/it] 37%|███▋      | 1833/5000 [8:31:17<14:24:52, 16.39s/it]                                                        {'loss': 20.8231, 'grad_norm': 30.25, 'learning_rate': 5.250668199337317e-05, 'epoch': 2.33}
 37%|███▋      | 1833/5000 [8:31:17<14:24:52, 16.39s/it] 37%|███▋      | 1834/5000 [8:31:34<14:38:48, 16.65s/it]                                                        {'loss': 21.8493, 'grad_norm': 286.0, 'learning_rate': 5.248663346188022e-05, 'epoch': 2.33}
 37%|███▋      | 1834/5000 [8:31:34<14:38:48, 16.65s/it] 37%|███▋      | 1835/5000 [8:31:51<14:43:38, 16.75s/it]                                                        {'loss': 20.9139, 'grad_norm': 42.0, 'learning_rate': 5.2466577281139716e-05, 'epoch': 2.33}
 37%|███▋      | 1835/5000 [8:31:51<14:43:38, 16.75s/it] 37%|███▋      | 1836/5000 [8:32:07<14:30:19, 16.50s/it]                                                        {'loss': 19.4115, 'grad_norm': 21.625, 'learning_rate': 5.24465134599249e-05, 'epoch': 2.33}
 37%|███▋      | 1836/5000 [8:32:07<14:30:19, 16.50s/it] 37%|███▋      | 1837/5000 [8:32:24<14:39:59, 16.69s/it]                                                        {'loss': 19.7115, 'grad_norm': 18.75, 'learning_rate': 5.242644200701236e-05, 'epoch': 2.33}
 37%|███▋      | 1837/5000 [8:32:24<14:39:59, 16.69s/it] 37%|███▋      | 1838/5000 [8:32:40<14:29:15, 16.49s/it]                                                        {'loss': 20.0377, 'grad_norm': 16.375, 'learning_rate': 5.240636293118204e-05, 'epoch': 2.33}
 37%|███▋      | 1838/5000 [8:32:40<14:29:15, 16.49s/it] 37%|███▋      | 1839/5000 [8:32:54<13:43:16, 15.63s/it]                                                        {'loss': 19.7904, 'grad_norm': 31.625, 'learning_rate': 5.2386276241217214e-05, 'epoch': 2.34}
 37%|███▋      | 1839/5000 [8:32:54<13:43:16, 15.63s/it] 37%|███▋      | 1840/5000 [8:33:09<13:40:23, 15.58s/it]                                                        {'loss': 18.5743, 'grad_norm': 46.75, 'learning_rate': 5.236618194590447e-05, 'epoch': 2.34}
 37%|███▋      | 1840/5000 [8:33:09<13:40:23, 15.58s/it] 37%|███▋      | 1841/5000 [8:33:26<13:56:04, 15.88s/it]                                                        {'loss': 18.7332, 'grad_norm': 83.0, 'learning_rate': 5.234608005403374e-05, 'epoch': 2.34}
 37%|███▋      | 1841/5000 [8:33:26<13:56:04, 15.88s/it] 37%|███▋      | 1842/5000 [8:33:41<13:45:43, 15.69s/it]                                                        {'loss': 19.6066, 'grad_norm': 40.25, 'learning_rate': 5.232597057439828e-05, 'epoch': 2.34}
 37%|███▋      | 1842/5000 [8:33:41<13:45:43, 15.69s/it] 37%|███▋      | 1843/5000 [8:34:00<14:43:56, 16.80s/it]                                                        {'loss': 21.5681, 'grad_norm': 31.375, 'learning_rate': 5.230585351579466e-05, 'epoch': 2.34}
 37%|███▋      | 1843/5000 [8:34:00<14:43:56, 16.80s/it] 37%|███▋      | 1844/5000 [8:34:27<17:21:15, 19.80s/it]                                                        {'loss': 17.8754, 'grad_norm': 10.25, 'learning_rate': 5.228572888702275e-05, 'epoch': 2.34}
 37%|███▋      | 1844/5000 [8:34:27<17:21:15, 19.80s/it] 37%|███▋      | 1845/5000 [8:34:47<17:20:30, 19.79s/it]                                                        {'loss': 19.5515, 'grad_norm': 39.25, 'learning_rate': 5.226559669688577e-05, 'epoch': 2.34}
 37%|███▋      | 1845/5000 [8:34:47<17:20:30, 19.79s/it] 37%|███▋      | 1846/5000 [8:35:02<15:57:31, 18.22s/it]                                                        {'loss': 19.8721, 'grad_norm': 19.0, 'learning_rate': 5.22454569541902e-05, 'epoch': 2.34}
 37%|███▋      | 1846/5000 [8:35:02<15:57:31, 18.22s/it] 37%|███▋      | 1847/5000 [8:35:18<15:23:00, 17.56s/it]                                                        {'loss': 18.3932, 'grad_norm': 12.5, 'learning_rate': 5.222530966774587e-05, 'epoch': 2.35}
 37%|███▋      | 1847/5000 [8:35:18<15:23:00, 17.56s/it] 37%|███▋      | 1848/5000 [8:35:33<14:49:42, 16.94s/it]                                                        {'loss': 20.9128, 'grad_norm': 109.5, 'learning_rate': 5.220515484636587e-05, 'epoch': 2.35}
 37%|███▋      | 1848/5000 [8:35:33<14:49:42, 16.94s/it] 37%|███▋      | 1849/5000 [8:35:48<14:15:19, 16.29s/it]                                                        {'loss': 19.9156, 'grad_norm': 27.75, 'learning_rate': 5.21849924988666e-05, 'epoch': 2.35}
 37%|███▋      | 1849/5000 [8:35:48<14:15:19, 16.29s/it] 37%|███▋      | 1850/5000 [8:36:03<13:57:58, 15.96s/it]                                                        {'loss': 19.0628, 'grad_norm': 33.5, 'learning_rate': 5.216482263406778e-05, 'epoch': 2.35}
 37%|███▋      | 1850/5000 [8:36:03<13:57:58, 15.96s/it] 37%|███▋      | 1851/5000 [8:36:19<13:50:47, 15.83s/it]                                                        {'loss': 19.2398, 'grad_norm': 62.0, 'learning_rate': 5.2144645260792376e-05, 'epoch': 2.35}
 37%|███▋      | 1851/5000 [8:36:19<13:50:47, 15.83s/it] 37%|███▋      | 1852/5000 [8:36:33<13:34:24, 15.52s/it]                                                        {'loss': 19.9459, 'grad_norm': 27.25, 'learning_rate': 5.212446038786666e-05, 'epoch': 2.35}
 37%|███▋      | 1852/5000 [8:36:33<13:34:24, 15.52s/it] 37%|███▋      | 1853/5000 [8:36:51<14:13:33, 16.27s/it]                                                        {'loss': 21.4078, 'grad_norm': 56.75, 'learning_rate': 5.210426802412018e-05, 'epoch': 2.35}
 37%|███▋      | 1853/5000 [8:36:51<14:13:33, 16.27s/it] 37%|███▋      | 1854/5000 [8:37:04<13:20:58, 15.28s/it]                                                        {'loss': 20.7848, 'grad_norm': 38.0, 'learning_rate': 5.2084068178385766e-05, 'epoch': 2.35}
 37%|███▋      | 1854/5000 [8:37:04<13:20:58, 15.28s/it] 37%|███▋      | 1855/5000 [8:37:22<14:03:11, 16.09s/it]                                                        {'loss': 19.4215, 'grad_norm': 30.125, 'learning_rate': 5.2063860859499516e-05, 'epoch': 2.36}
 37%|███▋      | 1855/5000 [8:37:22<14:03:11, 16.09s/it] 37%|███▋      | 1856/5000 [8:37:40<14:24:52, 16.51s/it]                                                        {'loss': 19.821, 'grad_norm': 26.625, 'learning_rate': 5.204364607630079e-05, 'epoch': 2.36}
 37%|███▋      | 1856/5000 [8:37:40<14:24:52, 16.51s/it] 37%|███▋      | 1857/5000 [8:38:13<18:42:55, 21.44s/it]                                                        {'loss': 20.1823, 'grad_norm': 15.25, 'learning_rate': 5.2023423837632234e-05, 'epoch': 2.36}
 37%|███▋      | 1857/5000 [8:38:13<18:42:55, 21.44s/it] 37%|███▋      | 1858/5000 [8:38:39<19:51:53, 22.76s/it]                                                        {'loss': 18.8939, 'grad_norm': 57.0, 'learning_rate': 5.200319415233974e-05, 'epoch': 2.36}
 37%|███▋      | 1858/5000 [8:38:39<19:51:53, 22.76s/it] 37%|███▋      | 1859/5000 [8:38:52<17:28:27, 20.03s/it]                                                        {'loss': 21.7417, 'grad_norm': 24.75, 'learning_rate': 5.1982957029272445e-05, 'epoch': 2.36}
 37%|███▋      | 1859/5000 [8:38:52<17:28:27, 20.03s/it] 37%|███▋      | 1860/5000 [8:39:06<15:53:20, 18.22s/it]                                                        {'loss': 17.7228, 'grad_norm': 17.5, 'learning_rate': 5.1962712477282773e-05, 'epoch': 2.36}
 37%|███▋      | 1860/5000 [8:39:06<15:53:20, 18.22s/it] 37%|███▋      | 1861/5000 [8:39:21<14:56:46, 17.14s/it]                                                        {'loss': 21.7795, 'grad_norm': 22.0, 'learning_rate': 5.1942460505226364e-05, 'epoch': 2.36}
 37%|███▋      | 1861/5000 [8:39:21<14:56:46, 17.14s/it] 37%|███▋      | 1862/5000 [8:39:40<15:25:20, 17.69s/it]                                                        {'loss': 19.8988, 'grad_norm': 16.625, 'learning_rate': 5.1922201121962124e-05, 'epoch': 2.36}
 37%|███▋      | 1862/5000 [8:39:40<15:25:20, 17.69s/it] 37%|███▋      | 1863/5000 [8:40:04<17:11:32, 19.73s/it]                                                        {'loss': 20.0632, 'grad_norm': 40.5, 'learning_rate': 5.1901934336352194e-05, 'epoch': 2.37}
 37%|███▋      | 1863/5000 [8:40:04<17:11:32, 19.73s/it] 37%|███▋      | 1864/5000 [8:40:17<15:22:29, 17.65s/it]                                                        {'loss': 19.1744, 'grad_norm': 16.75, 'learning_rate': 5.188166015726195e-05, 'epoch': 2.37}
 37%|███▋      | 1864/5000 [8:40:17<15:22:29, 17.65s/it] 37%|███▋      | 1865/5000 [8:40:31<14:28:23, 16.62s/it]                                                        {'loss': 18.6364, 'grad_norm': 28.0, 'learning_rate': 5.186137859356003e-05, 'epoch': 2.37}
 37%|███▋      | 1865/5000 [8:40:31<14:28:23, 16.62s/it] 37%|███▋      | 1866/5000 [8:40:44<13:23:57, 15.39s/it]                                                        {'loss': 20.556, 'grad_norm': 20.125, 'learning_rate': 5.184108965411826e-05, 'epoch': 2.37}
 37%|███▋      | 1866/5000 [8:40:44<13:23:57, 15.39s/it] 37%|███▋      | 1867/5000 [8:40:57<12:46:39, 14.68s/it]                                                        {'loss': 21.8647, 'grad_norm': 29.25, 'learning_rate': 5.18207933478117e-05, 'epoch': 2.37}
 37%|███▋      | 1867/5000 [8:40:57<12:46:39, 14.68s/it] 37%|███▋      | 1868/5000 [8:41:12<12:58:17, 14.91s/it]                                                        {'loss': 21.0381, 'grad_norm': 19.125, 'learning_rate': 5.1800489683518674e-05, 'epoch': 2.37}
 37%|███▋      | 1868/5000 [8:41:12<12:58:17, 14.91s/it] 37%|███▋      | 1869/5000 [8:41:41<16:39:25, 19.15s/it]                                                        {'loss': 18.3492, 'grad_norm': 13.25, 'learning_rate': 5.178017867012067e-05, 'epoch': 2.37}
 37%|███▋      | 1869/5000 [8:41:41<16:39:25, 19.15s/it] 37%|███▋      | 1870/5000 [8:41:57<15:47:10, 18.16s/it]                                                        {'loss': 18.9748, 'grad_norm': 63.75, 'learning_rate': 5.175986031650242e-05, 'epoch': 2.37}
 37%|███▋      | 1870/5000 [8:41:57<15:47:10, 18.16s/it] 37%|███▋      | 1871/5000 [8:42:12<14:51:58, 17.10s/it]                                                        {'loss': 20.3721, 'grad_norm': 22.75, 'learning_rate': 5.173953463155188e-05, 'epoch': 2.38}
 37%|███▋      | 1871/5000 [8:42:12<14:51:58, 17.10s/it] 37%|███▋      | 1872/5000 [8:42:39<17:30:48, 20.16s/it]                                                        {'loss': 18.1346, 'grad_norm': 11.0, 'learning_rate': 5.1719201624160175e-05, 'epoch': 2.38}
 37%|███▋      | 1872/5000 [8:42:39<17:30:48, 20.16s/it] 37%|███▋      | 1873/5000 [8:42:55<16:16:29, 18.74s/it]                                                        {'loss': 20.0467, 'grad_norm': 20.75, 'learning_rate': 5.1698861303221655e-05, 'epoch': 2.38}
 37%|███▋      | 1873/5000 [8:42:55<16:16:29, 18.74s/it] 37%|███▋      | 1874/5000 [8:43:10<15:17:25, 17.61s/it]                                                        {'loss': 19.4954, 'grad_norm': 18.75, 'learning_rate': 5.167851367763388e-05, 'epoch': 2.38}
 37%|███▋      | 1874/5000 [8:43:10<15:17:25, 17.61s/it] 38%|███▊      | 1875/5000 [8:43:34<17:01:40, 19.62s/it]                                                        {'loss': 19.8179, 'grad_norm': 13.125, 'learning_rate': 5.1658158756297576e-05, 'epoch': 2.38}
 38%|███▊      | 1875/5000 [8:43:34<17:01:40, 19.62s/it] 38%|███▊      | 1876/5000 [8:43:46<15:10:01, 17.48s/it]                                                        {'loss': 21.089, 'grad_norm': 41.75, 'learning_rate': 5.1637796548116685e-05, 'epoch': 2.38}
 38%|███▊      | 1876/5000 [8:43:46<15:10:01, 17.48s/it] 38%|███▊      | 1877/5000 [8:44:08<16:10:27, 18.64s/it]                                                        {'loss': 19.204, 'grad_norm': 14.125, 'learning_rate': 5.1617427061998325e-05, 'epoch': 2.38}
 38%|███▊      | 1877/5000 [8:44:08<16:10:27, 18.64s/it] 38%|███▊      | 1878/5000 [8:44:23<15:13:37, 17.56s/it]                                                        {'loss': 20.8378, 'grad_norm': 24.875, 'learning_rate': 5.159705030685281e-05, 'epoch': 2.38}
 38%|███▊      | 1878/5000 [8:44:23<15:13:37, 17.56s/it] 38%|███▊      | 1879/5000 [8:44:38<14:44:58, 17.01s/it]                                                        {'loss': 19.9207, 'grad_norm': 19.0, 'learning_rate': 5.157666629159363e-05, 'epoch': 2.39}
 38%|███▊      | 1879/5000 [8:44:38<14:44:58, 17.01s/it] 38%|███▊      | 1880/5000 [8:44:54<14:14:33, 16.43s/it]                                                        {'loss': 18.7249, 'grad_norm': 36.25, 'learning_rate': 5.155627502513743e-05, 'epoch': 2.39}
 38%|███▊      | 1880/5000 [8:44:54<14:14:33, 16.43s/it] 38%|███▊      | 1881/5000 [8:45:09<13:55:02, 16.06s/it]                                                        {'loss': 21.0483, 'grad_norm': 131.0, 'learning_rate': 5.153587651640404e-05, 'epoch': 2.39}
 38%|███▊      | 1881/5000 [8:45:09<13:55:02, 16.06s/it] 38%|███▊      | 1882/5000 [8:45:26<14:16:06, 16.47s/it]                                                        {'loss': 18.6086, 'grad_norm': 48.25, 'learning_rate': 5.1515470774316486e-05, 'epoch': 2.39}
 38%|███▊      | 1882/5000 [8:45:26<14:16:06, 16.47s/it] 38%|███▊      | 1883/5000 [8:45:41<13:56:36, 16.10s/it]                                                        {'loss': 19.215, 'grad_norm': 21.125, 'learning_rate': 5.1495057807800905e-05, 'epoch': 2.39}
 38%|███▊      | 1883/5000 [8:45:41<13:56:36, 16.10s/it] 38%|███▊      | 1884/5000 [8:45:56<13:29:06, 15.58s/it]                                                        {'loss': 19.6462, 'grad_norm': 22.75, 'learning_rate': 5.1474637625786645e-05, 'epoch': 2.39}
 38%|███▊      | 1884/5000 [8:45:56<13:29:06, 15.58s/it] 38%|███▊      | 1885/5000 [8:46:11<13:28:05, 15.57s/it]                                                        {'loss': 20.1111, 'grad_norm': 21.625, 'learning_rate': 5.145421023720616e-05, 'epoch': 2.39}
 38%|███▊      | 1885/5000 [8:46:11<13:28:05, 15.57s/it] 38%|███▊      | 1886/5000 [8:46:26<13:20:07, 15.42s/it]                                                        {'loss': 20.5221, 'grad_norm': 50.75, 'learning_rate': 5.143377565099511e-05, 'epoch': 2.39}
 38%|███▊      | 1886/5000 [8:46:26<13:20:07, 15.42s/it] 38%|███▊      | 1887/5000 [8:46:42<13:16:28, 15.35s/it]                                                        {'loss': 22.182, 'grad_norm': 22.375, 'learning_rate': 5.1413333876092264e-05, 'epoch': 2.4}
 38%|███▊      | 1887/5000 [8:46:42<13:16:28, 15.35s/it] 38%|███▊      | 1888/5000 [8:47:13<17:29:06, 20.23s/it]                                                        {'loss': 18.7938, 'grad_norm': 17.25, 'learning_rate': 5.1392884921439544e-05, 'epoch': 2.4}
 38%|███▊      | 1888/5000 [8:47:13<17:29:06, 20.23s/it] 38%|███▊      | 1889/5000 [8:47:28<16:06:08, 18.63s/it]                                                        {'loss': 19.2761, 'grad_norm': 34.75, 'learning_rate': 5.1372428795982034e-05, 'epoch': 2.4}
 38%|███▊      | 1889/5000 [8:47:28<16:06:08, 18.63s/it] 38%|███▊      | 1890/5000 [8:47:54<17:59:55, 20.83s/it]                                                        {'loss': 17.5127, 'grad_norm': 10.9375, 'learning_rate': 5.1351965508667927e-05, 'epoch': 2.4}
 38%|███▊      | 1890/5000 [8:47:54<17:59:55, 20.83s/it] 38%|███▊      | 1891/5000 [8:48:09<16:27:12, 19.05s/it]                                                        {'loss': 22.6449, 'grad_norm': 50.5, 'learning_rate': 5.133149506844857e-05, 'epoch': 2.4}
 38%|███▊      | 1891/5000 [8:48:09<16:27:12, 19.05s/it] 38%|███▊      | 1892/5000 [8:48:24<15:18:16, 17.73s/it]                                                        {'loss': 20.6309, 'grad_norm': 25.125, 'learning_rate': 5.131101748427842e-05, 'epoch': 2.4}
 38%|███▊      | 1892/5000 [8:48:24<15:18:16, 17.73s/it] 38%|███▊      | 1893/5000 [8:48:49<17:12:51, 19.95s/it]                                                        {'loss': 18.1289, 'grad_norm': 23.125, 'learning_rate': 5.129053276511507e-05, 'epoch': 2.4}
 38%|███▊      | 1893/5000 [8:48:49<17:12:51, 19.95s/it] 38%|███▊      | 1894/5000 [8:49:09<17:17:23, 20.04s/it]                                                        {'loss': 19.4148, 'grad_norm': 27.25, 'learning_rate': 5.127004091991922e-05, 'epoch': 2.41}
 38%|███▊      | 1894/5000 [8:49:09<17:17:23, 20.04s/it] 38%|███▊      | 1895/5000 [8:49:32<18:03:00, 20.93s/it]                                                        {'loss': 20.4659, 'grad_norm': 26.875, 'learning_rate': 5.124954195765473e-05, 'epoch': 2.41}
 38%|███▊      | 1895/5000 [8:49:32<18:03:00, 20.93s/it] 38%|███▊      | 1896/5000 [8:49:48<16:39:14, 19.32s/it]                                                        {'loss': 22.4258, 'grad_norm': 34.5, 'learning_rate': 5.12290358872885e-05, 'epoch': 2.41}
 38%|███▊      | 1896/5000 [8:49:48<16:39:14, 19.32s/it] 38%|███▊      | 1897/5000 [8:50:04<15:52:56, 18.43s/it]                                                        {'loss': 18.2851, 'grad_norm': 18.75, 'learning_rate': 5.1208522717790623e-05, 'epoch': 2.41}
 38%|███▊      | 1897/5000 [8:50:04<15:52:56, 18.43s/it] 38%|███▊      | 1898/5000 [8:50:19<14:57:10, 17.35s/it]                                                        {'loss': 18.9059, 'grad_norm': 18.5, 'learning_rate': 5.118800245813424e-05, 'epoch': 2.41}
 38%|███▊      | 1898/5000 [8:50:19<14:57:10, 17.35s/it] 38%|███▊      | 1899/5000 [8:50:33<14:06:37, 16.38s/it]                                                        {'loss': 21.0482, 'grad_norm': 21.875, 'learning_rate': 5.11674751172956e-05, 'epoch': 2.41}
 38%|███▊      | 1899/5000 [8:50:33<14:06:37, 16.38s/it] 38%|███▊      | 1900/5000 [8:50:47<13:38:49, 15.85s/it]                                                        {'loss': 17.521, 'grad_norm': 14.125, 'learning_rate': 5.114694070425407e-05, 'epoch': 2.41}
 38%|███▊      | 1900/5000 [8:50:47<13:38:49, 15.85s/it] 38%|███▊      | 1901/5000 [8:51:01<13:05:24, 15.21s/it]                                                        {'loss': 19.7874, 'grad_norm': 54.5, 'learning_rate': 5.11263992279921e-05, 'epoch': 2.41}
 38%|███▊      | 1901/5000 [8:51:01<13:05:24, 15.21s/it] 38%|███▊      | 1902/5000 [8:51:15<12:42:59, 14.78s/it]                                                        {'loss': 20.0983, 'grad_norm': 89.0, 'learning_rate': 5.110585069749522e-05, 'epoch': 2.42}
 38%|███▊      | 1902/5000 [8:51:15<12:42:59, 14.78s/it] 38%|███▊      | 1903/5000 [8:51:36<14:13:26, 16.53s/it]                                                        {'loss': 19.9677, 'grad_norm': 26.125, 'learning_rate': 5.108529512175208e-05, 'epoch': 2.42}
 38%|███▊      | 1903/5000 [8:51:36<14:13:26, 16.53s/it] 38%|███▊      | 1904/5000 [8:51:50<13:42:33, 15.94s/it]                                                        {'loss': 21.2821, 'grad_norm': 26.875, 'learning_rate': 5.1064732509754355e-05, 'epoch': 2.42}
 38%|███▊      | 1904/5000 [8:51:50<13:42:33, 15.94s/it] 38%|███▊      | 1905/5000 [8:52:08<14:05:02, 16.38s/it]                                                        {'loss': 18.7689, 'grad_norm': 51.0, 'learning_rate': 5.104416287049684e-05, 'epoch': 2.42}
 38%|███▊      | 1905/5000 [8:52:08<14:05:02, 16.38s/it] 38%|███▊      | 1906/5000 [8:52:27<14:55:34, 17.37s/it]                                                        {'loss': 19.7376, 'grad_norm': 28.0, 'learning_rate': 5.1023586212977414e-05, 'epoch': 2.42}
 38%|███▊      | 1906/5000 [8:52:27<14:55:34, 17.37s/it] 38%|███▊      | 1907/5000 [8:52:45<14:55:25, 17.37s/it]                                                        {'loss': 18.1259, 'grad_norm': 12.375, 'learning_rate': 5.1003002546196983e-05, 'epoch': 2.42}
 38%|███▊      | 1907/5000 [8:52:45<14:55:25, 17.37s/it] 38%|███▊      | 1908/5000 [8:52:59<14:05:36, 16.41s/it]                                                        {'loss': 19.0209, 'grad_norm': 66.0, 'learning_rate': 5.098241187915955e-05, 'epoch': 2.42}
 38%|███▊      | 1908/5000 [8:52:59<14:05:36, 16.41s/it] 38%|███▊      | 1909/5000 [8:53:20<15:27:00, 17.99s/it]                                                        {'loss': 18.522, 'grad_norm': 17.25, 'learning_rate': 5.096181422087218e-05, 'epoch': 2.42}
 38%|███▊      | 1909/5000 [8:53:20<15:27:00, 17.99s/it] 38%|███▊      | 1910/5000 [8:53:40<15:46:20, 18.38s/it]                                                        {'loss': 20.1199, 'grad_norm': 71.5, 'learning_rate': 5.094120958034497e-05, 'epoch': 2.43}
 38%|███▊      | 1910/5000 [8:53:40<15:46:20, 18.38s/it] 38%|███▊      | 1911/5000 [8:53:52<14:13:16, 16.57s/it]                                                        {'loss': 20.826, 'grad_norm': 36.25, 'learning_rate': 5.09205979665911e-05, 'epoch': 2.43}
 38%|███▊      | 1911/5000 [8:53:52<14:13:16, 16.57s/it] 38%|███▊      | 1912/5000 [8:54:07<13:42:45, 15.99s/it]                                                        {'loss': 19.2305, 'grad_norm': 22.875, 'learning_rate': 5.08999793886268e-05, 'epoch': 2.43}
 38%|███▊      | 1912/5000 [8:54:07<13:42:45, 15.99s/it] 38%|███▊      | 1913/5000 [8:54:33<16:23:15, 19.11s/it]                                                        {'loss': 19.2174, 'grad_norm': 13.3125, 'learning_rate': 5.0879353855471324e-05, 'epoch': 2.43}
 38%|███▊      | 1913/5000 [8:54:33<16:23:15, 19.11s/it] 38%|███▊      | 1914/5000 [8:54:48<15:23:53, 17.96s/it]                                                        {'loss': 20.0819, 'grad_norm': 21.25, 'learning_rate': 5.085872137614699e-05, 'epoch': 2.43}
 38%|███▊      | 1914/5000 [8:54:48<15:23:53, 17.96s/it] 38%|███▊      | 1915/5000 [8:55:02<14:15:20, 16.64s/it]                                                        {'loss': 19.2375, 'grad_norm': 24.5, 'learning_rate': 5.0838081959679125e-05, 'epoch': 2.43}
 38%|███▊      | 1915/5000 [8:55:02<14:15:20, 16.64s/it] 38%|███▊      | 1916/5000 [8:55:16<13:29:54, 15.76s/it]                                                        {'loss': 21.0544, 'grad_norm': 52.75, 'learning_rate': 5.081743561509613e-05, 'epoch': 2.43}
 38%|███▊      | 1916/5000 [8:55:16<13:29:54, 15.76s/it] 38%|███▊      | 1917/5000 [8:55:41<16:02:13, 18.73s/it]                                                        {'loss': 19.2116, 'grad_norm': 19.5, 'learning_rate': 5.079678235142943e-05, 'epoch': 2.43}
 38%|███▊      | 1917/5000 [8:55:41<16:02:13, 18.73s/it] 38%|███▊      | 1918/5000 [8:56:02<16:38:33, 19.44s/it]                                                        {'loss': 19.9795, 'grad_norm': 29.125, 'learning_rate': 5.077612217771343e-05, 'epoch': 2.44}
 38%|███▊      | 1918/5000 [8:56:02<16:38:33, 19.44s/it] 38%|███▊      | 1919/5000 [8:56:16<15:03:44, 17.60s/it]                                                        {'loss': 20.3252, 'grad_norm': 22.5, 'learning_rate': 5.0755455102985616e-05, 'epoch': 2.44}
 38%|███▊      | 1919/5000 [8:56:16<15:03:44, 17.60s/it] 38%|███▊      | 1920/5000 [8:56:40<16:45:48, 19.59s/it]                                                        {'loss': 19.9954, 'grad_norm': 820.0, 'learning_rate': 5.0734781136286465e-05, 'epoch': 2.44}
 38%|███▊      | 1920/5000 [8:56:40<16:45:48, 19.59s/it] 38%|███▊      | 1921/5000 [8:56:59<16:41:07, 19.51s/it]                                                        {'loss': 18.5332, 'grad_norm': 25.875, 'learning_rate': 5.0714100286659466e-05, 'epoch': 2.44}
 38%|███▊      | 1921/5000 [8:56:59<16:41:07, 19.51s/it] 38%|███▊      | 1922/5000 [8:57:14<15:29:01, 18.11s/it]                                                        {'loss': 22.3495, 'grad_norm': 78.0, 'learning_rate': 5.069341256315113e-05, 'epoch': 2.44}
 38%|███▊      | 1922/5000 [8:57:14<15:29:01, 18.11s/it] 38%|███▊      | 1923/5000 [8:57:29<14:40:24, 17.17s/it]                                                        {'loss': 19.412, 'grad_norm': 26.5, 'learning_rate': 5.0672717974810964e-05, 'epoch': 2.44}
 38%|███▊      | 1923/5000 [8:57:29<14:40:24, 17.17s/it] 38%|███▊      | 1924/5000 [8:57:45<14:24:29, 16.86s/it]                                                        {'loss': 19.0396, 'grad_norm': 27.875, 'learning_rate': 5.065201653069149e-05, 'epoch': 2.44}
 38%|███▊      | 1924/5000 [8:57:45<14:24:29, 16.86s/it] 38%|███▊      | 1925/5000 [8:58:16<17:55:13, 20.98s/it]                                                        {'loss': 23.336, 'grad_norm': 376.0, 'learning_rate': 5.063130823984823e-05, 'epoch': 2.44}
 38%|███▊      | 1925/5000 [8:58:16<17:55:13, 20.98s/it] 39%|███▊      | 1926/5000 [8:58:34<17:08:02, 20.07s/it]                                                        {'loss': 18.0694, 'grad_norm': 18.5, 'learning_rate': 5.0610593111339676e-05, 'epoch': 2.45}
 39%|███▊      | 1926/5000 [8:58:34<17:08:02, 20.07s/it] 39%|███▊      | 1927/5000 [8:58:49<15:49:45, 18.54s/it]                                                        {'loss': 22.965, 'grad_norm': 133.0, 'learning_rate': 5.0589871154227334e-05, 'epoch': 2.45}
 39%|███▊      | 1927/5000 [8:58:49<15:49:45, 18.54s/it] 39%|███▊      | 1928/5000 [8:59:03<14:50:08, 17.39s/it]                                                        {'loss': 20.0311, 'grad_norm': 28.75, 'learning_rate': 5.0569142377575716e-05, 'epoch': 2.45}
 39%|███▊      | 1928/5000 [8:59:03<14:50:08, 17.39s/it] 39%|███▊      | 1929/5000 [8:59:18<14:03:09, 16.47s/it]                                                        {'loss': 21.5716, 'grad_norm': 233.0, 'learning_rate': 5.054840679045226e-05, 'epoch': 2.45}
 39%|███▊      | 1929/5000 [8:59:18<14:03:09, 16.47s/it] 39%|███▊      | 1930/5000 [8:59:36<14:30:21, 17.01s/it]                                                        {'loss': 19.0981, 'grad_norm': 56.25, 'learning_rate': 5.0527664401927436e-05, 'epoch': 2.45}
 39%|███▊      | 1930/5000 [8:59:36<14:30:21, 17.01s/it] 39%|███▊      | 1931/5000 [8:59:49<13:35:09, 15.94s/it]                                                        {'loss': 23.5083, 'grad_norm': 32.5, 'learning_rate': 5.050691522107466e-05, 'epoch': 2.45}
 39%|███▊      | 1931/5000 [8:59:49<13:35:09, 15.94s/it] 39%|███▊      | 1932/5000 [9:00:05<13:21:44, 15.68s/it]                                                        {'loss': 20.8857, 'grad_norm': 22.5, 'learning_rate': 5.048615925697033e-05, 'epoch': 2.45}
 39%|███▊      | 1932/5000 [9:00:05<13:21:44, 15.68s/it] 39%|███▊      | 1933/5000 [9:00:17<12:29:00, 14.65s/it]                                                        {'loss': 21.1593, 'grad_norm': 35.0, 'learning_rate': 5.046539651869383e-05, 'epoch': 2.45}
 39%|███▊      | 1933/5000 [9:00:17<12:29:00, 14.65s/it] 39%|███▊      | 1934/5000 [9:00:32<12:32:10, 14.72s/it]                                                        {'loss': 18.9252, 'grad_norm': 13.6875, 'learning_rate': 5.044462701532744e-05, 'epoch': 2.46}
 39%|███▊      | 1934/5000 [9:00:32<12:32:10, 14.72s/it] 39%|███▊      | 1935/5000 [9:00:45<12:03:37, 14.17s/it]                                                        {'loss': 19.9896, 'grad_norm': 31.5, 'learning_rate': 5.0423850755956495e-05, 'epoch': 2.46}
 39%|███▊      | 1935/5000 [9:00:45<12:03:37, 14.17s/it] 39%|███▊      | 1936/5000 [9:00:57<11:41:42, 13.74s/it]                                                        {'loss': 20.3583, 'grad_norm': 20.0, 'learning_rate': 5.0403067749669215e-05, 'epoch': 2.46}
 39%|███▊      | 1936/5000 [9:00:57<11:41:42, 13.74s/it] 39%|███▊      | 1937/5000 [9:01:11<11:36:45, 13.65s/it]                                                        {'loss': 21.0003, 'grad_norm': 74.5, 'learning_rate': 5.038227800555679e-05, 'epoch': 2.46}
 39%|███▊      | 1937/5000 [9:01:11<11:36:45, 13.65s/it] 39%|███▉      | 1938/5000 [9:01:34<14:06:29, 16.59s/it]                                                        {'loss': 18.8113, 'grad_norm': 33.25, 'learning_rate': 5.036148153271337e-05, 'epoch': 2.46}
 39%|███▉      | 1938/5000 [9:01:34<14:06:29, 16.59s/it] 39%|███▉      | 1939/5000 [9:01:48<13:24:03, 15.76s/it]                                                        {'loss': 20.5432, 'grad_norm': 21.125, 'learning_rate': 5.0340678340236036e-05, 'epoch': 2.46}
 39%|███▉      | 1939/5000 [9:01:48<13:24:03, 15.76s/it] 39%|███▉      | 1940/5000 [9:02:03<13:13:01, 15.55s/it]                                                        {'loss': 20.4476, 'grad_norm': 26.75, 'learning_rate': 5.031986843722481e-05, 'epoch': 2.46}
 39%|███▉      | 1940/5000 [9:02:03<13:13:01, 15.55s/it] 39%|███▉      | 1941/5000 [9:02:20<13:28:00, 15.85s/it]                                                        {'loss': 18.999, 'grad_norm': 21.625, 'learning_rate': 5.029905183278265e-05, 'epoch': 2.46}
 39%|███▉      | 1941/5000 [9:02:20<13:28:00, 15.85s/it] 39%|███▉      | 1942/5000 [9:02:44<15:31:17, 18.27s/it]                                                        {'loss': 19.5011, 'grad_norm': 21.875, 'learning_rate': 5.0278228536015445e-05, 'epoch': 2.47}
 39%|███▉      | 1942/5000 [9:02:44<15:31:17, 18.27s/it] 39%|███▉      | 1943/5000 [9:03:05<16:16:08, 19.16s/it]                                                        {'loss': 19.4181, 'grad_norm': 14.625, 'learning_rate': 5.0257398556032006e-05, 'epoch': 2.47}
 39%|███▉      | 1943/5000 [9:03:05<16:16:08, 19.16s/it] 39%|███▉      | 1944/5000 [9:03:30<17:42:12, 20.85s/it]                                                        {'loss': 20.7624, 'grad_norm': 26.75, 'learning_rate': 5.0236561901944094e-05, 'epoch': 2.47}
 39%|███▉      | 1944/5000 [9:03:30<17:42:12, 20.85s/it] 39%|███▉      | 1945/5000 [9:03:55<18:47:18, 22.14s/it]                                                        {'loss': 19.5641, 'grad_norm': 45.25, 'learning_rate': 5.021571858286636e-05, 'epoch': 2.47}
 39%|███▉      | 1945/5000 [9:03:55<18:47:18, 22.14s/it] 39%|███▉      | 1946/5000 [9:04:10<17:09:18, 20.22s/it]                                                        {'loss': 20.1205, 'grad_norm': 20.75, 'learning_rate': 5.0194868607916375e-05, 'epoch': 2.47}
 39%|███▉      | 1946/5000 [9:04:10<17:09:18, 20.22s/it] 39%|███▉      | 1947/5000 [9:04:34<17:52:48, 21.08s/it]                                                        {'loss': 18.0056, 'grad_norm': 14.5, 'learning_rate': 5.017401198621462e-05, 'epoch': 2.47}
 39%|███▉      | 1947/5000 [9:04:34<17:52:48, 21.08s/it] 39%|███▉      | 1948/5000 [9:04:58<18:40:58, 22.04s/it]                                                        {'loss': 17.8942, 'grad_norm': 110.5, 'learning_rate': 5.01531487268845e-05, 'epoch': 2.47}
 39%|███▉      | 1948/5000 [9:04:58<18:40:58, 22.04s/it] 39%|███▉      | 1949/5000 [9:05:13<16:57:08, 20.00s/it]                                                        {'loss': 20.2172, 'grad_norm': 1240.0, 'learning_rate': 5.013227883905231e-05, 'epoch': 2.47}
 39%|███▉      | 1949/5000 [9:05:13<16:57:08, 20.00s/it] 39%|███▉      | 1950/5000 [9:05:40<18:37:07, 21.98s/it]                                                        {'loss': 17.7054, 'grad_norm': 15.5625, 'learning_rate': 5.011140233184724e-05, 'epoch': 2.48}
 39%|███▉      | 1950/5000 [9:05:40<18:37:07, 21.98s/it] 39%|███▉      | 1951/5000 [9:06:04<19:16:01, 22.75s/it]                                                        {'loss': 18.4312, 'grad_norm': 12.5625, 'learning_rate': 5.0090519214401416e-05, 'epoch': 2.48}
 39%|███▉      | 1951/5000 [9:06:04<19:16:01, 22.75s/it] 39%|███▉      | 1952/5000 [9:06:25<18:45:33, 22.16s/it]                                                        {'loss': 18.7482, 'grad_norm': 24.625, 'learning_rate': 5.006962949584979e-05, 'epoch': 2.48}
 39%|███▉      | 1952/5000 [9:06:25<18:45:33, 22.16s/it] 39%|███▉      | 1953/5000 [9:06:40<16:52:52, 19.94s/it]                                                        {'loss': 21.4021, 'grad_norm': 111.5, 'learning_rate': 5.0048733185330244e-05, 'epoch': 2.48}
 39%|███▉      | 1953/5000 [9:06:40<16:52:52, 19.94s/it] 39%|███▉      | 1954/5000 [9:06:54<15:30:49, 18.34s/it]                                                        {'loss': 22.1855, 'grad_norm': 35.75, 'learning_rate': 5.0027830291983544e-05, 'epoch': 2.48}
 39%|███▉      | 1954/5000 [9:06:54<15:30:49, 18.34s/it] 39%|███▉      | 1955/5000 [9:07:12<15:22:01, 18.17s/it]                                                        {'loss': 17.4892, 'grad_norm': 33.25, 'learning_rate': 5.000692082495331e-05, 'epoch': 2.48}
 39%|███▉      | 1955/5000 [9:07:12<15:22:01, 18.17s/it] 39%|███▉      | 1956/5000 [9:07:31<15:37:42, 18.48s/it]                                                        {'loss': 18.6922, 'grad_norm': 18.75, 'learning_rate': 4.998600479338606e-05, 'epoch': 2.48}
 39%|███▉      | 1956/5000 [9:07:31<15:37:42, 18.48s/it] 39%|███▉      | 1957/5000 [9:07:58<17:35:23, 20.81s/it]                                                        {'loss': 18.931, 'grad_norm': 29.75, 'learning_rate': 4.996508220643117e-05, 'epoch': 2.49}
 39%|███▉      | 1957/5000 [9:07:58<17:35:23, 20.81s/it] 39%|███▉      | 1958/5000 [9:08:12<15:55:50, 18.85s/it]                                                        {'loss': 21.8016, 'grad_norm': 22.0, 'learning_rate': 4.994415307324091e-05, 'epoch': 2.49}
 39%|███▉      | 1958/5000 [9:08:12<15:55:50, 18.85s/it] 39%|███▉      | 1959/5000 [9:08:25<14:30:09, 17.17s/it]                                                        {'loss': 20.1875, 'grad_norm': 25.875, 'learning_rate': 4.992321740297037e-05, 'epoch': 2.49}
 39%|███▉      | 1959/5000 [9:08:25<14:30:09, 17.17s/it] 39%|███▉      | 1960/5000 [9:08:40<13:58:20, 16.55s/it]                                                        {'loss': 18.9167, 'grad_norm': 17.125, 'learning_rate': 4.990227520477754e-05, 'epoch': 2.49}
 39%|███▉      | 1960/5000 [9:08:40<13:58:20, 16.55s/it] 39%|███▉      | 1961/5000 [9:08:55<13:38:56, 16.17s/it]                                                        {'loss': 20.1374, 'grad_norm': 21.125, 'learning_rate': 4.9881326487823246e-05, 'epoch': 2.49}
 39%|███▉      | 1961/5000 [9:08:55<13:38:56, 16.17s/it] 39%|███▉      | 1962/5000 [9:09:20<15:47:31, 18.71s/it]                                                        {'loss': 18.8445, 'grad_norm': 25.125, 'learning_rate': 4.9860371261271163e-05, 'epoch': 2.49}
 39%|███▉      | 1962/5000 [9:09:20<15:47:31, 18.71s/it] 39%|███▉      | 1963/5000 [9:09:42<16:33:52, 19.64s/it]                                                        {'loss': 19.5884, 'grad_norm': 17.625, 'learning_rate': 4.9839409534287824e-05, 'epoch': 2.49}
 39%|███▉      | 1963/5000 [9:09:42<16:33:52, 19.64s/it] 39%|███▉      | 1964/5000 [9:09:57<15:22:30, 18.23s/it]                                                        {'loss': 22.4747, 'grad_norm': 39.25, 'learning_rate': 4.981844131604259e-05, 'epoch': 2.49}
 39%|███▉      | 1964/5000 [9:09:57<15:22:30, 18.23s/it] 39%|███▉      | 1965/5000 [9:10:14<15:07:52, 17.95s/it]                                                        {'loss': 19.7807, 'grad_norm': 26.125, 'learning_rate': 4.979746661570769e-05, 'epoch': 2.5}
 39%|███▉      | 1965/5000 [9:10:14<15:07:52, 17.95s/it] 39%|███▉      | 1966/5000 [9:10:29<14:14:49, 16.90s/it]                                                        {'loss': 19.1845, 'grad_norm': 16.625, 'learning_rate': 4.977648544245816e-05, 'epoch': 2.5}
 39%|███▉      | 1966/5000 [9:10:29<14:14:49, 16.90s/it] 39%|███▉      | 1967/5000 [9:10:43<13:30:13, 16.03s/it]                                                        {'loss': 19.3405, 'grad_norm': 23.0, 'learning_rate': 4.975549780547187e-05, 'epoch': 2.5}
 39%|███▉      | 1967/5000 [9:10:43<13:30:13, 16.03s/it] 39%|███▉      | 1968/5000 [9:11:04<14:43:38, 17.49s/it]                                                        {'loss': 20.4467, 'grad_norm': 26.125, 'learning_rate': 4.973450371392955e-05, 'epoch': 2.5}
 39%|███▉      | 1968/5000 [9:11:04<14:43:38, 17.49s/it] 39%|███▉      | 1969/5000 [9:11:21<14:40:42, 17.43s/it]                                                        {'loss': 19.0255, 'grad_norm': 16.125, 'learning_rate': 4.971350317701471e-05, 'epoch': 2.5}
 39%|███▉      | 1969/5000 [9:11:21<14:40:42, 17.43s/it] 39%|███▉      | 1970/5000 [9:11:55<18:52:02, 22.42s/it]                                                        {'loss': 60.0466, 'grad_norm': 209.0, 'learning_rate': 4.96924962039137e-05, 'epoch': 2.5}
 39%|███▉      | 1970/5000 [9:11:55<18:52:02, 22.42s/it] 39%|███▉      | 1971/5000 [9:12:12<17:28:01, 20.76s/it]                                                        {'loss': 18.8001, 'grad_norm': 35.5, 'learning_rate': 4.967148280381568e-05, 'epoch': 2.5}
 39%|███▉      | 1971/5000 [9:12:12<17:28:01, 20.76s/it] 39%|███▉      | 1972/5000 [9:12:27<16:08:36, 19.19s/it]                                                        {'loss': 19.1787, 'grad_norm': 18.875, 'learning_rate': 4.965046298591263e-05, 'epoch': 2.5}
 39%|███▉      | 1972/5000 [9:12:27<16:08:36, 19.19s/it] 39%|███▉      | 1973/5000 [9:12:44<15:32:35, 18.49s/it]                                                        {'loss': 20.6643, 'grad_norm': 18.875, 'learning_rate': 4.962943675939935e-05, 'epoch': 2.51}
 39%|███▉      | 1973/5000 [9:12:44<15:32:35, 18.49s/it] 39%|███▉      | 1974/5000 [9:13:04<15:55:04, 18.94s/it]                                                        {'loss': 18.5016, 'grad_norm': 24.125, 'learning_rate': 4.9608404133473394e-05, 'epoch': 2.51}
 39%|███▉      | 1974/5000 [9:13:04<15:55:04, 18.94s/it] 40%|███▉      | 1975/5000 [9:13:20<15:02:19, 17.90s/it]                                                        {'loss': 20.5724, 'grad_norm': 45.25, 'learning_rate': 4.958736511733516e-05, 'epoch': 2.51}
 40%|███▉      | 1975/5000 [9:13:20<15:02:19, 17.90s/it] 40%|███▉      | 1976/5000 [9:13:33<13:57:12, 16.61s/it]                                                        {'loss': 21.6678, 'grad_norm': 28.125, 'learning_rate': 4.956631972018784e-05, 'epoch': 2.51}
 40%|███▉      | 1976/5000 [9:13:33<13:57:12, 16.61s/it] 40%|███▉      | 1977/5000 [9:14:00<16:24:36, 19.54s/it]                                                        {'loss': 18.2593, 'grad_norm': 25.0, 'learning_rate': 4.954526795123739e-05, 'epoch': 2.51}
 40%|███▉      | 1977/5000 [9:14:00<16:24:36, 19.54s/it] 40%|███▉      | 1978/5000 [9:14:13<14:52:02, 17.71s/it]                                                        {'loss': 19.8058, 'grad_norm': 17.875, 'learning_rate': 4.9524209819692574e-05, 'epoch': 2.51}
 40%|███▉      | 1978/5000 [9:14:13<14:52:02, 17.71s/it] 40%|███▉      | 1979/5000 [9:14:28<14:11:22, 16.91s/it]                                                        {'loss': 19.648, 'grad_norm': 24.0, 'learning_rate': 4.9503145334764944e-05, 'epoch': 2.51}
 40%|███▉      | 1979/5000 [9:14:28<14:11:22, 16.91s/it] 40%|███▉      | 1980/5000 [9:14:51<15:46:22, 18.80s/it]                                                        {'loss': 20.906, 'grad_norm': 24.375, 'learning_rate': 4.9482074505668804e-05, 'epoch': 2.51}
 40%|███▉      | 1980/5000 [9:14:51<15:46:22, 18.80s/it] 40%|███▉      | 1981/5000 [9:15:03<13:59:41, 16.69s/it]                                                        {'loss': 22.3488, 'grad_norm': 54.0, 'learning_rate': 4.9460997341621274e-05, 'epoch': 2.52}
 40%|███▉      | 1981/5000 [9:15:03<13:59:41, 16.69s/it] 40%|███▉      | 1982/5000 [9:15:19<13:41:48, 16.34s/it]                                                        {'loss': 20.3654, 'grad_norm': 17.625, 'learning_rate': 4.943991385184219e-05, 'epoch': 2.52}
 40%|███▉      | 1982/5000 [9:15:19<13:41:48, 16.34s/it] 40%|███▉      | 1983/5000 [9:15:34<13:23:53, 15.99s/it]                                                        {'loss': 23.2263, 'grad_norm': 700.0, 'learning_rate': 4.9418824045554205e-05, 'epoch': 2.52}
 40%|███▉      | 1983/5000 [9:15:34<13:23:53, 15.99s/it] 40%|███▉      | 1984/5000 [9:15:48<12:57:05, 15.46s/it]                                                        {'loss': 20.5869, 'grad_norm': 203.0, 'learning_rate': 4.939772793198272e-05, 'epoch': 2.52}
 40%|███▉      | 1984/5000 [9:15:48<12:57:05, 15.46s/it] 40%|███▉      | 1985/5000 [9:16:06<13:29:09, 16.10s/it]                                                        {'loss': 20.154, 'grad_norm': 39.5, 'learning_rate': 4.9376625520355886e-05, 'epoch': 2.52}
 40%|███▉      | 1985/5000 [9:16:06<13:29:09, 16.10s/it] 40%|███▉      | 1986/5000 [9:16:19<12:54:03, 15.41s/it]                                                        {'loss': 19.5199, 'grad_norm': 20.125, 'learning_rate': 4.935551681990462e-05, 'epoch': 2.52}
 40%|███▉      | 1986/5000 [9:16:19<12:54:03, 15.41s/it] 40%|███▉      | 1987/5000 [9:16:38<13:47:56, 16.49s/it]                                                        {'loss': 18.3243, 'grad_norm': 25.75, 'learning_rate': 4.933440183986258e-05, 'epoch': 2.52}
 40%|███▉      | 1987/5000 [9:16:38<13:47:56, 16.49s/it] 40%|███▉      | 1988/5000 [9:17:04<16:01:59, 19.16s/it]                                                        {'loss': 18.5617, 'grad_norm': 15.75, 'learning_rate': 4.931328058946617e-05, 'epoch': 2.52}
 40%|███▉      | 1988/5000 [9:17:04<16:01:59, 19.16s/it] 40%|███▉      | 1989/5000 [9:17:16<14:20:09, 17.14s/it]                                                        {'loss': 18.6346, 'grad_norm': 38.0, 'learning_rate': 4.929215307795456e-05, 'epoch': 2.53}
 40%|███▉      | 1989/5000 [9:17:16<14:20:09, 17.14s/it] 40%|███▉      | 1990/5000 [9:17:32<13:52:51, 16.60s/it]                                                        {'loss': 19.5934, 'grad_norm': 20.0, 'learning_rate': 4.927101931456963e-05, 'epoch': 2.53}
 40%|███▉      | 1990/5000 [9:17:32<13:52:51, 16.60s/it] 40%|███▉      | 1991/5000 [9:17:45<13:03:58, 15.63s/it]                                                        {'loss': 22.1599, 'grad_norm': 75.5, 'learning_rate': 4.924987930855599e-05, 'epoch': 2.53}
 40%|███▉      | 1991/5000 [9:17:45<13:03:58, 15.63s/it] 40%|███▉      | 1992/5000 [9:18:02<13:18:44, 15.93s/it]                                                        {'loss': 18.9465, 'grad_norm': 18.0, 'learning_rate': 4.922873306916103e-05, 'epoch': 2.53}
 40%|███▉      | 1992/5000 [9:18:02<13:18:44, 15.93s/it] 40%|███▉      | 1993/5000 [9:18:26<15:20:27, 18.37s/it]                                                        {'loss': 19.5014, 'grad_norm': 20.625, 'learning_rate': 4.920758060563481e-05, 'epoch': 2.53}
 40%|███▉      | 1993/5000 [9:18:26<15:20:27, 18.37s/it] 40%|███▉      | 1994/5000 [9:18:38<13:52:59, 16.63s/it]                                                        {'loss': 22.3226, 'grad_norm': 28.5, 'learning_rate': 4.918642192723014e-05, 'epoch': 2.53}
 40%|███▉      | 1994/5000 [9:18:38<13:52:59, 16.63s/it] 40%|███▉      | 1995/5000 [9:18:52<13:12:11, 15.82s/it]                                                        {'loss': 19.7565, 'grad_norm': 22.625, 'learning_rate': 4.916525704320255e-05, 'epoch': 2.53}
 40%|███▉      | 1995/5000 [9:18:52<13:12:11, 15.82s/it] 40%|███▉      | 1996/5000 [9:19:10<13:48:43, 16.55s/it]                                                        {'loss': 17.6436, 'grad_norm': 8.4375, 'learning_rate': 4.914408596281027e-05, 'epoch': 2.53}
 40%|███▉      | 1996/5000 [9:19:10<13:48:43, 16.55s/it] 40%|███▉      | 1997/5000 [9:19:27<13:47:24, 16.53s/it]                                                        {'loss': 19.7035, 'grad_norm': 19.75, 'learning_rate': 4.9122908695314275e-05, 'epoch': 2.54}
 40%|███▉      | 1997/5000 [9:19:27<13:47:24, 16.53s/it] 40%|███▉      | 1998/5000 [9:19:39<12:41:11, 15.21s/it]                                                        {'loss': 23.2454, 'grad_norm': 40.75, 'learning_rate': 4.910172524997819e-05, 'epoch': 2.54}
 40%|███▉      | 1998/5000 [9:19:39<12:41:11, 15.21s/it] 40%|███▉      | 1999/5000 [9:19:54<12:38:40, 15.17s/it]                                                        {'loss': 19.1542, 'grad_norm': 87.5, 'learning_rate': 4.908053563606839e-05, 'epoch': 2.54}
 40%|███▉      | 1999/5000 [9:19:54<12:38:40, 15.17s/it] 40%|████      | 2000/5000 [9:20:07<12:02:45, 14.46s/it]                                                        {'loss': 20.8821, 'grad_norm': 55.5, 'learning_rate': 4.905933986285393e-05, 'epoch': 2.54}
 40%|████      | 2000/5000 [9:20:07<12:02:45, 14.46s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:23,  4.46s/it][A
  3%|▎         | 3/88 [00:16<08:08,  5.74s/it][A
  5%|▍         | 4/88 [00:20<07:16,  5.20s/it][A
  6%|▌         | 5/88 [00:24<06:30,  4.70s/it][A
  7%|▋         | 6/88 [00:30<06:48,  4.98s/it][A
  8%|▊         | 7/88 [00:34<06:16,  4.65s/it][A
  9%|▉         | 8/88 [00:37<05:50,  4.38s/it][A
 10%|█         | 9/88 [00:41<05:23,  4.10s/it][A
 11%|█▏        | 10/88 [00:44<04:53,  3.76s/it][A
 12%|█▎        | 11/88 [00:47<04:28,  3.49s/it][A
 14%|█▎        | 12/88 [00:49<03:57,  3.13s/it][A
 15%|█▍        | 13/88 [00:51<03:40,  2.94s/it][A
 16%|█▌        | 14/88 [00:57<04:35,  3.72s/it][A
 17%|█▋        | 15/88 [01:02<05:10,  4.25s/it][A
 18%|█▊        | 16/88 [01:06<04:46,  3.98s/it][A
 19%|█▉        | 17/88 [01:11<05:11,  4.39s/it][A
 20%|██        | 18/88 [01:14<04:41,  4.03s/it][A
 22%|██▏       | 19/88 [01:19<04:44,  4.12s/it][A
 23%|██▎       | 20/88 [01:22<04:33,  4.02s/it][A
 24%|██▍       | 21/88 [01:26<04:14,  3.80s/it][A
 25%|██▌       | 22/88 [01:30<04:11,  3.81s/it][A
 26%|██▌       | 23/88 [01:32<03:44,  3.45s/it][A
 27%|██▋       | 24/88 [01:41<05:20,  5.01s/it][A
 28%|██▊       | 25/88 [01:44<04:45,  4.53s/it][A
 30%|██▉       | 26/88 [01:51<05:16,  5.11s/it][A
 31%|███       | 27/88 [01:55<04:58,  4.90s/it][A
 32%|███▏      | 28/88 [02:04<06:02,  6.05s/it][A
 33%|███▎      | 29/88 [02:10<06:06,  6.22s/it][A
 34%|███▍      | 30/88 [02:16<05:43,  5.93s/it][A
 35%|███▌      | 31/88 [02:20<05:06,  5.38s/it][A
 36%|███▋      | 32/88 [02:29<06:04,  6.51s/it][A
 38%|███▊      | 33/88 [02:34<05:27,  5.95s/it][A
 39%|███▊      | 34/88 [02:42<06:07,  6.80s/it][A
 40%|███▉      | 35/88 [02:45<04:59,  5.65s/it][A
 41%|████      | 36/88 [02:51<04:46,  5.51s/it][A
 42%|████▏     | 37/88 [02:54<04:12,  4.95s/it][A
 43%|████▎     | 38/88 [02:58<03:55,  4.72s/it][A
 44%|████▍     | 39/88 [03:02<03:29,  4.28s/it][A
 45%|████▌     | 40/88 [03:08<03:59,  4.99s/it][A
 47%|████▋     | 41/88 [03:18<05:00,  6.39s/it][A
 48%|████▊     | 42/88 [03:21<04:14,  5.53s/it][A
 49%|████▉     | 43/88 [03:30<04:50,  6.45s/it][A
 50%|█████     | 44/88 [03:33<04:02,  5.51s/it][A
 51%|█████     | 45/88 [03:38<03:45,  5.24s/it][A
 52%|█████▏    | 46/88 [03:42<03:22,  4.83s/it][A
 53%|█████▎    | 47/88 [03:44<02:50,  4.16s/it][A
 55%|█████▍    | 48/88 [03:47<02:24,  3.60s/it][A
 56%|█████▌    | 49/88 [03:52<02:36,  4.02s/it][A
 57%|█████▋    | 50/88 [03:56<02:33,  4.05s/it][A
 58%|█████▊    | 51/88 [04:00<02:31,  4.09s/it][A
 59%|█████▉    | 52/88 [04:06<02:45,  4.59s/it][A
 60%|██████    | 53/88 [04:11<02:46,  4.74s/it][A
 61%|██████▏   | 54/88 [04:21<03:30,  6.20s/it][A
 62%|██████▎   | 55/88 [04:25<03:11,  5.81s/it][A
 64%|██████▎   | 56/88 [04:28<02:34,  4.83s/it][A
 65%|██████▍   | 57/88 [04:32<02:21,  4.55s/it][A
 66%|██████▌   | 58/88 [04:41<02:55,  5.84s/it][A
 67%|██████▋   | 59/88 [04:46<02:40,  5.55s/it][A
 68%|██████▊   | 60/88 [04:49<02:15,  4.85s/it][A
 69%|██████▉   | 61/88 [04:53<02:04,  4.61s/it][A
 70%|███████   | 62/88 [04:56<01:47,  4.14s/it][A
 72%|███████▏  | 63/88 [05:01<01:53,  4.55s/it][A
 73%|███████▎  | 64/88 [05:05<01:45,  4.40s/it][A
 74%|███████▍  | 65/88 [05:10<01:40,  4.38s/it][A
 75%|███████▌  | 66/88 [05:13<01:31,  4.18s/it][A
 76%|███████▌  | 67/88 [05:17<01:20,  3.85s/it][A
 77%|███████▋  | 68/88 [05:22<01:23,  4.18s/it][A
 78%|███████▊  | 69/88 [05:28<01:31,  4.82s/it][A
 80%|███████▉  | 70/88 [05:32<01:22,  4.56s/it][A
 81%|████████  | 71/88 [05:36<01:13,  4.33s/it][A
 82%|████████▏ | 72/88 [05:39<01:07,  4.20s/it][A
 83%|████████▎ | 73/88 [05:43<00:57,  3.85s/it][A
 84%|████████▍ | 74/88 [05:45<00:49,  3.53s/it][A
 85%|████████▌ | 75/88 [05:50<00:50,  3.90s/it][A
 86%|████████▋ | 76/88 [05:54<00:46,  3.87s/it][A
 88%|████████▊ | 77/88 [06:01<00:53,  4.88s/it][A
 89%|████████▊ | 78/88 [06:05<00:45,  4.58s/it][A
 90%|████████▉ | 79/88 [06:10<00:41,  4.61s/it][A
 91%|█████████ | 80/88 [06:13<00:32,  4.11s/it][A
 92%|█████████▏| 81/88 [06:18<00:30,  4.37s/it][A
 93%|█████████▎| 82/88 [06:21<00:25,  4.22s/it][A
 94%|█████████▍| 83/88 [06:25<00:20,  4.09s/it][A
 95%|█████████▌| 84/88 [06:29<00:16,  4.02s/it][A
 97%|█████████▋| 85/88 [06:32<00:11,  3.74s/it][A
 98%|█████████▊| 86/88 [06:35<00:06,  3.47s/it][A
 99%|█████████▉| 87/88 [06:39<00:03,  3.65s/it][A
100%|██████████| 88/88 [06:43<00:00,  3.70s/it][A                                                        
                                               [A{'eval_loss': 19.20503044128418, 'eval_runtime': 407.2919, 'eval_samples_per_second': 6.875, 'eval_steps_per_second': 0.216, 'epoch': 2.54}
 40%|████      | 2000/5000 [9:26:54<12:02:45, 14.46s/it]
100%|██████████| 88/88 [06:43<00:00,  3.70s/it][A
                                               [A2024-06-13 19:02:31,772 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-13 19:02:42,094 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 40%|████      | 2001/5000 [9:27:29<118:54:41, 142.74s/it]                                                          {'loss': 18.1169, 'grad_norm': 31.625, 'learning_rate': 4.903813793960656e-05, 'epoch': 2.54}
 40%|████      | 2001/5000 [9:27:29<118:54:41, 142.74s/it] 40%|████      | 2002/5000 [9:27:42<86:34:10, 103.95s/it]                                                          {'loss': 22.7667, 'grad_norm': 30.375, 'learning_rate': 4.901692987560072e-05, 'epoch': 2.54}
 40%|████      | 2002/5000 [9:27:42<86:34:10, 103.95s/it] 40%|████      | 2003/5000 [9:28:02<65:34:03, 78.76s/it]                                                         {'loss': 19.3276, 'grad_norm': 30.625, 'learning_rate': 4.8995715680113535e-05, 'epoch': 2.54}
 40%|████      | 2003/5000 [9:28:02<65:34:03, 78.76s/it] 40%|████      | 2004/5000 [9:28:20<50:19:12, 60.46s/it]                                                        {'loss': 19.3454, 'grad_norm': 27.375, 'learning_rate': 4.897449536242482e-05, 'epoch': 2.54}
 40%|████      | 2004/5000 [9:28:20<50:19:12, 60.46s/it] 40%|████      | 2005/5000 [9:28:37<39:31:55, 47.52s/it]                                                        {'loss': 18.5807, 'grad_norm': 20.25, 'learning_rate': 4.895326893181707e-05, 'epoch': 2.55}
 40%|████      | 2005/5000 [9:28:37<39:31:55, 47.52s/it] 40%|████      | 2006/5000 [9:28:50<30:53:13, 37.14s/it]                                                        {'loss': 22.4075, 'grad_norm': 41.25, 'learning_rate': 4.893203639757544e-05, 'epoch': 2.55}
 40%|████      | 2006/5000 [9:28:50<30:53:13, 37.14s/it] 40%|████      | 2007/5000 [9:29:15<27:43:47, 33.35s/it]                                                        {'loss': 20.5605, 'grad_norm': 18.375, 'learning_rate': 4.8910797768987756e-05, 'epoch': 2.55}
 40%|████      | 2007/5000 [9:29:15<27:43:47, 33.35s/it] 40%|████      | 2008/5000 [9:29:32<23:48:15, 28.64s/it]                                                        {'loss': 18.8437, 'grad_norm': 12.875, 'learning_rate': 4.8889553055344514e-05, 'epoch': 2.55}
 40%|████      | 2008/5000 [9:29:32<23:48:15, 28.64s/it] 40%|████      | 2009/5000 [9:29:49<20:42:45, 24.93s/it]                                                        {'loss': 20.2908, 'grad_norm': 19.5, 'learning_rate': 4.886830226593889e-05, 'epoch': 2.55}
 40%|████      | 2009/5000 [9:29:49<20:42:45, 24.93s/it] 40%|████      | 2010/5000 [9:30:04<18:10:19, 21.88s/it]                                                        {'loss': 20.0081, 'grad_norm': 69.5, 'learning_rate': 4.8847045410066686e-05, 'epoch': 2.55}
 40%|████      | 2010/5000 [9:30:04<18:10:19, 21.88s/it] 40%|████      | 2011/5000 [9:30:18<16:12:06, 19.51s/it]                                                        {'loss': 20.7811, 'grad_norm': 53.75, 'learning_rate': 4.8825782497026384e-05, 'epoch': 2.55}
 40%|████      | 2011/5000 [9:30:18<16:12:06, 19.51s/it] 40%|████      | 2012/5000 [9:30:34<15:23:11, 18.54s/it]                                                        {'loss': 19.3522, 'grad_norm': 27.0, 'learning_rate': 4.8804513536119095e-05, 'epoch': 2.55}
 40%|████      | 2012/5000 [9:30:34<15:23:11, 18.54s/it] 40%|████      | 2013/5000 [9:30:58<16:49:56, 20.29s/it]                                                        {'loss': 20.3939, 'grad_norm': 22.375, 'learning_rate': 4.878323853664859e-05, 'epoch': 2.56}
 40%|████      | 2013/5000 [9:30:58<16:49:56, 20.29s/it] 40%|████      | 2014/5000 [9:31:13<15:26:51, 18.62s/it]                                                        {'loss': 19.051, 'grad_norm': 19.125, 'learning_rate': 4.876195750792127e-05, 'epoch': 2.56}
 40%|████      | 2014/5000 [9:31:13<15:26:51, 18.62s/it] 40%|████      | 2015/5000 [9:31:28<14:30:51, 17.50s/it]                                                        {'loss': 20.6838, 'grad_norm': 30.75, 'learning_rate': 4.874067045924619e-05, 'epoch': 2.56}
 40%|████      | 2015/5000 [9:31:28<14:30:51, 17.50s/it] 40%|████      | 2016/5000 [9:31:42<13:39:32, 16.48s/it]                                                        {'loss': 21.0362, 'grad_norm': 22.25, 'learning_rate': 4.871937739993503e-05, 'epoch': 2.56}
 40%|████      | 2016/5000 [9:31:42<13:39:32, 16.48s/it] 40%|████      | 2017/5000 [9:32:05<15:20:53, 18.52s/it]                                                        {'loss': 18.5202, 'grad_norm': 26.0, 'learning_rate': 4.869807833930209e-05, 'epoch': 2.56}
 40%|████      | 2017/5000 [9:32:05<15:20:53, 18.52s/it] 40%|████      | 2018/5000 [9:32:20<14:28:17, 17.47s/it]                                                        {'loss': 19.3429, 'grad_norm': 34.25, 'learning_rate': 4.867677328666429e-05, 'epoch': 2.56}
 40%|████      | 2018/5000 [9:32:20<14:28:17, 17.47s/it] 40%|████      | 2019/5000 [9:32:35<13:44:20, 16.59s/it]                                                        {'loss': 19.4396, 'grad_norm': 23.25, 'learning_rate': 4.865546225134121e-05, 'epoch': 2.56}
 40%|████      | 2019/5000 [9:32:35<13:44:20, 16.59s/it] 40%|████      | 2020/5000 [9:32:57<15:08:02, 18.28s/it]                                                        {'loss': 19.3505, 'grad_norm': 37.0, 'learning_rate': 4.8634145242655e-05, 'epoch': 2.57}
 40%|████      | 2020/5000 [9:32:57<15:08:02, 18.28s/it] 40%|████      | 2021/5000 [9:33:13<14:29:13, 17.51s/it]                                                        {'loss': 19.4125, 'grad_norm': 19.0, 'learning_rate': 4.861282226993046e-05, 'epoch': 2.57}
 40%|████      | 2021/5000 [9:33:13<14:29:13, 17.51s/it] 40%|████      | 2022/5000 [9:33:31<14:43:35, 17.80s/it]                                                        {'loss': 19.7635, 'grad_norm': 18.0, 'learning_rate': 4.859149334249497e-05, 'epoch': 2.57}
 40%|████      | 2022/5000 [9:33:31<14:43:35, 17.80s/it] 40%|████      | 2023/5000 [9:33:57<16:46:52, 20.29s/it]                                                        {'loss': 21.8008, 'grad_norm': 55.25, 'learning_rate': 4.857015846967854e-05, 'epoch': 2.57}
 40%|████      | 2023/5000 [9:33:57<16:46:52, 20.29s/it] 40%|████      | 2024/5000 [9:34:11<15:11:45, 18.38s/it]                                                        {'loss': 19.8747, 'grad_norm': 61.75, 'learning_rate': 4.854881766081376e-05, 'epoch': 2.57}
 40%|████      | 2024/5000 [9:34:11<15:11:45, 18.38s/it] 40%|████      | 2025/5000 [9:34:28<14:49:45, 17.94s/it]                                                        {'loss': 19.0069, 'grad_norm': 15.5625, 'learning_rate': 4.8527470925235824e-05, 'epoch': 2.57}
 40%|████      | 2025/5000 [9:34:28<14:49:45, 17.94s/it] 41%|████      | 2026/5000 [9:34:44<14:21:45, 17.39s/it]                                                        {'loss': 17.5737, 'grad_norm': 11.25, 'learning_rate': 4.850611827228252e-05, 'epoch': 2.57}
 41%|████      | 2026/5000 [9:34:44<14:21:45, 17.39s/it] 41%|████      | 2027/5000 [9:34:58<13:31:09, 16.37s/it]                                                        {'loss': 20.2871, 'grad_norm': 30.75, 'learning_rate': 4.848475971129423e-05, 'epoch': 2.57}
 41%|████      | 2027/5000 [9:34:58<13:31:09, 16.37s/it] 41%|████      | 2028/5000 [9:35:14<13:20:55, 16.17s/it]                                                        {'loss': 19.4661, 'grad_norm': 17.375, 'learning_rate': 4.846339525161391e-05, 'epoch': 2.58}
 41%|████      | 2028/5000 [9:35:14<13:20:55, 16.17s/it] 41%|████      | 2029/5000 [9:35:32<13:44:34, 16.65s/it]                                                        {'loss': 18.1518, 'grad_norm': 15.0625, 'learning_rate': 4.8442024902587094e-05, 'epoch': 2.58}
 41%|████      | 2029/5000 [9:35:32<13:44:34, 16.65s/it] 41%|████      | 2030/5000 [9:35:46<13:11:23, 15.99s/it]                                                        {'loss': 19.7638, 'grad_norm': 79.0, 'learning_rate': 4.84206486735619e-05, 'epoch': 2.58}
 41%|████      | 2030/5000 [9:35:46<13:11:23, 15.99s/it] 41%|████      | 2031/5000 [9:36:00<12:45:33, 15.47s/it]                                                        {'loss': 18.6236, 'grad_norm': 24.625, 'learning_rate': 4.839926657388903e-05, 'epoch': 2.58}
 41%|████      | 2031/5000 [9:36:00<12:45:33, 15.47s/it] 41%|████      | 2032/5000 [9:36:13<12:01:14, 14.58s/it]                                                        {'loss': 20.4699, 'grad_norm': 50.0, 'learning_rate': 4.837787861292173e-05, 'epoch': 2.58}
 41%|████      | 2032/5000 [9:36:13<12:01:14, 14.58s/it] 41%|████      | 2033/5000 [9:36:31<12:47:31, 15.52s/it]                                                        {'loss': 18.6145, 'grad_norm': 17.375, 'learning_rate': 4.8356484800015815e-05, 'epoch': 2.58}
 41%|████      | 2033/5000 [9:36:31<12:47:31, 15.52s/it] 41%|████      | 2034/5000 [9:36:44<12:15:40, 14.88s/it]                                                        {'loss': 20.6766, 'grad_norm': 24.625, 'learning_rate': 4.833508514452968e-05, 'epoch': 2.58}
 41%|████      | 2034/5000 [9:36:44<12:15:40, 14.88s/it] 41%|████      | 2035/5000 [9:37:07<14:09:22, 17.19s/it]                                                        {'loss': 18.258, 'grad_norm': 18.0, 'learning_rate': 4.8313679655824266e-05, 'epoch': 2.58}
 41%|████      | 2035/5000 [9:37:07<14:09:22, 17.19s/it] 41%|████      | 2036/5000 [9:37:23<13:59:00, 16.98s/it]                                                        {'loss': 17.7234, 'grad_norm': 27.125, 'learning_rate': 4.829226834326304e-05, 'epoch': 2.59}
 41%|████      | 2036/5000 [9:37:23<13:59:00, 16.98s/it] 41%|████      | 2037/5000 [9:37:36<12:52:10, 15.64s/it]                                                        {'loss': 20.3129, 'grad_norm': 18.625, 'learning_rate': 4.8270851216212036e-05, 'epoch': 2.59}
 41%|████      | 2037/5000 [9:37:36<12:52:10, 15.64s/it] 41%|████      | 2038/5000 [9:37:51<12:44:39, 15.49s/it]                                                        {'loss': 21.1752, 'grad_norm': 223.0, 'learning_rate': 4.824942828403987e-05, 'epoch': 2.59}
 41%|████      | 2038/5000 [9:37:51<12:44:39, 15.49s/it] 41%|████      | 2039/5000 [9:38:12<14:17:40, 17.38s/it]                                                        {'loss': 20.1827, 'grad_norm': 25.5, 'learning_rate': 4.822799955611761e-05, 'epoch': 2.59}
 41%|████      | 2039/5000 [9:38:12<14:17:40, 17.38s/it] 41%|████      | 2040/5000 [9:38:30<14:17:02, 17.37s/it]                                                        {'loss': 21.2361, 'grad_norm': 21.625, 'learning_rate': 4.820656504181895e-05, 'epoch': 2.59}
 41%|████      | 2040/5000 [9:38:30<14:17:02, 17.37s/it] 41%|████      | 2041/5000 [9:38:46<13:59:04, 17.01s/it]                                                        {'loss': 18.9839, 'grad_norm': 32.0, 'learning_rate': 4.818512475052005e-05, 'epoch': 2.59}
 41%|████      | 2041/5000 [9:38:46<13:59:04, 17.01s/it] 41%|████      | 2042/5000 [9:38:59<13:02:17, 15.87s/it]                                                        {'loss': 19.6501, 'grad_norm': 14.625, 'learning_rate': 4.816367869159964e-05, 'epoch': 2.59}
 41%|████      | 2042/5000 [9:38:59<13:02:17, 15.87s/it] 41%|████      | 2043/5000 [9:39:22<14:41:38, 17.89s/it]                                                        {'loss': 18.4293, 'grad_norm': 16.75, 'learning_rate': 4.8142226874438934e-05, 'epoch': 2.59}
 41%|████      | 2043/5000 [9:39:22<14:41:38, 17.89s/it] 41%|████      | 2044/5000 [9:39:37<13:55:40, 16.96s/it]                                                        {'loss': 20.7153, 'grad_norm': 26.0, 'learning_rate': 4.81207693084217e-05, 'epoch': 2.6}
 41%|████      | 2044/5000 [9:39:37<13:55:40, 16.96s/it] 41%|████      | 2045/5000 [9:39:54<14:09:13, 17.24s/it]                                                        {'loss': 20.2749, 'grad_norm': 18.0, 'learning_rate': 4.80993060029342e-05, 'epoch': 2.6}
 41%|████      | 2045/5000 [9:39:54<14:09:13, 17.24s/it] 41%|████      | 2046/5000 [9:40:19<15:49:37, 19.29s/it]                                                        {'loss': 19.4429, 'grad_norm': 53.0, 'learning_rate': 4.807783696736524e-05, 'epoch': 2.6}
 41%|████      | 2046/5000 [9:40:19<15:49:37, 19.29s/it] 41%|████      | 2047/5000 [9:40:34<14:55:47, 18.20s/it]                                                        {'loss': 26.7375, 'grad_norm': 59.25, 'learning_rate': 4.805636221110606e-05, 'epoch': 2.6}
 41%|████      | 2047/5000 [9:40:34<14:55:47, 18.20s/it] 41%|████      | 2048/5000 [9:40:53<15:00:39, 18.31s/it]                                                        {'loss': 18.9448, 'grad_norm': 23.0, 'learning_rate': 4.8034881743550476e-05, 'epoch': 2.6}
 41%|████      | 2048/5000 [9:40:53<15:00:39, 18.31s/it] 41%|████      | 2049/5000 [9:41:05<13:32:45, 16.52s/it]                                                        {'loss': 20.6333, 'grad_norm': 65.5, 'learning_rate': 4.801339557409478e-05, 'epoch': 2.6}
 41%|████      | 2049/5000 [9:41:05<13:32:45, 16.52s/it] 41%|████      | 2050/5000 [9:41:20<13:10:17, 16.07s/it]                                                        {'loss': 19.2703, 'grad_norm': 40.5, 'learning_rate': 4.799190371213772e-05, 'epoch': 2.6}
 41%|████      | 2050/5000 [9:41:20<13:10:17, 16.07s/it] 41%|████      | 2051/5000 [9:41:35<12:45:41, 15.58s/it]                                                        {'loss': 19.7029, 'grad_norm': 18.0, 'learning_rate': 4.7970406167080604e-05, 'epoch': 2.6}
 41%|████      | 2051/5000 [9:41:35<12:45:41, 15.58s/it] 41%|████      | 2052/5000 [9:41:50<12:48:44, 15.65s/it]                                                        {'loss': 18.0747, 'grad_norm': 19.375, 'learning_rate': 4.794890294832716e-05, 'epoch': 2.61}
 41%|████      | 2052/5000 [9:41:50<12:48:44, 15.65s/it] 41%|████      | 2053/5000 [9:42:03<12:09:53, 14.86s/it]                                                        {'loss': 21.5368, 'grad_norm': 24.5, 'learning_rate': 4.792739406528365e-05, 'epoch': 2.61}
 41%|████      | 2053/5000 [9:42:03<12:09:53, 14.86s/it] 41%|████      | 2054/5000 [9:42:23<13:17:25, 16.24s/it]                                                        {'loss': 18.9854, 'grad_norm': 15.5, 'learning_rate': 4.790587952735878e-05, 'epoch': 2.61}
 41%|████      | 2054/5000 [9:42:23<13:17:25, 16.24s/it] 41%|████      | 2055/5000 [9:42:48<15:35:08, 19.05s/it]                                                        {'loss': 19.7504, 'grad_norm': 18.375, 'learning_rate': 4.788435934396373e-05, 'epoch': 2.61}
 41%|████      | 2055/5000 [9:42:48<15:35:08, 19.05s/it] 41%|████      | 2056/5000 [9:43:03<14:29:35, 17.72s/it]                                                        {'loss': 20.0647, 'grad_norm': 15.5, 'learning_rate': 4.7862833524512165e-05, 'epoch': 2.61}
 41%|████      | 2056/5000 [9:43:03<14:29:35, 17.72s/it] 41%|████      | 2057/5000 [9:43:27<16:00:19, 19.58s/it]                                                        {'loss': 19.1271, 'grad_norm': 13.125, 'learning_rate': 4.784130207842022e-05, 'epoch': 2.61}
 41%|████      | 2057/5000 [9:43:27<16:00:19, 19.58s/it] 41%|████      | 2058/5000 [9:43:46<15:44:41, 19.27s/it]                                                        {'loss': 18.1353, 'grad_norm': 12.0625, 'learning_rate': 4.781976501510646e-05, 'epoch': 2.61}
 41%|████      | 2058/5000 [9:43:46<15:44:41, 19.27s/it] 41%|████      | 2059/5000 [9:43:59<14:23:32, 17.62s/it]                                                        {'loss': 20.7784, 'grad_norm': 21.0, 'learning_rate': 4.779822234399197e-05, 'epoch': 2.61}
 41%|████      | 2059/5000 [9:43:59<14:23:32, 17.62s/it] 41%|████      | 2060/5000 [9:44:14<13:43:16, 16.80s/it]                                                        {'loss': 17.817, 'grad_norm': 15.0625, 'learning_rate': 4.77766740745002e-05, 'epoch': 2.62}
 41%|████      | 2060/5000 [9:44:14<13:43:16, 16.80s/it] 41%|████      | 2061/5000 [9:44:31<13:40:42, 16.75s/it]                                                        {'loss': 18.3385, 'grad_norm': 26.125, 'learning_rate': 4.775512021605712e-05, 'epoch': 2.62}
 41%|████      | 2061/5000 [9:44:31<13:40:42, 16.75s/it] 41%|████      | 2062/5000 [9:45:02<17:13:34, 21.11s/it]                                                        {'loss': 19.7587, 'grad_norm': 27.25, 'learning_rate': 4.773356077809111e-05, 'epoch': 2.62}
 41%|████      | 2062/5000 [9:45:02<17:13:34, 21.11s/it] 41%|████▏     | 2063/5000 [9:45:17<15:38:36, 19.18s/it]                                                        {'loss': 20.5253, 'grad_norm': 22.875, 'learning_rate': 4.7711995770033e-05, 'epoch': 2.62}
 41%|████▏     | 2063/5000 [9:45:17<15:38:36, 19.18s/it] 41%|████▏     | 2064/5000 [9:45:32<14:33:31, 17.85s/it]                                                        {'loss': 19.3707, 'grad_norm': 21.75, 'learning_rate': 4.7690425201316064e-05, 'epoch': 2.62}
 41%|████▏     | 2064/5000 [9:45:32<14:33:31, 17.85s/it] 41%|████▏     | 2065/5000 [9:45:47<13:52:59, 17.03s/it]                                                        {'loss': 19.7404, 'grad_norm': 27.75, 'learning_rate': 4.7668849081376e-05, 'epoch': 2.62}
 41%|████▏     | 2065/5000 [9:45:47<13:52:59, 17.03s/it] 41%|████▏     | 2066/5000 [9:46:13<16:12:31, 19.89s/it]                                                        {'loss': 17.3876, 'grad_norm': 15.25, 'learning_rate': 4.764726741965093e-05, 'epoch': 2.62}
 41%|████▏     | 2066/5000 [9:46:13<16:12:31, 19.89s/it] 41%|████▏     | 2067/5000 [9:46:28<14:58:51, 18.39s/it]                                                        {'loss': 21.2889, 'grad_norm': 58.25, 'learning_rate': 4.762568022558142e-05, 'epoch': 2.62}
 41%|████▏     | 2067/5000 [9:46:28<14:58:51, 18.39s/it] 41%|████▏     | 2068/5000 [9:46:44<14:16:39, 17.53s/it]                                                        {'loss': 18.6769, 'grad_norm': 13.4375, 'learning_rate': 4.760408750861042e-05, 'epoch': 2.63}
 41%|████▏     | 2068/5000 [9:46:44<14:16:39, 17.53s/it] 41%|████▏     | 2069/5000 [9:46:58<13:24:26, 16.47s/it]                                                        {'loss': 19.603, 'grad_norm': 97.5, 'learning_rate': 4.7582489278183344e-05, 'epoch': 2.63}
 41%|████▏     | 2069/5000 [9:46:58<13:24:26, 16.47s/it] 41%|████▏     | 2070/5000 [9:47:14<13:18:25, 16.35s/it]                                                        {'loss': 18.8186, 'grad_norm': 31.25, 'learning_rate': 4.756088554374797e-05, 'epoch': 2.63}
 41%|████▏     | 2070/5000 [9:47:14<13:18:25, 16.35s/it] 41%|████▏     | 2071/5000 [9:47:29<13:01:50, 16.02s/it]                                                        {'loss': 18.7927, 'grad_norm': 27.875, 'learning_rate': 4.753927631475452e-05, 'epoch': 2.63}
 41%|████▏     | 2071/5000 [9:47:29<13:01:50, 16.02s/it] 41%|████▏     | 2072/5000 [9:47:45<13:03:01, 16.05s/it]                                                        {'loss': 19.13, 'grad_norm': 16.5, 'learning_rate': 4.751766160065559e-05, 'epoch': 2.63}
 41%|████▏     | 2072/5000 [9:47:45<13:03:01, 16.05s/it] 41%|████▏     | 2073/5000 [9:47:58<12:17:18, 15.11s/it]                                                        {'loss': 20.4468, 'grad_norm': 73.5, 'learning_rate': 4.749604141090621e-05, 'epoch': 2.63}
 41%|████▏     | 2073/5000 [9:47:58<12:17:18, 15.11s/it] 41%|████▏     | 2074/5000 [9:48:16<12:57:30, 15.94s/it]                                                        {'loss': 18.6242, 'grad_norm': 18.75, 'learning_rate': 4.747441575496377e-05, 'epoch': 2.63}
 41%|████▏     | 2074/5000 [9:48:16<12:57:30, 15.94s/it] 42%|████▏     | 2075/5000 [9:48:33<13:09:04, 16.19s/it]                                                        {'loss': 18.7791, 'grad_norm': 16.375, 'learning_rate': 4.745278464228808e-05, 'epoch': 2.63}
 42%|████▏     | 2075/5000 [9:48:33<13:09:04, 16.19s/it] 42%|████▏     | 2076/5000 [9:48:49<13:04:13, 16.09s/it]                                                        {'loss': 19.7651, 'grad_norm': 26.625, 'learning_rate': 4.74311480823413e-05, 'epoch': 2.64}
 42%|████▏     | 2076/5000 [9:48:49<13:04:13, 16.09s/it] 42%|████▏     | 2077/5000 [9:49:05<13:09:54, 16.21s/it]                                                        {'loss': 22.3792, 'grad_norm': 37.0, 'learning_rate': 4.7409506084588014e-05, 'epoch': 2.64}
 42%|████▏     | 2077/5000 [9:49:05<13:09:54, 16.21s/it] 42%|████▏     | 2078/5000 [9:49:31<15:32:01, 19.14s/it]                                                        {'loss': 19.4633, 'grad_norm': 30.625, 'learning_rate': 4.738785865849517e-05, 'epoch': 2.64}
 42%|████▏     | 2078/5000 [9:49:31<15:32:01, 19.14s/it] 42%|████▏     | 2079/5000 [9:49:46<14:29:44, 17.87s/it]                                                        {'loss': 20.0034, 'grad_norm': 40.75, 'learning_rate': 4.7366205813532084e-05, 'epoch': 2.64}
 42%|████▏     | 2079/5000 [9:49:46<14:29:44, 17.87s/it] 42%|████▏     | 2080/5000 [9:50:01<13:43:06, 16.91s/it]                                                        {'loss': 19.154, 'grad_norm': 21.5, 'learning_rate': 4.734454755917044e-05, 'epoch': 2.64}
 42%|████▏     | 2080/5000 [9:50:01<13:43:06, 16.91s/it] 42%|████▏     | 2081/5000 [9:50:18<13:54:26, 17.15s/it]                                                        {'loss': 20.8194, 'grad_norm': 30.75, 'learning_rate': 4.7322883904884295e-05, 'epoch': 2.64}
 42%|████▏     | 2081/5000 [9:50:18<13:54:26, 17.15s/it] 42%|████▏     | 2082/5000 [9:50:40<15:04:29, 18.60s/it]                                                        {'loss': 17.795, 'grad_norm': 19.25, 'learning_rate': 4.730121486015007e-05, 'epoch': 2.64}
 42%|████▏     | 2082/5000 [9:50:40<15:04:29, 18.60s/it] 42%|████▏     | 2083/5000 [9:50:53<13:45:48, 16.99s/it]                                                        {'loss': 20.1733, 'grad_norm': 36.0, 'learning_rate': 4.727954043444654e-05, 'epoch': 2.65}
 42%|████▏     | 2083/5000 [9:50:53<13:45:48, 16.99s/it] 42%|████▏     | 2084/5000 [9:51:14<14:30:44, 17.92s/it]                                                        {'loss': 19.0853, 'grad_norm': 17.25, 'learning_rate': 4.7257860637254854e-05, 'epoch': 2.65}
 42%|████▏     | 2084/5000 [9:51:14<14:30:44, 17.92s/it] 42%|████▏     | 2085/5000 [9:51:32<14:45:23, 18.22s/it]                                                        {'loss': 21.0089, 'grad_norm': 24.0, 'learning_rate': 4.723617547805846e-05, 'epoch': 2.65}
 42%|████▏     | 2085/5000 [9:51:32<14:45:23, 18.22s/it] 42%|████▏     | 2086/5000 [9:51:47<13:54:13, 17.18s/it]                                                        {'loss': 19.113, 'grad_norm': 15.5, 'learning_rate': 4.721448496634321e-05, 'epoch': 2.65}
 42%|████▏     | 2086/5000 [9:51:47<13:54:13, 17.18s/it] 42%|████▏     | 2087/5000 [9:52:11<15:24:30, 19.04s/it]                                                        {'loss': 19.2095, 'grad_norm': 16.5, 'learning_rate': 4.719278911159725e-05, 'epoch': 2.65}
 42%|████▏     | 2087/5000 [9:52:11<15:24:30, 19.04s/it] 42%|████▏     | 2088/5000 [9:52:36<17:03:16, 21.08s/it]                                                        {'loss': 19.564, 'grad_norm': 21.375, 'learning_rate': 4.7171087923311094e-05, 'epoch': 2.65}
 42%|████▏     | 2088/5000 [9:52:36<17:03:16, 21.08s/it] 42%|████▏     | 2089/5000 [9:52:58<17:05:53, 21.15s/it]                                                        {'loss': 18.9745, 'grad_norm': 15.3125, 'learning_rate': 4.714938141097759e-05, 'epoch': 2.65}
 42%|████▏     | 2089/5000 [9:52:58<17:05:53, 21.15s/it] 42%|████▏     | 2090/5000 [9:53:12<15:28:57, 19.15s/it]                                                        {'loss': 19.3324, 'grad_norm': 26.0, 'learning_rate': 4.712766958409188e-05, 'epoch': 2.65}
 42%|████▏     | 2090/5000 [9:53:12<15:28:57, 19.15s/it] 42%|████▏     | 2091/5000 [9:53:29<14:47:27, 18.30s/it]                                                        {'loss': 20.5254, 'grad_norm': 29.375, 'learning_rate': 4.710595245215147e-05, 'epoch': 2.66}
 42%|████▏     | 2091/5000 [9:53:29<14:47:27, 18.30s/it] 42%|████▏     | 2092/5000 [9:53:48<14:57:53, 18.53s/it]                                                        {'loss': 18.2964, 'grad_norm': 223.0, 'learning_rate': 4.7084230024656154e-05, 'epoch': 2.66}
 42%|████▏     | 2092/5000 [9:53:48<14:57:53, 18.53s/it] 42%|████▏     | 2093/5000 [9:54:02<13:50:23, 17.14s/it]                                                        {'loss': 21.1875, 'grad_norm': 37.25, 'learning_rate': 4.706250231110809e-05, 'epoch': 2.66}
 42%|████▏     | 2093/5000 [9:54:02<13:50:23, 17.14s/it] 42%|████▏     | 2094/5000 [9:54:20<14:13:26, 17.62s/it]                                                        {'loss': 19.9076, 'grad_norm': 46.75, 'learning_rate': 4.7040769321011705e-05, 'epoch': 2.66}
 42%|████▏     | 2094/5000 [9:54:20<14:13:26, 17.62s/it] 42%|████▏     | 2095/5000 [9:54:37<14:05:09, 17.46s/it]                                                        {'loss': 19.5286, 'grad_norm': 82.0, 'learning_rate': 4.701903106387374e-05, 'epoch': 2.66}
 42%|████▏     | 2095/5000 [9:54:37<14:05:09, 17.46s/it] 42%|████▏     | 2096/5000 [9:54:52<13:17:14, 16.47s/it]                                                        {'loss': 21.2334, 'grad_norm': 54.25, 'learning_rate': 4.6997287549203236e-05, 'epoch': 2.66}
 42%|████▏     | 2096/5000 [9:54:52<13:17:14, 16.47s/it] 42%|████▏     | 2097/5000 [9:55:16<15:11:37, 18.84s/it]                                                        {'loss': 20.2475, 'grad_norm': 39.5, 'learning_rate': 4.697553878651159e-05, 'epoch': 2.66}
 42%|████▏     | 2097/5000 [9:55:16<15:11:37, 18.84s/it] 42%|████▏     | 2098/5000 [9:55:41<16:37:23, 20.62s/it]                                                        {'loss': 20.558, 'grad_norm': 27.375, 'learning_rate': 4.69537847853124e-05, 'epoch': 2.66}
 42%|████▏     | 2098/5000 [9:55:41<16:37:23, 20.62s/it] 42%|████▏     | 2099/5000 [9:56:08<18:19:36, 22.74s/it]                                                        {'loss': 21.2715, 'grad_norm': 27.125, 'learning_rate': 4.693202555512163e-05, 'epoch': 2.67}
 42%|████▏     | 2099/5000 [9:56:08<18:19:36, 22.74s/it] 42%|████▏     | 2100/5000 [9:56:36<19:24:58, 24.10s/it]                                                        {'loss': 18.2106, 'grad_norm': 11.0, 'learning_rate': 4.69102611054575e-05, 'epoch': 2.67}
 42%|████▏     | 2100/5000 [9:56:36<19:24:58, 24.10s/it] 42%|████▏     | 2101/5000 [9:56:52<17:35:50, 21.85s/it]                                                        {'loss': 17.4634, 'grad_norm': 23.125, 'learning_rate': 4.6888491445840534e-05, 'epoch': 2.67}
 42%|████▏     | 2101/5000 [9:56:52<17:35:50, 21.85s/it] 42%|████▏     | 2102/5000 [9:57:06<15:37:59, 19.42s/it]                                                        {'loss': 19.1264, 'grad_norm': 28.625, 'learning_rate': 4.68667165857935e-05, 'epoch': 2.67}
 42%|████▏     | 2102/5000 [9:57:06<15:37:59, 19.42s/it] 42%|████▏     | 2103/5000 [9:57:23<15:04:44, 18.74s/it]                                                        {'loss': 20.2997, 'grad_norm': 18.5, 'learning_rate': 4.6844936534841466e-05, 'epoch': 2.67}
 42%|████▏     | 2103/5000 [9:57:23<15:04:44, 18.74s/it] 42%|████▏     | 2104/5000 [9:57:38<14:02:49, 17.46s/it]                                                        {'loss': 18.8145, 'grad_norm': 15.125, 'learning_rate': 4.682315130251176e-05, 'epoch': 2.67}
 42%|████▏     | 2104/5000 [9:57:38<14:02:49, 17.46s/it] 42%|████▏     | 2105/5000 [9:57:51<13:00:56, 16.19s/it]                                                        {'loss': 89.831, 'grad_norm': 2096.0, 'learning_rate': 4.680136089833401e-05, 'epoch': 2.67}
 42%|████▏     | 2105/5000 [9:57:51<13:00:56, 16.19s/it] 42%|████▏     | 2106/5000 [9:58:05<12:30:37, 15.56s/it]                                                        {'loss': 19.6382, 'grad_norm': 36.25, 'learning_rate': 4.677956533184006e-05, 'epoch': 2.67}
 42%|████▏     | 2106/5000 [9:58:05<12:30:37, 15.56s/it] 42%|████▏     | 2107/5000 [9:58:17<11:38:28, 14.49s/it]                                                        {'loss': 20.5165, 'grad_norm': 43.5, 'learning_rate': 4.6757764612564025e-05, 'epoch': 2.68}
 42%|████▏     | 2107/5000 [9:58:17<11:38:28, 14.49s/it] 42%|████▏     | 2108/5000 [9:58:32<11:41:28, 14.55s/it]                                                        {'loss': 21.4438, 'grad_norm': 35.5, 'learning_rate': 4.67359587500423e-05, 'epoch': 2.68}
 42%|████▏     | 2108/5000 [9:58:32<11:41:28, 14.55s/it] 42%|████▏     | 2109/5000 [9:58:48<12:11:00, 15.17s/it]                                                        {'loss': 19.7923, 'grad_norm': 384.0, 'learning_rate': 4.67141477538135e-05, 'epoch': 2.68}
 42%|████▏     | 2109/5000 [9:58:48<12:11:00, 15.17s/it] 42%|████▏     | 2110/5000 [9:59:02<11:48:49, 14.72s/it]                                                        {'loss': 20.4132, 'grad_norm': 23.0, 'learning_rate': 4.669233163341849e-05, 'epoch': 2.68}
 42%|████▏     | 2110/5000 [9:59:02<11:48:49, 14.72s/it] 42%|████▏     | 2111/5000 [9:59:15<11:24:11, 14.21s/it]                                                        {'loss': 19.4562, 'grad_norm': 19.125, 'learning_rate': 4.6670510398400375e-05, 'epoch': 2.68}
 42%|████▏     | 2111/5000 [9:59:15<11:24:11, 14.21s/it] 42%|████▏     | 2112/5000 [9:59:29<11:20:15, 14.13s/it]                                                        {'loss': 19.3714, 'grad_norm': 40.0, 'learning_rate': 4.664868405830453e-05, 'epoch': 2.68}
 42%|████▏     | 2112/5000 [9:59:29<11:20:15, 14.13s/it] 42%|████▏     | 2113/5000 [9:59:42<11:08:00, 13.88s/it]                                                        {'loss': 21.6582, 'grad_norm': 40.0, 'learning_rate': 4.6626852622678525e-05, 'epoch': 2.68}
 42%|████▏     | 2113/5000 [9:59:42<11:08:00, 13.88s/it] 42%|████▏     | 2114/5000 [10:00:00<12:06:17, 15.10s/it]                                                         {'loss': 18.7345, 'grad_norm': 24.125, 'learning_rate': 4.660501610107217e-05, 'epoch': 2.68}
 42%|████▏     | 2114/5000 [10:00:00<12:06:17, 15.10s/it] 42%|████▏     | 2115/5000 [10:00:19<13:03:10, 16.29s/it]                                                         {'loss': 19.0704, 'grad_norm': 123.5, 'learning_rate': 4.6583174503037486e-05, 'epoch': 2.69}
 42%|████▏     | 2115/5000 [10:00:19<13:03:10, 16.29s/it] 42%|████▏     | 2116/5000 [10:00:37<13:26:45, 16.78s/it]                                                         {'loss': 19.4976, 'grad_norm': 14.125, 'learning_rate': 4.656132783812876e-05, 'epoch': 2.69}
 42%|████▏     | 2116/5000 [10:00:37<13:26:45, 16.78s/it] 42%|████▏     | 2117/5000 [10:00:59<14:44:09, 18.40s/it]                                                         {'loss': 16.7324, 'grad_norm': 20.0, 'learning_rate': 4.653947611590245e-05, 'epoch': 2.69}
 42%|████▏     | 2117/5000 [10:00:59<14:44:09, 18.40s/it] 42%|████▏     | 2118/5000 [10:01:15<14:10:39, 17.71s/it]                                                         {'loss': 18.9368, 'grad_norm': 1104.0, 'learning_rate': 4.6517619345917246e-05, 'epoch': 2.69}
 42%|████▏     | 2118/5000 [10:01:15<14:10:39, 17.71s/it] 42%|████▏     | 2119/5000 [10:01:34<14:30:34, 18.13s/it]                                                         {'loss': 18.2472, 'grad_norm': 33.75, 'learning_rate': 4.6495757537734034e-05, 'epoch': 2.69}
 42%|████▏     | 2119/5000 [10:01:34<14:30:34, 18.13s/it] 42%|████▏     | 2120/5000 [10:01:51<14:01:53, 17.54s/it]                                                         {'loss': 19.1199, 'grad_norm': 28.25, 'learning_rate': 4.647389070091591e-05, 'epoch': 2.69}
 42%|████▏     | 2120/5000 [10:01:51<14:01:53, 17.54s/it] 42%|████▏     | 2121/5000 [10:02:07<13:39:50, 17.09s/it]                                                         {'loss': 18.6583, 'grad_norm': 21.375, 'learning_rate': 4.6452018845028194e-05, 'epoch': 2.69}
 42%|████▏     | 2121/5000 [10:02:07<13:39:50, 17.09s/it] 42%|████▏     | 2122/5000 [10:02:21<13:04:28, 16.35s/it]                                                         {'loss': 19.4783, 'grad_norm': 23.125, 'learning_rate': 4.6430141979638354e-05, 'epoch': 2.69}
 42%|████▏     | 2122/5000 [10:02:21<13:04:28, 16.35s/it] 42%|████▏     | 2123/5000 [10:02:45<14:48:14, 18.52s/it]                                                         {'loss': 19.6261, 'grad_norm': 15.75, 'learning_rate': 4.6408260114316086e-05, 'epoch': 2.7}
 42%|████▏     | 2123/5000 [10:02:45<14:48:14, 18.52s/it] 42%|████▏     | 2124/5000 [10:03:00<13:53:15, 17.38s/it]                                                         {'loss': 20.3287, 'grad_norm': 189.0, 'learning_rate': 4.638637325863326e-05, 'epoch': 2.7}
 42%|████▏     | 2124/5000 [10:03:00<13:53:15, 17.38s/it] 42%|████▎     | 2125/5000 [10:03:16<13:40:25, 17.12s/it]                                                         {'loss': 18.8652, 'grad_norm': 15.25, 'learning_rate': 4.6364481422163926e-05, 'epoch': 2.7}
 42%|████▎     | 2125/5000 [10:03:16<13:40:25, 17.12s/it] 43%|████▎     | 2126/5000 [10:03:42<15:52:46, 19.89s/it]                                                         {'loss': 20.5302, 'grad_norm': 35.0, 'learning_rate': 4.6342584614484316e-05, 'epoch': 2.7}
 43%|████▎     | 2126/5000 [10:03:42<15:52:46, 19.89s/it] 43%|████▎     | 2127/5000 [10:03:55<14:12:06, 17.80s/it]                                                         {'loss': 21.3731, 'grad_norm': 43.5, 'learning_rate': 4.632068284517284e-05, 'epoch': 2.7}
 43%|████▎     | 2127/5000 [10:03:55<14:12:06, 17.80s/it] 43%|████▎     | 2128/5000 [10:04:22<16:17:05, 20.41s/it]                                                         {'loss': 18.0461, 'grad_norm': 18.375, 'learning_rate': 4.629877612381007e-05, 'epoch': 2.7}
 43%|████▎     | 2128/5000 [10:04:22<16:17:05, 20.41s/it] 43%|████▎     | 2129/5000 [10:04:38<15:20:19, 19.23s/it]                                                         {'loss': 19.8375, 'grad_norm': 23.625, 'learning_rate': 4.627686445997878e-05, 'epoch': 2.7}
 43%|████▎     | 2129/5000 [10:04:38<15:20:19, 19.23s/it] 43%|████▎     | 2130/5000 [10:04:55<14:47:11, 18.55s/it]                                                         {'loss': 17.6239, 'grad_norm': 14.5, 'learning_rate': 4.625494786326384e-05, 'epoch': 2.7}
 43%|████▎     | 2130/5000 [10:04:55<14:47:11, 18.55s/it] 43%|████▎     | 2131/5000 [10:05:13<14:34:16, 18.28s/it]                                                         {'loss': 18.1543, 'grad_norm': 12.625, 'learning_rate': 4.623302634325232e-05, 'epoch': 2.71}
 43%|████▎     | 2131/5000 [10:05:13<14:34:16, 18.28s/it] 43%|████▎     | 2132/5000 [10:05:32<14:47:06, 18.56s/it]                                                         {'loss': 18.4399, 'grad_norm': 21.875, 'learning_rate': 4.621109990953346e-05, 'epoch': 2.71}
 43%|████▎     | 2132/5000 [10:05:32<14:47:06, 18.56s/it] 43%|████▎     | 2133/5000 [10:05:49<14:25:18, 18.11s/it]                                                         {'loss': 34.9673, 'grad_norm': 70.0, 'learning_rate': 4.6189168571698614e-05, 'epoch': 2.71}
 43%|████▎     | 2133/5000 [10:05:49<14:25:18, 18.11s/it] 43%|████▎     | 2134/5000 [10:06:04<13:40:51, 17.18s/it]                                                         {'loss': 19.7128, 'grad_norm': 33.0, 'learning_rate': 4.616723233934128e-05, 'epoch': 2.71}
 43%|████▎     | 2134/5000 [10:06:04<13:40:51, 17.18s/it] 43%|████▎     | 2135/5000 [10:06:29<15:32:59, 19.54s/it]                                                         {'loss': 17.7733, 'grad_norm': 20.5, 'learning_rate': 4.6145291222057136e-05, 'epoch': 2.71}
 43%|████▎     | 2135/5000 [10:06:29<15:32:59, 19.54s/it] 43%|████▎     | 2136/5000 [10:06:43<14:14:41, 17.91s/it]                                                         {'loss': 20.2044, 'grad_norm': 24.75, 'learning_rate': 4.612334522944395e-05, 'epoch': 2.71}
 43%|████▎     | 2136/5000 [10:06:43<14:14:41, 17.91s/it] 43%|████▎     | 2137/5000 [10:06:59<13:43:32, 17.26s/it]                                                         {'loss': 20.5975, 'grad_norm': 18.375, 'learning_rate': 4.6101394371101664e-05, 'epoch': 2.71}
 43%|████▎     | 2137/5000 [10:06:59<13:43:32, 17.26s/it] 43%|████▎     | 2138/5000 [10:07:13<12:50:12, 16.15s/it]                                                         {'loss': 18.7102, 'grad_norm': 14.125, 'learning_rate': 4.607943865663231e-05, 'epoch': 2.71}
 43%|████▎     | 2138/5000 [10:07:13<12:50:12, 16.15s/it] 43%|████▎     | 2139/5000 [10:07:27<12:16:16, 15.44s/it]                                                         {'loss': 19.8965, 'grad_norm': 33.75, 'learning_rate': 4.605747809564008e-05, 'epoch': 2.72}
 43%|████▎     | 2139/5000 [10:07:27<12:16:16, 15.44s/it] 43%|████▎     | 2140/5000 [10:07:43<12:28:36, 15.70s/it]                                                         {'loss': 18.7118, 'grad_norm': 18.875, 'learning_rate': 4.603551269773126e-05, 'epoch': 2.72}
 43%|████▎     | 2140/5000 [10:07:43<12:28:36, 15.70s/it] 43%|████▎     | 2141/5000 [10:07:56<11:49:52, 14.90s/it]                                                         {'loss': 20.7132, 'grad_norm': 26.25, 'learning_rate': 4.601354247251425e-05, 'epoch': 2.72}
 43%|████▎     | 2141/5000 [10:07:56<11:49:52, 14.90s/it] 43%|████▎     | 2142/5000 [10:08:11<11:57:57, 15.07s/it]                                                         {'loss': 18.6546, 'grad_norm': 14.3125, 'learning_rate': 4.59915674295996e-05, 'epoch': 2.72}
 43%|████▎     | 2142/5000 [10:08:11<11:57:57, 15.07s/it] 43%|████▎     | 2143/5000 [10:08:26<11:48:21, 14.88s/it]                                                         {'loss': 18.0722, 'grad_norm': 17.0, 'learning_rate': 4.596958757859994e-05, 'epoch': 2.72}
 43%|████▎     | 2143/5000 [10:08:26<11:48:21, 14.88s/it] 43%|████▎     | 2144/5000 [10:08:43<12:28:29, 15.72s/it]                                                         {'loss': 17.7899, 'grad_norm': 11.75, 'learning_rate': 4.594760292912997e-05, 'epoch': 2.72}
 43%|████▎     | 2144/5000 [10:08:43<12:28:29, 15.72s/it] 43%|████▎     | 2145/5000 [10:08:59<12:25:02, 15.66s/it]                                                         {'loss': 24.0396, 'grad_norm': 1040.0, 'learning_rate': 4.5925613490806555e-05, 'epoch': 2.72}
 43%|████▎     | 2145/5000 [10:08:59<12:25:02, 15.66s/it] 43%|████▎     | 2146/5000 [10:09:17<13:02:00, 16.44s/it]                                                         {'loss': 18.2313, 'grad_norm': 15.25, 'learning_rate': 4.5903619273248605e-05, 'epoch': 2.73}
 43%|████▎     | 2146/5000 [10:09:17<13:02:00, 16.44s/it] 43%|████▎     | 2147/5000 [10:09:32<12:36:53, 15.92s/it]                                                         {'loss': 19.9129, 'grad_norm': 20.75, 'learning_rate': 4.5881620286077144e-05, 'epoch': 2.73}
 43%|████▎     | 2147/5000 [10:09:32<12:36:53, 15.92s/it] 43%|████▎     | 2148/5000 [10:09:47<12:28:44, 15.75s/it]                                                         {'loss': 18.992, 'grad_norm': 12.9375, 'learning_rate': 4.5859616538915295e-05, 'epoch': 2.73}
 43%|████▎     | 2148/5000 [10:09:47<12:28:44, 15.75s/it] 43%|████▎     | 2149/5000 [10:10:00<11:46:30, 14.87s/it]                                                         {'loss': 18.9037, 'grad_norm': 18.25, 'learning_rate': 4.58376080413882e-05, 'epoch': 2.73}
 43%|████▎     | 2149/5000 [10:10:00<11:46:30, 14.87s/it] 43%|████▎     | 2150/5000 [10:10:17<12:15:40, 15.49s/it]                                                         {'loss': 19.5157, 'grad_norm': 17.5, 'learning_rate': 4.581559480312316e-05, 'epoch': 2.73}
 43%|████▎     | 2150/5000 [10:10:17<12:15:40, 15.49s/it] 43%|████▎     | 2151/5000 [10:10:47<15:48:17, 19.97s/it]                                                         {'loss': 16.7194, 'grad_norm': 17.625, 'learning_rate': 4.57935768337495e-05, 'epoch': 2.73}
 43%|████▎     | 2151/5000 [10:10:47<15:48:17, 19.97s/it] 43%|████▎     | 2152/5000 [10:11:14<17:19:37, 21.90s/it]                                                         {'loss': 18.1896, 'grad_norm': 17.0, 'learning_rate': 4.5771554142898634e-05, 'epoch': 2.73}
 43%|████▎     | 2152/5000 [10:11:14<17:19:37, 21.90s/it] 43%|████▎     | 2153/5000 [10:11:32<16:29:25, 20.85s/it]                                                         {'loss': 20.078, 'grad_norm': 924.0, 'learning_rate': 4.5749526740204024e-05, 'epoch': 2.73}
 43%|████▎     | 2153/5000 [10:11:32<16:29:25, 20.85s/it] 43%|████▎     | 2154/5000 [10:11:49<15:34:40, 19.70s/it]                                                         {'loss': 19.1322, 'grad_norm': 16.875, 'learning_rate': 4.572749463530122e-05, 'epoch': 2.74}
 43%|████▎     | 2154/5000 [10:11:49<15:34:40, 19.70s/it] 43%|████▎     | 2155/5000 [10:12:05<14:30:57, 18.37s/it]                                                         {'loss': 20.4127, 'grad_norm': 23.75, 'learning_rate': 4.570545783782781e-05, 'epoch': 2.74}
 43%|████▎     | 2155/5000 [10:12:05<14:30:57, 18.37s/it] 43%|████▎     | 2156/5000 [10:12:21<14:07:21, 17.88s/it]                                                         {'loss': 18.1405, 'grad_norm': 12.625, 'learning_rate': 4.568341635742343e-05, 'epoch': 2.74}
 43%|████▎     | 2156/5000 [10:12:21<14:07:21, 17.88s/it] 43%|████▎     | 2157/5000 [10:12:38<13:53:38, 17.59s/it]                                                         {'loss': 20.0176, 'grad_norm': 22.625, 'learning_rate': 4.566137020372976e-05, 'epoch': 2.74}
 43%|████▎     | 2157/5000 [10:12:38<13:53:38, 17.59s/it] 43%|████▎     | 2158/5000 [10:12:53<13:14:01, 16.76s/it]                                                         {'loss': 19.0988, 'grad_norm': 29.75, 'learning_rate': 4.5639319386390566e-05, 'epoch': 2.74}
 43%|████▎     | 2158/5000 [10:12:53<13:14:01, 16.76s/it] 43%|████▎     | 2159/5000 [10:13:20<15:32:16, 19.69s/it]                                                         {'loss': 19.6346, 'grad_norm': 21.125, 'learning_rate': 4.561726391505162e-05, 'epoch': 2.74}
 43%|████▎     | 2159/5000 [10:13:20<15:32:16, 19.69s/it] 43%|████▎     | 2160/5000 [10:13:40<15:39:52, 19.86s/it]                                                         {'loss': 18.4371, 'grad_norm': 13.875, 'learning_rate': 4.559520379936071e-05, 'epoch': 2.74}
 43%|████▎     | 2160/5000 [10:13:40<15:39:52, 19.86s/it] 43%|████▎     | 2161/5000 [10:14:05<16:57:26, 21.50s/it]                                                         {'loss': 19.6475, 'grad_norm': 51.25, 'learning_rate': 4.55731390489677e-05, 'epoch': 2.74}
 43%|████▎     | 2161/5000 [10:14:05<16:57:26, 21.50s/it] 43%|████▎     | 2162/5000 [10:14:20<15:17:04, 19.39s/it]                                                         {'loss': 19.5053, 'grad_norm': 25.125, 'learning_rate': 4.5551069673524455e-05, 'epoch': 2.75}
 43%|████▎     | 2162/5000 [10:14:20<15:17:04, 19.39s/it] 43%|████▎     | 2163/5000 [10:14:45<16:48:32, 21.33s/it]                                                         {'loss': 18.2338, 'grad_norm': 22.75, 'learning_rate': 4.552899568268487e-05, 'epoch': 2.75}
 43%|████▎     | 2163/5000 [10:14:45<16:48:32, 21.33s/it] 43%|████▎     | 2164/5000 [10:15:03<16:00:02, 20.31s/it]                                                         {'loss': 16.3527, 'grad_norm': 16.875, 'learning_rate': 4.5506917086104866e-05, 'epoch': 2.75}
 43%|████▎     | 2164/5000 [10:15:03<16:00:02, 20.31s/it] 43%|████▎     | 2165/5000 [10:15:24<16:01:24, 20.35s/it]                                                         {'loss': 22.7603, 'grad_norm': 39.0, 'learning_rate': 4.548483389344236e-05, 'epoch': 2.75}
 43%|████▎     | 2165/5000 [10:15:24<16:01:24, 20.35s/it] 43%|████▎     | 2166/5000 [10:15:37<14:25:05, 18.32s/it]                                                         {'loss': 18.6788, 'grad_norm': 14.875, 'learning_rate': 4.5462746114357295e-05, 'epoch': 2.75}
 43%|████▎     | 2166/5000 [10:15:37<14:25:05, 18.32s/it] 43%|████▎     | 2167/5000 [10:16:03<16:01:19, 20.36s/it]                                                         {'loss': 19.0191, 'grad_norm': 19.375, 'learning_rate': 4.544065375851162e-05, 'epoch': 2.75}
 43%|████▎     | 2167/5000 [10:16:03<16:01:19, 20.36s/it] 43%|████▎     | 2168/5000 [10:16:27<17:04:46, 21.71s/it]                                                         {'loss': 19.6923, 'grad_norm': 46.75, 'learning_rate': 4.5418556835569283e-05, 'epoch': 2.75}
 43%|████▎     | 2168/5000 [10:16:27<17:04:46, 21.71s/it] 43%|████▎     | 2169/5000 [10:16:46<16:15:06, 20.67s/it]                                                         {'loss': 18.6605, 'grad_norm': 17.375, 'learning_rate': 4.5396455355196214e-05, 'epoch': 2.75}
 43%|████▎     | 2169/5000 [10:16:46<16:15:06, 20.67s/it] 43%|████▎     | 2170/5000 [10:17:03<15:28:55, 19.69s/it]                                                         {'loss': 18.6184, 'grad_norm': 12.875, 'learning_rate': 4.537434932706038e-05, 'epoch': 2.76}
 43%|████▎     | 2170/5000 [10:17:03<15:28:55, 19.69s/it] 43%|████▎     | 2171/5000 [10:17:27<16:33:39, 21.07s/it]                                                         {'loss': 19.1386, 'grad_norm': 29.5, 'learning_rate': 4.535223876083168e-05, 'epoch': 2.76}
 43%|████▎     | 2171/5000 [10:17:27<16:33:39, 21.07s/it] 43%|████▎     | 2172/5000 [10:17:52<17:17:17, 22.01s/it]                                                         {'loss': 19.5338, 'grad_norm': 27.25, 'learning_rate': 4.533012366618203e-05, 'epoch': 2.76}
 43%|████▎     | 2172/5000 [10:17:52<17:17:17, 22.01s/it] 43%|████▎     | 2173/5000 [10:18:07<15:43:25, 20.02s/it]                                                         {'loss': 19.9343, 'grad_norm': 23.125, 'learning_rate': 4.5308004052785336e-05, 'epoch': 2.76}
 43%|████▎     | 2173/5000 [10:18:07<15:43:25, 20.02s/it] 43%|████▎     | 2174/5000 [10:18:20<14:08:16, 18.01s/it]                                                         {'loss': 19.9225, 'grad_norm': 31.125, 'learning_rate': 4.528587993031747e-05, 'epoch': 2.76}
 43%|████▎     | 2174/5000 [10:18:20<14:08:16, 18.01s/it] 44%|████▎     | 2175/5000 [10:18:35<13:16:08, 16.91s/it]                                                         {'loss': 21.0592, 'grad_norm': 41.25, 'learning_rate': 4.526375130845627e-05, 'epoch': 2.76}
 44%|████▎     | 2175/5000 [10:18:35<13:16:08, 16.91s/it] 44%|████▎     | 2176/5000 [10:18:49<12:41:18, 16.18s/it]                                                         {'loss': 20.0635, 'grad_norm': 33.75, 'learning_rate': 4.524161819688153e-05, 'epoch': 2.76}
 44%|████▎     | 2176/5000 [10:18:49<12:41:18, 16.18s/it] 44%|████▎     | 2177/5000 [10:19:06<12:58:51, 16.55s/it]                                                         {'loss': 18.5951, 'grad_norm': 15.0625, 'learning_rate': 4.521948060527505e-05, 'epoch': 2.76}
 44%|████▎     | 2177/5000 [10:19:06<12:58:51, 16.55s/it] 44%|████▎     | 2178/5000 [10:19:29<14:28:27, 18.46s/it]                                                         {'loss': 18.9507, 'grad_norm': 19.625, 'learning_rate': 4.519733854332056e-05, 'epoch': 2.77}
 44%|████▎     | 2178/5000 [10:19:29<14:28:27, 18.46s/it] 44%|████▎     | 2179/5000 [10:19:43<13:22:48, 17.07s/it]                                                         {'loss': 19.6939, 'grad_norm': 25.125, 'learning_rate': 4.517519202070375e-05, 'epoch': 2.77}
 44%|████▎     | 2179/5000 [10:19:43<13:22:48, 17.07s/it] 44%|████▎     | 2180/5000 [10:20:03<14:05:57, 18.00s/it]                                                         {'loss': 18.7918, 'grad_norm': 68.0, 'learning_rate': 4.515304104711225e-05, 'epoch': 2.77}
 44%|████▎     | 2180/5000 [10:20:03<14:05:57, 18.00s/it] 44%|████▎     | 2181/5000 [10:20:18<13:16:03, 16.94s/it]                                                         {'loss': 20.3955, 'grad_norm': 32.5, 'learning_rate': 4.513088563223564e-05, 'epoch': 2.77}
 44%|████▎     | 2181/5000 [10:20:18<13:16:03, 16.94s/it] 44%|████▎     | 2182/5000 [10:20:34<12:59:38, 16.60s/it]                                                         {'loss': 19.3447, 'grad_norm': 41.25, 'learning_rate': 4.510872578576549e-05, 'epoch': 2.77}
 44%|████▎     | 2182/5000 [10:20:34<12:59:38, 16.60s/it] 44%|████▎     | 2183/5000 [10:20:51<13:09:24, 16.81s/it]                                                         {'loss': 18.8629, 'grad_norm': 19.125, 'learning_rate': 4.508656151739524e-05, 'epoch': 2.77}
 44%|████▎     | 2183/5000 [10:20:51<13:09:24, 16.81s/it] 44%|████▎     | 2184/5000 [10:21:09<13:21:44, 17.08s/it]                                                         {'loss': 20.1001, 'grad_norm': 25.25, 'learning_rate': 4.506439283682029e-05, 'epoch': 2.77}
 44%|████▎     | 2184/5000 [10:21:09<13:21:44, 17.08s/it] 44%|████▎     | 2185/5000 [10:21:24<12:53:31, 16.49s/it]                                                         {'loss': 21.2463, 'grad_norm': 40.75, 'learning_rate': 4.504221975373799e-05, 'epoch': 2.77}
 44%|████▎     | 2185/5000 [10:21:24<12:53:31, 16.49s/it] 44%|████▎     | 2186/5000 [10:21:37<12:05:56, 15.48s/it]                                                         {'loss': 19.6976, 'grad_norm': 53.5, 'learning_rate': 4.50200422778476e-05, 'epoch': 2.78}
 44%|████▎     | 2186/5000 [10:21:37<12:05:56, 15.48s/it] 44%|████▎     | 2187/5000 [10:21:53<12:13:34, 15.65s/it]                                                         {'loss': 19.3103, 'grad_norm': 21.625, 'learning_rate': 4.499786041885029e-05, 'epoch': 2.78}
 44%|████▎     | 2187/5000 [10:21:53<12:13:34, 15.65s/it] 44%|████▍     | 2188/5000 [10:22:14<13:23:21, 17.14s/it]                                                         {'loss': 20.5064, 'grad_norm': 15.4375, 'learning_rate': 4.497567418644916e-05, 'epoch': 2.78}
 44%|████▍     | 2188/5000 [10:22:14<13:23:21, 17.14s/it] 44%|████▍     | 2189/5000 [10:22:33<13:51:46, 17.75s/it]                                                         {'loss': 18.9935, 'grad_norm': 20.5, 'learning_rate': 4.495348359034923e-05, 'epoch': 2.78}
 44%|████▍     | 2189/5000 [10:22:33<13:51:46, 17.75s/it] 44%|████▍     | 2190/5000 [10:23:00<15:58:47, 20.47s/it]                                                         {'loss': 16.1546, 'grad_norm': 17.375, 'learning_rate': 4.4931288640257414e-05, 'epoch': 2.78}
 44%|████▍     | 2190/5000 [10:23:00<15:58:47, 20.47s/it] 44%|████▍     | 2191/5000 [10:23:35<19:32:31, 25.04s/it]                                                         {'loss': 17.7354, 'grad_norm': 15.5625, 'learning_rate': 4.4909089345882546e-05, 'epoch': 2.78}
 44%|████▍     | 2191/5000 [10:23:35<19:32:31, 25.04s/it] 44%|████▍     | 2192/5000 [10:23:55<18:22:16, 23.55s/it]                                                         {'loss': 18.5985, 'grad_norm': 17.75, 'learning_rate': 4.4886885716935345e-05, 'epoch': 2.78}
 44%|████▍     | 2192/5000 [10:23:55<18:22:16, 23.55s/it] 44%|████▍     | 2193/5000 [10:24:21<18:54:26, 24.25s/it]                                                         {'loss': 21.4672, 'grad_norm': 52.25, 'learning_rate': 4.4864677763128426e-05, 'epoch': 2.78}
 44%|████▍     | 2193/5000 [10:24:21<18:54:26, 24.25s/it] 44%|████▍     | 2194/5000 [10:24:38<17:09:08, 22.01s/it]                                                         {'loss': 19.7571, 'grad_norm': 29.75, 'learning_rate': 4.4842465494176315e-05, 'epoch': 2.79}
 44%|████▍     | 2194/5000 [10:24:38<17:09:08, 22.01s/it] 44%|████▍     | 2195/5000 [10:24:56<16:18:04, 20.92s/it]                                                         {'loss': 19.983, 'grad_norm': 21.0, 'learning_rate': 4.482024891979541e-05, 'epoch': 2.79}
 44%|████▍     | 2195/5000 [10:24:56<16:18:04, 20.92s/it] 44%|████▍     | 2196/5000 [10:25:23<17:30:49, 22.49s/it]                                                         {'loss': 19.6768, 'grad_norm': 14.125, 'learning_rate': 4.4798028049704e-05, 'epoch': 2.79}
 44%|████▍     | 2196/5000 [10:25:23<17:30:49, 22.49s/it] 44%|████▍     | 2197/5000 [10:25:40<16:17:24, 20.92s/it]                                                         {'loss': 18.6802, 'grad_norm': 31.625, 'learning_rate': 4.4775802893622224e-05, 'epoch': 2.79}
 44%|████▍     | 2197/5000 [10:25:40<16:17:24, 20.92s/it] 44%|████▍     | 2198/5000 [10:25:59<15:55:09, 20.45s/it]                                                         {'loss': 19.7954, 'grad_norm': 22.5, 'learning_rate': 4.4753573461272145e-05, 'epoch': 2.79}
 44%|████▍     | 2198/5000 [10:25:59<15:55:09, 20.45s/it] 44%|████▍     | 2199/5000 [10:26:14<14:42:08, 18.90s/it]                                                         {'loss': 19.4172, 'grad_norm': 18.25, 'learning_rate': 4.473133976237766e-05, 'epoch': 2.79}
 44%|████▍     | 2199/5000 [10:26:14<14:42:08, 18.90s/it] 44%|████▍     | 2200/5000 [10:26:29<13:37:12, 17.51s/it]                                                         {'loss': 20.0646, 'grad_norm': 26.875, 'learning_rate': 4.4709101806664554e-05, 'epoch': 2.79}
 44%|████▍     | 2200/5000 [10:26:29<13:37:12, 17.51s/it] 44%|████▍     | 2201/5000 [10:26:44<13:05:06, 16.83s/it]                                                         {'loss': 19.5011, 'grad_norm': 17.375, 'learning_rate': 4.4686859603860456e-05, 'epoch': 2.79}
 44%|████▍     | 2201/5000 [10:26:44<13:05:06, 16.83s/it] 44%|████▍     | 2202/5000 [10:26:56<12:00:32, 15.45s/it]                                                         {'loss': 22.2817, 'grad_norm': 26.375, 'learning_rate': 4.466461316369487e-05, 'epoch': 2.8}
 44%|████▍     | 2202/5000 [10:26:56<12:00:32, 15.45s/it] 44%|████▍     | 2203/5000 [10:27:13<12:17:41, 15.82s/it]                                                         {'loss': 21.2955, 'grad_norm': 21.0, 'learning_rate': 4.4642362495899146e-05, 'epoch': 2.8}
 44%|████▍     | 2203/5000 [10:27:13<12:17:41, 15.82s/it] 44%|████▍     | 2204/5000 [10:27:29<12:20:42, 15.89s/it]                                                         {'loss': 18.4498, 'grad_norm': 14.75, 'learning_rate': 4.4620107610206454e-05, 'epoch': 2.8}
 44%|████▍     | 2204/5000 [10:27:29<12:20:42, 15.89s/it] 44%|████▍     | 2205/5000 [10:27:42<11:42:24, 15.08s/it]                                                         {'loss': 25.0655, 'grad_norm': 116.5, 'learning_rate': 4.459784851635188e-05, 'epoch': 2.8}
 44%|████▍     | 2205/5000 [10:27:42<11:42:24, 15.08s/it] 44%|████▍     | 2206/5000 [10:28:01<12:36:00, 16.23s/it]                                                         {'loss': 20.4482, 'grad_norm': 97.5, 'learning_rate': 4.4575585224072286e-05, 'epoch': 2.8}
 44%|████▍     | 2206/5000 [10:28:01<12:36:00, 16.23s/it] 44%|████▍     | 2207/5000 [10:28:17<12:25:40, 16.02s/it]                                                         {'loss': 18.9199, 'grad_norm': 29.0, 'learning_rate': 4.455331774310638e-05, 'epoch': 2.8}
 44%|████▍     | 2207/5000 [10:28:17<12:25:40, 16.02s/it] 44%|████▍     | 2208/5000 [10:28:41<14:29:41, 18.69s/it]                                                         {'loss': 19.8483, 'grad_norm': 24.875, 'learning_rate': 4.453104608319474e-05, 'epoch': 2.8}
 44%|████▍     | 2208/5000 [10:28:41<14:29:41, 18.69s/it] 44%|████▍     | 2209/5000 [10:28:58<13:53:35, 17.92s/it]                                                         {'loss': 19.0533, 'grad_norm': 14.875, 'learning_rate': 4.450877025407972e-05, 'epoch': 2.81}
 44%|████▍     | 2209/5000 [10:28:58<13:53:35, 17.92s/it] 44%|████▍     | 2210/5000 [10:29:15<13:48:26, 17.82s/it]                                                         {'loss': 19.5468, 'grad_norm': 15.6875, 'learning_rate': 4.448649026550555e-05, 'epoch': 2.81}
 44%|████▍     | 2210/5000 [10:29:15<13:48:26, 17.82s/it] 44%|████▍     | 2211/5000 [10:29:39<15:15:54, 19.70s/it]                                                         {'loss': 19.3004, 'grad_norm': 20.625, 'learning_rate': 4.446420612721824e-05, 'epoch': 2.81}
 44%|████▍     | 2211/5000 [10:29:39<15:15:54, 19.70s/it] 44%|████▍     | 2212/5000 [10:29:55<14:19:36, 18.50s/it]                                                         {'loss': 20.1805, 'grad_norm': 123.0, 'learning_rate': 4.4441917848965624e-05, 'epoch': 2.81}
 44%|████▍     | 2212/5000 [10:29:55<14:19:36, 18.50s/it] 44%|████▍     | 2213/5000 [10:30:12<13:58:05, 18.04s/it]                                                         {'loss': 17.4969, 'grad_norm': 13.8125, 'learning_rate': 4.441962544049736e-05, 'epoch': 2.81}
 44%|████▍     | 2213/5000 [10:30:12<13:58:05, 18.04s/it] 44%|████▍     | 2214/5000 [10:30:29<13:47:12, 17.81s/it]                                                         {'loss': 18.7261, 'grad_norm': 19.375, 'learning_rate': 4.439732891156491e-05, 'epoch': 2.81}
 44%|████▍     | 2214/5000 [10:30:29<13:47:12, 17.81s/it] 44%|████▍     | 2215/5000 [10:30:45<13:13:29, 17.10s/it]                                                         {'loss': 20.1018, 'grad_norm': 22.25, 'learning_rate': 4.4375028271921535e-05, 'epoch': 2.81}
 44%|████▍     | 2215/5000 [10:30:45<13:13:29, 17.10s/it] 44%|████▍     | 2216/5000 [10:31:00<12:50:15, 16.60s/it]                                                         {'loss': 18.0927, 'grad_norm': 8.875, 'learning_rate': 4.4352723531322276e-05, 'epoch': 2.81}
 44%|████▍     | 2216/5000 [10:31:00<12:50:15, 16.60s/it] 44%|████▍     | 2217/5000 [10:31:14<12:10:46, 15.76s/it]                                                         {'loss': 21.7764, 'grad_norm': 30.25, 'learning_rate': 4.433041469952399e-05, 'epoch': 2.82}
 44%|████▍     | 2217/5000 [10:31:14<12:10:46, 15.76s/it] 44%|████▍     | 2218/5000 [10:31:27<11:31:19, 14.91s/it]                                                         {'loss': 18.7339, 'grad_norm': 19.375, 'learning_rate': 4.430810178628534e-05, 'epoch': 2.82}
 44%|████▍     | 2218/5000 [10:31:27<11:31:19, 14.91s/it] 44%|████▍     | 2219/5000 [10:31:43<11:53:02, 15.38s/it]                                                         {'loss': 19.4386, 'grad_norm': 31.0, 'learning_rate': 4.428578480136672e-05, 'epoch': 2.82}
 44%|████▍     | 2219/5000 [10:31:43<11:53:02, 15.38s/it] 44%|████▍     | 2220/5000 [10:31:56<11:10:54, 14.48s/it]                                                         {'loss': 23.3094, 'grad_norm': 26.125, 'learning_rate': 4.4263463754530356e-05, 'epoch': 2.82}
 44%|████▍     | 2220/5000 [10:31:56<11:10:54, 14.48s/it] 44%|████▍     | 2221/5000 [10:32:10<11:06:37, 14.39s/it]                                                         {'loss': 20.0529, 'grad_norm': 18.125, 'learning_rate': 4.424113865554023e-05, 'epoch': 2.82}
 44%|████▍     | 2221/5000 [10:32:10<11:06:37, 14.39s/it] 44%|████▍     | 2222/5000 [10:32:23<10:42:26, 13.88s/it]                                                         {'loss': 19.1476, 'grad_norm': 31.875, 'learning_rate': 4.4218809514162086e-05, 'epoch': 2.82}
 44%|████▍     | 2222/5000 [10:32:23<10:42:26, 13.88s/it] 44%|████▍     | 2223/5000 [10:32:39<11:11:29, 14.51s/it]                                                         {'loss': 19.5994, 'grad_norm': 23.875, 'learning_rate': 4.4196476340163455e-05, 'epoch': 2.82}
 44%|████▍     | 2223/5000 [10:32:39<11:11:29, 14.51s/it] 44%|████▍     | 2224/5000 [10:32:59<12:30:38, 16.22s/it]                                                         {'loss': 18.7512, 'grad_norm': 23.5, 'learning_rate': 4.4174139143313646e-05, 'epoch': 2.82}
 44%|████▍     | 2224/5000 [10:32:59<12:30:38, 16.22s/it] 44%|████▍     | 2225/5000 [10:33:14<12:23:36, 16.08s/it]                                                         {'loss': 19.1365, 'grad_norm': 13.625, 'learning_rate': 4.4151797933383685e-05, 'epoch': 2.83}
 44%|████▍     | 2225/5000 [10:33:15<12:23:36, 16.08s/it] 45%|████▍     | 2226/5000 [10:33:31<12:32:45, 16.28s/it]                                                         {'loss': 18.4287, 'grad_norm': 13.4375, 'learning_rate': 4.412945272014639e-05, 'epoch': 2.83}
 45%|████▍     | 2226/5000 [10:33:31<12:32:45, 16.28s/it] 45%|████▍     | 2227/5000 [10:33:48<12:32:37, 16.28s/it]                                                         {'loss': 21.5751, 'grad_norm': 24.625, 'learning_rate': 4.41071035133763e-05, 'epoch': 2.83}
 45%|████▍     | 2227/5000 [10:33:48<12:32:37, 16.28s/it] 45%|████▍     | 2228/5000 [10:34:03<12:26:44, 16.16s/it]                                                         {'loss': 19.0444, 'grad_norm': 18.125, 'learning_rate': 4.408475032284973e-05, 'epoch': 2.83}
 45%|████▍     | 2228/5000 [10:34:03<12:26:44, 16.16s/it] 45%|████▍     | 2229/5000 [10:34:30<14:46:32, 19.20s/it]                                                         {'loss': 17.7333, 'grad_norm': 12.625, 'learning_rate': 4.406239315834474e-05, 'epoch': 2.83}
 45%|████▍     | 2229/5000 [10:34:30<14:46:32, 19.20s/it] 45%|████▍     | 2230/5000 [10:34:45<13:55:59, 18.11s/it]                                                         {'loss': 20.5554, 'grad_norm': 77.5, 'learning_rate': 4.404003202964108e-05, 'epoch': 2.83}
 45%|████▍     | 2230/5000 [10:34:45<13:55:59, 18.11s/it] 45%|████▍     | 2231/5000 [10:35:09<15:19:18, 19.92s/it]                                                         {'loss': 19.6026, 'grad_norm': 25.875, 'learning_rate': 4.401766694652028e-05, 'epoch': 2.83}
 45%|████▍     | 2231/5000 [10:35:09<15:19:18, 19.92s/it] 45%|████▍     | 2232/5000 [10:35:25<14:15:00, 18.53s/it]                                                         {'loss': 19.0353, 'grad_norm': 29.625, 'learning_rate': 4.399529791876559e-05, 'epoch': 2.83}
 45%|████▍     | 2232/5000 [10:35:25<14:15:00, 18.53s/it] 45%|████▍     | 2233/5000 [10:35:44<14:20:08, 18.65s/it]                                                         {'loss': 18.0991, 'grad_norm': 23.0, 'learning_rate': 4.3972924956161995e-05, 'epoch': 2.84}
 45%|████▍     | 2233/5000 [10:35:44<14:20:08, 18.65s/it] 45%|████▍     | 2234/5000 [10:35:59<13:36:15, 17.71s/it]                                                         {'loss': 18.3793, 'grad_norm': 17.0, 'learning_rate': 4.395054806849617e-05, 'epoch': 2.84}
 45%|████▍     | 2234/5000 [10:35:59<13:36:15, 17.71s/it] 45%|████▍     | 2235/5000 [10:36:17<13:37:19, 17.74s/it]                                                         {'loss': 18.3491, 'grad_norm': 58.5, 'learning_rate': 4.3928167265556525e-05, 'epoch': 2.84}
 45%|████▍     | 2235/5000 [10:36:17<13:37:19, 17.74s/it] 45%|████▍     | 2236/5000 [10:36:32<12:56:56, 16.87s/it]                                                         {'loss': 18.8206, 'grad_norm': 60.25, 'learning_rate': 4.39057825571332e-05, 'epoch': 2.84}
 45%|████▍     | 2236/5000 [10:36:32<12:56:56, 16.87s/it] 45%|████▍     | 2237/5000 [10:36:47<12:36:58, 16.44s/it]                                                         {'loss': 19.4143, 'grad_norm': 17.625, 'learning_rate': 4.3883393953018014e-05, 'epoch': 2.84}
 45%|████▍     | 2237/5000 [10:36:47<12:36:58, 16.44s/it] 45%|████▍     | 2238/5000 [10:37:12<14:34:29, 19.00s/it]                                                         {'loss': 20.2305, 'grad_norm': 22.75, 'learning_rate': 4.386100146300451e-05, 'epoch': 2.84}
 45%|████▍     | 2238/5000 [10:37:12<14:34:29, 19.00s/it] 45%|████▍     | 2239/5000 [10:37:28<13:55:13, 18.15s/it]                                                         {'loss': 18.1095, 'grad_norm': 17.0, 'learning_rate': 4.3838605096887916e-05, 'epoch': 2.84}
 45%|████▍     | 2239/5000 [10:37:28<13:55:13, 18.15s/it] 45%|████▍     | 2240/5000 [10:37:43<13:00:17, 16.96s/it]                                                         {'loss': 20.1267, 'grad_norm': 34.75, 'learning_rate': 4.3816204864465183e-05, 'epoch': 2.84}
 45%|████▍     | 2240/5000 [10:37:43<13:00:17, 16.96s/it] 45%|████▍     | 2241/5000 [10:38:08<14:56:11, 19.49s/it]                                                         {'loss': 18.6405, 'grad_norm': 21.375, 'learning_rate': 4.37938007755349e-05, 'epoch': 2.85}
 45%|████▍     | 2241/5000 [10:38:08<14:56:11, 19.49s/it] 45%|████▍     | 2242/5000 [10:38:23<13:50:33, 18.07s/it]                                                         {'loss': 19.0111, 'grad_norm': 46.75, 'learning_rate': 4.377139283989741e-05, 'epoch': 2.85}
 45%|████▍     | 2242/5000 [10:38:23<13:50:33, 18.07s/it] 45%|████▍     | 2243/5000 [10:38:48<15:26:06, 20.15s/it]                                                         {'loss': 17.1172, 'grad_norm': 10.3125, 'learning_rate': 4.374898106735469e-05, 'epoch': 2.85}
 45%|████▍     | 2243/5000 [10:38:48<15:26:06, 20.15s/it] 45%|████▍     | 2244/5000 [10:39:08<15:22:56, 20.09s/it]                                                         {'loss': 18.3776, 'grad_norm': 31.875, 'learning_rate': 4.372656546771041e-05, 'epoch': 2.85}
 45%|████▍     | 2244/5000 [10:39:08<15:22:56, 20.09s/it] 45%|████▍     | 2245/5000 [10:39:24<14:24:25, 18.83s/it]                                                         {'loss': 17.295, 'grad_norm': 13.8125, 'learning_rate': 4.370414605076992e-05, 'epoch': 2.85}
 45%|████▍     | 2245/5000 [10:39:24<14:24:25, 18.83s/it] 45%|████▍     | 2246/5000 [10:39:40<13:56:36, 18.23s/it]                                                         {'loss': 20.4883, 'grad_norm': 22.5, 'learning_rate': 4.368172282634023e-05, 'epoch': 2.85}
 45%|████▍     | 2246/5000 [10:39:40<13:56:36, 18.23s/it] 45%|████▍     | 2247/5000 [10:39:53<12:39:49, 16.56s/it]                                                         {'loss': 20.451, 'grad_norm': 28.125, 'learning_rate': 4.365929580423001e-05, 'epoch': 2.85}
 45%|████▍     | 2247/5000 [10:39:53<12:39:49, 16.56s/it] 45%|████▍     | 2248/5000 [10:40:11<12:57:12, 16.94s/it]                                                         {'loss': 18.2988, 'grad_norm': 13.3125, 'learning_rate': 4.363686499424963e-05, 'epoch': 2.85}
 45%|████▍     | 2248/5000 [10:40:11<12:57:12, 16.94s/it] 45%|████▍     | 2249/5000 [10:40:29<13:17:03, 17.38s/it]                                                         {'loss': 17.1287, 'grad_norm': 12.8125, 'learning_rate': 4.361443040621106e-05, 'epoch': 2.86}
 45%|████▍     | 2249/5000 [10:40:29<13:17:03, 17.38s/it] 45%|████▌     | 2250/5000 [10:40:45<12:56:45, 16.95s/it]                                                         {'loss': 18.8476, 'grad_norm': 26.375, 'learning_rate': 4.359199204992797e-05, 'epoch': 2.86}
 45%|████▌     | 2250/5000 [10:40:45<12:56:45, 16.95s/it] 45%|████▌     | 2251/5000 [10:41:08<14:10:53, 18.57s/it]                                                         {'loss': 19.3647, 'grad_norm': 21.25, 'learning_rate': 4.356954993521565e-05, 'epoch': 2.86}
 45%|████▌     | 2251/5000 [10:41:08<14:10:53, 18.57s/it] 45%|████▌     | 2252/5000 [10:41:30<15:01:43, 19.69s/it]                                                         {'loss': 19.4997, 'grad_norm': 15.3125, 'learning_rate': 4.354710407189104e-05, 'epoch': 2.86}
 45%|████▌     | 2252/5000 [10:41:30<15:01:43, 19.69s/it] 45%|████▌     | 2253/5000 [10:41:44<13:40:51, 17.93s/it]                                                         {'loss': 20.2967, 'grad_norm': 13.6875, 'learning_rate': 4.3524654469772734e-05, 'epoch': 2.86}
 45%|████▌     | 2253/5000 [10:41:44<13:40:51, 17.93s/it] 45%|████▌     | 2254/5000 [10:42:02<13:42:43, 17.98s/it]                                                         {'loss': 18.7551, 'grad_norm': 15.0625, 'learning_rate': 4.350220113868094e-05, 'epoch': 2.86}
 45%|████▌     | 2254/5000 [10:42:02<13:42:43, 17.98s/it] 45%|████▌     | 2255/5000 [10:42:18<13:12:42, 17.33s/it]                                                         {'loss': 19.4482, 'grad_norm': 13.5625, 'learning_rate': 4.347974408843752e-05, 'epoch': 2.86}
 45%|████▌     | 2255/5000 [10:42:18<13:12:42, 17.33s/it] 45%|████▌     | 2256/5000 [10:42:30<12:09:51, 15.96s/it]                                                         {'loss': 19.1325, 'grad_norm': 17.875, 'learning_rate': 4.3457283328865944e-05, 'epoch': 2.86}
 45%|████▌     | 2256/5000 [10:42:30<12:09:51, 15.96s/it] 45%|████▌     | 2257/5000 [10:42:46<12:11:43, 16.01s/it]                                                         {'loss': 19.0101, 'grad_norm': 12.25, 'learning_rate': 4.343481886979131e-05, 'epoch': 2.87}
 45%|████▌     | 2257/5000 [10:42:46<12:11:43, 16.01s/it] 45%|████▌     | 2258/5000 [10:43:01<11:56:46, 15.68s/it]                                                         {'loss': 21.355, 'grad_norm': 27.75, 'learning_rate': 4.341235072104034e-05, 'epoch': 2.87}
 45%|████▌     | 2258/5000 [10:43:01<11:56:46, 15.68s/it] 45%|████▌     | 2259/5000 [10:43:17<11:50:26, 15.55s/it]                                                         {'loss': 21.2233, 'grad_norm': 31.0, 'learning_rate': 4.338987889244134e-05, 'epoch': 2.87}
 45%|████▌     | 2259/5000 [10:43:17<11:50:26, 15.55s/it] 45%|████▌     | 2260/5000 [10:43:30<11:23:29, 14.97s/it]                                                         {'loss': 18.4724, 'grad_norm': 15.0, 'learning_rate': 4.336740339382429e-05, 'epoch': 2.87}
 45%|████▌     | 2260/5000 [10:43:30<11:23:29, 14.97s/it] 45%|████▌     | 2261/5000 [10:43:44<11:05:38, 14.58s/it]                                                         {'loss': 19.2612, 'grad_norm': 15.1875, 'learning_rate': 4.334492423502073e-05, 'epoch': 2.87}
 45%|████▌     | 2261/5000 [10:43:44<11:05:38, 14.58s/it] 45%|████▌     | 2262/5000 [10:44:08<13:16:56, 17.46s/it]                                                         {'loss': 21.2229, 'grad_norm': 31.25, 'learning_rate': 4.332244142586379e-05, 'epoch': 2.87}
 45%|████▌     | 2262/5000 [10:44:08<13:16:56, 17.46s/it] 45%|████▌     | 2263/5000 [10:44:22<12:31:25, 16.47s/it]                                                         {'loss': 20.189, 'grad_norm': 37.5, 'learning_rate': 4.329995497618822e-05, 'epoch': 2.87}
 45%|████▌     | 2263/5000 [10:44:22<12:31:25, 16.47s/it] 45%|████▌     | 2264/5000 [10:44:37<12:13:11, 16.08s/it]                                                         {'loss': 19.1224, 'grad_norm': 32.5, 'learning_rate': 4.327746489583036e-05, 'epoch': 2.87}
 45%|████▌     | 2264/5000 [10:44:37<12:13:11, 16.08s/it] 45%|████▌     | 2265/5000 [10:45:02<14:10:17, 18.65s/it]                                                         {'loss': 21.3094, 'grad_norm': 36.25, 'learning_rate': 4.325497119462813e-05, 'epoch': 2.88}
 45%|████▌     | 2265/5000 [10:45:02<14:10:17, 18.65s/it] 45%|████▌     | 2266/5000 [10:45:19<13:41:12, 18.02s/it]                                                         {'loss': 18.7407, 'grad_norm': 21.75, 'learning_rate': 4.3232473882421065e-05, 'epoch': 2.88}
 45%|████▌     | 2266/5000 [10:45:19<13:41:12, 18.02s/it] 45%|████▌     | 2267/5000 [10:45:35<13:17:11, 17.50s/it]                                                         {'loss': 18.0498, 'grad_norm': 10.5, 'learning_rate': 4.3209972969050217e-05, 'epoch': 2.88}
 45%|████▌     | 2267/5000 [10:45:35<13:17:11, 17.50s/it] 45%|████▌     | 2268/5000 [10:45:52<13:08:29, 17.32s/it]                                                         {'loss': 19.3182, 'grad_norm': 17.5, 'learning_rate': 4.3187468464358266e-05, 'epoch': 2.88}
 45%|████▌     | 2268/5000 [10:45:52<13:08:29, 17.32s/it] 45%|████▌     | 2269/5000 [10:46:28<17:25:41, 22.97s/it]                                                         {'loss': 19.4115, 'grad_norm': 42.25, 'learning_rate': 4.316496037818945e-05, 'epoch': 2.88}
 45%|████▌     | 2269/5000 [10:46:28<17:25:41, 22.97s/it] 45%|████▌     | 2270/5000 [10:46:43<15:39:52, 20.66s/it]                                                         {'loss': 19.0638, 'grad_norm': 21.75, 'learning_rate': 4.3142448720389555e-05, 'epoch': 2.88}
 45%|████▌     | 2270/5000 [10:46:43<15:39:52, 20.66s/it] 45%|████▌     | 2271/5000 [10:47:03<15:33:55, 20.53s/it]                                                         {'loss': 18.0074, 'grad_norm': 14.0, 'learning_rate': 4.311993350080597e-05, 'epoch': 2.88}
 45%|████▌     | 2271/5000 [10:47:04<15:33:55, 20.53s/it] 45%|████▌     | 2272/5000 [10:47:28<16:30:03, 21.78s/it]                                                         {'loss': 20.3336, 'grad_norm': 19.25, 'learning_rate': 4.30974147292876e-05, 'epoch': 2.89}
 45%|████▌     | 2272/5000 [10:47:28<16:30:03, 21.78s/it] 45%|████▌     | 2273/5000 [10:47:45<15:25:23, 20.36s/it]                                                         {'loss': 17.8497, 'grad_norm': 23.625, 'learning_rate': 4.307489241568492e-05, 'epoch': 2.89}
 45%|████▌     | 2273/5000 [10:47:45<15:25:23, 20.36s/it] 45%|████▌     | 2274/5000 [10:48:05<15:10:12, 20.03s/it]                                                         {'loss': 18.5695, 'grad_norm': 16.5, 'learning_rate': 4.3052366569849975e-05, 'epoch': 2.89}
 45%|████▌     | 2274/5000 [10:48:05<15:10:12, 20.03s/it] 46%|████▌     | 2275/5000 [10:48:19<13:56:08, 18.41s/it]                                                         {'loss': 19.6669, 'grad_norm': 16.625, 'learning_rate': 4.30298372016363e-05, 'epoch': 2.89}
 46%|████▌     | 2275/5000 [10:48:19<13:56:08, 18.41s/it] 46%|████▌     | 2276/5000 [10:48:36<13:36:03, 17.97s/it]                                                         {'loss': 19.5295, 'grad_norm': 75.0, 'learning_rate': 4.300730432089904e-05, 'epoch': 2.89}
 46%|████▌     | 2276/5000 [10:48:36<13:36:03, 17.97s/it] 46%|████▌     | 2277/5000 [10:48:50<12:38:20, 16.71s/it]                                                         {'loss': 20.9653, 'grad_norm': 35.5, 'learning_rate': 4.298476793749483e-05, 'epoch': 2.89}
 46%|████▌     | 2277/5000 [10:48:50<12:38:20, 16.71s/it] 46%|████▌     | 2278/5000 [10:49:07<12:47:38, 16.92s/it]                                                         {'loss': 18.919, 'grad_norm': 44.0, 'learning_rate': 4.296222806128184e-05, 'epoch': 2.89}
 46%|████▌     | 2278/5000 [10:49:07<12:47:38, 16.92s/it] 46%|████▌     | 2279/5000 [10:49:22<12:13:31, 16.17s/it]                                                         {'loss': 20.4216, 'grad_norm': 40.25, 'learning_rate': 4.293968470211979e-05, 'epoch': 2.89}
 46%|████▌     | 2279/5000 [10:49:22<12:13:31, 16.17s/it] 46%|████▌     | 2280/5000 [10:49:36<11:43:53, 15.53s/it]                                                         {'loss': 18.7005, 'grad_norm': 25.25, 'learning_rate': 4.291713786986991e-05, 'epoch': 2.9}
 46%|████▌     | 2280/5000 [10:49:36<11:43:53, 15.53s/it] 46%|████▌     | 2281/5000 [10:49:50<11:21:31, 15.04s/it]                                                         {'loss': 20.8045, 'grad_norm': 29.25, 'learning_rate': 4.289458757439495e-05, 'epoch': 2.9}
 46%|████▌     | 2281/5000 [10:49:50<11:21:31, 15.04s/it] 46%|████▌     | 2282/5000 [10:50:13<13:20:29, 17.67s/it]                                                         {'loss': 19.1402, 'grad_norm': 21.625, 'learning_rate': 4.287203382555916e-05, 'epoch': 2.9}
 46%|████▌     | 2282/5000 [10:50:13<13:20:29, 17.67s/it] 46%|████▌     | 2283/5000 [10:50:28<12:35:18, 16.68s/it]                                                         {'loss': 18.6855, 'grad_norm': 23.375, 'learning_rate': 4.284947663322834e-05, 'epoch': 2.9}
 46%|████▌     | 2283/5000 [10:50:28<12:35:18, 16.68s/it] 46%|████▌     | 2284/5000 [10:50:45<12:45:42, 16.92s/it]                                                         {'loss': 17.6035, 'grad_norm': 21.375, 'learning_rate': 4.282691600726976e-05, 'epoch': 2.9}
 46%|████▌     | 2284/5000 [10:50:45<12:45:42, 16.92s/it] 46%|████▌     | 2285/5000 [10:51:00<12:10:12, 16.14s/it]                                                         {'loss': 18.577, 'grad_norm': 15.125, 'learning_rate': 4.2804351957552206e-05, 'epoch': 2.9}
 46%|████▌     | 2285/5000 [10:51:00<12:10:12, 16.14s/it] 46%|████▌     | 2286/5000 [10:51:16<12:19:19, 16.34s/it]                                                         {'loss': 18.8808, 'grad_norm': 22.625, 'learning_rate': 4.278178449394595e-05, 'epoch': 2.9}
 46%|████▌     | 2286/5000 [10:51:16<12:19:19, 16.34s/it] 46%|████▌     | 2287/5000 [10:51:34<12:31:31, 16.62s/it]                                                         {'loss': 18.5903, 'grad_norm': 154.0, 'learning_rate': 4.275921362632279e-05, 'epoch': 2.9}
 46%|████▌     | 2287/5000 [10:51:34<12:31:31, 16.62s/it] 46%|████▌     | 2288/5000 [10:51:50<12:33:03, 16.66s/it]                                                         {'loss': 18.8583, 'grad_norm': 20.875, 'learning_rate': 4.2736639364555974e-05, 'epoch': 2.91}
 46%|████▌     | 2288/5000 [10:51:50<12:33:03, 16.66s/it] 46%|████▌     | 2289/5000 [10:52:06<12:21:14, 16.41s/it]                                                         {'loss': 19.851, 'grad_norm': 15.625, 'learning_rate': 4.271406171852025e-05, 'epoch': 2.91}
 46%|████▌     | 2289/5000 [10:52:06<12:21:14, 16.41s/it] 46%|████▌     | 2290/5000 [10:52:22<12:14:33, 16.26s/it]                                                         {'loss': 20.6463, 'grad_norm': 20.375, 'learning_rate': 4.2691480698091854e-05, 'epoch': 2.91}
 46%|████▌     | 2290/5000 [10:52:22<12:14:33, 16.26s/it] 46%|████▌     | 2291/5000 [10:52:36<11:41:05, 15.53s/it]                                                         {'loss': 18.7011, 'grad_norm': 13.125, 'learning_rate': 4.26688963131485e-05, 'epoch': 2.91}
 46%|████▌     | 2291/5000 [10:52:36<11:41:05, 15.53s/it] 46%|████▌     | 2292/5000 [10:52:50<11:26:29, 15.21s/it]                                                         {'loss': 20.5248, 'grad_norm': 16.625, 'learning_rate': 4.2646308573569337e-05, 'epoch': 2.91}
 46%|████▌     | 2292/5000 [10:52:50<11:26:29, 15.21s/it] 46%|████▌     | 2293/5000 [10:53:06<11:29:17, 15.28s/it]                                                         {'loss': 19.069, 'grad_norm': 18.875, 'learning_rate': 4.2623717489235046e-05, 'epoch': 2.91}
 46%|████▌     | 2293/5000 [10:53:06<11:29:17, 15.28s/it] 46%|████▌     | 2294/5000 [10:53:20<11:18:19, 15.04s/it]                                                         {'loss': 20.3, 'grad_norm': 27.0, 'learning_rate': 4.260112307002771e-05, 'epoch': 2.91}
 46%|████▌     | 2294/5000 [10:53:20<11:18:19, 15.04s/it] 46%|████▌     | 2295/5000 [10:53:44<13:18:16, 17.71s/it]                                                         {'loss': 21.3516, 'grad_norm': 21.375, 'learning_rate': 4.257852532583089e-05, 'epoch': 2.91}
 46%|████▌     | 2295/5000 [10:53:44<13:18:16, 17.71s/it] 46%|████▌     | 2296/5000 [10:54:01<13:08:42, 17.50s/it]                                                         {'loss': 20.456, 'grad_norm': 29.5, 'learning_rate': 4.2555924266529626e-05, 'epoch': 2.92}
 46%|████▌     | 2296/5000 [10:54:01<13:08:42, 17.50s/it] 46%|████▌     | 2297/5000 [10:54:22<13:57:55, 18.60s/it]                                                         {'loss': 19.1909, 'grad_norm': 36.25, 'learning_rate': 4.253331990201038e-05, 'epoch': 2.92}
 46%|████▌     | 2297/5000 [10:54:22<13:57:55, 18.60s/it] 46%|████▌     | 2298/5000 [10:54:36<12:54:08, 17.19s/it]                                                         {'loss': 18.6403, 'grad_norm': 19.125, 'learning_rate': 4.251071224216107e-05, 'epoch': 2.92}
 46%|████▌     | 2298/5000 [10:54:36<12:54:08, 17.19s/it] 46%|████▌     | 2299/5000 [10:54:52<12:31:44, 16.70s/it]                                                         {'loss': 19.0665, 'grad_norm': 13.5625, 'learning_rate': 4.2488101296871055e-05, 'epoch': 2.92}
 46%|████▌     | 2299/5000 [10:54:52<12:31:44, 16.70s/it] 46%|████▌     | 2300/5000 [10:55:06<12:01:59, 16.04s/it]                                                         {'loss': 18.3643, 'grad_norm': 13.5, 'learning_rate': 4.246548707603114e-05, 'epoch': 2.92}
 46%|████▌     | 2300/5000 [10:55:06<12:01:59, 16.04s/it] 46%|████▌     | 2301/5000 [10:55:22<12:00:26, 16.02s/it]                                                         {'loss': 19.8823, 'grad_norm': 20.75, 'learning_rate': 4.244286958953354e-05, 'epoch': 2.92}
 46%|████▌     | 2301/5000 [10:55:22<12:00:26, 16.02s/it] 46%|████▌     | 2302/5000 [10:55:38<11:50:49, 15.81s/it]                                                         {'loss': 17.8321, 'grad_norm': 12.6875, 'learning_rate': 4.2420248847271914e-05, 'epoch': 2.92}
 46%|████▌     | 2302/5000 [10:55:38<11:50:49, 15.81s/it] 46%|████▌     | 2303/5000 [10:56:02<13:46:51, 18.40s/it]                                                         {'loss': 19.7438, 'grad_norm': 20.5, 'learning_rate': 4.2397624859141345e-05, 'epoch': 2.92}
 46%|████▌     | 2303/5000 [10:56:02<13:46:51, 18.40s/it] 46%|████▌     | 2304/5000 [10:56:17<13:01:54, 17.40s/it]                                                         {'loss': 19.3778, 'grad_norm': 25.125, 'learning_rate': 4.237499763503833e-05, 'epoch': 2.93}
 46%|████▌     | 2304/5000 [10:56:17<13:01:54, 17.40s/it] 46%|████▌     | 2305/5000 [10:56:37<13:32:03, 18.08s/it]                                                         {'loss': 19.2272, 'grad_norm': 25.375, 'learning_rate': 4.23523671848608e-05, 'epoch': 2.93}
 46%|████▌     | 2305/5000 [10:56:37<13:32:03, 18.08s/it] 46%|████▌     | 2306/5000 [10:56:52<12:51:10, 17.18s/it]                                                         {'loss': 21.0859, 'grad_norm': 29.25, 'learning_rate': 4.232973351850807e-05, 'epoch': 2.93}
 46%|████▌     | 2306/5000 [10:56:52<12:51:10, 17.18s/it] 46%|████▌     | 2307/5000 [10:57:08<12:30:26, 16.72s/it]                                                         {'loss': 18.4311, 'grad_norm': 12.5625, 'learning_rate': 4.2307096645880873e-05, 'epoch': 2.93}
 46%|████▌     | 2307/5000 [10:57:08<12:30:26, 16.72s/it] 46%|████▌     | 2308/5000 [10:57:24<12:27:46, 16.67s/it]                                                         {'loss': 19.2826, 'grad_norm': 18.5, 'learning_rate': 4.228445657688136e-05, 'epoch': 2.93}
 46%|████▌     | 2308/5000 [10:57:24<12:27:46, 16.67s/it] 46%|████▌     | 2309/5000 [10:57:51<14:41:46, 19.66s/it]                                                         {'loss': 19.2255, 'grad_norm': 21.375, 'learning_rate': 4.226181332141305e-05, 'epoch': 2.93}
 46%|████▌     | 2309/5000 [10:57:51<14:41:46, 19.66s/it] 46%|████▌     | 2310/5000 [10:58:05<13:32:59, 18.13s/it]                                                         {'loss': 20.3028, 'grad_norm': 17.5, 'learning_rate': 4.223916688938088e-05, 'epoch': 2.93}
 46%|████▌     | 2310/5000 [10:58:05<13:32:59, 18.13s/it] 46%|████▌     | 2311/5000 [10:58:23<13:27:51, 18.03s/it]                                                         {'loss': 18.4122, 'grad_norm': 22.875, 'learning_rate': 4.2216517290691175e-05, 'epoch': 2.93}
 46%|████▌     | 2311/5000 [10:58:23<13:27:51, 18.03s/it] 46%|████▌     | 2312/5000 [10:58:49<15:06:59, 20.25s/it]                                                         {'loss': 17.7419, 'grad_norm': 13.25, 'learning_rate': 4.219386453525163e-05, 'epoch': 2.94}
 46%|████▌     | 2312/5000 [10:58:49<15:06:59, 20.25s/it] 46%|████▋     | 2313/5000 [10:59:05<14:18:58, 19.18s/it]                                                         {'loss': 18.8112, 'grad_norm': 27.625, 'learning_rate': 4.2171208632971325e-05, 'epoch': 2.94}
 46%|████▋     | 2313/5000 [10:59:05<14:18:58, 19.18s/it] 46%|████▋     | 2314/5000 [10:59:21<13:35:29, 18.22s/it]                                                         {'loss': 19.313, 'grad_norm': 28.25, 'learning_rate': 4.214854959376073e-05, 'epoch': 2.94}
 46%|████▋     | 2314/5000 [10:59:21<13:35:29, 18.22s/it] 46%|████▋     | 2315/5000 [10:59:38<13:19:01, 17.86s/it]                                                         {'loss': 16.9181, 'grad_norm': 11.5, 'learning_rate': 4.212588742753168e-05, 'epoch': 2.94}
 46%|████▋     | 2315/5000 [10:59:38<13:19:01, 17.86s/it] 46%|████▋     | 2316/5000 [10:59:50<11:57:13, 16.03s/it]                                                         {'loss': 20.5658, 'grad_norm': 20.625, 'learning_rate': 4.210322214419738e-05, 'epoch': 2.94}
 46%|████▋     | 2316/5000 [10:59:50<11:57:13, 16.03s/it] 46%|████▋     | 2317/5000 [11:00:03<11:13:28, 15.06s/it]                                                         {'loss': 21.104, 'grad_norm': 30.125, 'learning_rate': 4.2080553753672376e-05, 'epoch': 2.94}
 46%|████▋     | 2317/5000 [11:00:03<11:13:28, 15.06s/it] 46%|████▋     | 2318/5000 [11:00:20<11:36:32, 15.58s/it]                                                         {'loss': 17.6283, 'grad_norm': 18.0, 'learning_rate': 4.205788226587262e-05, 'epoch': 2.94}
 46%|████▋     | 2318/5000 [11:00:20<11:36:32, 15.58s/it] 46%|████▋     | 2319/5000 [11:00:36<11:51:33, 15.92s/it]                                                         {'loss': 18.5501, 'grad_norm': 14.4375, 'learning_rate': 4.2035207690715354e-05, 'epoch': 2.94}
 46%|████▋     | 2319/5000 [11:00:36<11:51:33, 15.92s/it] 46%|████▋     | 2320/5000 [11:00:53<12:00:05, 16.12s/it]                                                         {'loss': 19.3692, 'grad_norm': 17.875, 'learning_rate': 4.201253003811924e-05, 'epoch': 2.95}
 46%|████▋     | 2320/5000 [11:00:53<12:00:05, 16.12s/it] 46%|████▋     | 2321/5000 [11:01:08<11:49:42, 15.89s/it]                                                         {'loss': 18.7953, 'grad_norm': 23.125, 'learning_rate': 4.198984931800424e-05, 'epoch': 2.95}
 46%|████▋     | 2321/5000 [11:01:08<11:49:42, 15.89s/it] 46%|████▋     | 2322/5000 [11:01:31<13:21:02, 17.95s/it]                                                         {'loss': 17.5973, 'grad_norm': 15.0625, 'learning_rate': 4.196716554029168e-05, 'epoch': 2.95}
 46%|████▋     | 2322/5000 [11:01:31<13:21:02, 17.95s/it] 46%|████▋     | 2323/5000 [11:01:47<12:55:19, 17.38s/it]                                                         {'loss': 19.963, 'grad_norm': 21.5, 'learning_rate': 4.194447871490421e-05, 'epoch': 2.95}
 46%|████▋     | 2323/5000 [11:01:47<12:55:19, 17.38s/it] 46%|████▋     | 2324/5000 [11:02:02<12:18:32, 16.56s/it]                                                         {'loss': 19.7634, 'grad_norm': 14.375, 'learning_rate': 4.192178885176581e-05, 'epoch': 2.95}
 46%|████▋     | 2324/5000 [11:02:02<12:18:32, 16.56s/it] 46%|████▋     | 2325/5000 [11:02:29<14:41:03, 19.76s/it]                                                         {'loss': 18.4193, 'grad_norm': 16.0, 'learning_rate': 4.1899095960801805e-05, 'epoch': 2.95}
 46%|████▋     | 2325/5000 [11:02:29<14:41:03, 19.76s/it] 47%|████▋     | 2326/5000 [11:02:45<13:46:49, 18.55s/it]                                                         {'loss': 18.439, 'grad_norm': 27.625, 'learning_rate': 4.1876400051938845e-05, 'epoch': 2.95}
 47%|████▋     | 2326/5000 [11:02:45<13:46:49, 18.55s/it] 47%|████▋     | 2327/5000 [11:03:11<15:28:58, 20.85s/it]                                                         {'loss': 19.7496, 'grad_norm': 50.0, 'learning_rate': 4.185370113510488e-05, 'epoch': 2.95}
 47%|████▋     | 2327/5000 [11:03:11<15:28:58, 20.85s/it] 47%|████▋     | 2328/5000 [11:03:26<14:11:38, 19.12s/it]                                                         {'loss': 17.9299, 'grad_norm': 24.25, 'learning_rate': 4.1830999220229194e-05, 'epoch': 2.96}
 47%|████▋     | 2328/5000 [11:03:26<14:11:38, 19.12s/it] 47%|████▋     | 2329/5000 [11:03:39<12:48:51, 17.27s/it]                                                         {'loss': 19.1369, 'grad_norm': 17.75, 'learning_rate': 4.1808294317242354e-05, 'epoch': 2.96}
 47%|████▋     | 2329/5000 [11:03:39<12:48:51, 17.27s/it] 47%|████▋     | 2330/5000 [11:04:00<13:39:37, 18.42s/it]                                                         {'loss': 17.4456, 'grad_norm': 16.625, 'learning_rate': 4.178558643607629e-05, 'epoch': 2.96}
 47%|████▋     | 2330/5000 [11:04:00<13:39:37, 18.42s/it] 47%|████▋     | 2331/5000 [11:04:13<12:29:35, 16.85s/it]                                                         {'loss': 19.0283, 'grad_norm': 39.25, 'learning_rate': 4.1762875586664165e-05, 'epoch': 2.96}
 47%|████▋     | 2331/5000 [11:04:13<12:29:35, 16.85s/it] 47%|████▋     | 2332/5000 [11:04:43<15:16:43, 20.62s/it]                                                         {'loss': 19.2153, 'grad_norm': 35.75, 'learning_rate': 4.1740161778940515e-05, 'epoch': 2.96}
 47%|████▋     | 2332/5000 [11:04:43<15:16:43, 20.62s/it] 47%|████▋     | 2333/5000 [11:05:00<14:33:24, 19.65s/it]                                                         {'loss': 18.1741, 'grad_norm': 16.5, 'learning_rate': 4.17174450228411e-05, 'epoch': 2.96}
 47%|████▋     | 2333/5000 [11:05:00<14:33:24, 19.65s/it] 47%|████▋     | 2334/5000 [11:05:13<13:06:28, 17.70s/it]                                                         {'loss': 22.4113, 'grad_norm': 26.125, 'learning_rate': 4.1694725328303026e-05, 'epoch': 2.96}
 47%|████▋     | 2334/5000 [11:05:13<13:06:28, 17.70s/it] 47%|████▋     | 2335/5000 [11:05:44<15:57:58, 21.57s/it]                                                         {'loss': 21.3987, 'grad_norm': 1184.0, 'learning_rate': 4.167200270526464e-05, 'epoch': 2.97}
 47%|████▋     | 2335/5000 [11:05:44<15:57:58, 21.57s/it] 47%|████▋     | 2336/5000 [11:06:01<14:58:45, 20.24s/it]                                                         {'loss': 18.7052, 'grad_norm': 16.125, 'learning_rate': 4.16492771636656e-05, 'epoch': 2.97}
 47%|████▋     | 2336/5000 [11:06:01<14:58:45, 20.24s/it] 47%|████▋     | 2337/5000 [11:06:28<16:28:27, 22.27s/it]                                                         {'loss': 21.1601, 'grad_norm': 73.0, 'learning_rate': 4.1626548713446815e-05, 'epoch': 2.97}
 47%|████▋     | 2337/5000 [11:06:28<16:28:27, 22.27s/it] 47%|████▋     | 2338/5000 [11:06:44<15:04:58, 20.40s/it]                                                         {'loss': 18.0804, 'grad_norm': 28.5, 'learning_rate': 4.160381736455049e-05, 'epoch': 2.97}
 47%|████▋     | 2338/5000 [11:06:44<15:04:58, 20.40s/it] 47%|████▋     | 2339/5000 [11:06:58<13:39:42, 18.48s/it]                                                         {'loss': 19.7752, 'grad_norm': 12.6875, 'learning_rate': 4.15810831269201e-05, 'epoch': 2.97}
 47%|████▋     | 2339/5000 [11:06:58<13:39:42, 18.48s/it] 47%|████▋     | 2340/5000 [11:07:18<14:01:31, 18.98s/it]                                                         {'loss': 18.8099, 'grad_norm': 15.9375, 'learning_rate': 4.1558346010500367e-05, 'epoch': 2.97}
 47%|████▋     | 2340/5000 [11:07:18<14:01:31, 18.98s/it] 47%|████▋     | 2341/5000 [11:07:33<13:08:00, 17.78s/it]                                                         {'loss': 18.2915, 'grad_norm': 16.75, 'learning_rate': 4.153560602523726e-05, 'epoch': 2.97}
 47%|████▋     | 2341/5000 [11:07:33<13:08:00, 17.78s/it] 47%|████▋     | 2342/5000 [11:07:52<13:20:10, 18.06s/it]                                                         {'loss': 18.8408, 'grad_norm': 17.0, 'learning_rate': 4.1512863181078045e-05, 'epoch': 2.97}
 47%|████▋     | 2342/5000 [11:07:52<13:20:10, 18.06s/it] 47%|████▋     | 2343/5000 [11:08:11<13:29:46, 18.29s/it]                                                         {'loss': 18.5224, 'grad_norm': 31.375, 'learning_rate': 4.1490117487971206e-05, 'epoch': 2.98}
 47%|████▋     | 2343/5000 [11:08:11<13:29:46, 18.29s/it] 47%|████▋     | 2344/5000 [11:08:29<13:35:33, 18.42s/it]                                                         {'loss': 16.8119, 'grad_norm': 10.5, 'learning_rate': 4.1467368955866484e-05, 'epoch': 2.98}
 47%|████▋     | 2344/5000 [11:08:29<13:35:33, 18.42s/it] 47%|████▋     | 2345/5000 [11:08:43<12:29:55, 16.95s/it]                                                         {'loss': 19.1199, 'grad_norm': 16.0, 'learning_rate': 4.144461759471484e-05, 'epoch': 2.98}
 47%|████▋     | 2345/5000 [11:08:43<12:29:55, 16.95s/it] 47%|████▋     | 2346/5000 [11:09:00<12:32:53, 17.02s/it]                                                         {'loss': 20.0801, 'grad_norm': 25.25, 'learning_rate': 4.142186341446853e-05, 'epoch': 2.98}
 47%|████▋     | 2346/5000 [11:09:00<12:32:53, 17.02s/it] 47%|████▋     | 2347/5000 [11:09:17<12:29:12, 16.94s/it]                                                         {'loss': 18.8505, 'grad_norm': 25.625, 'learning_rate': 4.139910642508099e-05, 'epoch': 2.98}
 47%|████▋     | 2347/5000 [11:09:17<12:29:12, 16.94s/it] 47%|████▋     | 2348/5000 [11:09:35<12:42:05, 17.24s/it]                                                         {'loss': 19.6295, 'grad_norm': 27.75, 'learning_rate': 4.1376346636506884e-05, 'epoch': 2.98}
 47%|████▋     | 2348/5000 [11:09:35<12:42:05, 17.24s/it] 47%|████▋     | 2349/5000 [11:09:56<13:28:13, 18.29s/it]                                                         {'loss': 19.7833, 'grad_norm': 30.375, 'learning_rate': 4.135358405870212e-05, 'epoch': 2.98}
 47%|████▋     | 2349/5000 [11:09:56<13:28:13, 18.29s/it] 47%|████▋     | 2350/5000 [11:10:12<13:06:49, 17.81s/it]                                                         {'loss': 17.701, 'grad_norm': 11.375, 'learning_rate': 4.133081870162385e-05, 'epoch': 2.98}
 47%|████▋     | 2350/5000 [11:10:12<13:06:49, 17.81s/it] 47%|████▋     | 2351/5000 [11:10:29<12:55:48, 17.57s/it]                                                         {'loss': 18.3299, 'grad_norm': 20.0, 'learning_rate': 4.1308050575230376e-05, 'epoch': 2.99}
 47%|████▋     | 2351/5000 [11:10:29<12:55:48, 17.57s/it] 47%|████▋     | 2352/5000 [11:10:43<12:09:33, 16.53s/it]                                                         {'loss': 19.3879, 'grad_norm': 24.75, 'learning_rate': 4.1285279689481294e-05, 'epoch': 2.99}
 47%|████▋     | 2352/5000 [11:10:43<12:09:33, 16.53s/it] 47%|████▋     | 2353/5000 [11:11:00<12:07:10, 16.48s/it]                                                         {'loss': 17.6884, 'grad_norm': 16.75, 'learning_rate': 4.1262506054337316e-05, 'epoch': 2.99}
 47%|████▋     | 2353/5000 [11:11:00<12:07:10, 16.48s/it] 47%|████▋     | 2354/5000 [11:11:17<12:13:43, 16.64s/it]                                                         {'loss': 18.7513, 'grad_norm': 17.5, 'learning_rate': 4.123972967976044e-05, 'epoch': 2.99}
 47%|████▋     | 2354/5000 [11:11:17<12:13:43, 16.64s/it] 47%|████▋     | 2355/5000 [11:11:31<11:45:58, 16.01s/it]                                                         {'loss': 18.5927, 'grad_norm': 26.875, 'learning_rate': 4.1216950575713805e-05, 'epoch': 2.99}
 47%|████▋     | 2355/5000 [11:11:31<11:45:58, 16.01s/it] 47%|████▋     | 2356/5000 [11:11:45<11:22:18, 15.48s/it]                                                         {'loss': 18.3814, 'grad_norm': 21.5, 'learning_rate': 4.1194168752161766e-05, 'epoch': 2.99}
 47%|████▋     | 2356/5000 [11:11:45<11:22:18, 15.48s/it] 47%|████▋     | 2357/5000 [11:12:12<13:41:40, 18.65s/it]                                                         {'loss': 18.1071, 'grad_norm': 11.5, 'learning_rate': 4.117138421906988e-05, 'epoch': 2.99}
 47%|████▋     | 2357/5000 [11:12:12<13:41:40, 18.65s/it] 47%|████▋     | 2358/5000 [11:12:27<12:56:40, 17.64s/it]                                                         {'loss': 19.1468, 'grad_norm': 27.0, 'learning_rate': 4.1148596986404884e-05, 'epoch': 2.99}
 47%|████▋     | 2358/5000 [11:12:27<12:56:40, 17.64s/it] 47%|████▋     | 2359/5000 [11:12:41<12:13:55, 16.67s/it]                                                         {'loss': 22.2785, 'grad_norm': 53.0, 'learning_rate': 4.112580706413466e-05, 'epoch': 3.0}
 47%|████▋     | 2359/5000 [11:12:41<12:13:55, 16.67s/it] 47%|████▋     | 2360/5000 [11:13:00<12:47:02, 17.43s/it]                                                         {'loss': 16.9997, 'grad_norm': 16.875, 'learning_rate': 4.1103014462228306e-05, 'epoch': 3.0}
 47%|████▋     | 2360/5000 [11:13:00<12:47:02, 17.43s/it] 47%|████▋     | 2361/5000 [11:13:21<13:32:10, 18.47s/it]                                                         {'loss': 16.9305, 'grad_norm': 12.0625, 'learning_rate': 4.1080219190656083e-05, 'epoch': 3.0}
 47%|████▋     | 2361/5000 [11:13:21<13:32:10, 18.47s/it] 47%|████▋     | 2362/5000 [11:13:49<15:30:26, 21.16s/it]                                                         {'loss': 17.8761, 'grad_norm': 17.25, 'learning_rate': 4.105742125938942e-05, 'epoch': 3.0}
 47%|████▋     | 2362/5000 [11:13:49<15:30:26, 21.16s/it] 47%|████▋     | 2363/5000 [11:14:06<14:38:43, 19.99s/it]                                                         {'loss': 22.4776, 'grad_norm': 144.0, 'learning_rate': 4.10346206784009e-05, 'epoch': 3.0}
 47%|████▋     | 2363/5000 [11:14:06<14:38:43, 19.99s/it] 47%|████▋     | 2364/5000 [11:14:32<16:01:22, 21.88s/it]                                                         {'loss': 18.236, 'grad_norm': 15.125, 'learning_rate': 4.101181745766428e-05, 'epoch': 3.0}
 47%|████▋     | 2364/5000 [11:14:32<16:01:22, 21.88s/it] 47%|████▋     | 2365/5000 [11:14:59<17:02:54, 23.29s/it]                                                         {'loss': 18.9915, 'grad_norm': 19.5, 'learning_rate': 4.098901160715444e-05, 'epoch': 3.0}
 47%|████▋     | 2365/5000 [11:14:59<17:02:54, 23.29s/it] 47%|████▋     | 2366/5000 [11:15:18<16:05:18, 21.99s/it]                                                         {'loss': 19.7116, 'grad_norm': 13.75, 'learning_rate': 4.096620313684748e-05, 'epoch': 3.0}
 47%|████▋     | 2366/5000 [11:15:18<16:05:18, 21.99s/it] 47%|████▋     | 2367/5000 [11:15:43<16:44:50, 22.90s/it]                                                         {'loss': 16.9742, 'grad_norm': 21.5, 'learning_rate': 4.094339205672054e-05, 'epoch': 3.01}
 47%|████▋     | 2367/5000 [11:15:43<16:44:50, 22.90s/it] 47%|████▋     | 2368/5000 [11:15:59<15:18:37, 20.94s/it]                                                         {'loss': 19.2083, 'grad_norm': 36.0, 'learning_rate': 4.092057837675201e-05, 'epoch': 3.01}
 47%|████▋     | 2368/5000 [11:15:59<15:18:37, 20.94s/it] 47%|████▋     | 2369/5000 [11:16:13<13:40:54, 18.72s/it]                                                         {'loss': 19.268, 'grad_norm': 16.875, 'learning_rate': 4.089776210692135e-05, 'epoch': 3.01}
 47%|████▋     | 2369/5000 [11:16:13<13:40:54, 18.72s/it] 47%|████▋     | 2370/5000 [11:16:30<13:23:03, 18.32s/it]                                                         {'loss': 18.131, 'grad_norm': 16.0, 'learning_rate': 4.0874943257209166e-05, 'epoch': 3.01}
 47%|████▋     | 2370/5000 [11:16:30<13:23:03, 18.32s/it] 47%|████▋     | 2371/5000 [11:16:44<12:17:27, 16.83s/it]                                                         {'loss': 20.4849, 'grad_norm': 19.75, 'learning_rate': 4.0852121837597216e-05, 'epoch': 3.01}
 47%|████▋     | 2371/5000 [11:16:44<12:17:27, 16.83s/it] 47%|████▋     | 2372/5000 [11:16:57<11:27:44, 15.70s/it]                                                         {'loss': 19.2977, 'grad_norm': 15.875, 'learning_rate': 4.082929785806835e-05, 'epoch': 3.01}
 47%|████▋     | 2372/5000 [11:16:57<11:27:44, 15.70s/it] 47%|████▋     | 2373/5000 [11:17:13<11:37:22, 15.93s/it]                                                         {'loss': 18.4018, 'grad_norm': 12.75, 'learning_rate': 4.0806471328606544e-05, 'epoch': 3.01}
 47%|████▋     | 2373/5000 [11:17:13<11:37:22, 15.93s/it] 47%|████▋     | 2374/5000 [11:17:33<12:29:12, 17.12s/it]                                                         {'loss': 19.5788, 'grad_norm': 16.125, 'learning_rate': 4.0783642259196936e-05, 'epoch': 3.01}
 47%|████▋     | 2374/5000 [11:17:33<12:29:12, 17.12s/it] 48%|████▊     | 2375/5000 [11:17:45<11:28:20, 15.73s/it]                                                         {'loss': 20.3459, 'grad_norm': 17.0, 'learning_rate': 4.076081065982569e-05, 'epoch': 3.02}
 48%|████▊     | 2375/5000 [11:17:45<11:28:20, 15.73s/it] 48%|████▊     | 2376/5000 [11:18:01<11:25:40, 15.68s/it]                                                         {'loss': 19.0777, 'grad_norm': 14.875, 'learning_rate': 4.073797654048014e-05, 'epoch': 3.02}
 48%|████▊     | 2376/5000 [11:18:01<11:25:40, 15.68s/it] 48%|████▊     | 2377/5000 [11:18:17<11:28:32, 15.75s/it]                                                         {'loss': 19.7224, 'grad_norm': 17.875, 'learning_rate': 4.071513991114872e-05, 'epoch': 3.02}
 48%|████▊     | 2377/5000 [11:18:17<11:28:32, 15.75s/it] 48%|████▊     | 2378/5000 [11:18:30<10:53:16, 14.95s/it]                                                         {'loss': 20.1982, 'grad_norm': 23.875, 'learning_rate': 4.069230078182093e-05, 'epoch': 3.02}
 48%|████▊     | 2378/5000 [11:18:30<10:53:16, 14.95s/it] 48%|████▊     | 2379/5000 [11:18:51<12:15:55, 16.85s/it]                                                         {'loss': 17.3863, 'grad_norm': 8.1875, 'learning_rate': 4.066945916248738e-05, 'epoch': 3.02}
 48%|████▊     | 2379/5000 [11:18:51<12:15:55, 16.85s/it] 48%|████▊     | 2380/5000 [11:19:09<12:23:43, 17.03s/it]                                                         {'loss': 18.371, 'grad_norm': 11.6875, 'learning_rate': 4.0646615063139787e-05, 'epoch': 3.02}
 48%|████▊     | 2380/5000 [11:19:09<12:23:43, 17.03s/it] 48%|████▊     | 2381/5000 [11:19:23<11:46:19, 16.18s/it]                                                         {'loss': 21.1102, 'grad_norm': 26.125, 'learning_rate': 4.062376849377091e-05, 'epoch': 3.02}
 48%|████▊     | 2381/5000 [11:19:23<11:46:19, 16.18s/it] 48%|████▊     | 2382/5000 [11:19:39<11:47:20, 16.21s/it]                                                         {'loss': 17.4629, 'grad_norm': 10.875, 'learning_rate': 4.0600919464374664e-05, 'epoch': 3.02}
 48%|████▊     | 2382/5000 [11:19:39<11:47:20, 16.21s/it] 48%|████▊     | 2383/5000 [11:19:54<11:31:48, 15.86s/it]                                                         {'loss': 17.6546, 'grad_norm': 9.6875, 'learning_rate': 4.057806798494593e-05, 'epoch': 3.03}
 48%|████▊     | 2383/5000 [11:19:54<11:31:48, 15.86s/it] 48%|████▊     | 2384/5000 [11:20:10<11:29:17, 15.81s/it]                                                         {'loss': 19.61, 'grad_norm': 21.375, 'learning_rate': 4.055521406548076e-05, 'epoch': 3.03}
 48%|████▊     | 2384/5000 [11:20:10<11:29:17, 15.81s/it] 48%|████▊     | 2385/5000 [11:20:24<11:00:13, 15.15s/it]                                                         {'loss': 19.7017, 'grad_norm': 19.0, 'learning_rate': 4.053235771597622e-05, 'epoch': 3.03}
 48%|████▊     | 2385/5000 [11:20:24<11:00:13, 15.15s/it] 48%|████▊     | 2386/5000 [11:20:40<11:22:35, 15.67s/it]                                                         {'loss': 19.1405, 'grad_norm': 14.0, 'learning_rate': 4.0509498946430465e-05, 'epoch': 3.03}
 48%|████▊     | 2386/5000 [11:20:40<11:22:35, 15.67s/it] 48%|████▊     | 2387/5000 [11:20:57<11:34:38, 15.95s/it]                                                         {'loss': 17.9822, 'grad_norm': 20.875, 'learning_rate': 4.0486637766842686e-05, 'epoch': 3.03}
 48%|████▊     | 2387/5000 [11:20:57<11:34:38, 15.95s/it] 48%|████▊     | 2388/5000 [11:21:13<11:30:59, 15.87s/it]                                                         {'loss': 18.8982, 'grad_norm': 346.0, 'learning_rate': 4.046377418721315e-05, 'epoch': 3.03}
 48%|████▊     | 2388/5000 [11:21:13<11:30:59, 15.87s/it] 48%|████▊     | 2389/5000 [11:21:29<11:34:45, 15.97s/it]                                                         {'loss': 18.5284, 'grad_norm': 39.75, 'learning_rate': 4.044090821754315e-05, 'epoch': 3.03}
 48%|████▊     | 2389/5000 [11:21:29<11:34:45, 15.97s/it] 48%|████▊     | 2390/5000 [11:21:45<11:35:14, 15.98s/it]                                                         {'loss': 18.0024, 'grad_norm': 24.25, 'learning_rate': 4.041803986783505e-05, 'epoch': 3.03}
 48%|████▊     | 2390/5000 [11:21:45<11:35:14, 15.98s/it] 48%|████▊     | 2391/5000 [11:22:00<11:16:28, 15.56s/it]                                                         {'loss': 20.6874, 'grad_norm': 37.75, 'learning_rate': 4.0395169148092236e-05, 'epoch': 3.04}
 48%|████▊     | 2391/5000 [11:22:00<11:16:28, 15.56s/it] 48%|████▊     | 2392/5000 [11:22:15<11:13:44, 15.50s/it]                                                         {'loss': 18.5967, 'grad_norm': 32.0, 'learning_rate': 4.037229606831914e-05, 'epoch': 3.04}
 48%|████▊     | 2392/5000 [11:22:15<11:13:44, 15.50s/it] 48%|████▊     | 2393/5000 [11:22:32<11:31:40, 15.92s/it]                                                         {'loss': 18.6429, 'grad_norm': 21.25, 'learning_rate': 4.034942063852122e-05, 'epoch': 3.04}
 48%|████▊     | 2393/5000 [11:22:32<11:31:40, 15.92s/it] 48%|████▊     | 2394/5000 [11:22:47<11:23:21, 15.73s/it]                                                         {'loss': 20.0403, 'grad_norm': 29.875, 'learning_rate': 4.0326542868704965e-05, 'epoch': 3.04}
 48%|████▊     | 2394/5000 [11:22:47<11:23:21, 15.73s/it] 48%|████▊     | 2395/5000 [11:23:03<11:30:50, 15.91s/it]                                                         {'loss': 17.7739, 'grad_norm': 24.25, 'learning_rate': 4.030366276887791e-05, 'epoch': 3.04}
 48%|████▊     | 2395/5000 [11:23:03<11:30:50, 15.91s/it] 48%|████▊     | 2396/5000 [11:23:24<12:34:54, 17.39s/it]                                                         {'loss': 20.4674, 'grad_norm': 25.625, 'learning_rate': 4.028078034904858e-05, 'epoch': 3.04}
 48%|████▊     | 2396/5000 [11:23:24<12:34:54, 17.39s/it] 48%|████▊     | 2397/5000 [11:23:51<14:40:07, 20.29s/it]                                                         {'loss': 16.9733, 'grad_norm': 11.375, 'learning_rate': 4.02578956192265e-05, 'epoch': 3.04}
 48%|████▊     | 2397/5000 [11:23:51<14:40:07, 20.29s/it] 48%|████▊     | 2398/5000 [11:24:07<13:40:38, 18.92s/it]                                                         {'loss': 19.5908, 'grad_norm': 20.0, 'learning_rate': 4.023500858942225e-05, 'epoch': 3.05}
 48%|████▊     | 2398/5000 [11:24:07<13:40:38, 18.92s/it] 48%|████▊     | 2399/5000 [11:24:29<14:24:17, 19.94s/it]                                                         {'loss': 19.4706, 'grad_norm': 16.25, 'learning_rate': 4.0212119269647375e-05, 'epoch': 3.05}
 48%|████▊     | 2399/5000 [11:24:29<14:24:17, 19.94s/it] 48%|████▊     | 2400/5000 [11:24:45<13:24:10, 18.56s/it]                                                         {'loss': 20.3484, 'grad_norm': 19.5, 'learning_rate': 4.018922766991447e-05, 'epoch': 3.05}
 48%|████▊     | 2400/5000 [11:24:45<13:24:10, 18.56s/it] 48%|████▊     | 2401/5000 [11:24:59<12:31:24, 17.35s/it]                                                         {'loss': 17.803, 'grad_norm': 16.375, 'learning_rate': 4.016633380023708e-05, 'epoch': 3.05}
 48%|████▊     | 2401/5000 [11:24:59<12:31:24, 17.35s/it] 48%|████▊     | 2402/5000 [11:25:26<14:36:24, 20.24s/it]                                                         {'loss': 19.7883, 'grad_norm': 15.625, 'learning_rate': 4.014343767062976e-05, 'epoch': 3.05}
 48%|████▊     | 2402/5000 [11:25:26<14:36:24, 20.24s/it] 48%|████▊     | 2403/5000 [11:25:41<13:26:55, 18.64s/it]                                                         {'loss': 18.4904, 'grad_norm': 14.0, 'learning_rate': 4.0120539291108056e-05, 'epoch': 3.05}
 48%|████▊     | 2403/5000 [11:25:41<13:26:55, 18.64s/it] 48%|████▊     | 2404/5000 [11:25:55<12:25:55, 17.24s/it]                                                         {'loss': 18.8738, 'grad_norm': 24.375, 'learning_rate': 4.00976386716885e-05, 'epoch': 3.05}
 48%|████▊     | 2404/5000 [11:25:55<12:25:55, 17.24s/it] 48%|████▊     | 2405/5000 [11:26:12<12:26:02, 17.25s/it]                                                         {'loss': 40.7998, 'grad_norm': 1640.0, 'learning_rate': 4.0074735822388605e-05, 'epoch': 3.05}
 48%|████▊     | 2405/5000 [11:26:12<12:26:02, 17.25s/it] 48%|████▊     | 2406/5000 [11:26:31<12:44:41, 17.69s/it]                                                         {'loss': 18.7117, 'grad_norm': 47.0, 'learning_rate': 4.005183075322686e-05, 'epoch': 3.06}
 48%|████▊     | 2406/5000 [11:26:31<12:44:41, 17.69s/it] 48%|████▊     | 2407/5000 [11:26:48<12:31:22, 17.39s/it]                                                         {'loss': 19.6207, 'grad_norm': 24.125, 'learning_rate': 4.00289234742227e-05, 'epoch': 3.06}
 48%|████▊     | 2407/5000 [11:26:48<12:31:22, 17.39s/it] 48%|████▊     | 2408/5000 [11:27:04<12:20:18, 17.14s/it]                                                         {'loss': 17.7815, 'grad_norm': 9.875, 'learning_rate': 4.000601399539656e-05, 'epoch': 3.06}
 48%|████▊     | 2408/5000 [11:27:04<12:20:18, 17.14s/it] 48%|████▊     | 2409/5000 [11:27:19<11:52:20, 16.50s/it]                                                         {'loss': 19.9726, 'grad_norm': 25.375, 'learning_rate': 3.9983102326769826e-05, 'epoch': 3.06}
 48%|████▊     | 2409/5000 [11:27:19<11:52:20, 16.50s/it] 48%|████▊     | 2410/5000 [11:27:35<11:35:57, 16.12s/it]                                                         {'loss': 19.0228, 'grad_norm': 12.875, 'learning_rate': 3.996018847836484e-05, 'epoch': 3.06}
 48%|████▊     | 2410/5000 [11:27:35<11:35:57, 16.12s/it] 48%|████▊     | 2411/5000 [11:27:49<11:11:50, 15.57s/it]                                                         {'loss': 21.8117, 'grad_norm': 21.0, 'learning_rate': 3.993727246020489e-05, 'epoch': 3.06}
 48%|████▊     | 2411/5000 [11:27:49<11:11:50, 15.57s/it] 48%|████▊     | 2412/5000 [11:28:04<11:10:01, 15.53s/it]                                                         {'loss': 19.8049, 'grad_norm': 210.0, 'learning_rate': 3.991435428231424e-05, 'epoch': 3.06}
 48%|████▊     | 2412/5000 [11:28:04<11:10:01, 15.53s/it] 48%|████▊     | 2413/5000 [11:28:19<11:03:03, 15.38s/it]                                                         {'loss': 19.264, 'grad_norm': 112.0, 'learning_rate': 3.9891433954718054e-05, 'epoch': 3.06}
 48%|████▊     | 2413/5000 [11:28:19<11:03:03, 15.38s/it] 48%|████▊     | 2414/5000 [11:28:33<10:46:28, 15.00s/it]                                                         {'loss': 18.4258, 'grad_norm': 24.75, 'learning_rate': 3.986851148744247e-05, 'epoch': 3.07}
 48%|████▊     | 2414/5000 [11:28:33<10:46:28, 15.00s/it] 48%|████▊     | 2415/5000 [11:28:50<11:06:02, 15.46s/it]                                                         {'loss': 17.9989, 'grad_norm': 32.75, 'learning_rate': 3.984558689051456e-05, 'epoch': 3.07}
 48%|████▊     | 2415/5000 [11:28:50<11:06:02, 15.46s/it] 48%|████▊     | 2416/5000 [11:29:09<11:50:06, 16.49s/it]                                                         {'loss': 16.9157, 'grad_norm': 84.5, 'learning_rate': 3.9822660173962324e-05, 'epoch': 3.07}
 48%|████▊     | 2416/5000 [11:29:09<11:50:06, 16.49s/it] 48%|████▊     | 2417/5000 [11:29:27<12:05:52, 16.86s/it]                                                         {'loss': 18.6602, 'grad_norm': 58.0, 'learning_rate': 3.979973134781469e-05, 'epoch': 3.07}
 48%|████▊     | 2417/5000 [11:29:27<12:05:52, 16.86s/it] 48%|████▊     | 2418/5000 [11:29:41<11:37:51, 16.22s/it]                                                         {'loss': 18.7211, 'grad_norm': 69.5, 'learning_rate': 3.977680042210148e-05, 'epoch': 3.07}
 48%|████▊     | 2418/5000 [11:29:41<11:37:51, 16.22s/it] 48%|████▊     | 2419/5000 [11:29:58<11:49:00, 16.48s/it]                                                         {'loss': 18.0548, 'grad_norm': 27.75, 'learning_rate': 3.9753867406853494e-05, 'epoch': 3.07}
 48%|████▊     | 2419/5000 [11:29:58<11:49:00, 16.48s/it] 48%|████▊     | 2420/5000 [11:30:12<11:18:21, 15.78s/it]                                                         {'loss': 21.5368, 'grad_norm': 39.5, 'learning_rate': 3.973093231210239e-05, 'epoch': 3.07}
 48%|████▊     | 2420/5000 [11:30:12<11:18:21, 15.78s/it] 48%|████▊     | 2421/5000 [11:30:29<11:25:57, 15.96s/it]                                                         {'loss': 25.8836, 'grad_norm': 2960.0, 'learning_rate': 3.970799514788076e-05, 'epoch': 3.07}
 48%|████▊     | 2421/5000 [11:30:29<11:25:57, 15.96s/it] 48%|████▊     | 2422/5000 [11:30:48<12:02:12, 16.81s/it]                                                         {'loss': 16.824, 'grad_norm': 8.6875, 'learning_rate': 3.968505592422211e-05, 'epoch': 3.08}
 48%|████▊     | 2422/5000 [11:30:48<12:02:12, 16.81s/it] 48%|████▊     | 2423/5000 [11:31:13<13:54:58, 19.44s/it]                                                         {'loss': 18.3412, 'grad_norm': 11.8125, 'learning_rate': 3.966211465116082e-05, 'epoch': 3.08}
 48%|████▊     | 2423/5000 [11:31:13<13:54:58, 19.44s/it] 48%|████▊     | 2424/5000 [11:31:41<15:39:50, 21.89s/it]                                                         {'loss': 18.169, 'grad_norm': 14.375, 'learning_rate': 3.9639171338732195e-05, 'epoch': 3.08}
 48%|████▊     | 2424/5000 [11:31:41<15:39:50, 21.89s/it] 48%|████▊     | 2425/5000 [11:32:02<15:25:16, 21.56s/it]                                                         {'loss': 17.0255, 'grad_norm': 8.625, 'learning_rate': 3.961622599697241e-05, 'epoch': 3.08}
 48%|████▊     | 2425/5000 [11:32:02<15:25:16, 21.56s/it] 49%|████▊     | 2426/5000 [11:32:22<15:15:44, 21.35s/it]                                                         {'loss': 18.7196, 'grad_norm': 105.5, 'learning_rate': 3.959327863591855e-05, 'epoch': 3.08}
 49%|████▊     | 2426/5000 [11:32:23<15:15:44, 21.35s/it] 49%|████▊     | 2427/5000 [11:32:38<13:57:22, 19.53s/it]                                                         {'loss': 18.6306, 'grad_norm': 14.875, 'learning_rate': 3.9570329265608555e-05, 'epoch': 3.08}
 49%|████▊     | 2427/5000 [11:32:38<13:57:22, 19.53s/it] 49%|████▊     | 2428/5000 [11:32:51<12:40:52, 17.75s/it]                                                         {'loss': 19.4616, 'grad_norm': 22.5, 'learning_rate': 3.954737789608126e-05, 'epoch': 3.08}
 49%|████▊     | 2428/5000 [11:32:51<12:40:52, 17.75s/it] 49%|████▊     | 2429/5000 [11:33:09<12:32:59, 17.57s/it]                                                         {'loss': 18.5162, 'grad_norm': 21.375, 'learning_rate': 3.952442453737639e-05, 'epoch': 3.08}
 49%|████▊     | 2429/5000 [11:33:09<12:32:59, 17.57s/it] 49%|████▊     | 2430/5000 [11:33:28<12:56:08, 18.12s/it]                                                         {'loss': 17.7276, 'grad_norm': 15.6875, 'learning_rate': 3.950146919953452e-05, 'epoch': 3.09}
 49%|████▊     | 2430/5000 [11:33:28<12:56:08, 18.12s/it] 49%|████▊     | 2431/5000 [11:33:43<12:16:09, 17.19s/it]                                                         {'loss': 18.5676, 'grad_norm': 19.875, 'learning_rate': 3.947851189259707e-05, 'epoch': 3.09}
 49%|████▊     | 2431/5000 [11:33:43<12:16:09, 17.19s/it] 49%|████▊     | 2432/5000 [11:33:57<11:38:50, 16.33s/it]                                                         {'loss': 21.2328, 'grad_norm': 49.25, 'learning_rate': 3.9455552626606376e-05, 'epoch': 3.09}
 49%|████▊     | 2432/5000 [11:33:57<11:38:50, 16.33s/it] 49%|████▊     | 2433/5000 [11:34:15<11:52:14, 16.65s/it]                                                         {'loss': 17.6803, 'grad_norm': 16.375, 'learning_rate': 3.943259141160559e-05, 'epoch': 3.09}
 49%|████▊     | 2433/5000 [11:34:15<11:52:14, 16.65s/it] 49%|████▊     | 2434/5000 [11:34:30<11:33:44, 16.22s/it]                                                         {'loss': 18.6094, 'grad_norm': 12.5625, 'learning_rate': 3.9409628257638725e-05, 'epoch': 3.09}
 49%|████▊     | 2434/5000 [11:34:30<11:33:44, 16.22s/it] 49%|████▊     | 2435/5000 [11:34:45<11:14:51, 15.79s/it]                                                         {'loss': 20.1609, 'grad_norm': 15.3125, 'learning_rate': 3.938666317475065e-05, 'epoch': 3.09}
 49%|████▊     | 2435/5000 [11:34:45<11:14:51, 15.79s/it] 49%|████▊     | 2436/5000 [11:35:00<11:09:33, 15.67s/it]                                                         {'loss': 17.3736, 'grad_norm': 10.375, 'learning_rate': 3.9363696172987065e-05, 'epoch': 3.09}
 49%|████▊     | 2436/5000 [11:35:00<11:09:33, 15.67s/it] 49%|████▊     | 2437/5000 [11:35:15<11:03:37, 15.54s/it]                                                         {'loss': 20.361, 'grad_norm': 29.25, 'learning_rate': 3.934072726239452e-05, 'epoch': 3.09}
 49%|████▊     | 2437/5000 [11:35:15<11:03:37, 15.54s/it] 49%|████▉     | 2438/5000 [11:35:28<10:25:11, 14.64s/it]                                                         {'loss': 18.2645, 'grad_norm': 21.5, 'learning_rate': 3.93177564530204e-05, 'epoch': 3.1}
 49%|████▉     | 2438/5000 [11:35:28<10:25:11, 14.64s/it] 49%|████▉     | 2439/5000 [11:35:52<12:31:51, 17.61s/it]                                                         {'loss': 19.4362, 'grad_norm': 18.625, 'learning_rate': 3.929478375491292e-05, 'epoch': 3.1}
 49%|████▉     | 2439/5000 [11:35:52<12:31:51, 17.61s/it] 49%|████▉     | 2440/5000 [11:36:14<13:27:55, 18.94s/it]                                                         {'loss': 17.6584, 'grad_norm': 12.1875, 'learning_rate': 3.92718091781211e-05, 'epoch': 3.1}
 49%|████▉     | 2440/5000 [11:36:14<13:27:55, 18.94s/it] 49%|████▉     | 2441/5000 [11:36:27<12:02:51, 16.95s/it]                                                         {'loss': 21.1691, 'grad_norm': 27.375, 'learning_rate': 3.924883273269483e-05, 'epoch': 3.1}
 49%|████▉     | 2441/5000 [11:36:27<12:02:51, 16.95s/it] 49%|████▉     | 2442/5000 [11:36:42<11:40:33, 16.43s/it]                                                         {'loss': 21.3446, 'grad_norm': 26.625, 'learning_rate': 3.922585442868477e-05, 'epoch': 3.1}
 49%|████▉     | 2442/5000 [11:36:42<11:40:33, 16.43s/it] 49%|████▉     | 2443/5000 [11:36:56<11:05:31, 15.62s/it]                                                         {'loss': 20.6067, 'grad_norm': 23.0, 'learning_rate': 3.920287427614241e-05, 'epoch': 3.1}
 49%|████▉     | 2443/5000 [11:36:56<11:05:31, 15.62s/it] 49%|████▉     | 2444/5000 [11:37:14<11:40:52, 16.45s/it]                                                         {'loss': 18.6051, 'grad_norm': 17.125, 'learning_rate': 3.917989228512005e-05, 'epoch': 3.1}
 49%|████▉     | 2444/5000 [11:37:14<11:40:52, 16.45s/it] 49%|████▉     | 2445/5000 [11:37:38<13:21:01, 18.81s/it]                                                         {'loss': 18.5165, 'grad_norm': 16.5, 'learning_rate': 3.91569084656708e-05, 'epoch': 3.1}
 49%|████▉     | 2445/5000 [11:37:38<13:21:01, 18.81s/it] 49%|████▉     | 2446/5000 [11:37:54<12:43:53, 17.95s/it]                                                         {'loss': 17.8089, 'grad_norm': 14.375, 'learning_rate': 3.913392282784856e-05, 'epoch': 3.11}
 49%|████▉     | 2446/5000 [11:37:54<12:43:53, 17.95s/it] 49%|████▉     | 2447/5000 [11:38:10<12:20:14, 17.40s/it]                                                         {'loss': 17.8307, 'grad_norm': 14.1875, 'learning_rate': 3.911093538170803e-05, 'epoch': 3.11}
 49%|████▉     | 2447/5000 [11:38:10<12:20:14, 17.40s/it] 49%|████▉     | 2448/5000 [11:38:25<11:45:17, 16.58s/it]                                                         {'loss': 19.34, 'grad_norm': 43.0, 'learning_rate': 3.90879461373047e-05, 'epoch': 3.11}
 49%|████▉     | 2448/5000 [11:38:25<11:45:17, 16.58s/it] 49%|████▉     | 2449/5000 [11:38:40<11:22:32, 16.05s/it]                                                         {'loss': 19.3683, 'grad_norm': 21.25, 'learning_rate': 3.906495510469485e-05, 'epoch': 3.11}
 49%|████▉     | 2449/5000 [11:38:40<11:22:32, 16.05s/it] 49%|████▉     | 2450/5000 [11:38:54<10:56:38, 15.45s/it]                                                         {'loss': 19.5228, 'grad_norm': 162.0, 'learning_rate': 3.9041962293935516e-05, 'epoch': 3.11}
 49%|████▉     | 2450/5000 [11:38:54<10:56:38, 15.45s/it] 49%|████▉     | 2451/5000 [11:39:08<10:44:35, 15.17s/it]                                                         {'loss': 18.1104, 'grad_norm': 14.5625, 'learning_rate': 3.901896771508456e-05, 'epoch': 3.11}
 49%|████▉     | 2451/5000 [11:39:08<10:44:35, 15.17s/it] 49%|████▉     | 2452/5000 [11:39:34<12:55:43, 18.27s/it]                                                         {'loss': 18.2768, 'grad_norm': 22.25, 'learning_rate': 3.899597137820058e-05, 'epoch': 3.11}
 49%|████▉     | 2452/5000 [11:39:34<12:55:43, 18.27s/it] 49%|████▉     | 2453/5000 [11:39:58<14:05:25, 19.92s/it]                                                         {'loss': 18.7403, 'grad_norm': 19.375, 'learning_rate': 3.897297329334297e-05, 'epoch': 3.11}
 49%|████▉     | 2453/5000 [11:39:58<14:05:25, 19.92s/it] 49%|████▉     | 2454/5000 [11:40:19<14:21:45, 20.31s/it]                                                         {'loss': 19.6255, 'grad_norm': 63.75, 'learning_rate': 3.8949973470571855e-05, 'epoch': 3.12}
 49%|████▉     | 2454/5000 [11:40:19<14:21:45, 20.31s/it] 49%|████▉     | 2455/5000 [11:40:35<13:28:42, 19.07s/it]                                                         {'loss': 18.0749, 'grad_norm': 17.375, 'learning_rate': 3.8926971919948155e-05, 'epoch': 3.12}
 49%|████▉     | 2455/5000 [11:40:35<13:28:42, 19.07s/it] 49%|████▉     | 2456/5000 [11:40:49<12:16:01, 17.36s/it]                                                         {'loss': 20.5548, 'grad_norm': 17.625, 'learning_rate': 3.890396865153351e-05, 'epoch': 3.12}
 49%|████▉     | 2456/5000 [11:40:49<12:16:01, 17.36s/it] 49%|████▉     | 2457/5000 [11:41:13<13:41:36, 19.38s/it]                                                         {'loss': 19.4849, 'grad_norm': 16.5, 'learning_rate': 3.888096367539036e-05, 'epoch': 3.12}
 49%|████▉     | 2457/5000 [11:41:13<13:41:36, 19.38s/it] 49%|████▉     | 2458/5000 [11:41:35<14:23:05, 20.37s/it]                                                         {'loss': 17.4044, 'grad_norm': 9.125, 'learning_rate': 3.885795700158183e-05, 'epoch': 3.12}
 49%|████▉     | 2458/5000 [11:41:35<14:23:05, 20.37s/it] 49%|████▉     | 2459/5000 [11:41:50<13:12:04, 18.70s/it]                                                         {'loss': 20.1357, 'grad_norm': 18.875, 'learning_rate': 3.883494864017185e-05, 'epoch': 3.12}
 49%|████▉     | 2459/5000 [11:41:50<13:12:04, 18.70s/it] 49%|████▉     | 2460/5000 [11:42:05<12:22:51, 17.55s/it]                                                         {'loss': 19.8758, 'grad_norm': 34.0, 'learning_rate': 3.881193860122504e-05, 'epoch': 3.12}
 49%|████▉     | 2460/5000 [11:42:05<12:22:51, 17.55s/it] 49%|████▉     | 2461/5000 [11:42:33<14:33:27, 20.64s/it]                                                         {'loss': 18.3062, 'grad_norm': 41.0, 'learning_rate': 3.878892689480678e-05, 'epoch': 3.13}
 49%|████▉     | 2461/5000 [11:42:33<14:33:27, 20.64s/it] 49%|████▉     | 2462/5000 [11:42:49<13:31:53, 19.19s/it]                                                         {'loss': 18.4441, 'grad_norm': 22.125, 'learning_rate': 3.876591353098316e-05, 'epoch': 3.13}
 49%|████▉     | 2462/5000 [11:42:49<13:31:53, 19.19s/it] 49%|████▉     | 2463/5000 [11:43:02<12:11:55, 17.31s/it]                                                         {'loss': 19.912, 'grad_norm': 28.75, 'learning_rate': 3.874289851982103e-05, 'epoch': 3.13}
 49%|████▉     | 2463/5000 [11:43:02<12:11:55, 17.31s/it] 49%|████▉     | 2464/5000 [11:43:17<11:53:51, 16.89s/it]                                                         {'loss': 16.8889, 'grad_norm': 18.75, 'learning_rate': 3.871988187138792e-05, 'epoch': 3.13}
 49%|████▉     | 2464/5000 [11:43:17<11:53:51, 16.89s/it] 49%|████▉     | 2465/5000 [11:43:47<14:28:08, 20.55s/it]                                                         {'loss': 19.281, 'grad_norm': 26.0, 'learning_rate': 3.86968635957521e-05, 'epoch': 3.13}
 49%|████▉     | 2465/5000 [11:43:47<14:28:08, 20.55s/it] 49%|████▉     | 2466/5000 [11:44:01<13:07:15, 18.64s/it]                                                         {'loss': 17.2395, 'grad_norm': 24.25, 'learning_rate': 3.8673843702982546e-05, 'epoch': 3.13}
 49%|████▉     | 2466/5000 [11:44:01<13:07:15, 18.64s/it] 49%|████▉     | 2467/5000 [11:44:18<12:47:50, 18.19s/it]                                                         {'loss': 17.5788, 'grad_norm': 13.0625, 'learning_rate': 3.8650822203148946e-05, 'epoch': 3.13}
 49%|████▉     | 2467/5000 [11:44:18<12:47:50, 18.19s/it] 49%|████▉     | 2468/5000 [11:44:37<12:58:37, 18.45s/it]                                                         {'loss': 23.6758, 'grad_norm': 440.0, 'learning_rate': 3.8627799106321676e-05, 'epoch': 3.13}
 49%|████▉     | 2468/5000 [11:44:37<12:58:37, 18.45s/it] 49%|████▉     | 2469/5000 [11:44:50<11:55:20, 16.96s/it]                                                         {'loss': 22.983, 'grad_norm': 364.0, 'learning_rate': 3.8604774422571824e-05, 'epoch': 3.14}
 49%|████▉     | 2469/5000 [11:44:50<11:55:20, 16.96s/it] 49%|████▉     | 2470/5000 [11:45:05<11:25:51, 16.27s/it]                                                         {'loss': 20.211, 'grad_norm': 88.5, 'learning_rate': 3.858174816197117e-05, 'epoch': 3.14}
 49%|████▉     | 2470/5000 [11:45:05<11:25:51, 16.27s/it] 49%|████▉     | 2471/5000 [11:45:25<12:17:53, 17.51s/it]                                                         {'loss': 19.5741, 'grad_norm': 46.5, 'learning_rate': 3.855872033459219e-05, 'epoch': 3.14}
 49%|████▉     | 2471/5000 [11:45:25<12:17:53, 17.51s/it] 49%|████▉     | 2472/5000 [11:45:47<13:04:25, 18.62s/it]                                                         {'loss': 18.6061, 'grad_norm': 18.25, 'learning_rate': 3.853569095050803e-05, 'epoch': 3.14}
 49%|████▉     | 2472/5000 [11:45:47<13:04:25, 18.62s/it] 49%|████▉     | 2473/5000 [11:46:12<14:23:41, 20.51s/it]                                                         {'loss': 17.924, 'grad_norm': 11.375, 'learning_rate': 3.8512660019792516e-05, 'epoch': 3.14}
 49%|████▉     | 2473/5000 [11:46:12<14:23:41, 20.51s/it] 49%|████▉     | 2474/5000 [11:46:26<13:10:25, 18.77s/it]                                                         {'loss': 18.669, 'grad_norm': 27.625, 'learning_rate': 3.8489627552520187e-05, 'epoch': 3.14}
 49%|████▉     | 2474/5000 [11:46:26<13:10:25, 18.77s/it] 50%|████▉     | 2475/5000 [11:46:55<15:20:50, 21.88s/it]                                                         {'loss': 17.5237, 'grad_norm': 22.0, 'learning_rate': 3.84665935587662e-05, 'epoch': 3.14}
 50%|████▉     | 2475/5000 [11:46:55<15:20:50, 21.88s/it] 50%|████▉     | 2476/5000 [11:47:11<13:54:39, 19.84s/it]                                                         {'loss': 18.1891, 'grad_norm': 27.25, 'learning_rate': 3.8443558048606414e-05, 'epoch': 3.14}
 50%|████▉     | 2476/5000 [11:47:11<13:54:39, 19.84s/it] 50%|████▉     | 2477/5000 [11:47:37<15:17:49, 21.83s/it]                                                         {'loss': 19.6394, 'grad_norm': 151.0, 'learning_rate': 3.8420521032117334e-05, 'epoch': 3.15}
 50%|████▉     | 2477/5000 [11:47:37<15:17:49, 21.83s/it] 50%|████▉     | 2478/5000 [11:47:55<14:33:06, 20.77s/it]                                                         {'loss': 18.4943, 'grad_norm': 45.0, 'learning_rate': 3.8397482519376165e-05, 'epoch': 3.15}
 50%|████▉     | 2478/5000 [11:47:55<14:33:06, 20.77s/it] 50%|████▉     | 2479/5000 [11:48:10<13:17:29, 18.98s/it]                                                         {'loss': 17.6646, 'grad_norm': 11.875, 'learning_rate': 3.8374442520460714e-05, 'epoch': 3.15}
 50%|████▉     | 2479/5000 [11:48:10<13:17:29, 18.98s/it] 50%|████▉     | 2480/5000 [11:48:25<12:19:56, 17.62s/it]                                                         {'loss': 21.1993, 'grad_norm': 144.0, 'learning_rate': 3.835140104544945e-05, 'epoch': 3.15}
 50%|████▉     | 2480/5000 [11:48:25<12:19:56, 17.62s/it] 50%|████▉     | 2481/5000 [11:48:49<13:40:14, 19.54s/it]                                                         {'loss': 20.1479, 'grad_norm': 18.5, 'learning_rate': 3.832835810442151e-05, 'epoch': 3.15}
 50%|████▉     | 2481/5000 [11:48:49<13:40:14, 19.54s/it] 50%|████▉     | 2482/5000 [11:49:03<12:40:48, 18.13s/it]                                                         {'loss': 19.7037, 'grad_norm': 19.5, 'learning_rate': 3.8305313707456666e-05, 'epoch': 3.15}
 50%|████▉     | 2482/5000 [11:49:03<12:40:48, 18.13s/it] 50%|████▉     | 2483/5000 [11:49:20<12:23:16, 17.72s/it]                                                         {'loss': 17.9196, 'grad_norm': 24.25, 'learning_rate': 3.82822678646353e-05, 'epoch': 3.15}
 50%|████▉     | 2483/5000 [11:49:20<12:23:16, 17.72s/it] 50%|████▉     | 2484/5000 [11:49:41<13:07:18, 18.78s/it]                                                         {'loss': 18.4776, 'grad_norm': 14.5625, 'learning_rate': 3.825922058603847e-05, 'epoch': 3.15}
 50%|████▉     | 2484/5000 [11:49:41<13:07:18, 18.78s/it] 50%|████▉     | 2485/5000 [11:49:54<11:51:08, 16.97s/it]                                                         {'loss': 20.0012, 'grad_norm': 20.625, 'learning_rate': 3.823617188174782e-05, 'epoch': 3.16}
 50%|████▉     | 2485/5000 [11:49:54<11:51:08, 16.97s/it] 50%|████▉     | 2486/5000 [11:50:11<11:48:55, 16.92s/it]                                                         {'loss': 17.641, 'grad_norm': 16.375, 'learning_rate': 3.8213121761845644e-05, 'epoch': 3.16}
 50%|████▉     | 2486/5000 [11:50:11<11:48:55, 16.92s/it] 50%|████▉     | 2487/5000 [11:50:30<12:09:22, 17.41s/it]                                                         {'loss': 18.7071, 'grad_norm': 13.0, 'learning_rate': 3.819007023641484e-05, 'epoch': 3.16}
 50%|████▉     | 2487/5000 [11:50:30<12:09:22, 17.41s/it] 50%|████▉     | 2488/5000 [11:50:46<12:01:15, 17.23s/it]                                                         {'loss': 17.6721, 'grad_norm': 13.4375, 'learning_rate': 3.816701731553894e-05, 'epoch': 3.16}
 50%|████▉     | 2488/5000 [11:50:46<12:01:15, 17.23s/it] 50%|████▉     | 2489/5000 [11:51:10<13:27:39, 19.30s/it]                                                         {'loss': 20.2962, 'grad_norm': 13.5625, 'learning_rate': 3.8143963009302067e-05, 'epoch': 3.16}
 50%|████▉     | 2489/5000 [11:51:10<13:27:39, 19.30s/it] 50%|████▉     | 2490/5000 [11:51:26<12:40:28, 18.18s/it]                                                         {'loss': 17.2868, 'grad_norm': 16.0, 'learning_rate': 3.812090732778897e-05, 'epoch': 3.16}
 50%|████▉     | 2490/5000 [11:51:26<12:40:28, 18.18s/it] 50%|████▉     | 2491/5000 [11:51:41<12:00:43, 17.24s/it]                                                         {'loss': 19.1022, 'grad_norm': 19.375, 'learning_rate': 3.8097850281084966e-05, 'epoch': 3.16}
 50%|████▉     | 2491/5000 [11:51:41<12:00:43, 17.24s/it] 50%|████▉     | 2492/5000 [11:52:06<13:33:00, 19.45s/it]                                                         {'loss': 18.0931, 'grad_norm': 27.25, 'learning_rate': 3.8074791879276014e-05, 'epoch': 3.16}
 50%|████▉     | 2492/5000 [11:52:06<13:33:00, 19.45s/it] 50%|████▉     | 2493/5000 [11:52:22<12:56:06, 18.57s/it]                                                         {'loss': 18.3416, 'grad_norm': 16.5, 'learning_rate': 3.8051732132448625e-05, 'epoch': 3.17}
 50%|████▉     | 2493/5000 [11:52:22<12:56:06, 18.57s/it] 50%|████▉     | 2494/5000 [11:52:39<12:35:12, 18.08s/it]                                                         {'loss': 17.2361, 'grad_norm': 11.6875, 'learning_rate': 3.802867105068991e-05, 'epoch': 3.17}
 50%|████▉     | 2494/5000 [11:52:39<12:35:12, 18.08s/it] 50%|████▉     | 2495/5000 [11:52:53<11:43:46, 16.86s/it]                                                         {'loss': 18.9927, 'grad_norm': 17.875, 'learning_rate': 3.8005608644087596e-05, 'epoch': 3.17}
 50%|████▉     | 2495/5000 [11:52:53<11:43:46, 16.86s/it] 50%|████▉     | 2496/5000 [11:53:14<12:36:40, 18.13s/it]                                                         {'loss': 28.625, 'grad_norm': 466.0, 'learning_rate': 3.798254492272994e-05, 'epoch': 3.17}
 50%|████▉     | 2496/5000 [11:53:14<12:36:40, 18.13s/it] 50%|████▉     | 2497/5000 [11:53:39<13:57:19, 20.07s/it]                                                         {'loss': 18.624, 'grad_norm': 15.875, 'learning_rate': 3.795947989670581e-05, 'epoch': 3.17}
 50%|████▉     | 2497/5000 [11:53:39<13:57:19, 20.07s/it] 50%|████▉     | 2498/5000 [11:53:53<12:38:49, 18.20s/it]                                                         {'loss': 18.483, 'grad_norm': 14.875, 'learning_rate': 3.7936413576104635e-05, 'epoch': 3.17}
 50%|████▉     | 2498/5000 [11:53:53<12:38:49, 18.20s/it] 50%|████▉     | 2499/5000 [11:54:08<12:02:15, 17.33s/it]                                                         {'loss': 18.2657, 'grad_norm': 14.625, 'learning_rate': 3.791334597101638e-05, 'epoch': 3.17}
 50%|████▉     | 2499/5000 [11:54:08<12:02:15, 17.33s/it] 50%|█████     | 2500/5000 [11:54:24<11:43:40, 16.89s/it]                                                         {'loss': 17.6553, 'grad_norm': 24.125, 'learning_rate': 3.7890277091531636e-05, 'epoch': 3.17}
 50%|█████     | 2500/5000 [11:54:24<11:43:40, 16.89s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:23,  4.46s/it][A
  3%|▎         | 3/88 [00:16<08:10,  5.77s/it][A
  5%|▍         | 4/88 [00:20<07:19,  5.23s/it][A
  6%|▌         | 5/88 [00:24<06:33,  4.74s/it][A
  7%|▋         | 6/88 [00:29<06:43,  4.92s/it][A
  8%|▊         | 7/88 [00:33<06:14,  4.62s/it][A
  9%|▉         | 8/88 [00:37<05:47,  4.35s/it][A
 10%|█         | 9/88 [00:41<05:22,  4.08s/it][A
 11%|█▏        | 10/88 [00:44<04:51,  3.74s/it][A
 12%|█▎        | 11/88 [00:47<04:30,  3.51s/it][A
 14%|█▎        | 12/88 [00:49<03:59,  3.15s/it][A
 15%|█▍        | 13/88 [00:51<03:42,  2.96s/it][A
 16%|█▌        | 14/88 [00:57<04:37,  3.74s/it][A
 17%|█▋        | 15/88 [01:03<05:11,  4.27s/it][A
 18%|█▊        | 16/88 [01:06<04:48,  4.00s/it][A
 19%|█▉        | 17/88 [01:11<05:13,  4.41s/it][A
 20%|██        | 18/88 [01:14<04:43,  4.04s/it][A
 22%|██▏       | 19/88 [01:19<04:44,  4.12s/it][A
 23%|██▎       | 20/88 [01:23<04:36,  4.07s/it][A
 24%|██▍       | 21/88 [01:26<04:16,  3.83s/it][A
 25%|██▌       | 22/88 [01:30<04:13,  3.84s/it][A
 26%|██▌       | 23/88 [01:32<03:45,  3.48s/it][A
 27%|██▋       | 24/88 [01:41<05:21,  5.03s/it][A
 28%|██▊       | 25/88 [01:45<04:46,  4.55s/it][A
 30%|██▉       | 26/88 [01:51<05:18,  5.13s/it][A
 31%|███       | 27/88 [01:55<04:59,  4.91s/it][A
 32%|███▏      | 28/88 [02:04<06:04,  6.08s/it][A
 33%|███▎      | 29/88 [02:09<05:43,  5.82s/it][A
 34%|███▍      | 30/88 [02:15<05:26,  5.63s/it][A
 35%|███▌      | 31/88 [02:19<04:55,  5.18s/it][A
 36%|███▋      | 32/88 [02:28<05:54,  6.34s/it][A
 38%|███▊      | 33/88 [02:33<05:22,  5.86s/it][A
 39%|███▊      | 34/88 [02:41<06:03,  6.73s/it][A
 40%|███▉      | 35/88 [02:44<04:57,  5.61s/it][A
 41%|████      | 36/88 [02:49<04:45,  5.48s/it][A
 42%|████▏     | 37/88 [02:53<04:11,  4.94s/it][A
 43%|████▎     | 38/88 [02:57<03:55,  4.71s/it][A
 44%|████▍     | 39/88 [03:00<03:27,  4.24s/it][A
 45%|████▌     | 40/88 [03:07<03:59,  4.99s/it][A
 47%|████▋     | 41/88 [03:17<04:58,  6.36s/it][A
 48%|████▊     | 42/88 [03:20<04:13,  5.51s/it][A
 49%|████▉     | 43/88 [03:29<04:50,  6.45s/it][A
 50%|█████     | 44/88 [03:32<04:02,  5.51s/it][A
 51%|█████     | 45/88 [03:37<03:44,  5.22s/it][A
 52%|█████▏    | 46/88 [03:41<03:24,  4.86s/it][A
 53%|█████▎    | 47/88 [03:43<02:45,  4.03s/it][A
 55%|█████▍    | 48/88 [03:45<02:20,  3.52s/it][A
 56%|█████▌    | 49/88 [03:50<02:34,  3.97s/it][A
 57%|█████▋    | 50/88 [03:54<02:32,  4.02s/it][A
 58%|█████▊    | 51/88 [03:59<02:30,  4.06s/it][A
 59%|█████▉    | 52/88 [04:04<02:45,  4.58s/it][A
 60%|██████    | 53/88 [04:09<02:45,  4.73s/it][A
 61%|██████▏   | 54/88 [04:19<03:30,  6.18s/it][A
 62%|██████▎   | 55/88 [04:24<03:11,  5.81s/it][A
 64%|██████▎   | 56/88 [04:26<02:34,  4.83s/it][A
 65%|██████▍   | 57/88 [04:30<02:21,  4.55s/it][A
 66%|██████▌   | 58/88 [04:39<02:55,  5.83s/it][A
 67%|██████▋   | 59/88 [04:44<02:40,  5.54s/it][A
 68%|██████▊   | 60/88 [04:47<02:15,  4.85s/it][A
 69%|██████▉   | 61/88 [04:51<02:05,  4.64s/it][A
 70%|███████   | 62/88 [04:54<01:47,  4.15s/it][A
 72%|███████▏  | 63/88 [05:00<01:54,  4.57s/it][A
 73%|███████▎  | 64/88 [05:04<01:45,  4.41s/it][A
 74%|███████▍  | 65/88 [05:08<01:41,  4.39s/it][A
 75%|███████▌  | 66/88 [05:12<01:32,  4.20s/it][A
 76%|███████▌  | 67/88 [05:15<01:21,  3.86s/it][A
 77%|███████▋  | 68/88 [05:20<01:23,  4.17s/it][A
 78%|███████▊  | 69/88 [05:27<01:32,  4.84s/it][A
 80%|███████▉  | 70/88 [05:31<01:22,  4.59s/it][A
 81%|████████  | 71/88 [05:34<01:13,  4.35s/it][A
 82%|████████▏ | 72/88 [05:38<01:07,  4.22s/it][A
 83%|████████▎ | 73/88 [05:41<00:58,  3.87s/it][A
 84%|████████▍ | 74/88 [05:44<00:49,  3.55s/it][A
 85%|████████▌ | 75/88 [05:49<00:50,  3.91s/it][A
 86%|████████▋ | 76/88 [05:53<00:46,  3.89s/it][A
 88%|████████▊ | 77/88 [06:00<00:53,  4.89s/it][A
 89%|████████▊ | 78/88 [06:04<00:45,  4.60s/it][A
 90%|████████▉ | 79/88 [06:09<00:41,  4.64s/it][A
 91%|█████████ | 80/88 [06:11<00:33,  4.13s/it][A
 92%|█████████▏| 81/88 [06:16<00:30,  4.39s/it][A
 93%|█████████▎| 82/88 [06:20<00:25,  4.24s/it][A
 94%|█████████▍| 83/88 [06:24<00:20,  4.12s/it][A
 95%|█████████▌| 84/88 [06:28<00:16,  4.04s/it][A
 97%|█████████▋| 85/88 [06:31<00:11,  3.71s/it][A
 98%|█████████▊| 86/88 [06:34<00:06,  3.47s/it][A
 99%|█████████▉| 87/88 [06:38<00:03,  3.67s/it][A
100%|██████████| 88/88 [06:42<00:00,  3.72s/it][A                                                         
                                               [A{'eval_loss': 18.248489379882812, 'eval_runtime': 406.3167, 'eval_samples_per_second': 6.891, 'eval_steps_per_second': 0.217, 'epoch': 3.17}
 50%|█████     | 2500/5000 [12:01:10<11:43:40, 16.89s/it]
100%|██████████| 88/88 [06:42<00:00,  3.72s/it][A
                                               [A2024-06-13 21:36:47,780 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-13 21:36:58,362 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 50%|█████     | 2501/5000 [12:01:54<101:57:43, 146.88s/it]                                                           {'loss': 17.7052, 'grad_norm': 53.0, 'learning_rate': 3.7867206947741484e-05, 'epoch': 3.18}
 50%|█████     | 2501/5000 [12:01:54<101:57:43, 146.88s/it] 50%|█████     | 2502/5000 [12:02:10<74:45:01, 107.73s/it]                                                           {'loss': 19.3924, 'grad_norm': 20.5, 'learning_rate': 3.7844135549737605e-05, 'epoch': 3.18}
 50%|█████     | 2502/5000 [12:02:10<74:45:01, 107.73s/it] 50%|█████     | 2503/5000 [12:02:25<55:20:22, 79.78s/it]                                                          {'loss': 20.0951, 'grad_norm': 15.4375, 'learning_rate': 3.782106290761222e-05, 'epoch': 3.18}
 50%|█████     | 2503/5000 [12:02:25<55:20:22, 79.78s/it] 50%|█████     | 2504/5000 [12:02:39<41:40:52, 60.12s/it]                                                         {'loss': 19.136, 'grad_norm': 18.875, 'learning_rate': 3.779798903145807e-05, 'epoch': 3.18}
 50%|█████     | 2504/5000 [12:02:39<41:40:52, 60.12s/it] 50%|█████     | 2505/5000 [12:02:59<33:11:47, 47.90s/it]                                                         {'loss': 17.4772, 'grad_norm': 11.875, 'learning_rate': 3.777491393136845e-05, 'epoch': 3.18}
 50%|█████     | 2505/5000 [12:02:59<33:11:47, 47.90s/it] 50%|█████     | 2506/5000 [12:03:14<26:25:22, 38.14s/it]                                                         {'loss': 19.5472, 'grad_norm': 47.25, 'learning_rate': 3.7751837617437214e-05, 'epoch': 3.18}
 50%|█████     | 2506/5000 [12:03:14<26:25:22, 38.14s/it] 50%|█████     | 2507/5000 [12:03:29<21:39:46, 31.28s/it]                                                         {'loss': 18.5862, 'grad_norm': 14.125, 'learning_rate': 3.772876009975871e-05, 'epoch': 3.18}
 50%|█████     | 2507/5000 [12:03:29<21:39:46, 31.28s/it] 50%|█████     | 2508/5000 [12:03:45<18:30:05, 26.73s/it]                                                         {'loss': 18.1127, 'grad_norm': 17.0, 'learning_rate': 3.770568138842784e-05, 'epoch': 3.18}
 50%|█████     | 2508/5000 [12:03:45<18:30:05, 26.73s/it] 50%|█████     | 2509/5000 [12:04:01<16:07:06, 23.29s/it]                                                         {'loss': 18.398, 'grad_norm': 17.5, 'learning_rate': 3.768260149353999e-05, 'epoch': 3.19}
 50%|█████     | 2509/5000 [12:04:01<16:07:06, 23.29s/it] 50%|█████     | 2510/5000 [12:04:15<14:14:57, 20.60s/it]                                                         {'loss': 18.4642, 'grad_norm': 30.0, 'learning_rate': 3.765952042519111e-05, 'epoch': 3.19}
 50%|█████     | 2510/5000 [12:04:15<14:14:57, 20.60s/it] 50%|█████     | 2511/5000 [12:04:29<12:52:07, 18.61s/it]                                                         {'loss': 21.6674, 'grad_norm': 22.75, 'learning_rate': 3.763643819347765e-05, 'epoch': 3.19}
 50%|█████     | 2511/5000 [12:04:29<12:52:07, 18.61s/it] 50%|█████     | 2512/5000 [12:04:47<12:47:47, 18.52s/it]                                                         {'loss': 17.8413, 'grad_norm': 16.875, 'learning_rate': 3.761335480849654e-05, 'epoch': 3.19}
 50%|█████     | 2512/5000 [12:04:47<12:47:47, 18.52s/it] 50%|█████     | 2513/5000 [12:05:05<12:37:11, 18.27s/it]                                                         {'loss': 18.3781, 'grad_norm': 18.0, 'learning_rate': 3.7590270280345255e-05, 'epoch': 3.19}
 50%|█████     | 2513/5000 [12:05:05<12:37:11, 18.27s/it] 50%|█████     | 2514/5000 [12:05:34<14:46:43, 21.40s/it]                                                         {'loss': 15.5987, 'grad_norm': 6.59375, 'learning_rate': 3.7567184619121756e-05, 'epoch': 3.19}
 50%|█████     | 2514/5000 [12:05:34<14:46:43, 21.40s/it] 50%|█████     | 2515/5000 [12:05:58<15:22:15, 22.27s/it]                                                         {'loss': 19.5249, 'grad_norm': 22.25, 'learning_rate': 3.7544097834924486e-05, 'epoch': 3.19}
 50%|█████     | 2515/5000 [12:05:58<15:22:15, 22.27s/it] 50%|█████     | 2516/5000 [12:06:11<13:28:12, 19.52s/it]                                                         {'loss': 20.7702, 'grad_norm': 26.5, 'learning_rate': 3.752100993785238e-05, 'epoch': 3.19}
 50%|█████     | 2516/5000 [12:06:11<13:28:12, 19.52s/it] 50%|█████     | 2517/5000 [12:06:31<13:30:03, 19.57s/it]                                                         {'loss': 18.3657, 'grad_norm': 15.8125, 'learning_rate': 3.749792093800489e-05, 'epoch': 3.2}
 50%|█████     | 2517/5000 [12:06:31<13:30:03, 19.57s/it] 50%|█████     | 2518/5000 [12:06:48<12:59:03, 18.83s/it]                                                         {'loss': 19.5422, 'grad_norm': 14.1875, 'learning_rate': 3.7474830845481906e-05, 'epoch': 3.2}
 50%|█████     | 2518/5000 [12:06:48<12:59:03, 18.83s/it] 50%|█████     | 2519/5000 [12:07:04<12:22:30, 17.96s/it]                                                         {'loss': 19.0366, 'grad_norm': 27.375, 'learning_rate': 3.7451739670383846e-05, 'epoch': 3.2}
 50%|█████     | 2519/5000 [12:07:04<12:22:30, 17.96s/it] 50%|█████     | 2520/5000 [12:07:20<12:06:43, 17.58s/it]                                                         {'loss': 18.3076, 'grad_norm': 17.625, 'learning_rate': 3.742864742281154e-05, 'epoch': 3.2}
 50%|█████     | 2520/5000 [12:07:20<12:06:43, 17.58s/it] 50%|█████     | 2521/5000 [12:07:47<13:58:02, 20.28s/it]                                                         {'loss': 17.8889, 'grad_norm': 85.0, 'learning_rate': 3.740555411286635e-05, 'epoch': 3.2}
 50%|█████     | 2521/5000 [12:07:47<13:58:02, 20.28s/it] 50%|█████     | 2522/5000 [12:08:06<13:46:26, 20.01s/it]                                                         {'loss': 19.157, 'grad_norm': 84.0, 'learning_rate': 3.738245975065006e-05, 'epoch': 3.2}
 50%|█████     | 2522/5000 [12:08:06<13:46:26, 20.01s/it] 50%|█████     | 2523/5000 [12:08:24<13:16:17, 19.29s/it]                                                         {'loss': 18.0873, 'grad_norm': 26.375, 'learning_rate': 3.7359364346264936e-05, 'epoch': 3.2}
 50%|█████     | 2523/5000 [12:08:24<13:16:17, 19.29s/it] 50%|█████     | 2524/5000 [12:08:39<12:25:23, 18.06s/it]                                                         {'loss': 19.5045, 'grad_norm': 21.0, 'learning_rate': 3.733626790981369e-05, 'epoch': 3.21}
 50%|█████     | 2524/5000 [12:08:39<12:25:23, 18.06s/it] 50%|█████     | 2525/5000 [12:08:55<12:03:04, 17.53s/it]                                                         {'loss': 17.902, 'grad_norm': 13.5625, 'learning_rate': 3.7313170451399475e-05, 'epoch': 3.21}
 50%|█████     | 2525/5000 [12:08:55<12:03:04, 17.53s/it] 51%|█████     | 2526/5000 [12:09:14<12:15:36, 17.84s/it]                                                         {'loss': 19.3702, 'grad_norm': 18.75, 'learning_rate': 3.729007198112593e-05, 'epoch': 3.21}
 51%|█████     | 2526/5000 [12:09:14<12:15:36, 17.84s/it] 51%|█████     | 2527/5000 [12:09:29<11:36:54, 16.91s/it]                                                         {'loss': 19.1713, 'grad_norm': 30.5, 'learning_rate': 3.7266972509097095e-05, 'epoch': 3.21}
 51%|█████     | 2527/5000 [12:09:29<11:36:54, 16.91s/it] 51%|█████     | 2528/5000 [12:09:42<10:53:05, 15.85s/it]                                                         {'loss': 18.6752, 'grad_norm': 20.5, 'learning_rate': 3.724387204541747e-05, 'epoch': 3.21}
 51%|█████     | 2528/5000 [12:09:42<10:53:05, 15.85s/it] 51%|█████     | 2529/5000 [12:10:10<13:16:35, 19.34s/it]                                                         {'loss': 15.4891, 'grad_norm': 7.6875, 'learning_rate': 3.722077060019197e-05, 'epoch': 3.21}
 51%|█████     | 2529/5000 [12:10:10<13:16:35, 19.34s/it] 51%|█████     | 2530/5000 [12:10:28<13:07:43, 19.14s/it]                                                         {'loss': 17.0823, 'grad_norm': 24.125, 'learning_rate': 3.719766818352597e-05, 'epoch': 3.21}
 51%|█████     | 2530/5000 [12:10:28<13:07:43, 19.14s/it] 51%|█████     | 2531/5000 [12:10:44<12:24:06, 18.08s/it]                                                         {'loss': 18.2752, 'grad_norm': 50.75, 'learning_rate': 3.717456480552523e-05, 'epoch': 3.21}
 51%|█████     | 2531/5000 [12:10:44<12:24:06, 18.08s/it] 51%|█████     | 2532/5000 [12:11:00<11:56:39, 17.42s/it]                                                         {'loss': 24.5406, 'grad_norm': 1032.0, 'learning_rate': 3.715146047629598e-05, 'epoch': 3.22}
 51%|█████     | 2532/5000 [12:11:00<11:56:39, 17.42s/it] 51%|█████     | 2533/5000 [12:11:15<11:24:02, 16.64s/it]                                                         {'loss': 17.9265, 'grad_norm': 18.875, 'learning_rate': 3.712835520594482e-05, 'epoch': 3.22}
 51%|█████     | 2533/5000 [12:11:15<11:24:02, 16.64s/it] 51%|█████     | 2534/5000 [12:11:28<10:41:36, 15.61s/it]                                                         {'loss': 19.8353, 'grad_norm': 52.75, 'learning_rate': 3.710524900457878e-05, 'epoch': 3.22}
 51%|█████     | 2534/5000 [12:11:28<10:41:36, 15.61s/it] 51%|█████     | 2535/5000 [12:11:43<10:38:49, 15.55s/it]                                                         {'loss': 19.2488, 'grad_norm': 30.375, 'learning_rate': 3.70821418823053e-05, 'epoch': 3.22}
 51%|█████     | 2535/5000 [12:11:43<10:38:49, 15.55s/it] 51%|█████     | 2536/5000 [12:11:57<10:21:01, 15.12s/it]                                                         {'loss': 17.5149, 'grad_norm': 17.875, 'learning_rate': 3.705903384923222e-05, 'epoch': 3.22}
 51%|█████     | 2536/5000 [12:11:57<10:21:01, 15.12s/it] 51%|█████     | 2537/5000 [12:12:12<10:10:21, 14.87s/it]                                                         {'loss': 19.1815, 'grad_norm': 18.0, 'learning_rate': 3.7035924915467776e-05, 'epoch': 3.22}
 51%|█████     | 2537/5000 [12:12:12<10:10:21, 14.87s/it] 51%|█████     | 2538/5000 [12:12:31<11:02:22, 16.14s/it]                                                         {'loss': 19.3017, 'grad_norm': 46.5, 'learning_rate': 3.701281509112062e-05, 'epoch': 3.22}
 51%|█████     | 2538/5000 [12:12:31<11:02:22, 16.14s/it] 51%|█████     | 2539/5000 [12:12:45<10:40:40, 15.62s/it]                                                         {'loss': 18.376, 'grad_norm': 17.0, 'learning_rate': 3.698970438629976e-05, 'epoch': 3.22}
 51%|█████     | 2539/5000 [12:12:45<10:40:40, 15.62s/it] 51%|█████     | 2540/5000 [12:13:09<12:26:37, 18.21s/it]                                                         {'loss': 17.837, 'grad_norm': 15.875, 'learning_rate': 3.696659281111459e-05, 'epoch': 3.23}
 51%|█████     | 2540/5000 [12:13:09<12:26:37, 18.21s/it] 51%|█████     | 2541/5000 [12:13:26<12:07:03, 17.74s/it]                                                         {'loss': 17.9694, 'grad_norm': 30.875, 'learning_rate': 3.6943480375674916e-05, 'epoch': 3.23}
 51%|█████     | 2541/5000 [12:13:26<12:07:03, 17.74s/it] 51%|█████     | 2542/5000 [12:13:43<11:53:52, 17.43s/it]                                                         {'loss': 18.2396, 'grad_norm': 20.25, 'learning_rate': 3.69203670900909e-05, 'epoch': 3.23}
 51%|█████     | 2542/5000 [12:13:43<11:53:52, 17.43s/it] 51%|█████     | 2543/5000 [12:14:10<13:49:58, 20.27s/it]                                                         {'loss': 18.2695, 'grad_norm': 15.125, 'learning_rate': 3.689725296447308e-05, 'epoch': 3.23}
 51%|█████     | 2543/5000 [12:14:10<13:49:58, 20.27s/it] 51%|█████     | 2544/5000 [12:14:25<12:44:18, 18.67s/it]                                                         {'loss': 19.3238, 'grad_norm': 18.375, 'learning_rate': 3.6874138008932346e-05, 'epoch': 3.23}
 51%|█████     | 2544/5000 [12:14:25<12:44:18, 18.67s/it] 51%|█████     | 2545/5000 [12:14:45<13:03:34, 19.15s/it]                                                         {'loss': 17.5452, 'grad_norm': 16.875, 'learning_rate': 3.6851022233579984e-05, 'epoch': 3.23}
 51%|█████     | 2545/5000 [12:14:45<13:03:34, 19.15s/it] 51%|█████     | 2546/5000 [12:15:00<12:08:37, 17.81s/it]                                                         {'loss': 16.7749, 'grad_norm': 15.1875, 'learning_rate': 3.6827905648527605e-05, 'epoch': 3.23}
 51%|█████     | 2546/5000 [12:15:00<12:08:37, 17.81s/it] 51%|█████     | 2547/5000 [12:15:14<11:29:48, 16.87s/it]                                                         {'loss': 17.3972, 'grad_norm': 15.875, 'learning_rate': 3.680478826388719e-05, 'epoch': 3.23}
 51%|█████     | 2547/5000 [12:15:14<11:29:48, 16.87s/it] 51%|█████     | 2548/5000 [12:15:28<10:55:09, 16.03s/it]                                                         {'loss': 18.47, 'grad_norm': 14.875, 'learning_rate': 3.6781670089771064e-05, 'epoch': 3.24}
 51%|█████     | 2548/5000 [12:15:28<10:55:09, 16.03s/it] 51%|█████     | 2549/5000 [12:15:44<10:55:40, 16.05s/it]                                                         {'loss': 18.2167, 'grad_norm': 58.0, 'learning_rate': 3.6758551136291935e-05, 'epoch': 3.24}
 51%|█████     | 2549/5000 [12:15:44<10:55:40, 16.05s/it] 51%|█████     | 2550/5000 [12:15:59<10:40:37, 15.69s/it]                                                         {'loss': 19.3324, 'grad_norm': 46.25, 'learning_rate': 3.673543141356278e-05, 'epoch': 3.24}
 51%|█████     | 2550/5000 [12:15:59<10:40:37, 15.69s/it] 51%|█████     | 2551/5000 [12:16:15<10:35:57, 15.58s/it]                                                         {'loss': 18.1664, 'grad_norm': 90.0, 'learning_rate': 3.6712310931696965e-05, 'epoch': 3.24}
 51%|█████     | 2551/5000 [12:16:15<10:35:57, 15.58s/it] 51%|█████     | 2552/5000 [12:16:28<10:06:14, 14.86s/it]                                                         {'loss': 19.5125, 'grad_norm': 82.0, 'learning_rate': 3.668918970080818e-05, 'epoch': 3.24}
 51%|█████     | 2552/5000 [12:16:28<10:06:14, 14.86s/it] 51%|█████     | 2553/5000 [12:16:41<9:46:09, 14.37s/it]                                                         {'loss': 18.3959, 'grad_norm': 22.625, 'learning_rate': 3.6666067731010414e-05, 'epoch': 3.24}
 51%|█████     | 2553/5000 [12:16:41<9:46:09, 14.37s/it] 51%|█████     | 2554/5000 [12:17:01<10:53:08, 16.02s/it]                                                         {'loss': 18.5309, 'grad_norm': 10.125, 'learning_rate': 3.664294503241804e-05, 'epoch': 3.24}
 51%|█████     | 2554/5000 [12:17:01<10:53:08, 16.02s/it] 51%|█████     | 2555/5000 [12:17:17<10:49:37, 15.94s/it]                                                         {'loss': 19.4885, 'grad_norm': 17.875, 'learning_rate': 3.661982161514568e-05, 'epoch': 3.24}
 51%|█████     | 2555/5000 [12:17:17<10:49:37, 15.94s/it] 51%|█████     | 2556/5000 [12:17:31<10:32:37, 15.53s/it]                                                         {'loss': 23.4669, 'grad_norm': 556.0, 'learning_rate': 3.6596697489308314e-05, 'epoch': 3.25}
 51%|█████     | 2556/5000 [12:17:31<10:32:37, 15.53s/it] 51%|█████     | 2557/5000 [12:17:46<10:29:26, 15.46s/it]                                                         {'loss': 20.229, 'grad_norm': 19.5, 'learning_rate': 3.6573572665021235e-05, 'epoch': 3.25}
 51%|█████     | 2557/5000 [12:17:46<10:29:26, 15.46s/it] 51%|█████     | 2558/5000 [12:18:04<10:59:01, 16.19s/it]                                                         {'loss': 19.0008, 'grad_norm': 20.0, 'learning_rate': 3.655044715239999e-05, 'epoch': 3.25}
 51%|█████     | 2558/5000 [12:18:04<10:59:01, 16.19s/it] 51%|█████     | 2559/5000 [12:18:17<10:15:25, 15.13s/it]                                                         {'loss': 20.1612, 'grad_norm': 101.5, 'learning_rate': 3.6527320961560495e-05, 'epoch': 3.25}
 51%|█████     | 2559/5000 [12:18:17<10:15:25, 15.13s/it] 51%|█████     | 2560/5000 [12:18:31<9:56:49, 14.68s/it]                                                         {'loss': 18.5003, 'grad_norm': 19.25, 'learning_rate': 3.6504194102618915e-05, 'epoch': 3.25}
 51%|█████     | 2560/5000 [12:18:31<9:56:49, 14.68s/it] 51%|█████     | 2561/5000 [12:18:46<10:04:36, 14.87s/it]                                                         {'loss': 17.9813, 'grad_norm': 11.625, 'learning_rate': 3.648106658569174e-05, 'epoch': 3.25}
 51%|█████     | 2561/5000 [12:18:46<10:04:36, 14.87s/it] 51%|█████     | 2562/5000 [12:19:01<10:05:08, 14.89s/it]                                                         {'loss': 19.0358, 'grad_norm': 25.25, 'learning_rate': 3.645793842089572e-05, 'epoch': 3.25}
 51%|█████     | 2562/5000 [12:19:01<10:05:08, 14.89s/it] 51%|█████▏    | 2563/5000 [12:19:17<10:22:44, 15.33s/it]                                                         {'loss': 18.2469, 'grad_norm': 41.0, 'learning_rate': 3.6434809618347906e-05, 'epoch': 3.25}
 51%|█████▏    | 2563/5000 [12:19:17<10:22:44, 15.33s/it] 51%|█████▏    | 2564/5000 [12:19:43<12:33:53, 18.57s/it]                                                         {'loss': 18.0465, 'grad_norm': 18.625, 'learning_rate': 3.641168018816561e-05, 'epoch': 3.26}
 51%|█████▏    | 2564/5000 [12:19:43<12:33:53, 18.57s/it] 51%|█████▏    | 2565/5000 [12:20:07<13:38:28, 20.17s/it]                                                         {'loss': 19.2992, 'grad_norm': 18.0, 'learning_rate': 3.6388550140466444e-05, 'epoch': 3.26}
 51%|█████▏    | 2565/5000 [12:20:07<13:38:28, 20.17s/it] 51%|█████▏    | 2566/5000 [12:20:24<12:50:08, 18.98s/it]                                                         {'loss': 18.5617, 'grad_norm': 13.1875, 'learning_rate': 3.636541948536828e-05, 'epoch': 3.26}
 51%|█████▏    | 2566/5000 [12:20:24<12:50:08, 18.98s/it] 51%|█████▏    | 2567/5000 [12:20:38<11:50:48, 17.53s/it]                                                         {'loss': 18.6547, 'grad_norm': 13.9375, 'learning_rate': 3.6342288232989226e-05, 'epoch': 3.26}
 51%|█████▏    | 2567/5000 [12:20:38<11:50:48, 17.53s/it] 51%|█████▏    | 2568/5000 [12:20:52<11:07:12, 16.46s/it]                                                         {'loss': 20.5797, 'grad_norm': 42.75, 'learning_rate': 3.6319156393447715e-05, 'epoch': 3.26}
 51%|█████▏    | 2568/5000 [12:20:52<11:07:12, 16.46s/it] 51%|█████▏    | 2569/5000 [12:21:14<12:19:35, 18.25s/it]                                                         {'loss': 19.5026, 'grad_norm': 18.625, 'learning_rate': 3.6296023976862365e-05, 'epoch': 3.26}
 51%|█████▏    | 2569/5000 [12:21:14<12:19:35, 18.25s/it] 51%|█████▏    | 2570/5000 [12:21:38<13:22:32, 19.82s/it]                                                         {'loss': 19.1854, 'grad_norm': 11.75, 'learning_rate': 3.6272890993352104e-05, 'epoch': 3.26}
 51%|█████▏    | 2570/5000 [12:21:38<13:22:32, 19.82s/it] 51%|█████▏    | 2571/5000 [12:21:53<12:27:30, 18.46s/it]                                                         {'loss': 18.2388, 'grad_norm': 15.75, 'learning_rate': 3.624975745303607e-05, 'epoch': 3.26}
 51%|█████▏    | 2571/5000 [12:21:53<12:27:30, 18.46s/it] 51%|█████▏    | 2572/5000 [12:22:15<13:10:20, 19.53s/it]                                                         {'loss': 19.4651, 'grad_norm': 24.75, 'learning_rate': 3.6226623366033666e-05, 'epoch': 3.27}
 51%|█████▏    | 2572/5000 [12:22:15<13:10:20, 19.53s/it] 51%|█████▏    | 2573/5000 [12:22:30<12:16:09, 18.20s/it]                                                         {'loss': 19.5529, 'grad_norm': 15.1875, 'learning_rate': 3.6203488742464535e-05, 'epoch': 3.27}
 51%|█████▏    | 2573/5000 [12:22:30<12:16:09, 18.20s/it] 51%|█████▏    | 2574/5000 [12:22:45<11:38:41, 17.28s/it]                                                         {'loss': 17.7342, 'grad_norm': 23.625, 'learning_rate': 3.618035359244854e-05, 'epoch': 3.27}
 51%|█████▏    | 2574/5000 [12:22:45<11:38:41, 17.28s/it] 52%|█████▏    | 2575/5000 [12:22:58<10:51:30, 16.12s/it]                                                         {'loss': 20.0128, 'grad_norm': 40.0, 'learning_rate': 3.6157217926105783e-05, 'epoch': 3.27}
 52%|█████▏    | 2575/5000 [12:22:58<10:51:30, 16.12s/it] 52%|█████▏    | 2576/5000 [12:23:13<10:32:54, 15.67s/it]                                                         {'loss': 18.967, 'grad_norm': 18.875, 'learning_rate': 3.613408175355659e-05, 'epoch': 3.27}
 52%|█████▏    | 2576/5000 [12:23:13<10:32:54, 15.67s/it] 52%|█████▏    | 2577/5000 [12:23:47<14:14:21, 21.16s/it]                                                         {'loss': 18.4362, 'grad_norm': 13.625, 'learning_rate': 3.611094508492151e-05, 'epoch': 3.27}
 52%|█████▏    | 2577/5000 [12:23:47<14:14:21, 21.16s/it] 52%|█████▏    | 2578/5000 [12:24:06<13:43:03, 20.39s/it]                                                         {'loss': 20.6781, 'grad_norm': 21.5, 'learning_rate': 3.608780793032131e-05, 'epoch': 3.27}
 52%|█████▏    | 2578/5000 [12:24:06<13:43:03, 20.39s/it] 52%|█████▏    | 2579/5000 [12:24:21<12:45:17, 18.97s/it]                                                         {'loss': 19.555, 'grad_norm': 20.75, 'learning_rate': 3.606467029987695e-05, 'epoch': 3.27}
 52%|█████▏    | 2579/5000 [12:24:21<12:45:17, 18.97s/it] 52%|█████▏    | 2580/5000 [12:24:38<12:12:58, 18.17s/it]                                                         {'loss': 19.2301, 'grad_norm': 19.25, 'learning_rate': 3.604153220370963e-05, 'epoch': 3.28}
 52%|█████▏    | 2580/5000 [12:24:38<12:12:58, 18.17s/it] 52%|█████▏    | 2581/5000 [12:24:59<12:55:01, 19.22s/it]                                                         {'loss': 19.46, 'grad_norm': 17.75, 'learning_rate': 3.6018393651940756e-05, 'epoch': 3.28}
 52%|█████▏    | 2581/5000 [12:24:59<12:55:01, 19.22s/it] 52%|█████▏    | 2582/5000 [12:25:14<12:01:35, 17.91s/it]                                                         {'loss': 20.3182, 'grad_norm': 22.625, 'learning_rate': 3.599525465469188e-05, 'epoch': 3.28}
 52%|█████▏    | 2582/5000 [12:25:14<12:01:35, 17.91s/it] 52%|█████▏    | 2583/5000 [12:25:38<13:17:56, 19.81s/it]                                                         {'loss': 18.9735, 'grad_norm': 16.375, 'learning_rate': 3.597211522208482e-05, 'epoch': 3.28}
 52%|█████▏    | 2583/5000 [12:25:38<13:17:56, 19.81s/it] 52%|█████▏    | 2584/5000 [12:25:54<12:28:20, 18.58s/it]                                                         {'loss': 18.1818, 'grad_norm': 40.5, 'learning_rate': 3.5948975364241524e-05, 'epoch': 3.28}
 52%|█████▏    | 2584/5000 [12:25:54<12:28:20, 18.58s/it] 52%|█████▏    | 2585/5000 [12:26:11<12:09:11, 18.12s/it]                                                         {'loss': 18.1145, 'grad_norm': 18.0, 'learning_rate': 3.592583509128417e-05, 'epoch': 3.28}
 52%|█████▏    | 2585/5000 [12:26:11<12:09:11, 18.12s/it] 52%|█████▏    | 2586/5000 [12:26:27<11:40:59, 17.42s/it]                                                         {'loss': 18.389, 'grad_norm': 46.75, 'learning_rate': 3.590269441333509e-05, 'epoch': 3.28}
 52%|█████▏    | 2586/5000 [12:26:27<11:40:59, 17.42s/it] 52%|█████▏    | 2587/5000 [12:26:39<10:38:42, 15.88s/it]                                                         {'loss': 19.2288, 'grad_norm': 23.25, 'learning_rate': 3.587955334051681e-05, 'epoch': 3.29}
 52%|█████▏    | 2587/5000 [12:26:39<10:38:42, 15.88s/it] 52%|█████▏    | 2588/5000 [12:26:55<10:37:14, 15.85s/it]                                                         {'loss': 19.8997, 'grad_norm': 26.125, 'learning_rate': 3.585641188295201e-05, 'epoch': 3.29}
 52%|█████▏    | 2588/5000 [12:26:55<10:37:14, 15.85s/it] 52%|█████▏    | 2589/5000 [12:27:12<10:56:03, 16.33s/it]                                                         {'loss': 19.7964, 'grad_norm': 12.8125, 'learning_rate': 3.583327005076357e-05, 'epoch': 3.29}
 52%|█████▏    | 2589/5000 [12:27:12<10:56:03, 16.33s/it] 52%|█████▏    | 2590/5000 [12:27:36<12:22:15, 18.48s/it]                                                         {'loss': 18.8907, 'grad_norm': 13.8125, 'learning_rate': 3.581012785407448e-05, 'epoch': 3.29}
 52%|█████▏    | 2590/5000 [12:27:36<12:22:15, 18.48s/it] 52%|█████▏    | 2591/5000 [12:27:49<11:11:09, 16.72s/it]                                                         {'loss': 19.0716, 'grad_norm': 16.875, 'learning_rate': 3.578698530300796e-05, 'epoch': 3.29}
 52%|█████▏    | 2591/5000 [12:27:49<11:11:09, 16.72s/it] 52%|█████▏    | 2592/5000 [12:28:12<12:26:15, 18.59s/it]                                                         {'loss': 19.052, 'grad_norm': 19.75, 'learning_rate': 3.5763842407687325e-05, 'epoch': 3.29}
 52%|█████▏    | 2592/5000 [12:28:12<12:26:15, 18.59s/it] 52%|█████▏    | 2593/5000 [12:28:29<12:15:44, 18.34s/it]                                                         {'loss': 18.7269, 'grad_norm': 26.25, 'learning_rate': 3.574069917823607e-05, 'epoch': 3.29}
 52%|█████▏    | 2593/5000 [12:28:29<12:15:44, 18.34s/it] 52%|█████▏    | 2594/5000 [12:28:47<12:11:55, 18.25s/it]                                                         {'loss': 19.1439, 'grad_norm': 33.0, 'learning_rate': 3.5717555624777826e-05, 'epoch': 3.29}
 52%|█████▏    | 2594/5000 [12:28:47<12:11:55, 18.25s/it] 52%|█████▏    | 2595/5000 [12:29:10<13:01:21, 19.49s/it]                                                         {'loss': 17.8461, 'grad_norm': 17.875, 'learning_rate': 3.5694411757436375e-05, 'epoch': 3.3}
 52%|█████▏    | 2595/5000 [12:29:10<13:01:21, 19.49s/it] 52%|█████▏    | 2596/5000 [12:29:27<12:28:45, 18.69s/it]                                                         {'loss': 18.453, 'grad_norm': 18.625, 'learning_rate': 3.567126758633563e-05, 'epoch': 3.3}
 52%|█████▏    | 2596/5000 [12:29:27<12:28:45, 18.69s/it] 52%|█████▏    | 2597/5000 [12:29:43<12:03:56, 18.08s/it]                                                         {'loss': 18.5168, 'grad_norm': 13.25, 'learning_rate': 3.564812312159965e-05, 'epoch': 3.3}
 52%|█████▏    | 2597/5000 [12:29:43<12:03:56, 18.08s/it] 52%|█████▏    | 2598/5000 [12:30:00<11:52:46, 17.80s/it]                                                         {'loss': 17.2158, 'grad_norm': 13.4375, 'learning_rate': 3.562497837335257e-05, 'epoch': 3.3}
 52%|█████▏    | 2598/5000 [12:30:00<11:52:46, 17.80s/it] 52%|█████▏    | 2599/5000 [12:30:26<13:27:08, 20.17s/it]                                                         {'loss': 17.9726, 'grad_norm': 12.375, 'learning_rate': 3.5601833351718734e-05, 'epoch': 3.3}
 52%|█████▏    | 2599/5000 [12:30:26<13:27:08, 20.17s/it] 52%|█████▏    | 2600/5000 [12:30:41<12:29:59, 18.75s/it]                                                         {'loss': 18.7646, 'grad_norm': 16.25, 'learning_rate': 3.557868806682255e-05, 'epoch': 3.3}
 52%|█████▏    | 2600/5000 [12:30:41<12:29:59, 18.75s/it] 52%|█████▏    | 2601/5000 [12:31:02<12:50:46, 19.28s/it]                                                         {'loss': 19.7756, 'grad_norm': 30.25, 'learning_rate': 3.555554252878853e-05, 'epoch': 3.3}
 52%|█████▏    | 2601/5000 [12:31:02<12:50:46, 19.28s/it] 52%|█████▏    | 2602/5000 [12:31:19<12:27:36, 18.71s/it]                                                         {'loss': 17.3683, 'grad_norm': 22.875, 'learning_rate': 3.5532396747741334e-05, 'epoch': 3.3}
 52%|█████▏    | 2602/5000 [12:31:19<12:27:36, 18.71s/it] 52%|█████▏    | 2603/5000 [12:31:33<11:24:04, 17.12s/it]                                                         {'loss': 19.796, 'grad_norm': 34.5, 'learning_rate': 3.550925073380573e-05, 'epoch': 3.31}
 52%|█████▏    | 2603/5000 [12:31:33<11:24:04, 17.12s/it] 52%|█████▏    | 2604/5000 [12:31:47<10:44:36, 16.14s/it]                                                         {'loss': 19.9286, 'grad_norm': 66.5, 'learning_rate': 3.548610449710654e-05, 'epoch': 3.31}
 52%|█████▏    | 2604/5000 [12:31:47<10:44:36, 16.14s/it] 52%|█████▏    | 2605/5000 [12:32:05<11:16:01, 16.94s/it]                                                         {'loss': 17.5736, 'grad_norm': 11.3125, 'learning_rate': 3.5462958047768736e-05, 'epoch': 3.31}
 52%|█████▏    | 2605/5000 [12:32:05<11:16:01, 16.94s/it] 52%|█████▏    | 2606/5000 [12:32:19<10:39:06, 16.02s/it]                                                         {'loss': 18.9062, 'grad_norm': 15.375, 'learning_rate': 3.5439811395917344e-05, 'epoch': 3.31}
 52%|█████▏    | 2606/5000 [12:32:19<10:39:06, 16.02s/it] 52%|█████▏    | 2607/5000 [12:32:32<10:00:12, 15.05s/it]                                                         {'loss': 20.2381, 'grad_norm': 31.25, 'learning_rate': 3.541666455167751e-05, 'epoch': 3.31}
 52%|█████▏    | 2607/5000 [12:32:32<10:00:12, 15.05s/it] 52%|█████▏    | 2608/5000 [12:32:56<11:48:19, 17.77s/it]                                                         {'loss': 19.1321, 'grad_norm': 16.75, 'learning_rate': 3.539351752517444e-05, 'epoch': 3.31}
 52%|█████▏    | 2608/5000 [12:32:56<11:48:19, 17.77s/it] 52%|█████▏    | 2609/5000 [12:33:11<11:07:07, 16.74s/it]                                                         {'loss': 18.1477, 'grad_norm': 11.25, 'learning_rate': 3.537037032653344e-05, 'epoch': 3.31}
 52%|█████▏    | 2609/5000 [12:33:11<11:07:07, 16.74s/it] 52%|█████▏    | 2610/5000 [12:33:27<11:02:55, 16.64s/it]                                                         {'loss': 17.1851, 'grad_norm': 19.125, 'learning_rate': 3.534722296587987e-05, 'epoch': 3.31}
 52%|█████▏    | 2610/5000 [12:33:27<11:02:55, 16.64s/it] 52%|█████▏    | 2611/5000 [12:33:42<10:44:38, 16.19s/it]                                                         {'loss': 17.8371, 'grad_norm': 19.375, 'learning_rate': 3.532407545333919e-05, 'epoch': 3.32}
 52%|█████▏    | 2611/5000 [12:33:42<10:44:38, 16.19s/it] 52%|█████▏    | 2612/5000 [12:34:04<11:48:15, 17.80s/it]                                                         {'loss': 16.9218, 'grad_norm': 14.1875, 'learning_rate': 3.530092779903689e-05, 'epoch': 3.32}
 52%|█████▏    | 2612/5000 [12:34:04<11:48:15, 17.80s/it] 52%|█████▏    | 2613/5000 [12:34:19<11:18:33, 17.06s/it]                                                         {'loss': 18.2725, 'grad_norm': 24.75, 'learning_rate': 3.5277780013098546e-05, 'epoch': 3.32}
 52%|█████▏    | 2613/5000 [12:34:19<11:18:33, 17.06s/it] 52%|█████▏    | 2614/5000 [12:34:35<11:08:44, 16.82s/it]                                                         {'loss': 18.0237, 'grad_norm': 14.25, 'learning_rate': 3.525463210564979e-05, 'epoch': 3.32}
 52%|█████▏    | 2614/5000 [12:34:35<11:08:44, 16.82s/it] 52%|█████▏    | 2615/5000 [12:34:51<10:57:07, 16.53s/it]                                                         {'loss': 18.8873, 'grad_norm': 17.0, 'learning_rate': 3.5231484086816296e-05, 'epoch': 3.32}
 52%|█████▏    | 2615/5000 [12:34:51<10:57:07, 16.53s/it] 52%|█████▏    | 2616/5000 [12:35:14<12:09:44, 18.37s/it]                                                         {'loss': 17.9535, 'grad_norm': 10.5, 'learning_rate': 3.52083359667238e-05, 'epoch': 3.32}
 52%|█████▏    | 2616/5000 [12:35:14<12:09:44, 18.37s/it] 52%|█████▏    | 2617/5000 [12:35:41<13:50:12, 20.90s/it]                                                         {'loss': 18.0583, 'grad_norm': 11.625, 'learning_rate': 3.5185187755498075e-05, 'epoch': 3.32}
 52%|█████▏    | 2617/5000 [12:35:41<13:50:12, 20.90s/it] 52%|█████▏    | 2618/5000 [12:35:56<12:40:37, 19.16s/it]                                                         {'loss': 19.1962, 'grad_norm': 21.375, 'learning_rate': 3.516203946326493e-05, 'epoch': 3.32}
 52%|█████▏    | 2618/5000 [12:35:56<12:40:37, 19.16s/it] 52%|█████▏    | 2619/5000 [12:36:22<14:01:04, 21.19s/it]                                                         {'loss': 20.1201, 'grad_norm': 28.25, 'learning_rate': 3.513889110015022e-05, 'epoch': 3.33}
 52%|█████▏    | 2619/5000 [12:36:22<14:01:04, 21.19s/it] 52%|█████▏    | 2620/5000 [12:36:37<12:56:16, 19.57s/it]                                                         {'loss': 19.1671, 'grad_norm': 22.25, 'learning_rate': 3.511574267627983e-05, 'epoch': 3.33}
 52%|█████▏    | 2620/5000 [12:36:37<12:56:16, 19.57s/it] 52%|█████▏    | 2621/5000 [12:36:53<12:06:03, 18.31s/it]                                                         {'loss': 18.3395, 'grad_norm': 15.5625, 'learning_rate': 3.509259420177966e-05, 'epoch': 3.33}
 52%|█████▏    | 2621/5000 [12:36:53<12:06:03, 18.31s/it] 52%|█████▏    | 2622/5000 [12:37:10<11:58:59, 18.14s/it]                                                         {'loss': 18.7076, 'grad_norm': 22.375, 'learning_rate': 3.506944568677563e-05, 'epoch': 3.33}
 52%|█████▏    | 2622/5000 [12:37:10<11:58:59, 18.14s/it] 52%|█████▏    | 2623/5000 [12:37:28<11:50:48, 17.94s/it]                                                         {'loss': 19.4784, 'grad_norm': 14.4375, 'learning_rate': 3.504629714139371e-05, 'epoch': 3.33}
 52%|█████▏    | 2623/5000 [12:37:28<11:50:48, 17.94s/it] 52%|█████▏    | 2624/5000 [12:37:41<10:54:19, 16.52s/it]                                                         {'loss': 18.8187, 'grad_norm': 14.1875, 'learning_rate': 3.502314857575985e-05, 'epoch': 3.33}
 52%|█████▏    | 2624/5000 [12:37:41<10:54:19, 16.52s/it] 52%|█████▎    | 2625/5000 [12:37:59<11:05:12, 16.81s/it]                                                         {'loss': 16.9668, 'grad_norm': 8.25, 'learning_rate': 3.5e-05, 'epoch': 3.33}
 52%|█████▎    | 2625/5000 [12:37:59<11:05:12, 16.81s/it] 53%|█████▎    | 2626/5000 [12:38:15<11:02:30, 16.74s/it]                                                         {'loss': 18.7119, 'grad_norm': 15.9375, 'learning_rate': 3.4976851424240165e-05, 'epoch': 3.33}
 53%|█████▎    | 2626/5000 [12:38:15<11:02:30, 16.74s/it] 53%|█████▎    | 2627/5000 [12:38:41<12:49:05, 19.45s/it]                                                         {'loss': 17.6361, 'grad_norm': 22.625, 'learning_rate': 3.4953702858606286e-05, 'epoch': 3.34}
 53%|█████▎    | 2627/5000 [12:38:41<12:49:05, 19.45s/it] 53%|█████▎    | 2628/5000 [12:38:59<12:35:58, 19.12s/it]                                                         {'loss': 17.3357, 'grad_norm': 9.5, 'learning_rate': 3.493055431322436e-05, 'epoch': 3.34}
 53%|█████▎    | 2628/5000 [12:38:59<12:35:58, 19.12s/it] 53%|█████▎    | 2629/5000 [12:39:26<14:08:55, 21.48s/it]                                                         {'loss': 17.8278, 'grad_norm': 17.25, 'learning_rate': 3.4907405798220354e-05, 'epoch': 3.34}
 53%|█████▎    | 2629/5000 [12:39:26<14:08:55, 21.48s/it] 53%|█████▎    | 2630/5000 [12:39:44<13:18:45, 20.22s/it]                                                         {'loss': 18.7563, 'grad_norm': 9.6875, 'learning_rate': 3.4884257323720166e-05, 'epoch': 3.34}
 53%|█████▎    | 2630/5000 [12:39:44<13:18:45, 20.22s/it] 53%|█████▎    | 2631/5000 [12:39:59<12:22:50, 18.81s/it]                                                         {'loss': 20.2091, 'grad_norm': 25.75, 'learning_rate': 3.4861108899849774e-05, 'epoch': 3.34}
 53%|█████▎    | 2631/5000 [12:39:59<12:22:50, 18.81s/it] 53%|█████▎    | 2632/5000 [12:40:12<11:11:57, 17.03s/it]                                                         {'loss': 20.0644, 'grad_norm': 19.25, 'learning_rate': 3.483796053673507e-05, 'epoch': 3.34}
 53%|█████▎    | 2632/5000 [12:40:12<11:11:57, 17.03s/it] 53%|█████▎    | 2633/5000 [12:40:36<12:35:53, 19.16s/it]                                                         {'loss': 18.3477, 'grad_norm': 51.25, 'learning_rate': 3.4814812244501925e-05, 'epoch': 3.34}
 53%|█████▎    | 2633/5000 [12:40:36<12:35:53, 19.16s/it] 53%|█████▎    | 2634/5000 [12:40:53<12:08:59, 18.49s/it]                                                         {'loss': 19.1484, 'grad_norm': 15.4375, 'learning_rate': 3.479166403327619e-05, 'epoch': 3.34}
 53%|█████▎    | 2634/5000 [12:40:53<12:08:59, 18.49s/it] 53%|█████▎    | 2635/5000 [12:41:09<11:32:50, 17.58s/it]                                                         {'loss': 17.9495, 'grad_norm': 13.9375, 'learning_rate': 3.4768515913183705e-05, 'epoch': 3.35}
 53%|█████▎    | 2635/5000 [12:41:09<11:32:50, 17.58s/it] 53%|█████▎    | 2636/5000 [12:41:33<12:54:54, 19.67s/it]                                                         {'loss': 18.3038, 'grad_norm': 15.875, 'learning_rate': 3.474536789435022e-05, 'epoch': 3.35}
 53%|█████▎    | 2636/5000 [12:41:33<12:54:54, 19.67s/it] 53%|█████▎    | 2637/5000 [12:41:52<12:41:36, 19.34s/it]                                                         {'loss': 18.5774, 'grad_norm': 11.9375, 'learning_rate': 3.4722219986901454e-05, 'epoch': 3.35}
 53%|█████▎    | 2637/5000 [12:41:52<12:41:36, 19.34s/it] 53%|█████▎    | 2638/5000 [12:42:05<11:28:00, 17.48s/it]                                                         {'loss': 20.5062, 'grad_norm': 38.25, 'learning_rate': 3.469907220096311e-05, 'epoch': 3.35}
 53%|█████▎    | 2638/5000 [12:42:05<11:28:00, 17.48s/it] 53%|█████▎    | 2639/5000 [12:42:22<11:23:18, 17.36s/it]                                                         {'loss': 19.7518, 'grad_norm': 25.0, 'learning_rate': 3.467592454666082e-05, 'epoch': 3.35}
 53%|█████▎    | 2639/5000 [12:42:22<11:23:18, 17.36s/it] 53%|█████▎    | 2640/5000 [12:42:37<11:01:02, 16.81s/it]                                                         {'loss': 28.0754, 'grad_norm': 1264.0, 'learning_rate': 3.4652777034120125e-05, 'epoch': 3.35}
 53%|█████▎    | 2640/5000 [12:42:37<11:01:02, 16.81s/it] 53%|█████▎    | 2641/5000 [12:42:53<10:52:04, 16.59s/it]                                                         {'loss': 18.05, 'grad_norm': 31.25, 'learning_rate': 3.462962967346656e-05, 'epoch': 3.35}
 53%|█████▎    | 2641/5000 [12:42:53<10:52:04, 16.59s/it] 53%|█████▎    | 2642/5000 [12:43:07<10:14:42, 15.64s/it]                                                         {'loss': 18.2881, 'grad_norm': 18.625, 'learning_rate': 3.460648247482556e-05, 'epoch': 3.35}
 53%|█████▎    | 2642/5000 [12:43:07<10:14:42, 15.64s/it] 53%|█████▎    | 2643/5000 [12:43:27<11:11:26, 17.09s/it]                                                         {'loss': 18.1259, 'grad_norm': 10.125, 'learning_rate': 3.458333544832249e-05, 'epoch': 3.36}
 53%|█████▎    | 2643/5000 [12:43:27<11:11:26, 17.09s/it] 53%|█████▎    | 2644/5000 [12:43:53<12:55:12, 19.74s/it]                                                         {'loss': 18.8168, 'grad_norm': 15.0, 'learning_rate': 3.456018860408265e-05, 'epoch': 3.36}
 53%|█████▎    | 2644/5000 [12:43:53<12:55:12, 19.74s/it] 53%|█████▎    | 2645/5000 [12:44:10<12:17:40, 18.79s/it]                                                         {'loss': 17.4732, 'grad_norm': 12.3125, 'learning_rate': 3.453704195223127e-05, 'epoch': 3.36}
 53%|█████▎    | 2645/5000 [12:44:10<12:17:40, 18.79s/it] 53%|█████▎    | 2646/5000 [12:44:28<12:15:05, 18.74s/it]                                                         {'loss': 18.0833, 'grad_norm': 10.5, 'learning_rate': 3.451389550289345e-05, 'epoch': 3.36}
 53%|█████▎    | 2646/5000 [12:44:28<12:15:05, 18.74s/it] 53%|█████▎    | 2647/5000 [12:44:41<11:06:50, 17.00s/it]                                                         {'loss': 18.7862, 'grad_norm': 20.0, 'learning_rate': 3.449074926619427e-05, 'epoch': 3.36}
 53%|█████▎    | 2647/5000 [12:44:41<11:06:50, 17.00s/it] 53%|█████▎    | 2648/5000 [12:44:59<11:18:30, 17.31s/it]                                                         {'loss': 18.5072, 'grad_norm': 11.0625, 'learning_rate': 3.446760325225866e-05, 'epoch': 3.36}
 53%|█████▎    | 2648/5000 [12:44:59<11:18:30, 17.31s/it] 53%|█████▎    | 2649/5000 [12:45:17<11:18:12, 17.31s/it]                                                         {'loss': 18.7697, 'grad_norm': 15.5, 'learning_rate': 3.444445747121146e-05, 'epoch': 3.36}
 53%|█████▎    | 2649/5000 [12:45:17<11:18:12, 17.31s/it] 53%|█████▎    | 2650/5000 [12:45:34<11:15:58, 17.26s/it]                                                         {'loss': 17.9357, 'grad_norm': 9.4375, 'learning_rate': 3.442131193317745e-05, 'epoch': 3.37}
 53%|█████▎    | 2650/5000 [12:45:34<11:15:58, 17.26s/it] 53%|█████▎    | 2651/5000 [12:45:51<11:13:44, 17.21s/it]                                                         {'loss': 19.4034, 'grad_norm': 17.375, 'learning_rate': 3.4398166648281266e-05, 'epoch': 3.37}
 53%|█████▎    | 2651/5000 [12:45:51<11:13:44, 17.21s/it] 53%|█████▎    | 2652/5000 [12:46:04<10:29:11, 16.08s/it]                                                         {'loss': 19.456, 'grad_norm': 18.25, 'learning_rate': 3.437502162664743e-05, 'epoch': 3.37}
 53%|█████▎    | 2652/5000 [12:46:04<10:29:11, 16.08s/it] 53%|█████▎    | 2653/5000 [12:46:18<10:03:30, 15.43s/it]                                                         {'loss': 17.461, 'grad_norm': 14.25, 'learning_rate': 3.435187687840036e-05, 'epoch': 3.37}
 53%|█████▎    | 2653/5000 [12:46:18<10:03:30, 15.43s/it] 53%|█████▎    | 2654/5000 [12:46:36<10:26:30, 16.02s/it]                                                         {'loss': 20.2775, 'grad_norm': 23.125, 'learning_rate': 3.432873241366437e-05, 'epoch': 3.37}
 53%|█████▎    | 2654/5000 [12:46:36<10:26:30, 16.02s/it] 53%|█████▎    | 2655/5000 [12:46:54<10:55:10, 16.76s/it]                                                         {'loss': 17.9317, 'grad_norm': 9.625, 'learning_rate': 3.4305588242563625e-05, 'epoch': 3.37}
 53%|█████▎    | 2655/5000 [12:46:54<10:55:10, 16.76s/it] 53%|█████▎    | 2656/5000 [12:47:21<12:49:09, 19.69s/it]                                                         {'loss': 17.8646, 'grad_norm': 15.25, 'learning_rate': 3.4282444375222175e-05, 'epoch': 3.37}
 53%|█████▎    | 2656/5000 [12:47:21<12:49:09, 19.69s/it] 53%|█████▎    | 2657/5000 [12:47:41<12:53:07, 19.80s/it]                                                         {'loss': 17.2433, 'grad_norm': 9.4375, 'learning_rate': 3.4259300821763934e-05, 'epoch': 3.37}
 53%|█████▎    | 2657/5000 [12:47:41<12:53:07, 19.80s/it] 53%|█████▎    | 2658/5000 [12:47:54<11:34:47, 17.80s/it]                                                         {'loss': 22.5837, 'grad_norm': 896.0, 'learning_rate': 3.4236157592312676e-05, 'epoch': 3.38}
 53%|█████▎    | 2658/5000 [12:47:54<11:34:47, 17.80s/it] 53%|█████▎    | 2659/5000 [12:48:09<11:00:48, 16.94s/it]                                                         {'loss': 18.1783, 'grad_norm': 19.5, 'learning_rate': 3.4213014696992036e-05, 'epoch': 3.38}
 53%|█████▎    | 2659/5000 [12:48:09<11:00:48, 16.94s/it] 53%|█████▎    | 2660/5000 [12:48:33<12:19:30, 18.96s/it]                                                         {'loss': 18.2087, 'grad_norm': 13.3125, 'learning_rate': 3.4189872145925514e-05, 'epoch': 3.38}
 53%|█████▎    | 2660/5000 [12:48:33<12:19:30, 18.96s/it] 53%|█████▎    | 2661/5000 [12:48:52<12:22:38, 19.05s/it]                                                         {'loss': 17.8842, 'grad_norm': 16.625, 'learning_rate': 3.416672994923644e-05, 'epoch': 3.38}
 53%|█████▎    | 2661/5000 [12:48:52<12:22:38, 19.05s/it] 53%|█████▎    | 2662/5000 [12:49:06<11:28:03, 17.66s/it]                                                         {'loss': 17.6699, 'grad_norm': 11.75, 'learning_rate': 3.414358811704798e-05, 'epoch': 3.38}
 53%|█████▎    | 2662/5000 [12:49:06<11:28:03, 17.66s/it] 53%|█████▎    | 2663/5000 [12:49:19<10:28:52, 16.15s/it]                                                         {'loss': 19.37, 'grad_norm': 18.5, 'learning_rate': 3.412044665948319e-05, 'epoch': 3.38}
 53%|█████▎    | 2663/5000 [12:49:19<10:28:52, 16.15s/it] 53%|█████▎    | 2664/5000 [12:49:34<10:15:54, 15.82s/it]                                                         {'loss': 17.7946, 'grad_norm': 15.75, 'learning_rate': 3.409730558666491e-05, 'epoch': 3.38}
 53%|█████▎    | 2664/5000 [12:49:34<10:15:54, 15.82s/it] 53%|█████▎    | 2665/5000 [12:49:59<11:59:59, 18.50s/it]                                                         {'loss': 17.7302, 'grad_norm': 8.875, 'learning_rate': 3.407416490871582e-05, 'epoch': 3.38}
 53%|█████▎    | 2665/5000 [12:49:59<11:59:59, 18.50s/it] 53%|█████▎    | 2666/5000 [12:50:13<11:15:03, 17.35s/it]                                                         {'loss': 18.2384, 'grad_norm': 22.5, 'learning_rate': 3.405102463575847e-05, 'epoch': 3.39}
 53%|█████▎    | 2666/5000 [12:50:13<11:15:03, 17.35s/it] 53%|█████▎    | 2667/5000 [12:50:30<11:01:17, 17.01s/it]                                                         {'loss': 18.4038, 'grad_norm': 14.125, 'learning_rate': 3.402788477791518e-05, 'epoch': 3.39}
 53%|█████▎    | 2667/5000 [12:50:30<11:01:17, 17.01s/it] 53%|█████▎    | 2668/5000 [12:50:43<10:21:03, 15.98s/it]                                                         {'loss': 18.5572, 'grad_norm': 21.25, 'learning_rate': 3.400474534530811e-05, 'epoch': 3.39}
 53%|█████▎    | 2668/5000 [12:50:43<10:21:03, 15.98s/it] 53%|█████▎    | 2669/5000 [12:50:56<9:40:34, 14.94s/it]                                                         {'loss': 18.9687, 'grad_norm': 14.5625, 'learning_rate': 3.398160634805924e-05, 'epoch': 3.39}
 53%|█████▎    | 2669/5000 [12:50:56<9:40:34, 14.94s/it] 53%|█████▎    | 2670/5000 [12:51:11<9:44:18, 15.05s/it]                                                        {'loss': 19.4442, 'grad_norm': 41.75, 'learning_rate': 3.395846779629036e-05, 'epoch': 3.39}
 53%|█████▎    | 2670/5000 [12:51:11<9:44:18, 15.05s/it] 53%|█████▎    | 2671/5000 [12:51:28<10:07:02, 15.64s/it]                                                         {'loss': 18.9623, 'grad_norm': 63.0, 'learning_rate': 3.393532970012306e-05, 'epoch': 3.39}
 53%|█████▎    | 2671/5000 [12:51:28<10:07:02, 15.64s/it] 53%|█████▎    | 2672/5000 [12:51:52<11:47:27, 18.23s/it]                                                         {'loss': 18.3767, 'grad_norm': 13.125, 'learning_rate': 3.3912192069678694e-05, 'epoch': 3.39}
 53%|█████▎    | 2672/5000 [12:51:52<11:47:27, 18.23s/it] 53%|█████▎    | 2673/5000 [12:52:08<11:23:15, 17.62s/it]                                                         {'loss': 17.8885, 'grad_norm': 14.625, 'learning_rate': 3.388905491507849e-05, 'epoch': 3.39}
 53%|█████▎    | 2673/5000 [12:52:08<11:23:15, 17.62s/it] 53%|█████▎    | 2674/5000 [12:52:30<12:09:25, 18.82s/it]                                                         {'loss': 19.3312, 'grad_norm': 12.5, 'learning_rate': 3.3865918246443416e-05, 'epoch': 3.4}
 53%|█████▎    | 2674/5000 [12:52:30<12:09:25, 18.82s/it] 54%|█████▎    | 2675/5000 [12:52:49<12:05:14, 18.72s/it]                                                         {'loss': 19.3008, 'grad_norm': 16.875, 'learning_rate': 3.384278207389421e-05, 'epoch': 3.4}
 54%|█████▎    | 2675/5000 [12:52:49<12:05:14, 18.72s/it] 54%|█████▎    | 2676/5000 [12:53:02<11:09:12, 17.28s/it]                                                         {'loss': 18.32, 'grad_norm': 13.125, 'learning_rate': 3.381964640755146e-05, 'epoch': 3.4}
 54%|█████▎    | 2676/5000 [12:53:02<11:09:12, 17.28s/it] 54%|█████▎    | 2677/5000 [12:53:15<10:14:19, 15.87s/it]                                                         {'loss': 20.4588, 'grad_norm': 21.5, 'learning_rate': 3.3796511257535466e-05, 'epoch': 3.4}
 54%|█████▎    | 2677/5000 [12:53:15<10:14:19, 15.87s/it] 54%|█████▎    | 2678/5000 [12:53:33<10:38:12, 16.49s/it]                                                         {'loss': 18.9558, 'grad_norm': 27.375, 'learning_rate': 3.377337663396632e-05, 'epoch': 3.4}
 54%|█████▎    | 2678/5000 [12:53:33<10:38:12, 16.49s/it] 54%|█████▎    | 2679/5000 [12:53:49<10:27:31, 16.22s/it]                                                         {'loss': 17.5148, 'grad_norm': 26.875, 'learning_rate': 3.375024254696393e-05, 'epoch': 3.4}
 54%|█████▎    | 2679/5000 [12:53:49<10:27:31, 16.22s/it] 54%|█████▎    | 2680/5000 [12:54:04<10:18:30, 16.00s/it]                                                         {'loss': 18.3143, 'grad_norm': 30.5, 'learning_rate': 3.3727109006647896e-05, 'epoch': 3.4}
 54%|█████▎    | 2680/5000 [12:54:04<10:18:30, 16.00s/it] 54%|█████▎    | 2681/5000 [12:54:19<10:01:46, 15.57s/it]                                                         {'loss': 19.9707, 'grad_norm': 22.625, 'learning_rate': 3.3703976023137636e-05, 'epoch': 3.4}
 54%|█████▎    | 2681/5000 [12:54:19<10:01:46, 15.57s/it] 54%|█████▎    | 2682/5000 [12:54:35<10:13:54, 15.89s/it]                                                         {'loss': 17.5974, 'grad_norm': 404.0, 'learning_rate': 3.3680843606552286e-05, 'epoch': 3.41}
 54%|█████▎    | 2682/5000 [12:54:35<10:13:54, 15.89s/it] 54%|█████▎    | 2683/5000 [12:54:49<9:43:47, 15.12s/it]                                                         {'loss': 19.2026, 'grad_norm': 24.25, 'learning_rate': 3.365771176701077e-05, 'epoch': 3.41}
 54%|█████▎    | 2683/5000 [12:54:49<9:43:47, 15.12s/it] 54%|█████▎    | 2684/5000 [12:55:05<9:59:10, 15.52s/it]                                                        {'loss': 18.7677, 'grad_norm': 35.5, 'learning_rate': 3.3634580514631716e-05, 'epoch': 3.41}
 54%|█████▎    | 2684/5000 [12:55:05<9:59:10, 15.52s/it] 54%|█████▎    | 2685/5000 [12:55:21<10:02:51, 15.62s/it]                                                         {'loss': 18.3873, 'grad_norm': 30.0, 'learning_rate': 3.361144985953355e-05, 'epoch': 3.41}
 54%|█████▎    | 2685/5000 [12:55:21<10:02:51, 15.62s/it] 54%|█████▎    | 2686/5000 [12:55:40<10:37:54, 16.54s/it]                                                         {'loss': 17.9425, 'grad_norm': 12.6875, 'learning_rate': 3.358831981183439e-05, 'epoch': 3.41}
 54%|█████▎    | 2686/5000 [12:55:40<10:37:54, 16.54s/it] 54%|█████▎    | 2687/5000 [12:56:04<12:11:43, 18.98s/it]                                                         {'loss': 18.7273, 'grad_norm': 21.125, 'learning_rate': 3.35651903816521e-05, 'epoch': 3.41}
 54%|█████▎    | 2687/5000 [12:56:04<12:11:43, 18.98s/it] 54%|█████▍    | 2688/5000 [12:56:23<12:06:08, 18.84s/it]                                                         {'loss': 17.6633, 'grad_norm': 11.5, 'learning_rate': 3.3542061579104275e-05, 'epoch': 3.41}
 54%|█████▍    | 2688/5000 [12:56:23<12:06:08, 18.84s/it] 54%|█████▍    | 2689/5000 [12:56:39<11:33:10, 18.00s/it]                                                         {'loss': 17.0964, 'grad_norm': 16.875, 'learning_rate': 3.351893341430826e-05, 'epoch': 3.41}
 54%|█████▍    | 2689/5000 [12:56:39<11:33:10, 18.00s/it] 54%|█████▍    | 2690/5000 [12:56:53<10:45:18, 16.76s/it]                                                         {'loss': 19.2337, 'grad_norm': 22.5, 'learning_rate': 3.349580589738108e-05, 'epoch': 3.42}
 54%|█████▍    | 2690/5000 [12:56:53<10:45:18, 16.76s/it] 54%|█████▍    | 2691/5000 [12:57:17<12:07:44, 18.91s/it]                                                         {'loss': 18.2126, 'grad_norm': 15.75, 'learning_rate': 3.3472679038439505e-05, 'epoch': 3.42}
 54%|█████▍    | 2691/5000 [12:57:17<12:07:44, 18.91s/it] 54%|█████▍    | 2692/5000 [12:57:43<13:36:24, 21.22s/it]                                                         {'loss': 18.3972, 'grad_norm': 15.375, 'learning_rate': 3.3449552847600006e-05, 'epoch': 3.42}
 54%|█████▍    | 2692/5000 [12:57:43<13:36:24, 21.22s/it] 54%|█████▍    | 2693/5000 [12:57:59<12:30:32, 19.52s/it]                                                         {'loss': 17.7577, 'grad_norm': 15.875, 'learning_rate': 3.342642733497877e-05, 'epoch': 3.42}
 54%|█████▍    | 2693/5000 [12:57:59<12:30:32, 19.52s/it] 54%|█████▍    | 2694/5000 [12:58:14<11:45:41, 18.36s/it]                                                         {'loss': 17.3977, 'grad_norm': 22.25, 'learning_rate': 3.340330251069167e-05, 'epoch': 3.42}
 54%|█████▍    | 2694/5000 [12:58:14<11:45:41, 18.36s/it] 54%|█████▍    | 2695/5000 [12:58:32<11:31:02, 17.99s/it]                                                         {'loss': 18.6409, 'grad_norm': 13.3125, 'learning_rate': 3.3380178384854315e-05, 'epoch': 3.42}
 54%|█████▍    | 2695/5000 [12:58:32<11:31:02, 17.99s/it] 54%|█████▍    | 2696/5000 [12:58:48<11:09:29, 17.43s/it]                                                         {'loss': 18.1175, 'grad_norm': 14.375, 'learning_rate': 3.3357054967581965e-05, 'epoch': 3.42}
 54%|█████▍    | 2696/5000 [12:58:48<11:09:29, 17.43s/it] 54%|█████▍    | 2697/5000 [12:59:06<11:23:26, 17.81s/it]                                                         {'loss': 18.5707, 'grad_norm': 19.5, 'learning_rate': 3.3333932268989567e-05, 'epoch': 3.42}
 54%|█████▍    | 2697/5000 [12:59:06<11:23:26, 17.81s/it] 54%|█████▍    | 2698/5000 [12:59:27<11:57:55, 18.71s/it]                                                         {'loss': 18.9667, 'grad_norm': 19.125, 'learning_rate': 3.3310810299191824e-05, 'epoch': 3.43}
 54%|█████▍    | 2698/5000 [12:59:27<11:57:55, 18.71s/it] 54%|█████▍    | 2699/5000 [12:59:42<11:18:42, 17.70s/it]                                                         {'loss': 18.8817, 'grad_norm': 25.75, 'learning_rate': 3.3287689068303036e-05, 'epoch': 3.43}
 54%|█████▍    | 2699/5000 [12:59:42<11:18:42, 17.70s/it] 54%|█████▍    | 2700/5000 [13:00:10<13:09:50, 20.60s/it]                                                         {'loss': 16.7526, 'grad_norm': 12.8125, 'learning_rate': 3.3264568586437216e-05, 'epoch': 3.43}
 54%|█████▍    | 2700/5000 [13:00:10<13:09:50, 20.60s/it] 54%|█████▍    | 2701/5000 [13:00:27<12:32:41, 19.64s/it]                                                         {'loss': 18.7063, 'grad_norm': 14.375, 'learning_rate': 3.3241448863708066e-05, 'epoch': 3.43}
 54%|█████▍    | 2701/5000 [13:00:27<12:32:41, 19.64s/it] 54%|█████▍    | 2702/5000 [13:00:42<11:33:39, 18.11s/it]                                                         {'loss': 18.5998, 'grad_norm': 20.125, 'learning_rate': 3.321832991022893e-05, 'epoch': 3.43}
 54%|█████▍    | 2702/5000 [13:00:42<11:33:39, 18.11s/it] 54%|█████▍    | 2703/5000 [13:00:57<11:02:02, 17.29s/it]                                                         {'loss': 17.3387, 'grad_norm': 11.0625, 'learning_rate': 3.3195211736112804e-05, 'epoch': 3.43}
 54%|█████▍    | 2703/5000 [13:00:57<11:02:02, 17.29s/it] 54%|█████▍    | 2704/5000 [13:01:12<10:38:33, 16.69s/it]                                                         {'loss': 18.6319, 'grad_norm': 16.75, 'learning_rate': 3.3172094351472395e-05, 'epoch': 3.43}
 54%|█████▍    | 2704/5000 [13:01:12<10:38:33, 16.69s/it] 54%|█████▍    | 2705/5000 [13:01:37<12:08:49, 19.05s/it]                                                         {'loss': 16.608, 'grad_norm': 11.3125, 'learning_rate': 3.314897776642002e-05, 'epoch': 3.43}
 54%|█████▍    | 2705/5000 [13:01:37<12:08:49, 19.05s/it] 54%|█████▍    | 2706/5000 [13:01:52<11:15:47, 17.68s/it]                                                         {'loss': 19.5385, 'grad_norm': 25.5, 'learning_rate': 3.3125861991067655e-05, 'epoch': 3.44}
 54%|█████▍    | 2706/5000 [13:01:52<11:15:47, 17.68s/it] 54%|█████▍    | 2707/5000 [13:02:13<12:02:18, 18.90s/it]                                                         {'loss': 18.8564, 'grad_norm': 27.0, 'learning_rate': 3.310274703552692e-05, 'epoch': 3.44}
 54%|█████▍    | 2707/5000 [13:02:13<12:02:18, 18.90s/it] 54%|█████▍    | 2708/5000 [13:02:29<11:22:00, 17.85s/it]                                                         {'loss': 17.9536, 'grad_norm': 12.0625, 'learning_rate': 3.30796329099091e-05, 'epoch': 3.44}
 54%|█████▍    | 2708/5000 [13:02:29<11:22:00, 17.85s/it] 54%|█████▍    | 2709/5000 [13:02:51<12:11:23, 19.15s/it]                                                         {'loss': 18.907, 'grad_norm': 11.75, 'learning_rate': 3.3056519624325085e-05, 'epoch': 3.44}
 54%|█████▍    | 2709/5000 [13:02:51<12:11:23, 19.15s/it] 54%|█████▍    | 2710/5000 [13:03:10<12:14:03, 19.23s/it]                                                         {'loss': 17.8361, 'grad_norm': 14.5625, 'learning_rate': 3.303340718888541e-05, 'epoch': 3.44}
 54%|█████▍    | 2710/5000 [13:03:10<12:14:03, 19.23s/it] 54%|█████▍    | 2711/5000 [13:03:36<13:22:47, 21.04s/it]                                                         {'loss': 17.8092, 'grad_norm': 13.8125, 'learning_rate': 3.3010295613700244e-05, 'epoch': 3.44}
 54%|█████▍    | 2711/5000 [13:03:36<13:22:47, 21.04s/it] 54%|█████▍    | 2712/5000 [13:04:05<14:54:44, 23.46s/it]                                                         {'loss': 16.5345, 'grad_norm': 18.5, 'learning_rate': 3.298718490887938e-05, 'epoch': 3.44}
 54%|█████▍    | 2712/5000 [13:04:05<14:54:44, 23.46s/it] 54%|█████▍    | 2713/5000 [13:04:17<12:51:37, 20.24s/it]                                                         {'loss': 20.8098, 'grad_norm': 27.375, 'learning_rate': 3.296407508453221e-05, 'epoch': 3.45}
 54%|█████▍    | 2713/5000 [13:04:17<12:51:37, 20.24s/it] 54%|█████▍    | 2714/5000 [13:04:31<11:35:42, 18.26s/it]                                                         {'loss': 17.9743, 'grad_norm': 11.5625, 'learning_rate': 3.294096615076778e-05, 'epoch': 3.45}
 54%|█████▍    | 2714/5000 [13:04:31<11:35:42, 18.26s/it] 54%|█████▍    | 2715/5000 [13:04:46<10:58:29, 17.29s/it]                                                         {'loss': 18.89, 'grad_norm': 18.0, 'learning_rate': 3.29178581176947e-05, 'epoch': 3.45}
 54%|█████▍    | 2715/5000 [13:04:46<10:58:29, 17.29s/it] 54%|█████▍    | 2716/5000 [13:05:02<10:44:37, 16.93s/it]                                                         {'loss': 17.3972, 'grad_norm': 12.1875, 'learning_rate': 3.289475099542122e-05, 'epoch': 3.45}
 54%|█████▍    | 2716/5000 [13:05:02<10:44:37, 16.93s/it] 54%|█████▍    | 2717/5000 [13:05:19<10:44:34, 16.94s/it]                                                         {'loss': 17.9612, 'grad_norm': 12.3125, 'learning_rate': 3.2871644794055176e-05, 'epoch': 3.45}
 54%|█████▍    | 2717/5000 [13:05:19<10:44:34, 16.94s/it] 54%|█████▍    | 2718/5000 [13:05:34<10:20:44, 16.32s/it]                                                         {'loss': 19.21, 'grad_norm': 17.0, 'learning_rate': 3.284853952370402e-05, 'epoch': 3.45}
 54%|█████▍    | 2718/5000 [13:05:34<10:20:44, 16.32s/it] 54%|█████▍    | 2719/5000 [13:05:51<10:24:07, 16.42s/it]                                                         {'loss': 18.3517, 'grad_norm': 18.25, 'learning_rate': 3.2825435194474755e-05, 'epoch': 3.45}
 54%|█████▍    | 2719/5000 [13:05:51<10:24:07, 16.42s/it] 54%|█████▍    | 2720/5000 [13:06:17<12:19:26, 19.46s/it]                                                         {'loss': 17.1719, 'grad_norm': 12.0625, 'learning_rate': 3.280233181647403e-05, 'epoch': 3.45}
 54%|█████▍    | 2720/5000 [13:06:17<12:19:26, 19.46s/it] 54%|█████▍    | 2721/5000 [13:06:33<11:32:45, 18.24s/it]                                                         {'loss': 17.8075, 'grad_norm': 13.375, 'learning_rate': 3.277922939980803e-05, 'epoch': 3.46}
 54%|█████▍    | 2721/5000 [13:06:33<11:32:45, 18.24s/it] 54%|█████▍    | 2722/5000 [13:06:47<10:53:58, 17.23s/it]                                                         {'loss': 20.4758, 'grad_norm': 16.125, 'learning_rate': 3.275612795458254e-05, 'epoch': 3.46}
 54%|█████▍    | 2722/5000 [13:06:47<10:53:58, 17.23s/it] 54%|█████▍    | 2723/5000 [13:07:03<10:39:45, 16.86s/it]                                                         {'loss': 18.1369, 'grad_norm': 24.75, 'learning_rate': 3.27330274909029e-05, 'epoch': 3.46}
 54%|█████▍    | 2723/5000 [13:07:03<10:39:45, 16.86s/it] 54%|█████▍    | 2724/5000 [13:07:23<11:06:14, 17.56s/it]                                                         {'loss': 17.6948, 'grad_norm': 21.625, 'learning_rate': 3.270992801887407e-05, 'epoch': 3.46}
 54%|█████▍    | 2724/5000 [13:07:23<11:06:14, 17.56s/it] 55%|█████▍    | 2725/5000 [13:07:38<10:41:22, 16.92s/it]                                                         {'loss': 18.7269, 'grad_norm': 18.75, 'learning_rate': 3.268682954860052e-05, 'epoch': 3.46}
 55%|█████▍    | 2725/5000 [13:07:38<10:41:22, 16.92s/it] 55%|█████▍    | 2726/5000 [13:07:52<10:07:17, 16.02s/it]                                                         {'loss': 18.8765, 'grad_norm': 21.625, 'learning_rate': 3.266373209018631e-05, 'epoch': 3.46}
 55%|█████▍    | 2726/5000 [13:07:52<10:07:17, 16.02s/it] 55%|█████▍    | 2727/5000 [13:08:06<9:42:13, 15.37s/it]                                                         {'loss': 17.8639, 'grad_norm': 14.5625, 'learning_rate': 3.2640635653735065e-05, 'epoch': 3.46}
 55%|█████▍    | 2727/5000 [13:08:06<9:42:13, 15.37s/it] 55%|█████▍    | 2728/5000 [13:08:20<9:25:50, 14.94s/it]                                                        {'loss': 18.6213, 'grad_norm': 12.25, 'learning_rate': 3.261754024934994e-05, 'epoch': 3.46}
 55%|█████▍    | 2728/5000 [13:08:20<9:25:50, 14.94s/it] 55%|█████▍    | 2729/5000 [13:08:42<10:47:39, 17.11s/it]                                                         {'loss': 18.4925, 'grad_norm': 15.0, 'learning_rate': 3.2594445887133646e-05, 'epoch': 3.47}
 55%|█████▍    | 2729/5000 [13:08:42<10:47:39, 17.11s/it] 55%|█████▍    | 2730/5000 [13:08:58<10:38:59, 16.89s/it]                                                         {'loss': 16.8838, 'grad_norm': 11.75, 'learning_rate': 3.257135257718846e-05, 'epoch': 3.47}
 55%|█████▍    | 2730/5000 [13:08:58<10:38:59, 16.89s/it] 55%|█████▍    | 2731/5000 [13:09:13<10:08:21, 16.09s/it]                                                         {'loss': 18.145, 'grad_norm': 10.6875, 'learning_rate': 3.254826032961617e-05, 'epoch': 3.47}
 55%|█████▍    | 2731/5000 [13:09:13<10:08:21, 16.09s/it] 55%|█████▍    | 2732/5000 [13:09:34<11:10:23, 17.74s/it]                                                         {'loss': 18.6323, 'grad_norm': 140.0, 'learning_rate': 3.252516915451808e-05, 'epoch': 3.47}
 55%|█████▍    | 2732/5000 [13:09:34<11:10:23, 17.74s/it] 55%|█████▍    | 2733/5000 [13:09:49<10:38:21, 16.90s/it]                                                         {'loss': 18.5875, 'grad_norm': 14.375, 'learning_rate': 3.250207906199511e-05, 'epoch': 3.47}
 55%|█████▍    | 2733/5000 [13:09:49<10:38:21, 16.90s/it] 55%|█████▍    | 2734/5000 [13:10:02<9:51:59, 15.68s/it]                                                         {'loss': 18.2229, 'grad_norm': 10.8125, 'learning_rate': 3.247899006214762e-05, 'epoch': 3.47}
 55%|█████▍    | 2734/5000 [13:10:02<9:51:59, 15.68s/it] 55%|█████▍    | 2735/5000 [13:10:16<9:39:10, 15.34s/it]                                                        {'loss': 17.8059, 'grad_norm': 16.125, 'learning_rate': 3.245590216507551e-05, 'epoch': 3.47}
 55%|█████▍    | 2735/5000 [13:10:16<9:39:10, 15.34s/it] 55%|█████▍    | 2736/5000 [13:10:29<9:06:06, 14.47s/it]                                                        {'loss': 18.0391, 'grad_norm': 19.875, 'learning_rate': 3.243281538087824e-05, 'epoch': 3.47}
 55%|█████▍    | 2736/5000 [13:10:29<9:06:06, 14.47s/it] 55%|█████▍    | 2737/5000 [13:10:46<9:32:43, 15.18s/it]                                                        {'loss': 17.539, 'grad_norm': 14.625, 'learning_rate': 3.240972971965474e-05, 'epoch': 3.48}
 55%|█████▍    | 2737/5000 [13:10:46<9:32:43, 15.18s/it] 55%|█████▍    | 2738/5000 [13:11:03<9:58:26, 15.87s/it]                                                        {'loss': 16.9689, 'grad_norm': 19.625, 'learning_rate': 3.238664519150346e-05, 'epoch': 3.48}
 55%|█████▍    | 2738/5000 [13:11:03<9:58:26, 15.87s/it] 55%|█████▍    | 2739/5000 [13:11:16<9:26:08, 15.02s/it]                                                        {'loss': 19.1434, 'grad_norm': 20.875, 'learning_rate': 3.236356180652236e-05, 'epoch': 3.48}
 55%|█████▍    | 2739/5000 [13:11:16<9:26:08, 15.02s/it] 55%|█████▍    | 2740/5000 [13:11:34<9:55:00, 15.80s/it]                                                        {'loss': 20.8317, 'grad_norm': 22.25, 'learning_rate': 3.2340479574808886e-05, 'epoch': 3.48}
 55%|█████▍    | 2740/5000 [13:11:34<9:55:00, 15.80s/it] 55%|█████▍    | 2741/5000 [13:12:00<11:49:34, 18.85s/it]                                                         {'loss': 16.7982, 'grad_norm': 10.75, 'learning_rate': 3.231739850646002e-05, 'epoch': 3.48}
 55%|█████▍    | 2741/5000 [13:12:00<11:49:34, 18.85s/it] 55%|█████▍    | 2742/5000 [13:12:19<11:47:56, 18.81s/it]                                                         {'loss': 16.8824, 'grad_norm': 10.1875, 'learning_rate': 3.229431861157216e-05, 'epoch': 3.48}
 55%|█████▍    | 2742/5000 [13:12:19<11:47:56, 18.81s/it] 55%|█████▍    | 2743/5000 [13:12:39<12:05:34, 19.29s/it]                                                         {'loss': 18.6718, 'grad_norm': 21.5, 'learning_rate': 3.227123990024129e-05, 'epoch': 3.48}
 55%|█████▍    | 2743/5000 [13:12:39<12:05:34, 19.29s/it] 55%|█████▍    | 2744/5000 [13:12:53<11:07:03, 17.74s/it]                                                         {'loss': 18.0247, 'grad_norm': 24.375, 'learning_rate': 3.2248162382562786e-05, 'epoch': 3.48}
 55%|█████▍    | 2744/5000 [13:12:53<11:07:03, 17.74s/it] 55%|█████▍    | 2745/5000 [13:13:07<10:21:34, 16.54s/it]                                                         {'loss': 17.5968, 'grad_norm': 18.25, 'learning_rate': 3.2225086068631535e-05, 'epoch': 3.49}
 55%|█████▍    | 2745/5000 [13:13:07<10:21:34, 16.54s/it] 55%|█████▍    | 2746/5000 [13:13:23<10:12:52, 16.31s/it]                                                         {'loss': 17.4346, 'grad_norm': 13.375, 'learning_rate': 3.220201096854193e-05, 'epoch': 3.49}
 55%|█████▍    | 2746/5000 [13:13:23<10:12:52, 16.31s/it] 55%|█████▍    | 2747/5000 [13:13:40<10:25:19, 16.65s/it]                                                         {'loss': 16.7569, 'grad_norm': 9.875, 'learning_rate': 3.217893709238779e-05, 'epoch': 3.49}
 55%|█████▍    | 2747/5000 [13:13:40<10:25:19, 16.65s/it] 55%|█████▍    | 2748/5000 [13:14:07<12:17:41, 19.65s/it]                                                         {'loss': 18.0052, 'grad_norm': 11.3125, 'learning_rate': 3.215586445026239e-05, 'epoch': 3.49}
 55%|█████▍    | 2748/5000 [13:14:07<12:17:41, 19.65s/it] 55%|█████▍    | 2749/5000 [13:14:22<11:33:24, 18.48s/it]                                                         {'loss': 18.2005, 'grad_norm': 10.4375, 'learning_rate': 3.213279305225852e-05, 'epoch': 3.49}
 55%|█████▍    | 2749/5000 [13:14:22<11:33:24, 18.48s/it] 55%|█████▌    | 2750/5000 [13:14:43<11:57:26, 19.13s/it]                                                         {'loss': 18.6106, 'grad_norm': 14.5, 'learning_rate': 3.210972290846837e-05, 'epoch': 3.49}
 55%|█████▌    | 2750/5000 [13:14:43<11:57:26, 19.13s/it] 55%|█████▌    | 2751/5000 [13:14:57<11:00:53, 17.63s/it]                                                         {'loss': 18.0572, 'grad_norm': 12.4375, 'learning_rate': 3.208665402898362e-05, 'epoch': 3.49}
 55%|█████▌    | 2751/5000 [13:14:57<11:00:53, 17.63s/it] 55%|█████▌    | 2752/5000 [13:15:12<10:28:57, 16.79s/it]                                                         {'loss': 27.6264, 'grad_norm': 149.0, 'learning_rate': 3.206358642389537e-05, 'epoch': 3.49}
 55%|█████▌    | 2752/5000 [13:15:12<10:28:57, 16.79s/it] 55%|█████▌    | 2753/5000 [13:15:27<10:07:33, 16.22s/it]                                                         {'loss': 18.2688, 'grad_norm': 15.625, 'learning_rate': 3.2040520103294185e-05, 'epoch': 3.5}
 55%|█████▌    | 2753/5000 [13:15:27<10:07:33, 16.22s/it] 55%|█████▌    | 2754/5000 [13:15:42<9:51:13, 15.79s/it]                                                         {'loss': 18.1041, 'grad_norm': 22.375, 'learning_rate': 3.2017455077270045e-05, 'epoch': 3.5}
 55%|█████▌    | 2754/5000 [13:15:42<9:51:13, 15.79s/it] 55%|█████▌    | 2755/5000 [13:15:54<9:14:32, 14.82s/it]                                                        {'loss': 20.1877, 'grad_norm': 18.0, 'learning_rate': 3.19943913559124e-05, 'epoch': 3.5}
 55%|█████▌    | 2755/5000 [13:15:54<9:14:32, 14.82s/it] 55%|█████▌    | 2756/5000 [13:16:19<11:07:50, 17.86s/it]                                                         {'loss': 18.8216, 'grad_norm': 29.0, 'learning_rate': 3.197132894931008e-05, 'epoch': 3.5}
 55%|█████▌    | 2756/5000 [13:16:19<11:07:50, 17.86s/it] 55%|█████▌    | 2757/5000 [13:16:55<14:28:51, 23.24s/it]                                                         {'loss': 17.9533, 'grad_norm': 12.875, 'learning_rate': 3.194826786755139e-05, 'epoch': 3.5}
 55%|█████▌    | 2757/5000 [13:16:55<14:28:51, 23.24s/it] 55%|█████▌    | 2758/5000 [13:17:12<13:18:50, 21.38s/it]                                                         {'loss': 17.9573, 'grad_norm': 13.875, 'learning_rate': 3.1925208120723987e-05, 'epoch': 3.5}
 55%|█████▌    | 2758/5000 [13:17:12<13:18:50, 21.38s/it] 55%|█████▌    | 2759/5000 [13:17:27<12:10:55, 19.57s/it]                                                         {'loss': 18.6258, 'grad_norm': 33.75, 'learning_rate': 3.190214971891503e-05, 'epoch': 3.5}
 55%|█████▌    | 2759/5000 [13:17:27<12:10:55, 19.57s/it] 55%|█████▌    | 2760/5000 [13:17:42<11:15:16, 18.09s/it]                                                         {'loss': 19.2517, 'grad_norm': 32.5, 'learning_rate': 3.1879092672211035e-05, 'epoch': 3.5}
 55%|█████▌    | 2760/5000 [13:17:42<11:15:16, 18.09s/it] 55%|█████▌    | 2761/5000 [13:17:58<10:49:56, 17.42s/it]                                                         {'loss': 17.9115, 'grad_norm': 19.375, 'learning_rate': 3.185603699069792e-05, 'epoch': 3.51}
 55%|█████▌    | 2761/5000 [13:17:58<10:49:56, 17.42s/it] 55%|█████▌    | 2762/5000 [13:18:22<12:08:31, 19.53s/it]                                                         {'loss': 19.5329, 'grad_norm': 16.375, 'learning_rate': 3.1832982684461056e-05, 'epoch': 3.51}
 55%|█████▌    | 2762/5000 [13:18:22<12:08:31, 19.53s/it] 55%|█████▌    | 2763/5000 [13:18:44<12:33:54, 20.22s/it]                                                         {'loss': 20.0455, 'grad_norm': 34.25, 'learning_rate': 3.180992976358515e-05, 'epoch': 3.51}
 55%|█████▌    | 2763/5000 [13:18:44<12:33:54, 20.22s/it] 55%|█████▌    | 2764/5000 [13:19:05<12:35:06, 20.26s/it]                                                         {'loss': 19.3765, 'grad_norm': 36.5, 'learning_rate': 3.178687823815436e-05, 'epoch': 3.51}
 55%|█████▌    | 2764/5000 [13:19:05<12:35:06, 20.26s/it] 55%|█████▌    | 2765/5000 [13:19:22<12:00:07, 19.33s/it]                                                         {'loss': 17.1776, 'grad_norm': 17.125, 'learning_rate': 3.1763828118252175e-05, 'epoch': 3.51}
 55%|█████▌    | 2765/5000 [13:19:22<12:00:07, 19.33s/it] 55%|█████▌    | 2766/5000 [13:19:35<10:51:01, 17.48s/it]                                                         {'loss': 19.4808, 'grad_norm': 22.0, 'learning_rate': 3.174077941396153e-05, 'epoch': 3.51}
 55%|█████▌    | 2766/5000 [13:19:35<10:51:01, 17.48s/it] 55%|█████▌    | 2767/5000 [13:19:51<10:39:52, 17.19s/it]                                                         {'loss': 19.2053, 'grad_norm': 31.875, 'learning_rate': 3.1717732135364685e-05, 'epoch': 3.51}
 55%|█████▌    | 2767/5000 [13:19:51<10:39:52, 17.19s/it] 55%|█████▌    | 2768/5000 [13:20:08<10:31:42, 16.98s/it]                                                         {'loss': 18.4045, 'grad_norm': 21.625, 'learning_rate': 3.169468629254333e-05, 'epoch': 3.51}
 55%|█████▌    | 2768/5000 [13:20:08<10:31:42, 16.98s/it] 55%|█████▌    | 2769/5000 [13:20:26<10:42:02, 17.27s/it]                                                         {'loss': 18.2447, 'grad_norm': 16.375, 'learning_rate': 3.167164189557848e-05, 'epoch': 3.52}
 55%|█████▌    | 2769/5000 [13:20:26<10:42:02, 17.27s/it] 55%|█████▌    | 2770/5000 [13:20:48<11:33:46, 18.67s/it]                                                         {'loss': 19.0236, 'grad_norm': 18.0, 'learning_rate': 3.164859895455054e-05, 'epoch': 3.52}
 55%|█████▌    | 2770/5000 [13:20:48<11:33:46, 18.67s/it] 55%|█████▌    | 2771/5000 [13:21:03<10:55:16, 17.64s/it]                                                         {'loss': 17.0479, 'grad_norm': 10.9375, 'learning_rate': 3.162555747953929e-05, 'epoch': 3.52}
 55%|█████▌    | 2771/5000 [13:21:03<10:55:16, 17.64s/it] 55%|█████▌    | 2772/5000 [13:21:17<10:12:32, 16.50s/it]                                                         {'loss': 17.8941, 'grad_norm': 14.0625, 'learning_rate': 3.1602517480623836e-05, 'epoch': 3.52}
 55%|█████▌    | 2772/5000 [13:21:17<10:12:32, 16.50s/it] 55%|█████▌    | 2773/5000 [13:21:34<10:18:33, 16.67s/it]                                                         {'loss': 17.7035, 'grad_norm': 10.8125, 'learning_rate': 3.157947896788266e-05, 'epoch': 3.52}
 55%|█████▌    | 2773/5000 [13:21:34<10:18:33, 16.67s/it] 55%|█████▌    | 2774/5000 [13:21:50<10:14:21, 16.56s/it]                                                         {'loss': 17.4058, 'grad_norm': 11.3125, 'learning_rate': 3.1556441951393594e-05, 'epoch': 3.52}
 55%|█████▌    | 2774/5000 [13:21:50<10:14:21, 16.56s/it] 56%|█████▌    | 2775/5000 [13:22:12<11:17:07, 18.26s/it]                                                         {'loss': 17.5755, 'grad_norm': 16.75, 'learning_rate': 3.15334064412338e-05, 'epoch': 3.52}
 56%|█████▌    | 2775/5000 [13:22:12<11:17:07, 18.26s/it] 56%|█████▌    | 2776/5000 [13:22:26<10:21:49, 16.78s/it]                                                         {'loss': 18.1532, 'grad_norm': 10.8125, 'learning_rate': 3.151037244747982e-05, 'epoch': 3.53}
 56%|█████▌    | 2776/5000 [13:22:26<10:21:49, 16.78s/it] 56%|█████▌    | 2777/5000 [13:22:49<11:31:50, 18.67s/it]                                                         {'loss': 18.3824, 'grad_norm': 17.5, 'learning_rate': 3.148733998020747e-05, 'epoch': 3.53}
 56%|█████▌    | 2777/5000 [13:22:49<11:31:50, 18.67s/it] 56%|█████▌    | 2778/5000 [13:23:02<10:33:03, 17.09s/it]                                                         {'loss': 18.0947, 'grad_norm': 15.4375, 'learning_rate': 3.146430904949197e-05, 'epoch': 3.53}
 56%|█████▌    | 2778/5000 [13:23:02<10:33:03, 17.09s/it] 56%|█████▌    | 2779/5000 [13:23:18<10:13:09, 16.56s/it]                                                         {'loss': 19.1469, 'grad_norm': 14.0625, 'learning_rate': 3.1441279665407815e-05, 'epoch': 3.53}
 56%|█████▌    | 2779/5000 [13:23:18<10:13:09, 16.56s/it] 56%|█████▌    | 2780/5000 [13:23:31<9:39:23, 15.66s/it]                                                         {'loss': 18.9742, 'grad_norm': 40.5, 'learning_rate': 3.141825183802882e-05, 'epoch': 3.53}
 56%|█████▌    | 2780/5000 [13:23:31<9:39:23, 15.66s/it] 56%|█████▌    | 2781/5000 [13:23:44<9:11:40, 14.92s/it]                                                        {'loss': 21.0292, 'grad_norm': 28.5, 'learning_rate': 3.139522557742817e-05, 'epoch': 3.53}
 56%|█████▌    | 2781/5000 [13:23:44<9:11:40, 14.92s/it] 56%|█████▌    | 2782/5000 [13:24:01<9:30:26, 15.43s/it]                                                        {'loss': 19.158, 'grad_norm': 19.125, 'learning_rate': 3.137220089367833e-05, 'epoch': 3.53}
 56%|█████▌    | 2782/5000 [13:24:01<9:30:26, 15.43s/it] 56%|█████▌    | 2783/5000 [13:24:15<9:13:15, 14.97s/it]                                                        {'loss': 19.8682, 'grad_norm': 17.75, 'learning_rate': 3.134917779685105e-05, 'epoch': 3.53}
 56%|█████▌    | 2783/5000 [13:24:15<9:13:15, 14.97s/it] 56%|█████▌    | 2784/5000 [13:24:30<9:14:17, 15.01s/it]                                                        {'loss': 17.5955, 'grad_norm': 10.4375, 'learning_rate': 3.1326156297017454e-05, 'epoch': 3.54}
 56%|█████▌    | 2784/5000 [13:24:30<9:14:17, 15.01s/it] 56%|█████▌    | 2785/5000 [13:24:50<10:05:52, 16.41s/it]                                                         {'loss': 33.5531, 'grad_norm': 544.0, 'learning_rate': 3.1303136404247904e-05, 'epoch': 3.54}
 56%|█████▌    | 2785/5000 [13:24:50<10:05:52, 16.41s/it] 56%|█████▌    | 2786/5000 [13:25:07<10:11:49, 16.58s/it]                                                         {'loss': 16.9225, 'grad_norm': 17.75, 'learning_rate': 3.1280118128612075e-05, 'epoch': 3.54}
 56%|█████▌    | 2786/5000 [13:25:07<10:11:49, 16.58s/it] 56%|█████▌    | 2787/5000 [13:25:20<9:37:32, 15.66s/it]                                                         {'loss': 19.0859, 'grad_norm': 15.9375, 'learning_rate': 3.125710148017897e-05, 'epoch': 3.54}
 56%|█████▌    | 2787/5000 [13:25:20<9:37:32, 15.66s/it] 56%|█████▌    | 2788/5000 [13:25:35<9:32:38, 15.53s/it]                                                        {'loss': 17.9514, 'grad_norm': 19.0, 'learning_rate': 3.1234086469016835e-05, 'epoch': 3.54}
 56%|█████▌    | 2788/5000 [13:25:35<9:32:38, 15.53s/it] 56%|█████▌    | 2789/5000 [13:25:50<9:19:19, 15.18s/it]                                                        {'loss': 18.1948, 'grad_norm': 43.0, 'learning_rate': 3.121107310519322e-05, 'epoch': 3.54}
 56%|█████▌    | 2789/5000 [13:25:50<9:19:19, 15.18s/it] 56%|█████▌    | 2790/5000 [13:26:06<9:29:59, 15.48s/it]                                                        {'loss': 17.3909, 'grad_norm': 20.0, 'learning_rate': 3.118806139877495e-05, 'epoch': 3.54}
 56%|█████▌    | 2790/5000 [13:26:06<9:29:59, 15.48s/it] 56%|█████▌    | 2791/5000 [13:26:20<9:11:59, 14.99s/it]                                                        {'loss': 17.3652, 'grad_norm': 12.1875, 'learning_rate': 3.116505135982815e-05, 'epoch': 3.54}
 56%|█████▌    | 2791/5000 [13:26:20<9:11:59, 14.99s/it] 56%|█████▌    | 2792/5000 [13:26:38<9:51:30, 16.07s/it]                                                        {'loss': 17.8444, 'grad_norm': 13.25, 'learning_rate': 3.114204299841817e-05, 'epoch': 3.55}
 56%|█████▌    | 2792/5000 [13:26:38<9:51:30, 16.07s/it] 56%|█████▌    | 2793/5000 [13:26:56<10:04:49, 16.44s/it]                                                         {'loss': 18.1992, 'grad_norm': 14.25, 'learning_rate': 3.111903632460964e-05, 'epoch': 3.55}
 56%|█████▌    | 2793/5000 [13:26:56<10:04:49, 16.44s/it] 56%|█████▌    | 2794/5000 [13:27:11<9:48:52, 16.02s/it]                                                         {'loss': 16.5962, 'grad_norm': 18.375, 'learning_rate': 3.109603134846649e-05, 'epoch': 3.55}
 56%|█████▌    | 2794/5000 [13:27:11<9:48:52, 16.02s/it] 56%|█████▌    | 2795/5000 [13:27:29<10:10:57, 16.62s/it]                                                         {'loss': 16.8628, 'grad_norm': 9.625, 'learning_rate': 3.107302808005185e-05, 'epoch': 3.55}
 56%|█████▌    | 2795/5000 [13:27:29<10:10:57, 16.62s/it] 56%|█████▌    | 2796/5000 [13:27:45<10:01:41, 16.38s/it]                                                         {'loss': 19.3083, 'grad_norm': 17.375, 'learning_rate': 3.105002652942814e-05, 'epoch': 3.55}
 56%|█████▌    | 2796/5000 [13:27:45<10:01:41, 16.38s/it] 56%|█████▌    | 2797/5000 [13:28:00<9:46:27, 15.97s/it]                                                         {'loss': 20.5328, 'grad_norm': 35.25, 'learning_rate': 3.1027026706657034e-05, 'epoch': 3.55}
 56%|█████▌    | 2797/5000 [13:28:00<9:46:27, 15.97s/it] 56%|█████▌    | 2798/5000 [13:28:19<10:26:13, 17.06s/it]                                                         {'loss': 20.012, 'grad_norm': 28.125, 'learning_rate': 3.100402862179942e-05, 'epoch': 3.55}
 56%|█████▌    | 2798/5000 [13:28:19<10:26:13, 17.06s/it] 56%|█████▌    | 2799/5000 [13:28:40<11:03:54, 18.10s/it]                                                         {'loss': 18.0404, 'grad_norm': 15.375, 'learning_rate': 3.098103228491544e-05, 'epoch': 3.55}
 56%|█████▌    | 2799/5000 [13:28:40<11:03:54, 18.10s/it] 56%|█████▌    | 2800/5000 [13:28:58<11:11:25, 18.31s/it]                                                         {'loss': 18.7882, 'grad_norm': 12.6875, 'learning_rate': 3.0958037706064485e-05, 'epoch': 3.56}
 56%|█████▌    | 2800/5000 [13:28:58<11:11:25, 18.31s/it] 56%|█████▌    | 2801/5000 [13:29:23<12:23:50, 20.30s/it]                                                         {'loss': 17.5506, 'grad_norm': 10.5, 'learning_rate': 3.093504489530516e-05, 'epoch': 3.56}
 56%|█████▌    | 2801/5000 [13:29:23<12:23:50, 20.30s/it] 56%|█████▌    | 2802/5000 [13:29:56<14:42:34, 24.09s/it]                                                         {'loss': 16.8764, 'grad_norm': 9.9375, 'learning_rate': 3.0912053862695294e-05, 'epoch': 3.56}
 56%|█████▌    | 2802/5000 [13:29:56<14:42:34, 24.09s/it] 56%|█████▌    | 2803/5000 [13:30:12<13:05:22, 21.45s/it]                                                         {'loss': 19.3605, 'grad_norm': 16.875, 'learning_rate': 3.088906461829197e-05, 'epoch': 3.56}
 56%|█████▌    | 2803/5000 [13:30:12<13:05:22, 21.45s/it] 56%|█████▌    | 2804/5000 [13:30:32<12:53:58, 21.15s/it]                                                         {'loss': 18.2568, 'grad_norm': 16.125, 'learning_rate': 3.086607717215144e-05, 'epoch': 3.56}
 56%|█████▌    | 2804/5000 [13:30:32<12:53:58, 21.15s/it] 56%|█████▌    | 2805/5000 [13:30:47<11:49:30, 19.39s/it]                                                         {'loss': 18.0904, 'grad_norm': 9.6875, 'learning_rate': 3.084309153432919e-05, 'epoch': 3.56}
 56%|█████▌    | 2805/5000 [13:30:47<11:49:30, 19.39s/it] 56%|█████▌    | 2806/5000 [13:31:03<11:10:31, 18.34s/it]                                                         {'loss': 17.8811, 'grad_norm': 10.3125, 'learning_rate': 3.082010771487994e-05, 'epoch': 3.56}
 56%|█████▌    | 2806/5000 [13:31:03<11:10:31, 18.34s/it] 56%|█████▌    | 2807/5000 [13:31:25<11:46:41, 19.33s/it]                                                         {'loss': 17.4219, 'grad_norm': 12.625, 'learning_rate': 3.079712572385759e-05, 'epoch': 3.56}
 56%|█████▌    | 2807/5000 [13:31:25<11:46:41, 19.33s/it] 56%|█████▌    | 2808/5000 [13:31:45<11:59:37, 19.70s/it]                                                         {'loss': 17.099, 'grad_norm': 13.9375, 'learning_rate': 3.0774145571315236e-05, 'epoch': 3.57}
 56%|█████▌    | 2808/5000 [13:31:45<11:59:37, 19.70s/it] 56%|█████▌    | 2809/5000 [13:32:00<11:01:55, 18.13s/it]                                                         {'loss': 30.6166, 'grad_norm': 1944.0, 'learning_rate': 3.075116726730516e-05, 'epoch': 3.57}
 56%|█████▌    | 2809/5000 [13:32:00<11:01:55, 18.13s/it] 56%|█████▌    | 2810/5000 [13:32:15<10:25:43, 17.14s/it]                                                         {'loss': 18.3784, 'grad_norm': 16.625, 'learning_rate': 3.0728190821878895e-05, 'epoch': 3.57}
 56%|█████▌    | 2810/5000 [13:32:15<10:25:43, 17.14s/it] 56%|█████▌    | 2811/5000 [13:32:28<9:43:47, 16.00s/it]                                                         {'loss': 19.0138, 'grad_norm': 23.5, 'learning_rate': 3.070521624508708e-05, 'epoch': 3.57}
 56%|█████▌    | 2811/5000 [13:32:28<9:43:47, 16.00s/it] 56%|█████▌    | 2812/5000 [13:32:52<11:11:22, 18.41s/it]                                                         {'loss': 19.592, 'grad_norm': 13.4375, 'learning_rate': 3.0682243546979596e-05, 'epoch': 3.57}
 56%|█████▌    | 2812/5000 [13:32:52<11:11:22, 18.41s/it] 56%|█████▋    | 2813/5000 [13:33:08<10:46:35, 17.74s/it]                                                         {'loss': 21.3408, 'grad_norm': 20.25, 'learning_rate': 3.0659272737605475e-05, 'epoch': 3.57}
 56%|█████▋    | 2813/5000 [13:33:08<10:46:35, 17.74s/it] 56%|█████▋    | 2814/5000 [13:33:34<12:10:43, 20.06s/it]                                                         {'loss': 17.75, 'grad_norm': 11.625, 'learning_rate': 3.0636303827012936e-05, 'epoch': 3.57}
 56%|█████▋    | 2814/5000 [13:33:34<12:10:43, 20.06s/it] 56%|█████▋    | 2815/5000 [13:33:46<10:48:02, 17.80s/it]                                                         {'loss': 20.1591, 'grad_norm': 17.75, 'learning_rate': 3.0613336825249346e-05, 'epoch': 3.57}
 56%|█████▋    | 2815/5000 [13:33:46<10:48:02, 17.80s/it] 56%|█████▋    | 2816/5000 [13:34:02<10:29:55, 17.31s/it]                                                         {'loss': 19.2861, 'grad_norm': 14.0, 'learning_rate': 3.059037174236127e-05, 'epoch': 3.58}
 56%|█████▋    | 2816/5000 [13:34:02<10:29:55, 17.31s/it] 56%|█████▋    | 2817/5000 [13:34:18<10:15:06, 16.91s/it]                                                         {'loss': 18.4362, 'grad_norm': 26.5, 'learning_rate': 3.0567408588394415e-05, 'epoch': 3.58}
 56%|█████▋    | 2817/5000 [13:34:18<10:15:06, 16.91s/it] 56%|█████▋    | 2818/5000 [13:34:41<11:16:18, 18.60s/it]                                                         {'loss': 17.8479, 'grad_norm': 14.9375, 'learning_rate': 3.054444737339362e-05, 'epoch': 3.58}
 56%|█████▋    | 2818/5000 [13:34:41<11:16:18, 18.60s/it] 56%|█████▋    | 2819/5000 [13:34:55<10:20:58, 17.08s/it]                                                         {'loss': 19.3445, 'grad_norm': 15.0, 'learning_rate': 3.052148810740293e-05, 'epoch': 3.58}
 56%|█████▋    | 2819/5000 [13:34:55<10:20:58, 17.08s/it] 56%|█████▋    | 2820/5000 [13:35:23<12:21:40, 20.41s/it]                                                         {'loss': 18.8309, 'grad_norm': 13.3125, 'learning_rate': 3.049853080046549e-05, 'epoch': 3.58}
 56%|█████▋    | 2820/5000 [13:35:23<12:21:40, 20.41s/it] 56%|█████▋    | 2821/5000 [13:35:41<11:58:36, 19.79s/it]                                                         {'loss': 17.1761, 'grad_norm': 8.6875, 'learning_rate': 3.0475575462623604e-05, 'epoch': 3.58}
 56%|█████▋    | 2821/5000 [13:35:41<11:58:36, 19.79s/it] 56%|█████▋    | 2822/5000 [13:35:57<11:17:10, 18.66s/it]                                                         {'loss': 17.6561, 'grad_norm': 13.625, 'learning_rate': 3.0452622103918736e-05, 'epoch': 3.58}
 56%|█████▋    | 2822/5000 [13:35:57<11:17:10, 18.66s/it] 56%|█████▋    | 2823/5000 [13:36:12<10:37:56, 17.58s/it]                                                         {'loss': 18.417, 'grad_norm': 14.625, 'learning_rate': 3.0429670734391445e-05, 'epoch': 3.58}
 56%|█████▋    | 2823/5000 [13:36:12<10:37:56, 17.58s/it] 56%|█████▋    | 2824/5000 [13:36:28<10:16:24, 17.00s/it]                                                         {'loss': 18.0064, 'grad_norm': 16.0, 'learning_rate': 3.040672136408145e-05, 'epoch': 3.59}
 56%|█████▋    | 2824/5000 [13:36:28<10:16:24, 17.00s/it] 56%|█████▋    | 2825/5000 [13:36:41<9:37:39, 15.94s/it]                                                         {'loss': 18.8884, 'grad_norm': 19.25, 'learning_rate': 3.038377400302758e-05, 'epoch': 3.59}
 56%|█████▋    | 2825/5000 [13:36:41<9:37:39, 15.94s/it] 57%|█████▋    | 2826/5000 [13:36:55<9:18:07, 15.40s/it]                                                        {'loss': 18.2476, 'grad_norm': 16.75, 'learning_rate': 3.0360828661267806e-05, 'epoch': 3.59}
 57%|█████▋    | 2826/5000 [13:36:55<9:18:07, 15.40s/it] 57%|█████▋    | 2827/5000 [13:37:10<9:07:57, 15.13s/it]                                                        {'loss': 18.0392, 'grad_norm': 19.875, 'learning_rate': 3.0337885348839185e-05, 'epoch': 3.59}
 57%|█████▋    | 2827/5000 [13:37:10<9:07:57, 15.13s/it] 57%|█████▋    | 2828/5000 [13:37:24<8:52:03, 14.70s/it]                                                        {'loss': 18.413, 'grad_norm': 21.5, 'learning_rate': 3.031494407577789e-05, 'epoch': 3.59}
 57%|█████▋    | 2828/5000 [13:37:24<8:52:03, 14.70s/it] 57%|█████▋    | 2829/5000 [13:37:37<8:42:12, 14.43s/it]                                                        {'loss': 17.7322, 'grad_norm': 30.5, 'learning_rate': 3.0292004852119236e-05, 'epoch': 3.59}
 57%|█████▋    | 2829/5000 [13:37:37<8:42:12, 14.43s/it] 57%|█████▋    | 2830/5000 [13:37:54<9:04:56, 15.07s/it]                                                        {'loss': 18.1344, 'grad_norm': 22.375, 'learning_rate': 3.0269067687897616e-05, 'epoch': 3.59}
 57%|█████▋    | 2830/5000 [13:37:54<9:04:56, 15.07s/it] 57%|█████▋    | 2831/5000 [13:38:09<9:03:23, 15.03s/it]                                                        {'loss': 17.5159, 'grad_norm': 16.375, 'learning_rate': 3.02461325931465e-05, 'epoch': 3.59}
 57%|█████▋    | 2831/5000 [13:38:09<9:03:23, 15.03s/it] 57%|█████▋    | 2832/5000 [13:38:32<10:35:34, 17.59s/it]                                                         {'loss': 17.3813, 'grad_norm': 9.1875, 'learning_rate': 3.022319957789851e-05, 'epoch': 3.6}
 57%|█████▋    | 2832/5000 [13:38:32<10:35:34, 17.59s/it] 57%|█████▋    | 2833/5000 [13:38:47<10:04:22, 16.73s/it]                                                         {'loss': 18.0931, 'grad_norm': 12.9375, 'learning_rate': 3.020026865218532e-05, 'epoch': 3.6}
 57%|█████▋    | 2833/5000 [13:38:47<10:04:22, 16.73s/it] 57%|█████▋    | 2834/5000 [13:39:02<9:45:13, 16.21s/it]                                                         {'loss': 19.126, 'grad_norm': 13.375, 'learning_rate': 3.0177339826037667e-05, 'epoch': 3.6}
 57%|█████▋    | 2834/5000 [13:39:02<9:45:13, 16.21s/it] 57%|█████▋    | 2835/5000 [13:39:16<9:15:48, 15.40s/it]                                                        {'loss': 18.4598, 'grad_norm': 10.8125, 'learning_rate': 3.0154413109485435e-05, 'epoch': 3.6}
 57%|█████▋    | 2835/5000 [13:39:16<9:15:48, 15.40s/it] 57%|█████▋    | 2836/5000 [13:39:37<10:23:29, 17.29s/it]                                                         {'loss': 18.5255, 'grad_norm': 22.125, 'learning_rate': 3.0131488512557524e-05, 'epoch': 3.6}
 57%|█████▋    | 2836/5000 [13:39:37<10:23:29, 17.29s/it] 57%|█████▋    | 2837/5000 [13:40:03<11:54:13, 19.81s/it]                                                         {'loss': 18.7089, 'grad_norm': 13.3125, 'learning_rate': 3.0108566045281946e-05, 'epoch': 3.6}
 57%|█████▋    | 2837/5000 [13:40:03<11:54:13, 19.81s/it] 57%|█████▋    | 2838/5000 [13:40:18<11:06:11, 18.49s/it]                                                         {'loss': 18.1391, 'grad_norm': 13.875, 'learning_rate': 3.008564571768576e-05, 'epoch': 3.6}
 57%|█████▋    | 2838/5000 [13:40:18<11:06:11, 18.49s/it] 57%|█████▋    | 2839/5000 [13:40:37<11:10:41, 18.62s/it]                                                         {'loss': 18.3343, 'grad_norm': 282.0, 'learning_rate': 3.0062727539795103e-05, 'epoch': 3.61}
 57%|█████▋    | 2839/5000 [13:40:37<11:10:41, 18.62s/it] 57%|█████▋    | 2840/5000 [13:41:00<11:58:52, 19.97s/it]                                                         {'loss': 17.4327, 'grad_norm': 12.9375, 'learning_rate': 3.0039811521635147e-05, 'epoch': 3.61}
 57%|█████▋    | 2840/5000 [13:41:00<11:58:52, 19.97s/it] 57%|█████▋    | 2841/5000 [13:41:13<10:42:13, 17.85s/it]                                                         {'loss': 20.8999, 'grad_norm': 19.25, 'learning_rate': 3.0016897673230164e-05, 'epoch': 3.61}
 57%|█████▋    | 2841/5000 [13:41:13<10:42:13, 17.85s/it] 57%|█████▋    | 2842/5000 [13:41:29<10:21:05, 17.27s/it]                                                         {'loss': 18.8344, 'grad_norm': 12.875, 'learning_rate': 2.999398600460344e-05, 'epoch': 3.61}
 57%|█████▋    | 2842/5000 [13:41:29<10:21:05, 17.27s/it] 57%|█████▋    | 2843/5000 [13:41:49<10:43:18, 17.89s/it]                                                         {'loss': 17.4578, 'grad_norm': 11.0625, 'learning_rate': 2.9971076525777307e-05, 'epoch': 3.61}
 57%|█████▋    | 2843/5000 [13:41:49<10:43:18, 17.89s/it] 57%|█████▋    | 2844/5000 [13:42:04<10:14:00, 17.09s/it]                                                         {'loss': 17.9632, 'grad_norm': 12.8125, 'learning_rate': 2.994816924677314e-05, 'epoch': 3.61}
 57%|█████▋    | 2844/5000 [13:42:04<10:14:00, 17.09s/it] 57%|█████▋    | 2845/5000 [13:42:20<10:03:36, 16.81s/it]                                                         {'loss': 18.503, 'grad_norm': 15.125, 'learning_rate': 2.9925264177611392e-05, 'epoch': 3.61}
 57%|█████▋    | 2845/5000 [13:42:20<10:03:36, 16.81s/it] 57%|█████▋    | 2846/5000 [13:42:34<9:33:29, 15.97s/it]                                                         {'loss': 17.7721, 'grad_norm': 11.4375, 'learning_rate': 2.99023613283115e-05, 'epoch': 3.61}
 57%|█████▋    | 2846/5000 [13:42:34<9:33:29, 15.97s/it] 57%|█████▋    | 2847/5000 [13:43:00<11:19:30, 18.94s/it]                                                         {'loss': 17.8597, 'grad_norm': 11.1875, 'learning_rate': 2.9879460708891944e-05, 'epoch': 3.62}
 57%|█████▋    | 2847/5000 [13:43:00<11:19:30, 18.94s/it] 57%|█████▋    | 2848/5000 [13:43:15<10:33:30, 17.66s/it]                                                         {'loss': 18.6159, 'grad_norm': 15.4375, 'learning_rate': 2.985656232937024e-05, 'epoch': 3.62}
 57%|█████▋    | 2848/5000 [13:43:15<10:33:30, 17.66s/it] 57%|█████▋    | 2849/5000 [13:43:28<9:52:19, 16.52s/it]                                                         {'loss': 18.4114, 'grad_norm': 15.75, 'learning_rate': 2.9833666199762928e-05, 'epoch': 3.62}
 57%|█████▋    | 2849/5000 [13:43:28<9:52:19, 16.52s/it] 57%|█████▋    | 2850/5000 [13:43:53<11:19:02, 18.95s/it]                                                         {'loss': 17.7333, 'grad_norm': 12.6875, 'learning_rate': 2.9810772330085524e-05, 'epoch': 3.62}
 57%|█████▋    | 2850/5000 [13:43:53<11:19:02, 18.95s/it] 57%|█████▋    | 2851/5000 [13:44:10<10:56:43, 18.34s/it]                                                         {'loss': 17.2829, 'grad_norm': 11.9375, 'learning_rate': 2.978788073035262e-05, 'epoch': 3.62}
 57%|█████▋    | 2851/5000 [13:44:10<10:56:43, 18.34s/it] 57%|█████▋    | 2852/5000 [13:44:26<10:31:56, 17.65s/it]                                                         {'loss': 18.0894, 'grad_norm': 17.5, 'learning_rate': 2.976499141057776e-05, 'epoch': 3.62}
 57%|█████▋    | 2852/5000 [13:44:26<10:31:56, 17.65s/it] 57%|█████▋    | 2853/5000 [13:44:44<10:34:27, 17.73s/it]                                                         {'loss': 16.9104, 'grad_norm': 8.1875, 'learning_rate': 2.97421043807735e-05, 'epoch': 3.62}
 57%|█████▋    | 2853/5000 [13:44:44<10:34:27, 17.73s/it] 57%|█████▋    | 2854/5000 [13:45:00<10:19:36, 17.32s/it]                                                         {'loss': 18.5655, 'grad_norm': 17.0, 'learning_rate': 2.9719219650951427e-05, 'epoch': 3.62}
 57%|█████▋    | 2854/5000 [13:45:00<10:19:36, 17.32s/it] 57%|█████▋    | 2855/5000 [13:45:16<10:02:01, 16.84s/it]                                                         {'loss': 17.6979, 'grad_norm': 16.75, 'learning_rate': 2.9696337231122088e-05, 'epoch': 3.63}
 57%|█████▋    | 2855/5000 [13:45:16<10:02:01, 16.84s/it] 57%|█████▋    | 2856/5000 [13:45:34<10:16:31, 17.25s/it]                                                         {'loss': 18.7571, 'grad_norm': 23.125, 'learning_rate': 2.967345713129502e-05, 'epoch': 3.63}
 57%|█████▋    | 2856/5000 [13:45:34<10:16:31, 17.25s/it] 57%|█████▋    | 2857/5000 [13:45:50<10:00:21, 16.81s/it]                                                         {'loss': 17.5783, 'grad_norm': 11.1875, 'learning_rate': 2.965057936147878e-05, 'epoch': 3.63}
 57%|█████▋    | 2857/5000 [13:45:50<10:00:21, 16.81s/it] 57%|█████▋    | 2858/5000 [13:46:05<9:37:42, 16.18s/it]                                                         {'loss': 18.061, 'grad_norm': 61.5, 'learning_rate': 2.9627703931680862e-05, 'epoch': 3.63}
 57%|█████▋    | 2858/5000 [13:46:05<9:37:42, 16.18s/it] 57%|█████▋    | 2859/5000 [13:46:20<9:24:07, 15.81s/it]                                                        {'loss': 18.1545, 'grad_norm': 12.25, 'learning_rate': 2.9604830851907764e-05, 'epoch': 3.63}
 57%|█████▋    | 2859/5000 [13:46:20<9:24:07, 15.81s/it] 57%|█████▋    | 2860/5000 [13:46:35<9:19:52, 15.70s/it]                                                        {'loss': 19.2148, 'grad_norm': 19.75, 'learning_rate': 2.958196013216495e-05, 'epoch': 3.63}
 57%|█████▋    | 2860/5000 [13:46:35<9:19:52, 15.70s/it] 57%|█████▋    | 2861/5000 [13:47:00<11:01:18, 18.55s/it]                                                         {'loss': 16.5113, 'grad_norm': 15.5, 'learning_rate': 2.955909178245685e-05, 'epoch': 3.63}
 57%|█████▋    | 2861/5000 [13:47:00<11:01:18, 18.55s/it] 57%|█████▋    | 2862/5000 [13:47:17<10:45:54, 18.13s/it]                                                         {'loss': 16.9684, 'grad_norm': 32.25, 'learning_rate': 2.9536225812786858e-05, 'epoch': 3.63}
 57%|█████▋    | 2862/5000 [13:47:17<10:45:54, 18.13s/it] 57%|█████▋    | 2863/5000 [13:47:34<10:32:42, 17.76s/it]                                                         {'loss': 17.7522, 'grad_norm': 16.875, 'learning_rate': 2.9513362233157305e-05, 'epoch': 3.64}
 57%|█████▋    | 2863/5000 [13:47:34<10:32:42, 17.76s/it] 57%|█████▋    | 2864/5000 [13:47:48<9:48:57, 16.54s/it]                                                         {'loss': 19.5289, 'grad_norm': 17.5, 'learning_rate': 2.9490501053569536e-05, 'epoch': 3.64}
 57%|█████▋    | 2864/5000 [13:47:48<9:48:57, 16.54s/it] 57%|█████▋    | 2865/5000 [13:48:01<9:14:17, 15.58s/it]                                                        {'loss': 18.3015, 'grad_norm': 14.625, 'learning_rate': 2.9467642284023778e-05, 'epoch': 3.64}
 57%|█████▋    | 2865/5000 [13:48:01<9:14:17, 15.58s/it] 57%|█████▋    | 2866/5000 [13:48:25<10:40:14, 18.00s/it]                                                         {'loss': 16.2785, 'grad_norm': 10.3125, 'learning_rate': 2.9444785934519232e-05, 'epoch': 3.64}
 57%|█████▋    | 2866/5000 [13:48:25<10:40:14, 18.00s/it] 57%|█████▋    | 2867/5000 [13:48:41<10:21:06, 17.47s/it]                                                         {'loss': 17.8854, 'grad_norm': 11.5, 'learning_rate': 2.9421932015054067e-05, 'epoch': 3.64}
 57%|█████▋    | 2867/5000 [13:48:41<10:21:06, 17.47s/it] 57%|█████▋    | 2868/5000 [13:48:55<9:45:42, 16.48s/it]                                                         {'loss': 18.9092, 'grad_norm': 34.0, 'learning_rate': 2.9399080535625346e-05, 'epoch': 3.64}
 57%|█████▋    | 2868/5000 [13:48:55<9:45:42, 16.48s/it] 57%|█████▋    | 2869/5000 [13:49:13<9:56:05, 16.78s/it]                                                        {'loss': 18.6407, 'grad_norm': 20.375, 'learning_rate': 2.937623150622907e-05, 'epoch': 3.64}
 57%|█████▋    | 2869/5000 [13:49:13<9:56:05, 16.78s/it] 57%|█████▋    | 2870/5000 [13:49:28<9:32:37, 16.13s/it]                                                        {'loss': 19.5819, 'grad_norm': 24.375, 'learning_rate': 2.9353384936860217e-05, 'epoch': 3.64}
 57%|█████▋    | 2870/5000 [13:49:28<9:32:37, 16.13s/it] 57%|█████▋    | 2871/5000 [13:49:43<9:20:48, 15.80s/it]                                                        {'loss': 18.0081, 'grad_norm': 10.3125, 'learning_rate': 2.9330540837512618e-05, 'epoch': 3.65}
 57%|█████▋    | 2871/5000 [13:49:43<9:20:48, 15.80s/it] 57%|█████▋    | 2872/5000 [13:49:56<8:56:41, 15.13s/it]                                                        {'loss': 18.6344, 'grad_norm': 15.3125, 'learning_rate': 2.9307699218179073e-05, 'epoch': 3.65}
 57%|█████▋    | 2872/5000 [13:49:56<8:56:41, 15.13s/it] 57%|█████▋    | 2873/5000 [13:50:10<8:45:18, 14.82s/it]                                                        {'loss': 18.8404, 'grad_norm': 10.8125, 'learning_rate': 2.928486008885128e-05, 'epoch': 3.65}
 57%|█████▋    | 2873/5000 [13:50:10<8:45:18, 14.82s/it] 57%|█████▋    | 2874/5000 [13:50:27<9:08:33, 15.48s/it]                                                        {'loss': 18.9767, 'grad_norm': 17.625, 'learning_rate': 2.926202345951986e-05, 'epoch': 3.65}
 57%|█████▋    | 2874/5000 [13:50:27<9:08:33, 15.48s/it] 57%|█████▊    | 2875/5000 [13:50:57<11:36:12, 19.66s/it]                                                         {'loss': 17.5568, 'grad_norm': 10.25, 'learning_rate': 2.9239189340174306e-05, 'epoch': 3.65}
 57%|█████▊    | 2875/5000 [13:50:57<11:36:12, 19.66s/it] 58%|█████▊    | 2876/5000 [13:51:10<10:24:13, 17.63s/it]                                                         {'loss': 19.2211, 'grad_norm': 16.25, 'learning_rate': 2.921635774080307e-05, 'epoch': 3.65}
 58%|█████▊    | 2876/5000 [13:51:10<10:24:13, 17.63s/it] 58%|█████▊    | 2877/5000 [13:51:25<10:02:36, 17.03s/it]                                                         {'loss': 18.8913, 'grad_norm': 15.4375, 'learning_rate': 2.9193528671393446e-05, 'epoch': 3.65}
 58%|█████▊    | 2877/5000 [13:51:25<10:02:36, 17.03s/it] 58%|█████▊    | 2878/5000 [13:51:39<9:32:29, 16.19s/it]                                                         {'loss': 16.3809, 'grad_norm': 15.0, 'learning_rate': 2.9170702141931657e-05, 'epoch': 3.65}
 58%|█████▊    | 2878/5000 [13:51:39<9:32:29, 16.19s/it] 58%|█████▊    | 2879/5000 [13:52:05<11:15:16, 19.10s/it]                                                         {'loss': 20.2214, 'grad_norm': 31.5, 'learning_rate': 2.9147878162402778e-05, 'epoch': 3.66}
 58%|█████▊    | 2879/5000 [13:52:05<11:15:16, 19.10s/it] 58%|█████▊    | 2880/5000 [13:52:20<10:29:43, 17.82s/it]                                                         {'loss': 18.5229, 'grad_norm': 27.375, 'learning_rate': 2.912505674279083e-05, 'epoch': 3.66}
 58%|█████▊    | 2880/5000 [13:52:20<10:29:43, 17.82s/it] 58%|█████▊    | 2881/5000 [13:52:35<10:00:47, 17.01s/it]                                                         {'loss': 18.2703, 'grad_norm': 13.75, 'learning_rate': 2.9102237893078653e-05, 'epoch': 3.66}
 58%|█████▊    | 2881/5000 [13:52:35<10:00:47, 17.01s/it] 58%|█████▊    | 2882/5000 [13:53:00<11:25:32, 19.42s/it]                                                         {'loss': 18.5623, 'grad_norm': 13.5625, 'learning_rate': 2.9079421623247987e-05, 'epoch': 3.66}
 58%|█████▊    | 2882/5000 [13:53:00<11:25:32, 19.42s/it] 58%|█████▊    | 2883/5000 [13:53:19<11:16:09, 19.16s/it]                                                         {'loss': 17.1595, 'grad_norm': 9.6875, 'learning_rate': 2.905660794327945e-05, 'epoch': 3.66}
 58%|█████▊    | 2883/5000 [13:53:19<11:16:09, 19.16s/it] 58%|█████▊    | 2884/5000 [13:53:33<10:24:00, 17.69s/it]                                                         {'loss': 19.8811, 'grad_norm': 16.0, 'learning_rate': 2.9033796863152532e-05, 'epoch': 3.66}
 58%|█████▊    | 2884/5000 [13:53:33<10:24:00, 17.69s/it] 58%|█████▊    | 2885/5000 [13:53:46<9:35:09, 16.32s/it]                                                         {'loss': 18.8681, 'grad_norm': 41.25, 'learning_rate': 2.9010988392845544e-05, 'epoch': 3.66}
 58%|█████▊    | 2885/5000 [13:53:46<9:35:09, 16.32s/it] 58%|█████▊    | 2886/5000 [13:54:17<12:08:32, 20.68s/it]                                                         {'loss': 17.2448, 'grad_norm': 16.375, 'learning_rate': 2.8988182542335723e-05, 'epoch': 3.66}
 58%|█████▊    | 2886/5000 [13:54:17<12:08:32, 20.68s/it] 58%|█████▊    | 2887/5000 [13:54:35<11:34:49, 19.73s/it]                                                         {'loss': 17.2129, 'grad_norm': 9.5625, 'learning_rate': 2.8965379321599103e-05, 'epoch': 3.67}
 58%|█████▊    | 2887/5000 [13:54:35<11:34:49, 19.73s/it] 58%|█████▊    | 2888/5000 [13:54:49<10:34:46, 18.03s/it]                                                         {'loss': 18.4038, 'grad_norm': 13.4375, 'learning_rate': 2.8942578740610572e-05, 'epoch': 3.67}
 58%|█████▊    | 2888/5000 [13:54:49<10:34:46, 18.03s/it] 58%|█████▊    | 2889/5000 [13:55:03<9:58:54, 17.02s/it]                                                         {'loss': 18.833, 'grad_norm': 15.5, 'learning_rate': 2.891978080934391e-05, 'epoch': 3.67}
 58%|█████▊    | 2889/5000 [13:55:03<9:58:54, 17.02s/it] 58%|█████▊    | 2890/5000 [13:55:20<9:57:23, 16.99s/it]                                                        {'loss': 18.14, 'grad_norm': 18.25, 'learning_rate': 2.8896985537771694e-05, 'epoch': 3.67}
 58%|█████▊    | 2890/5000 [13:55:20<9:57:23, 16.99s/it] 58%|█████▊    | 2891/5000 [13:55:37<9:55:22, 16.94s/it]                                                        {'loss': 16.3531, 'grad_norm': 16.375, 'learning_rate': 2.887419293586533e-05, 'epoch': 3.67}
 58%|█████▊    | 2891/5000 [13:55:37<9:55:22, 16.94s/it] 58%|█████▊    | 2892/5000 [13:55:56<10:12:33, 17.44s/it]                                                         {'loss': 18.6527, 'grad_norm': 12.5625, 'learning_rate': 2.8851403013595117e-05, 'epoch': 3.67}
 58%|█████▊    | 2892/5000 [13:55:56<10:12:33, 17.44s/it] 58%|█████▊    | 2893/5000 [13:56:11<9:51:16, 16.84s/it]                                                         {'loss': 19.4185, 'grad_norm': 19.25, 'learning_rate': 2.882861578093011e-05, 'epoch': 3.67}
 58%|█████▊    | 2893/5000 [13:56:11<9:51:16, 16.84s/it] 58%|█████▊    | 2894/5000 [13:56:35<11:08:23, 19.04s/it]                                                         {'loss': 18.6544, 'grad_norm': 13.1875, 'learning_rate': 2.8805831247838228e-05, 'epoch': 3.67}
 58%|█████▊    | 2894/5000 [13:56:35<11:08:23, 19.04s/it] 58%|█████▊    | 2895/5000 [13:56:51<10:31:20, 18.00s/it]                                                         {'loss': 18.64, 'grad_norm': 22.0, 'learning_rate': 2.8783049424286196e-05, 'epoch': 3.68}
 58%|█████▊    | 2895/5000 [13:56:51<10:31:20, 18.00s/it] 58%|█████▊    | 2896/5000 [13:57:06<9:57:21, 17.04s/it]                                                         {'loss': 21.0885, 'grad_norm': 18.625, 'learning_rate': 2.8760270320239563e-05, 'epoch': 3.68}
 58%|█████▊    | 2896/5000 [13:57:06<9:57:21, 17.04s/it] 58%|█████▊    | 2897/5000 [13:57:21<9:41:59, 16.60s/it]                                                        {'loss': 18.6216, 'grad_norm': 11.875, 'learning_rate': 2.8737493945662685e-05, 'epoch': 3.68}
 58%|█████▊    | 2897/5000 [13:57:21<9:41:59, 16.60s/it] 58%|█████▊    | 2898/5000 [13:57:35<9:09:41, 15.69s/it]                                                        {'loss': 20.3323, 'grad_norm': 18.25, 'learning_rate': 2.871472031051871e-05, 'epoch': 3.68}
 58%|█████▊    | 2898/5000 [13:57:35<9:09:41, 15.69s/it] 58%|█████▊    | 2899/5000 [13:57:52<9:20:18, 16.00s/it]                                                        {'loss': 18.4356, 'grad_norm': 18.375, 'learning_rate': 2.8691949424769618e-05, 'epoch': 3.68}
 58%|█████▊    | 2899/5000 [13:57:52<9:20:18, 16.00s/it] 58%|█████▊    | 2900/5000 [13:58:07<9:18:50, 15.97s/it]                                                        {'loss': 19.6176, 'grad_norm': 15.125, 'learning_rate': 2.8669181298376163e-05, 'epoch': 3.68}
 58%|█████▊    | 2900/5000 [13:58:07<9:18:50, 15.97s/it] 58%|█████▊    | 2901/5000 [13:58:26<9:40:57, 16.61s/it]                                                        {'loss': 16.3099, 'grad_norm': 7.59375, 'learning_rate': 2.864641594129787e-05, 'epoch': 3.68}
 58%|█████▊    | 2901/5000 [13:58:26<9:40:57, 16.61s/it] 58%|█████▊    | 2902/5000 [13:58:42<9:40:32, 16.60s/it]                                                        {'loss': 18.2379, 'grad_norm': 15.5, 'learning_rate': 2.862365336349312e-05, 'epoch': 3.69}
 58%|█████▊    | 2902/5000 [13:58:42<9:40:32, 16.60s/it] 58%|█████▊    | 2903/5000 [13:58:56<9:07:51, 15.68s/it]                                                        {'loss': 18.5543, 'grad_norm': 15.9375, 'learning_rate': 2.8600893574919022e-05, 'epoch': 3.69}
 58%|█████▊    | 2903/5000 [13:58:56<9:07:51, 15.68s/it] 58%|█████▊    | 2904/5000 [13:59:11<9:03:59, 15.57s/it]                                                        {'loss': 17.481, 'grad_norm': 21.875, 'learning_rate': 2.857813658553146e-05, 'epoch': 3.69}
 58%|█████▊    | 2904/5000 [13:59:11<9:03:59, 15.57s/it] 58%|█████▊    | 2905/5000 [13:59:39<11:11:58, 19.25s/it]                                                         {'loss': 18.3312, 'grad_norm': 124.5, 'learning_rate': 2.855538240528515e-05, 'epoch': 3.69}
 58%|█████▊    | 2905/5000 [13:59:39<11:11:58, 19.25s/it] 58%|█████▊    | 2906/5000 [14:00:05<12:27:42, 21.42s/it]                                                         {'loss': 16.8261, 'grad_norm': 25.75, 'learning_rate': 2.853263104413352e-05, 'epoch': 3.69}
 58%|█████▊    | 2906/5000 [14:00:05<12:27:42, 21.42s/it] 58%|█████▊    | 2907/5000 [14:00:20<11:18:42, 19.46s/it]                                                         {'loss': 18.9467, 'grad_norm': 18.875, 'learning_rate': 2.850988251202879e-05, 'epoch': 3.69}
 58%|█████▊    | 2907/5000 [14:00:20<11:18:42, 19.46s/it] 58%|█████▊    | 2908/5000 [14:00:39<11:08:39, 19.18s/it]                                                         {'loss': 18.734, 'grad_norm': 15.125, 'learning_rate': 2.848713681892195e-05, 'epoch': 3.69}
 58%|█████▊    | 2908/5000 [14:00:39<11:08:39, 19.18s/it] 58%|█████▊    | 2909/5000 [14:00:56<10:48:00, 18.59s/it]                                                         {'loss': 19.7277, 'grad_norm': 17.5, 'learning_rate': 2.846439397476274e-05, 'epoch': 3.69}
 58%|█████▊    | 2909/5000 [14:00:56<10:48:00, 18.59s/it] 58%|█████▊    | 2910/5000 [14:01:18<11:22:33, 19.59s/it]                                                         {'loss': 16.8176, 'grad_norm': 14.25, 'learning_rate': 2.8441653989499627e-05, 'epoch': 3.7}
 58%|█████▊    | 2910/5000 [14:01:18<11:22:33, 19.59s/it] 58%|█████▊    | 2911/5000 [14:01:42<12:11:47, 21.02s/it]                                                         {'loss': 19.0604, 'grad_norm': 12.3125, 'learning_rate': 2.8418916873079892e-05, 'epoch': 3.7}
 58%|█████▊    | 2911/5000 [14:01:42<12:11:47, 21.02s/it] 58%|█████▊    | 2912/5000 [14:01:57<11:07:05, 19.17s/it]                                                         {'loss': 19.0374, 'grad_norm': 11.6875, 'learning_rate': 2.8396182635449506e-05, 'epoch': 3.7}
 58%|█████▊    | 2912/5000 [14:01:57<11:07:05, 19.17s/it] 58%|█████▊    | 2913/5000 [14:02:11<10:11:52, 17.59s/it]                                                         {'loss': 19.6567, 'grad_norm': 16.5, 'learning_rate': 2.8373451286553192e-05, 'epoch': 3.7}
 58%|█████▊    | 2913/5000 [14:02:11<10:11:52, 17.59s/it] 58%|█████▊    | 2914/5000 [14:02:27<9:52:55, 17.05s/it]                                                         {'loss': 16.7357, 'grad_norm': 12.8125, 'learning_rate': 2.8350722836334402e-05, 'epoch': 3.7}
 58%|█████▊    | 2914/5000 [14:02:27<9:52:55, 17.05s/it] 58%|█████▊    | 2915/5000 [14:02:41<9:21:22, 16.15s/it]                                                        {'loss': 18.8711, 'grad_norm': 18.25, 'learning_rate': 2.8327997294735362e-05, 'epoch': 3.7}
 58%|█████▊    | 2915/5000 [14:02:41<9:21:22, 16.15s/it] 58%|█████▊    | 2916/5000 [14:02:57<9:18:59, 16.09s/it]                                                        {'loss': 19.2943, 'grad_norm': 50.0, 'learning_rate': 2.8305274671696982e-05, 'epoch': 3.7}
 58%|█████▊    | 2916/5000 [14:02:57<9:18:59, 16.09s/it] 58%|█████▊    | 2917/5000 [14:03:20<10:33:41, 18.25s/it]                                                         {'loss': 17.7721, 'grad_norm': 15.75, 'learning_rate': 2.828255497715889e-05, 'epoch': 3.7}
 58%|█████▊    | 2917/5000 [14:03:20<10:33:41, 18.25s/it] 58%|█████▊    | 2918/5000 [14:03:36<10:06:55, 17.49s/it]                                                         {'loss': 17.5457, 'grad_norm': 14.4375, 'learning_rate': 2.8259838221059486e-05, 'epoch': 3.71}
 58%|█████▊    | 2918/5000 [14:03:36<10:06:55, 17.49s/it] 58%|█████▊    | 2919/5000 [14:04:02<11:42:10, 20.25s/it]                                                         {'loss': 17.7195, 'grad_norm': 24.5, 'learning_rate': 2.8237124413335825e-05, 'epoch': 3.71}
 58%|█████▊    | 2919/5000 [14:04:02<11:42:10, 20.25s/it] 58%|█████▊    | 2920/5000 [14:04:25<12:03:32, 20.87s/it]                                                         {'loss': 17.5427, 'grad_norm': 13.9375, 'learning_rate': 2.8214413563923712e-05, 'epoch': 3.71}
 58%|█████▊    | 2920/5000 [14:04:25<12:03:32, 20.87s/it] 58%|█████▊    | 2921/5000 [14:04:38<10:43:07, 18.56s/it]                                                         {'loss': 19.1967, 'grad_norm': 21.25, 'learning_rate': 2.819170568275764e-05, 'epoch': 3.71}
 58%|█████▊    | 2921/5000 [14:04:38<10:43:07, 18.56s/it] 58%|█████▊    | 2922/5000 [14:04:57<10:47:41, 18.70s/it]                                                         {'loss': 17.9469, 'grad_norm': 14.0625, 'learning_rate': 2.8169000779770817e-05, 'epoch': 3.71}
 58%|█████▊    | 2922/5000 [14:04:57<10:47:41, 18.70s/it] 58%|█████▊    | 2923/5000 [14:05:11<9:59:26, 17.32s/it]                                                         {'loss': 19.123, 'grad_norm': 22.0, 'learning_rate': 2.8146298864895113e-05, 'epoch': 3.71}
 58%|█████▊    | 2923/5000 [14:05:11<9:59:26, 17.32s/it] 58%|█████▊    | 2924/5000 [14:05:28<9:51:26, 17.09s/it]                                                        {'loss': 19.1141, 'grad_norm': 15.3125, 'learning_rate': 2.8123599948061156e-05, 'epoch': 3.71}
 58%|█████▊    | 2924/5000 [14:05:28<9:51:26, 17.09s/it] 58%|█████▊    | 2925/5000 [14:05:47<10:09:43, 17.63s/it]                                                         {'loss': 19.0352, 'grad_norm': 40.5, 'learning_rate': 2.8100904039198193e-05, 'epoch': 3.71}
 58%|█████▊    | 2925/5000 [14:05:47<10:09:43, 17.63s/it] 59%|█████▊    | 2926/5000 [14:06:01<9:39:58, 16.78s/it]                                                         {'loss': 17.8194, 'grad_norm': 14.125, 'learning_rate': 2.807821114823418e-05, 'epoch': 3.72}
 59%|█████▊    | 2926/5000 [14:06:01<9:39:58, 16.78s/it] 59%|█████▊    | 2927/5000 [14:06:18<9:37:49, 16.72s/it]                                                        {'loss': 17.3904, 'grad_norm': 15.0625, 'learning_rate': 2.805552128509579e-05, 'epoch': 3.72}
 59%|█████▊    | 2927/5000 [14:06:18<9:37:49, 16.72s/it] 59%|█████▊    | 2928/5000 [14:06:31<9:02:51, 15.72s/it]                                                        {'loss': 20.2086, 'grad_norm': 19.125, 'learning_rate': 2.803283445970832e-05, 'epoch': 3.72}
 59%|█████▊    | 2928/5000 [14:06:31<9:02:51, 15.72s/it] 59%|█████▊    | 2929/5000 [14:06:48<9:10:55, 15.96s/it]                                                        {'loss': 18.2263, 'grad_norm': 12.5625, 'learning_rate': 2.8010150681995755e-05, 'epoch': 3.72}
 59%|█████▊    | 2929/5000 [14:06:48<9:10:55, 15.96s/it] 59%|█████▊    | 2930/5000 [14:07:04<9:10:22, 15.95s/it]                                                        {'loss': 17.7288, 'grad_norm': 10.625, 'learning_rate': 2.7987469961880757e-05, 'epoch': 3.72}
 59%|█████▊    | 2930/5000 [14:07:04<9:10:22, 15.95s/it] 59%|█████▊    | 2931/5000 [14:07:18<8:47:54, 15.31s/it]                                                        {'loss': 19.0655, 'grad_norm': 30.375, 'learning_rate': 2.796479230928464e-05, 'epoch': 3.72}
 59%|█████▊    | 2931/5000 [14:07:18<8:47:54, 15.31s/it] 59%|█████▊    | 2932/5000 [14:07:33<8:47:39, 15.31s/it]                                                        {'loss': 18.6882, 'grad_norm': 159.0, 'learning_rate': 2.7942117734127394e-05, 'epoch': 3.72}
 59%|█████▊    | 2932/5000 [14:07:33<8:47:39, 15.31s/it] 59%|█████▊    | 2933/5000 [14:08:02<11:05:37, 19.32s/it]                                                         {'loss': 16.6944, 'grad_norm': 9.5625, 'learning_rate': 2.7919446246327615e-05, 'epoch': 3.72}
 59%|█████▊    | 2933/5000 [14:08:02<11:05:37, 19.32s/it] 59%|█████▊    | 2934/5000 [14:08:27<12:03:53, 21.02s/it]                                                         {'loss': 17.3762, 'grad_norm': 10.9375, 'learning_rate': 2.7896777855802622e-05, 'epoch': 3.73}
 59%|█████▊    | 2934/5000 [14:08:27<12:03:53, 21.02s/it] 59%|█████▊    | 2935/5000 [14:08:41<10:51:29, 18.93s/it]                                                         {'loss': 19.4846, 'grad_norm': 18.75, 'learning_rate': 2.787411257246832e-05, 'epoch': 3.73}
 59%|█████▊    | 2935/5000 [14:08:41<10:51:29, 18.93s/it] 59%|█████▊    | 2936/5000 [14:08:53<9:41:26, 16.90s/it]                                                         {'loss': 20.4532, 'grad_norm': 49.0, 'learning_rate': 2.785145040623926e-05, 'epoch': 3.73}
 59%|█████▊    | 2936/5000 [14:08:53<9:41:26, 16.90s/it] 59%|█████▊    | 2937/5000 [14:09:09<9:37:14, 16.79s/it]                                                        {'loss': 17.0908, 'grad_norm': 37.5, 'learning_rate': 2.7828791367028675e-05, 'epoch': 3.73}
 59%|█████▊    | 2937/5000 [14:09:09<9:37:14, 16.79s/it] 59%|█████▉    | 2938/5000 [14:09:26<9:35:49, 16.76s/it]                                                        {'loss': 17.1937, 'grad_norm': 13.5, 'learning_rate': 2.7806135464748378e-05, 'epoch': 3.73}
 59%|█████▉    | 2938/5000 [14:09:26<9:35:49, 16.76s/it] 59%|█████▉    | 2939/5000 [14:09:44<9:44:02, 17.00s/it]                                                        {'loss': 18.9239, 'grad_norm': 21.125, 'learning_rate': 2.7783482709308822e-05, 'epoch': 3.73}
 59%|█████▉    | 2939/5000 [14:09:44<9:44:02, 17.00s/it] 59%|█████▉    | 2940/5000 [14:10:00<9:34:28, 16.73s/it]                                                        {'loss': 18.5756, 'grad_norm': 11.8125, 'learning_rate': 2.7760833110619115e-05, 'epoch': 3.73}
 59%|█████▉    | 2940/5000 [14:10:00<9:34:28, 16.73s/it] 59%|█████▉    | 2941/5000 [14:10:15<9:15:21, 16.18s/it]                                                        {'loss': 18.017, 'grad_norm': 12.8125, 'learning_rate': 2.773818667858695e-05, 'epoch': 3.73}
 59%|█████▉    | 2941/5000 [14:10:15<9:15:21, 16.18s/it] 59%|█████▉    | 2942/5000 [14:10:30<9:07:53, 15.97s/it]                                                        {'loss': 20.0137, 'grad_norm': 15.5, 'learning_rate': 2.7715543423118636e-05, 'epoch': 3.74}
 59%|█████▉    | 2942/5000 [14:10:30<9:07:53, 15.97s/it] 59%|█████▉    | 2943/5000 [14:10:54<10:31:13, 18.41s/it]                                                         {'loss': 16.8053, 'grad_norm': 11.375, 'learning_rate': 2.769290335411912e-05, 'epoch': 3.74}
 59%|█████▉    | 2943/5000 [14:10:54<10:31:13, 18.41s/it] 59%|█████▉    | 2944/5000 [14:11:09<9:53:10, 17.31s/it]                                                         {'loss': 20.892, 'grad_norm': 20.875, 'learning_rate': 2.7670266481491927e-05, 'epoch': 3.74}
 59%|█████▉    | 2944/5000 [14:11:09<9:53:10, 17.31s/it] 59%|█████▉    | 2945/5000 [14:11:24<9:27:51, 16.58s/it]                                                        {'loss': 17.6226, 'grad_norm': 13.125, 'learning_rate': 2.7647632815139196e-05, 'epoch': 3.74}
 59%|█████▉    | 2945/5000 [14:11:24<9:27:51, 16.58s/it] 59%|█████▉    | 2946/5000 [14:11:44<10:02:02, 17.59s/it]                                                         {'loss': 17.5537, 'grad_norm': 18.25, 'learning_rate': 2.7625002364961658e-05, 'epoch': 3.74}
 59%|█████▉    | 2946/5000 [14:11:44<10:02:02, 17.59s/it] 59%|█████▉    | 2947/5000 [14:12:10<11:31:26, 20.21s/it]                                                         {'loss': 18.9909, 'grad_norm': 11.25, 'learning_rate': 2.7602375140858655e-05, 'epoch': 3.74}
 59%|█████▉    | 2947/5000 [14:12:10<11:31:26, 20.21s/it] 59%|█████▉    | 2948/5000 [14:12:25<10:42:20, 18.78s/it]                                                         {'loss': 18.6189, 'grad_norm': 12.5625, 'learning_rate': 2.7579751152728093e-05, 'epoch': 3.74}
 59%|█████▉    | 2948/5000 [14:12:25<10:42:20, 18.78s/it] 59%|█████▉    | 2949/5000 [14:12:53<12:07:10, 21.27s/it]                                                         {'loss': 18.3624, 'grad_norm': 29.5, 'learning_rate': 2.7557130410466457e-05, 'epoch': 3.74}
 59%|█████▉    | 2949/5000 [14:12:53<12:07:10, 21.27s/it] 59%|█████▉    | 2950/5000 [14:13:08<11:09:46, 19.60s/it]                                                         {'loss': 19.1742, 'grad_norm': 51.0, 'learning_rate': 2.7534512923968863e-05, 'epoch': 3.75}
 59%|█████▉    | 2950/5000 [14:13:08<11:09:46, 19.60s/it] 59%|█████▉    | 2951/5000 [14:13:28<11:10:43, 19.64s/it]                                                         {'loss': 18.0897, 'grad_norm': 11.625, 'learning_rate': 2.7511898703128946e-05, 'epoch': 3.75}
 59%|█████▉    | 2951/5000 [14:13:28<11:10:43, 19.64s/it] 59%|█████▉    | 2952/5000 [14:13:41<10:07:22, 17.79s/it]                                                         {'loss': 19.7416, 'grad_norm': 22.875, 'learning_rate': 2.7489287757838923e-05, 'epoch': 3.75}
 59%|█████▉    | 2952/5000 [14:13:41<10:07:22, 17.79s/it] 59%|█████▉    | 2953/5000 [14:13:58<9:55:29, 17.45s/it]                                                         {'loss': 17.5092, 'grad_norm': 10.9375, 'learning_rate': 2.7466680097989624e-05, 'epoch': 3.75}
 59%|█████▉    | 2953/5000 [14:13:58<9:55:29, 17.45s/it] 59%|█████▉    | 2954/5000 [14:14:13<9:30:25, 16.73s/it]                                                        {'loss': 18.23, 'grad_norm': 13.0, 'learning_rate': 2.7444075733470375e-05, 'epoch': 3.75}
 59%|█████▉    | 2954/5000 [14:14:13<9:30:25, 16.73s/it] 59%|█████▉    | 2955/5000 [14:14:29<9:24:01, 16.55s/it]                                                        {'loss': 18.3024, 'grad_norm': 9.375, 'learning_rate': 2.7421474674169112e-05, 'epoch': 3.75}
 59%|█████▉    | 2955/5000 [14:14:29<9:24:01, 16.55s/it] 59%|█████▉    | 2956/5000 [14:14:46<9:22:44, 16.52s/it]                                                        {'loss': 17.7885, 'grad_norm': 24.125, 'learning_rate': 2.7398876929972296e-05, 'epoch': 3.75}
 59%|█████▉    | 2956/5000 [14:14:46<9:22:44, 16.52s/it] 59%|█████▉    | 2957/5000 [14:15:03<9:27:48, 16.68s/it]                                                        {'loss': 17.9105, 'grad_norm': 27.5, 'learning_rate': 2.7376282510764958e-05, 'epoch': 3.75}
 59%|█████▉    | 2957/5000 [14:15:03<9:27:48, 16.68s/it] 59%|█████▉    | 2958/5000 [14:15:17<9:00:57, 15.90s/it]                                                        {'loss': 17.9321, 'grad_norm': 54.5, 'learning_rate': 2.7353691426430647e-05, 'epoch': 3.76}
 59%|█████▉    | 2958/5000 [14:15:17<9:00:57, 15.90s/it] 59%|█████▉    | 2959/5000 [14:15:31<8:44:32, 15.42s/it]                                                        {'loss': 18.0285, 'grad_norm': 76.0, 'learning_rate': 2.73311036868515e-05, 'epoch': 3.76}
 59%|█████▉    | 2959/5000 [14:15:31<8:44:32, 15.42s/it] 59%|█████▉    | 2960/5000 [14:15:43<8:09:31, 14.40s/it]                                                        {'loss': 21.039, 'grad_norm': 27.5, 'learning_rate': 2.730851930190814e-05, 'epoch': 3.76}
 59%|█████▉    | 2960/5000 [14:15:43<8:09:31, 14.40s/it] 59%|█████▉    | 2961/5000 [14:16:13<10:43:39, 18.94s/it]                                                         {'loss': 18.7923, 'grad_norm': 11.9375, 'learning_rate': 2.7285938281479738e-05, 'epoch': 3.76}
 59%|█████▉    | 2961/5000 [14:16:13<10:43:39, 18.94s/it] 59%|█████▉    | 2962/5000 [14:16:36<11:29:41, 20.31s/it]                                                         {'loss': 18.7562, 'grad_norm': 12.5625, 'learning_rate': 2.7263360635444023e-05, 'epoch': 3.76}
 59%|█████▉    | 2962/5000 [14:16:36<11:29:41, 20.31s/it] 59%|█████▉    | 2963/5000 [14:16:54<10:58:58, 19.41s/it]                                                         {'loss': 16.7557, 'grad_norm': 9.375, 'learning_rate': 2.724078637367721e-05, 'epoch': 3.76}
 59%|█████▉    | 2963/5000 [14:16:54<10:58:58, 19.41s/it] 59%|█████▉    | 2964/5000 [14:17:08<10:10:49, 18.00s/it]                                                         {'loss': 18.5818, 'grad_norm': 12.8125, 'learning_rate': 2.7218215506054048e-05, 'epoch': 3.76}
 59%|█████▉    | 2964/5000 [14:17:08<10:10:49, 18.00s/it] 59%|█████▉    | 2965/5000 [14:17:24<9:48:50, 17.36s/it]                                                         {'loss': 16.1137, 'grad_norm': 7.75, 'learning_rate': 2.719564804244779e-05, 'epoch': 3.77}
 59%|█████▉    | 2965/5000 [14:17:24<9:48:50, 17.36s/it] 59%|█████▉    | 2966/5000 [14:17:40<9:28:57, 16.78s/it]                                                        {'loss': 17.9033, 'grad_norm': 11.875, 'learning_rate': 2.7173083992730247e-05, 'epoch': 3.77}
 59%|█████▉    | 2966/5000 [14:17:40<9:28:57, 16.78s/it] 59%|█████▉    | 2967/5000 [14:17:55<9:11:46, 16.28s/it]                                                        {'loss': 19.6174, 'grad_norm': 22.625, 'learning_rate': 2.7150523366771663e-05, 'epoch': 3.77}
 59%|█████▉    | 2967/5000 [14:17:55<9:11:46, 16.28s/it] 59%|█████▉    | 2968/5000 [14:18:14<9:41:37, 17.17s/it]                                                        {'loss': 16.8471, 'grad_norm': 11.75, 'learning_rate': 2.7127966174440837e-05, 'epoch': 3.77}
 59%|█████▉    | 2968/5000 [14:18:14<9:41:37, 17.17s/it] 59%|█████▉    | 2969/5000 [14:18:33<10:04:56, 17.87s/it]                                                         {'loss': 17.5552, 'grad_norm': 14.0, 'learning_rate': 2.7105412425605056e-05, 'epoch': 3.77}
 59%|█████▉    | 2969/5000 [14:18:33<10:04:56, 17.87s/it] 59%|█████▉    | 2970/5000 [14:18:47<9:25:38, 16.72s/it]                                                         {'loss': 18.1524, 'grad_norm': 11.9375, 'learning_rate': 2.7082862130130096e-05, 'epoch': 3.77}
 59%|█████▉    | 2970/5000 [14:18:47<9:25:38, 16.72s/it] 59%|█████▉    | 2971/5000 [14:19:15<11:11:10, 19.85s/it]                                                         {'loss': 17.3983, 'grad_norm': 6.78125, 'learning_rate': 2.7060315297880203e-05, 'epoch': 3.77}
 59%|█████▉    | 2971/5000 [14:19:15<11:11:10, 19.85s/it] 59%|█████▉    | 2972/5000 [14:19:39<12:00:41, 21.32s/it]                                                         {'loss': 19.6798, 'grad_norm': 20.5, 'learning_rate': 2.7037771938718156e-05, 'epoch': 3.77}
 59%|█████▉    | 2972/5000 [14:19:39<12:00:41, 21.32s/it] 59%|█████▉    | 2973/5000 [14:19:53<10:44:55, 19.09s/it]                                                         {'loss': 18.6773, 'grad_norm': 13.5, 'learning_rate': 2.7015232062505176e-05, 'epoch': 3.78}
 59%|█████▉    | 2973/5000 [14:19:53<10:44:55, 19.09s/it] 59%|█████▉    | 2974/5000 [14:20:08<9:57:11, 17.69s/it]                                                         {'loss': 17.4779, 'grad_norm': 79.5, 'learning_rate': 2.6992695679100954e-05, 'epoch': 3.78}
 59%|█████▉    | 2974/5000 [14:20:08<9:57:11, 17.69s/it] 60%|█████▉    | 2975/5000 [14:20:33<11:13:06, 19.94s/it]                                                         {'loss': 16.76, 'grad_norm': 9.6875, 'learning_rate': 2.6970162798363695e-05, 'epoch': 3.78}
 60%|█████▉    | 2975/5000 [14:20:33<11:13:06, 19.94s/it] 60%|█████▉    | 2976/5000 [14:20:49<10:33:10, 18.77s/it]                                                         {'loss': 18.2973, 'grad_norm': 17.0, 'learning_rate': 2.6947633430150035e-05, 'epoch': 3.78}
 60%|█████▉    | 2976/5000 [14:20:49<10:33:10, 18.77s/it] 60%|█████▉    | 2977/5000 [14:21:09<10:51:14, 19.31s/it]                                                         {'loss': 16.6009, 'grad_norm': 8.875, 'learning_rate': 2.692510758431507e-05, 'epoch': 3.78}
 60%|█████▉    | 2977/5000 [14:21:09<10:51:14, 19.31s/it] 60%|█████▉    | 2978/5000 [14:21:30<11:01:18, 19.62s/it]                                                         {'loss': 17.1761, 'grad_norm': 19.875, 'learning_rate': 2.6902585270712402e-05, 'epoch': 3.78}
 60%|█████▉    | 2978/5000 [14:21:30<11:01:18, 19.62s/it] 60%|█████▉    | 2979/5000 [14:21:49<10:53:16, 19.39s/it]                                                         {'loss': 17.333, 'grad_norm': 10.0, 'learning_rate': 2.688006649919403e-05, 'epoch': 3.78}
 60%|█████▉    | 2979/5000 [14:21:49<10:53:16, 19.39s/it] 60%|█████▉    | 2980/5000 [14:22:04<10:10:24, 18.13s/it]                                                         {'loss': 17.7677, 'grad_norm': 15.9375, 'learning_rate': 2.685755127961044e-05, 'epoch': 3.78}
 60%|█████▉    | 2980/5000 [14:22:04<10:10:24, 18.13s/it] 60%|█████▉    | 2981/5000 [14:22:19<9:40:48, 17.26s/it]                                                         {'loss': 19.3727, 'grad_norm': 18.0, 'learning_rate': 2.6835039621810552e-05, 'epoch': 3.79}
 60%|█████▉    | 2981/5000 [14:22:19<9:40:48, 17.26s/it] 60%|█████▉    | 2982/5000 [14:22:35<9:27:16, 16.87s/it]                                                        {'loss': 19.4615, 'grad_norm': 21.0, 'learning_rate': 2.6812531535641738e-05, 'epoch': 3.79}
 60%|█████▉    | 2982/5000 [14:22:35<9:27:16, 16.87s/it] 60%|█████▉    | 2983/5000 [14:22:52<9:26:32, 16.85s/it]                                                        {'loss': 18.9618, 'grad_norm': 19.875, 'learning_rate': 2.679002703094979e-05, 'epoch': 3.79}
 60%|█████▉    | 2983/5000 [14:22:52<9:26:32, 16.85s/it] 60%|█████▉    | 2984/5000 [14:23:08<9:18:53, 16.63s/it]                                                        {'loss': 19.0001, 'grad_norm': 18.625, 'learning_rate': 2.6767526117578936e-05, 'epoch': 3.79}
 60%|█████▉    | 2984/5000 [14:23:08<9:18:53, 16.63s/it] 60%|█████▉    | 2985/5000 [14:23:24<9:16:58, 16.58s/it]                                                        {'loss': 17.0155, 'grad_norm': 16.25, 'learning_rate': 2.674502880537186e-05, 'epoch': 3.79}
 60%|█████▉    | 2985/5000 [14:23:24<9:16:58, 16.58s/it] 60%|█████▉    | 2986/5000 [14:23:44<9:47:52, 17.51s/it]                                                        {'loss': 15.6106, 'grad_norm': 10.8125, 'learning_rate': 2.6722535104169643e-05, 'epoch': 3.79}
 60%|█████▉    | 2986/5000 [14:23:44<9:47:52, 17.51s/it] 60%|█████▉    | 2987/5000 [14:23:58<9:15:57, 16.57s/it]                                                        {'loss': 19.4645, 'grad_norm': 13.75, 'learning_rate': 2.6700045023811775e-05, 'epoch': 3.79}
 60%|█████▉    | 2987/5000 [14:23:59<9:15:57, 16.57s/it] 60%|█████▉    | 2988/5000 [14:24:16<9:20:36, 16.72s/it]                                                        {'loss': 19.0186, 'grad_norm': 11.625, 'learning_rate': 2.6677558574136213e-05, 'epoch': 3.79}
 60%|█████▉    | 2988/5000 [14:24:16<9:20:36, 16.72s/it] 60%|█████▉    | 2989/5000 [14:24:34<9:41:20, 17.34s/it]                                                        {'loss': 18.0205, 'grad_norm': 12.0625, 'learning_rate': 2.6655075764979273e-05, 'epoch': 3.8}
 60%|█████▉    | 2989/5000 [14:24:34<9:41:20, 17.34s/it] 60%|█████▉    | 2990/5000 [14:24:57<10:39:10, 19.08s/it]                                                         {'loss': 16.799, 'grad_norm': 8.375, 'learning_rate': 2.6632596606175695e-05, 'epoch': 3.8}
 60%|█████▉    | 2990/5000 [14:24:57<10:39:10, 19.08s/it] 60%|█████▉    | 2991/5000 [14:25:21<11:22:40, 20.39s/it]                                                         {'loss': 17.6447, 'grad_norm': 12.875, 'learning_rate': 2.6610121107558655e-05, 'epoch': 3.8}
 60%|█████▉    | 2991/5000 [14:25:21<11:22:40, 20.39s/it] 60%|█████▉    | 2992/5000 [14:25:36<10:24:07, 18.65s/it]                                                         {'loss': 18.5775, 'grad_norm': 16.5, 'learning_rate': 2.658764927895967e-05, 'epoch': 3.8}
 60%|█████▉    | 2992/5000 [14:25:36<10:24:07, 18.65s/it] 60%|█████▉    | 2993/5000 [14:25:53<10:10:36, 18.25s/it]                                                         {'loss': 16.9752, 'grad_norm': 12.1875, 'learning_rate': 2.6565181130208685e-05, 'epoch': 3.8}
 60%|█████▉    | 2993/5000 [14:25:53<10:10:36, 18.25s/it] 60%|█████▉    | 2994/5000 [14:26:06<9:20:40, 16.77s/it]                                                         {'loss': 20.3613, 'grad_norm': 19.25, 'learning_rate': 2.654271667113405e-05, 'epoch': 3.8}
 60%|█████▉    | 2994/5000 [14:26:06<9:20:40, 16.77s/it] 60%|█████▉    | 2995/5000 [14:26:21<9:01:01, 16.19s/it]                                                        {'loss': 17.821, 'grad_norm': 9.0625, 'learning_rate': 2.652025591156248e-05, 'epoch': 3.8}
 60%|█████▉    | 2995/5000 [14:26:21<9:01:01, 16.19s/it] 60%|█████▉    | 2996/5000 [14:26:38<9:03:57, 16.29s/it]                                                        {'loss': 16.8045, 'grad_norm': 8.1875, 'learning_rate': 2.6497798861319046e-05, 'epoch': 3.8}
 60%|█████▉    | 2996/5000 [14:26:38<9:03:57, 16.29s/it] 60%|█████▉    | 2997/5000 [14:26:50<8:28:59, 15.25s/it]                                                        {'loss': 19.2199, 'grad_norm': 16.75, 'learning_rate': 2.647534553022726e-05, 'epoch': 3.81}
 60%|█████▉    | 2997/5000 [14:26:50<8:28:59, 15.25s/it] 60%|█████▉    | 2998/5000 [14:27:06<8:31:03, 15.32s/it]                                                        {'loss': 16.5984, 'grad_norm': 8.125, 'learning_rate': 2.6452895928108957e-05, 'epoch': 3.81}
 60%|█████▉    | 2998/5000 [14:27:06<8:31:03, 15.32s/it] 60%|█████▉    | 2999/5000 [14:27:24<8:56:59, 16.10s/it]                                                        {'loss': 18.321, 'grad_norm': 12.6875, 'learning_rate': 2.643045006478436e-05, 'epoch': 3.81}
 60%|█████▉    | 2999/5000 [14:27:24<8:56:59, 16.10s/it] 60%|██████    | 3000/5000 [14:27:42<9:14:24, 16.63s/it]                                                        {'loss': 18.0386, 'grad_norm': 36.75, 'learning_rate': 2.640800795007203e-05, 'epoch': 3.81}
 60%|██████    | 3000/5000 [14:27:42<9:14:24, 16.63s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:20,  4.43s/it][A
  3%|▎         | 3/88 [00:16<08:04,  5.70s/it][A
  5%|▍         | 4/88 [00:20<07:15,  5.18s/it][A
  6%|▌         | 5/88 [00:24<06:30,  4.71s/it][A
  7%|▋         | 6/88 [00:29<06:42,  4.90s/it][A
  8%|▊         | 7/88 [00:33<06:13,  4.61s/it][A
  9%|▉         | 8/88 [00:37<05:47,  4.34s/it][A
 10%|█         | 9/88 [00:40<05:21,  4.07s/it][A
 11%|█▏        | 10/88 [00:43<04:50,  3.72s/it][A
 12%|█▎        | 11/88 [00:46<04:26,  3.46s/it][A
 14%|█▎        | 12/88 [00:49<03:57,  3.12s/it][A
 15%|█▍        | 13/88 [00:51<03:41,  2.95s/it][A
 16%|█▌        | 14/88 [00:57<04:36,  3.74s/it][A
 17%|█▋        | 15/88 [01:02<05:10,  4.26s/it][A
 18%|█▊        | 16/88 [01:06<04:47,  4.00s/it][A
 19%|█▉        | 17/88 [01:11<05:12,  4.40s/it][A
 20%|██        | 18/88 [01:14<04:42,  4.04s/it][A
 22%|██▏       | 19/88 [01:18<04:43,  4.11s/it][A
 23%|██▎       | 20/88 [01:22<04:33,  4.02s/it][A
 24%|██▍       | 21/88 [01:25<04:14,  3.80s/it][A
 25%|██▌       | 22/88 [01:29<04:11,  3.81s/it][A
 26%|██▌       | 23/88 [01:32<03:44,  3.46s/it][A
 27%|██▋       | 24/88 [01:41<05:20,  5.01s/it][A
 28%|██▊       | 25/88 [01:44<04:46,  4.54s/it][A
 30%|██▉       | 26/88 [01:51<05:17,  5.12s/it][A
 31%|███       | 27/88 [01:55<04:58,  4.89s/it][A
 32%|███▏      | 28/88 [02:04<06:02,  6.05s/it][A
 33%|███▎      | 29/88 [02:09<05:41,  5.79s/it][A
 34%|███▍      | 30/88 [02:14<05:25,  5.62s/it][A
 35%|███▌      | 31/88 [02:18<04:54,  5.17s/it][A
 36%|███▋      | 32/88 [02:27<05:54,  6.33s/it][A
 38%|███▊      | 33/88 [02:32<05:20,  5.83s/it][A
 39%|███▊      | 34/88 [02:41<06:02,  6.71s/it][A
 40%|███▉      | 35/88 [02:44<04:56,  5.59s/it][A
 41%|████      | 36/88 [02:49<04:44,  5.47s/it][A
 42%|████▏     | 37/88 [02:52<04:11,  4.94s/it][A
 43%|████▎     | 38/88 [02:57<03:55,  4.71s/it][A
 44%|████▍     | 39/88 [03:00<03:27,  4.24s/it][A
 45%|████▌     | 40/88 [03:06<03:57,  4.95s/it][A
 47%|████▋     | 41/88 [03:16<04:58,  6.36s/it][A
 48%|████▊     | 42/88 [03:20<04:13,  5.51s/it][A
 49%|████▉     | 43/88 [03:28<04:49,  6.44s/it][A
 50%|█████     | 44/88 [03:32<04:02,  5.50s/it][A
 51%|█████     | 45/88 [03:36<03:44,  5.22s/it][A
 52%|█████▏    | 46/88 [03:40<03:22,  4.83s/it][A
 53%|█████▎    | 47/88 [03:42<02:44,  4.02s/it][A
 55%|█████▍    | 48/88 [03:44<02:20,  3.51s/it][A
 56%|█████▌    | 49/88 [03:49<02:34,  3.96s/it][A
 57%|█████▋    | 50/88 [03:54<02:32,  4.02s/it][A
 58%|█████▊    | 51/88 [03:58<02:30,  4.07s/it][A
 59%|█████▉    | 52/88 [04:04<02:45,  4.59s/it][A
 60%|██████    | 53/88 [04:09<02:45,  4.72s/it][A
 61%|██████▏   | 54/88 [04:18<03:30,  6.20s/it][A
 62%|██████▎   | 55/88 [04:23<03:12,  5.83s/it][A
 64%|██████▎   | 56/88 [04:26<02:34,  4.84s/it][A
 65%|██████▍   | 57/88 [04:30<02:21,  4.57s/it][A
 66%|██████▌   | 58/88 [04:39<02:55,  5.85s/it][A
 67%|██████▋   | 59/88 [04:43<02:40,  5.54s/it][A
 68%|██████▊   | 60/88 [04:47<02:15,  4.86s/it][A
 69%|██████▉   | 61/88 [04:51<02:04,  4.62s/it][A
 70%|███████   | 62/88 [04:54<01:47,  4.13s/it][A
 72%|███████▏  | 63/88 [04:59<01:53,  4.55s/it][A
 73%|███████▎  | 64/88 [05:03<01:45,  4.40s/it][A
 74%|███████▍  | 65/88 [05:08<01:40,  4.39s/it][A
 75%|███████▌  | 66/88 [05:11<01:32,  4.20s/it][A
 76%|███████▌  | 67/88 [05:14<01:21,  3.86s/it][A
 77%|███████▋  | 68/88 [05:19<01:23,  4.17s/it][A
 78%|███████▊  | 69/88 [05:26<01:31,  4.81s/it][A
 80%|███████▉  | 70/88 [05:30<01:22,  4.56s/it][A
 81%|████████  | 71/88 [05:33<01:13,  4.33s/it][A
 82%|████████▏ | 72/88 [05:37<01:07,  4.20s/it][A
 83%|████████▎ | 73/88 [05:40<00:57,  3.86s/it][A
 84%|████████▍ | 74/88 [05:43<00:49,  3.54s/it][A
 85%|████████▌ | 75/88 [05:48<00:50,  3.91s/it][A
 86%|████████▋ | 76/88 [05:52<00:46,  3.89s/it][A
 88%|████████▊ | 77/88 [05:59<00:53,  4.86s/it][A
 89%|████████▊ | 78/88 [06:03<00:45,  4.57s/it][A
 90%|████████▉ | 79/88 [06:08<00:41,  4.61s/it][A
 91%|█████████ | 80/88 [06:10<00:32,  4.11s/it][A
 92%|█████████▏| 81/88 [06:15<00:30,  4.38s/it][A
 93%|█████████▎| 82/88 [06:19<00:25,  4.23s/it][A
 94%|█████████▍| 83/88 [06:23<00:20,  4.10s/it][A
 95%|█████████▌| 84/88 [06:27<00:16,  4.04s/it][A
 97%|█████████▋| 85/88 [06:30<00:11,  3.71s/it][A
 98%|█████████▊| 86/88 [06:33<00:06,  3.45s/it][A
 99%|█████████▉| 87/88 [06:37<00:03,  3.64s/it][A
100%|██████████| 88/88 [06:41<00:00,  3.70s/it][A                                                        
                                               [A{'eval_loss': 17.86752700805664, 'eval_runtime': 405.211, 'eval_samples_per_second': 6.91, 'eval_steps_per_second': 0.217, 'epoch': 3.81}
 60%|██████    | 3000/5000 [14:34:27<9:14:24, 16.63s/it]
100%|██████████| 88/88 [06:41<00:00,  3.70s/it][A
                                               [A2024-06-14 00:10:04,485 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-14 00:10:14,958 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 60%|██████    | 3001/5000 [14:34:56<78:44:35, 141.81s/it]                                                          {'loss': 18.743, 'grad_norm': 47.5, 'learning_rate': 2.6385569593788943e-05, 'epoch': 3.81}
 60%|██████    | 3001/5000 [14:34:56<78:44:35, 141.81s/it] 60%|██████    | 3002/5000 [14:35:09<57:20:13, 103.31s/it]                                                          {'loss': 18.2886, 'grad_norm': 66.5, 'learning_rate': 2.6363135005750375e-05, 'epoch': 3.81}
 60%|██████    | 3002/5000 [14:35:09<57:20:13, 103.31s/it] 60%|██████    | 3003/5000 [14:35:24<42:33:22, 76.72s/it]                                                          {'loss': 17.4391, 'grad_norm': 39.5, 'learning_rate': 2.6340704195769986e-05, 'epoch': 3.81}
 60%|██████    | 3003/5000 [14:35:24<42:33:22, 76.72s/it] 60%|██████    | 3004/5000 [14:35:46<33:31:11, 60.46s/it]                                                         {'loss': 17.3598, 'grad_norm': 18.0, 'learning_rate': 2.6318277173659774e-05, 'epoch': 3.81}
 60%|██████    | 3004/5000 [14:35:46<33:31:11, 60.46s/it] 60%|██████    | 3005/5000 [14:36:01<25:56:56, 46.83s/it]                                                         {'loss': 18.972, 'grad_norm': 13.0, 'learning_rate': 2.6295853949230086e-05, 'epoch': 3.82}
 60%|██████    | 3005/5000 [14:36:01<25:56:56, 46.83s/it] 60%|██████    | 3006/5000 [14:36:16<20:33:02, 37.10s/it]                                                         {'loss': 18.3583, 'grad_norm': 13.5625, 'learning_rate': 2.6273434532289583e-05, 'epoch': 3.82}
 60%|██████    | 3006/5000 [14:36:16<20:33:02, 37.10s/it] 60%|██████    | 3007/5000 [14:36:31<16:51:57, 30.47s/it]                                                         {'loss': 18.15, 'grad_norm': 10.3125, 'learning_rate': 2.6251018932645307e-05, 'epoch': 3.82}
 60%|██████    | 3007/5000 [14:36:31<16:51:57, 30.47s/it] 60%|██████    | 3008/5000 [14:36:47<14:30:18, 26.21s/it]                                                         {'loss': 18.6984, 'grad_norm': 15.5, 'learning_rate': 2.622860716010259e-05, 'epoch': 3.82}
 60%|██████    | 3008/5000 [14:36:47<14:30:18, 26.21s/it] 60%|██████    | 3009/5000 [14:37:06<13:22:14, 24.18s/it]                                                         {'loss': 19.3136, 'grad_norm': 22.875, 'learning_rate': 2.6206199224465087e-05, 'epoch': 3.82}
 60%|██████    | 3009/5000 [14:37:06<13:22:14, 24.18s/it] 60%|██████    | 3010/5000 [14:37:27<12:51:44, 23.27s/it]                                                         {'loss': 18.8571, 'grad_norm': 15.125, 'learning_rate': 2.618379513553482e-05, 'epoch': 3.82}
 60%|██████    | 3010/5000 [14:37:27<12:51:44, 23.27s/it] 60%|██████    | 3011/5000 [14:37:42<11:26:32, 20.71s/it]                                                         {'loss': 19.254, 'grad_norm': 16.875, 'learning_rate': 2.6161394903112082e-05, 'epoch': 3.82}
 60%|██████    | 3011/5000 [14:37:42<11:26:32, 20.71s/it] 60%|██████    | 3012/5000 [14:37:56<10:19:33, 18.70s/it]                                                         {'loss': 18.5401, 'grad_norm': 72.5, 'learning_rate': 2.6138998536995486e-05, 'epoch': 3.82}
 60%|██████    | 3012/5000 [14:37:56<10:19:33, 18.70s/it] 60%|██████    | 3013/5000 [14:38:22<11:30:31, 20.85s/it]                                                         {'loss': 18.0769, 'grad_norm': 13.0625, 'learning_rate': 2.6116606046981983e-05, 'epoch': 3.83}
 60%|██████    | 3013/5000 [14:38:22<11:30:31, 20.85s/it] 60%|██████    | 3014/5000 [14:38:41<11:12:04, 20.30s/it]                                                         {'loss': 17.4686, 'grad_norm': 11.125, 'learning_rate': 2.6094217442866797e-05, 'epoch': 3.83}
 60%|██████    | 3014/5000 [14:38:41<11:12:04, 20.30s/it] 60%|██████    | 3015/5000 [14:39:05<11:52:05, 21.52s/it]                                                         {'loss': 16.3151, 'grad_norm': 11.0, 'learning_rate': 2.607183273444347e-05, 'epoch': 3.83}
 60%|██████    | 3015/5000 [14:39:05<11:52:05, 21.52s/it] 60%|██████    | 3016/5000 [14:39:20<10:43:20, 19.46s/it]                                                         {'loss': 19.3951, 'grad_norm': 16.875, 'learning_rate': 2.6049451931503827e-05, 'epoch': 3.83}
 60%|██████    | 3016/5000 [14:39:20<10:43:20, 19.46s/it] 60%|██████    | 3017/5000 [14:39:38<10:22:45, 18.84s/it]                                                         {'loss': 18.1162, 'grad_norm': 14.125, 'learning_rate': 2.602707504383801e-05, 'epoch': 3.83}
 60%|██████    | 3017/5000 [14:39:38<10:22:45, 18.84s/it] 60%|██████    | 3018/5000 [14:39:51<9:31:52, 17.31s/it]                                                         {'loss': 18.4116, 'grad_norm': 16.375, 'learning_rate': 2.600470208123441e-05, 'epoch': 3.83}
 60%|██████    | 3018/5000 [14:39:51<9:31:52, 17.31s/it] 60%|██████    | 3019/5000 [14:40:08<9:28:21, 17.21s/it]                                                        {'loss': 18.1199, 'grad_norm': 11.5625, 'learning_rate': 2.5982333053479713e-05, 'epoch': 3.83}
 60%|██████    | 3019/5000 [14:40:08<9:28:21, 17.21s/it] 60%|██████    | 3020/5000 [14:40:30<10:11:17, 18.52s/it]                                                         {'loss': 17.6124, 'grad_norm': 24.5, 'learning_rate': 2.5959967970358924e-05, 'epoch': 3.83}
 60%|██████    | 3020/5000 [14:40:30<10:11:17, 18.52s/it] 60%|██████    | 3021/5000 [14:40:46<9:52:37, 17.97s/it]                                                         {'loss': 30.0318, 'grad_norm': 286.0, 'learning_rate': 2.593760684165527e-05, 'epoch': 3.84}
 60%|██████    | 3021/5000 [14:40:46<9:52:37, 17.97s/it] 60%|██████    | 3022/5000 [14:41:04<9:47:14, 17.81s/it]                                                        {'loss': 17.7965, 'grad_norm': 15.125, 'learning_rate': 2.5915249677150254e-05, 'epoch': 3.84}
 60%|██████    | 3022/5000 [14:41:04<9:47:14, 17.81s/it] 60%|██████    | 3023/5000 [14:41:17<9:00:47, 16.41s/it]                                                        {'loss': 18.6217, 'grad_norm': 17.375, 'learning_rate': 2.589289648662369e-05, 'epoch': 3.84}
 60%|██████    | 3023/5000 [14:41:17<9:00:47, 16.41s/it] 60%|██████    | 3024/5000 [14:41:31<8:33:20, 15.59s/it]                                                        {'loss': 18.7325, 'grad_norm': 16.125, 'learning_rate': 2.5870547279853615e-05, 'epoch': 3.84}
 60%|██████    | 3024/5000 [14:41:31<8:33:20, 15.59s/it] 60%|██████    | 3025/5000 [14:41:44<8:06:35, 14.78s/it]                                                        {'loss': 19.4568, 'grad_norm': 15.25, 'learning_rate': 2.5848202066616305e-05, 'epoch': 3.84}
 60%|██████    | 3025/5000 [14:41:44<8:06:35, 14.78s/it] 61%|██████    | 3026/5000 [14:42:00<8:20:18, 15.21s/it]                                                        {'loss': 18.5732, 'grad_norm': 13.375, 'learning_rate': 2.582586085668635e-05, 'epoch': 3.84}
 61%|██████    | 3026/5000 [14:42:00<8:20:18, 15.21s/it] 61%|██████    | 3027/5000 [14:42:24<9:49:56, 17.94s/it]                                                        {'loss': 17.5272, 'grad_norm': 19.0, 'learning_rate': 2.5803523659836536e-05, 'epoch': 3.84}
 61%|██████    | 3027/5000 [14:42:24<9:49:56, 17.94s/it] 61%|██████    | 3028/5000 [14:42:50<11:06:54, 20.29s/it]                                                         {'loss': 18.9395, 'grad_norm': 32.25, 'learning_rate': 2.5781190485837914e-05, 'epoch': 3.85}
 61%|██████    | 3028/5000 [14:42:50<11:06:54, 20.29s/it] 61%|██████    | 3029/5000 [14:43:05<10:12:05, 18.63s/it]                                                         {'loss': 17.0918, 'grad_norm': 25.75, 'learning_rate': 2.5758861344459773e-05, 'epoch': 3.85}
 61%|██████    | 3029/5000 [14:43:05<10:12:05, 18.63s/it] 61%|██████    | 3030/5000 [14:43:18<9:22:25, 17.13s/it]                                                         {'loss': 18.3122, 'grad_norm': 24.5, 'learning_rate': 2.5736536245469645e-05, 'epoch': 3.85}
 61%|██████    | 3030/5000 [14:43:18<9:22:25, 17.13s/it] 61%|██████    | 3031/5000 [14:43:37<9:33:20, 17.47s/it]                                                        {'loss': 17.1894, 'grad_norm': 27.75, 'learning_rate': 2.571421519863327e-05, 'epoch': 3.85}
 61%|██████    | 3031/5000 [14:43:37<9:33:20, 17.47s/it] 61%|██████    | 3032/5000 [14:43:54<9:27:45, 17.31s/it]                                                        {'loss': 18.4384, 'grad_norm': 20.5, 'learning_rate': 2.569189821371466e-05, 'epoch': 3.85}
 61%|██████    | 3032/5000 [14:43:54<9:27:45, 17.31s/it] 61%|██████    | 3033/5000 [14:44:06<8:40:36, 15.88s/it]                                                        {'loss': 19.545, 'grad_norm': 22.75, 'learning_rate': 2.5669585300476e-05, 'epoch': 3.85}
 61%|██████    | 3033/5000 [14:44:06<8:40:36, 15.88s/it] 61%|██████    | 3034/5000 [14:44:22<8:40:21, 15.88s/it]                                                        {'loss': 18.3245, 'grad_norm': 42.0, 'learning_rate': 2.564727646867773e-05, 'epoch': 3.85}
 61%|██████    | 3034/5000 [14:44:22<8:40:21, 15.88s/it] 61%|██████    | 3035/5000 [14:44:42<9:16:05, 16.98s/it]                                                        {'loss': 17.8508, 'grad_norm': 14.3125, 'learning_rate': 2.5624971728078463e-05, 'epoch': 3.85}
 61%|██████    | 3035/5000 [14:44:42<9:16:05, 16.98s/it] 61%|██████    | 3036/5000 [14:45:04<10:11:40, 18.69s/it]                                                         {'loss': 17.4135, 'grad_norm': 10.125, 'learning_rate': 2.5602671088435083e-05, 'epoch': 3.86}
 61%|██████    | 3036/5000 [14:45:04<10:11:40, 18.69s/it] 61%|██████    | 3037/5000 [14:45:21<9:54:49, 18.18s/it]                                                         {'loss': 19.7263, 'grad_norm': 82.0, 'learning_rate': 2.5580374559502635e-05, 'epoch': 3.86}
 61%|██████    | 3037/5000 [14:45:21<9:54:49, 18.18s/it] 61%|██████    | 3038/5000 [14:45:40<10:05:28, 18.52s/it]                                                         {'loss': 17.6526, 'grad_norm': 10.6875, 'learning_rate': 2.5558082151034373e-05, 'epoch': 3.86}
 61%|██████    | 3038/5000 [14:45:40<10:05:28, 18.52s/it] 61%|██████    | 3039/5000 [14:45:53<9:08:30, 16.78s/it]                                                         {'loss': 18.2384, 'grad_norm': 12.6875, 'learning_rate': 2.553579387278176e-05, 'epoch': 3.86}
 61%|██████    | 3039/5000 [14:45:53<9:08:30, 16.78s/it] 61%|██████    | 3040/5000 [14:46:08<8:52:20, 16.30s/it]                                                        {'loss': 18.5179, 'grad_norm': 11.375, 'learning_rate': 2.5513509734494456e-05, 'epoch': 3.86}
 61%|██████    | 3040/5000 [14:46:08<8:52:20, 16.30s/it] 61%|██████    | 3041/5000 [14:46:24<8:46:05, 16.11s/it]                                                        {'loss': 18.4306, 'grad_norm': 12.375, 'learning_rate': 2.549122974592027e-05, 'epoch': 3.86}
 61%|██████    | 3041/5000 [14:46:24<8:46:05, 16.11s/it] 61%|██████    | 3042/5000 [14:46:45<9:28:41, 17.43s/it]                                                        {'loss': 17.2485, 'grad_norm': 19.5, 'learning_rate': 2.5468953916805265e-05, 'epoch': 3.86}
 61%|██████    | 3042/5000 [14:46:45<9:28:41, 17.43s/it] 61%|██████    | 3043/5000 [14:46:58<8:53:04, 16.34s/it]                                                        {'loss': 20.7381, 'grad_norm': 17.125, 'learning_rate': 2.544668225689362e-05, 'epoch': 3.86}
 61%|██████    | 3043/5000 [14:46:58<8:53:04, 16.34s/it] 61%|██████    | 3044/5000 [14:47:14<8:47:41, 16.19s/it]                                                        {'loss': 18.5488, 'grad_norm': 12.375, 'learning_rate': 2.5424414775927708e-05, 'epoch': 3.87}
 61%|██████    | 3044/5000 [14:47:14<8:47:41, 16.19s/it] 61%|██████    | 3045/5000 [14:47:41<10:27:33, 19.26s/it]                                                         {'loss': 18.6891, 'grad_norm': 20.75, 'learning_rate': 2.5402151483648113e-05, 'epoch': 3.87}
 61%|██████    | 3045/5000 [14:47:41<10:27:33, 19.26s/it] 61%|██████    | 3046/5000 [14:47:58<10:07:43, 18.66s/it]                                                         {'loss': 18.8976, 'grad_norm': 25.5, 'learning_rate': 2.5379892389793536e-05, 'epoch': 3.87}
 61%|██████    | 3046/5000 [14:47:58<10:07:43, 18.66s/it] 61%|██████    | 3047/5000 [14:48:20<10:43:07, 19.76s/it]                                                         {'loss': 18.1066, 'grad_norm': 10.5625, 'learning_rate': 2.5357637504100848e-05, 'epoch': 3.87}
 61%|██████    | 3047/5000 [14:48:20<10:43:07, 19.76s/it] 61%|██████    | 3048/5000 [14:48:35<9:56:03, 18.32s/it]                                                         {'loss': 17.2954, 'grad_norm': 17.0, 'learning_rate': 2.533538683630512e-05, 'epoch': 3.87}
 61%|██████    | 3048/5000 [14:48:35<9:56:03, 18.32s/it] 61%|██████    | 3049/5000 [14:49:01<11:13:14, 20.70s/it]                                                         {'loss': 16.936, 'grad_norm': 24.625, 'learning_rate': 2.5313140396139538e-05, 'epoch': 3.87}
 61%|██████    | 3049/5000 [14:49:01<11:13:14, 20.70s/it] 61%|██████    | 3050/5000 [14:49:18<10:32:49, 19.47s/it]                                                         {'loss': 17.9444, 'grad_norm': 26.625, 'learning_rate': 2.5290898193335446e-05, 'epoch': 3.87}
 61%|██████    | 3050/5000 [14:49:18<10:32:49, 19.47s/it] 61%|██████    | 3051/5000 [14:49:36<10:22:47, 19.17s/it]                                                         {'loss': 16.9066, 'grad_norm': 10.0, 'learning_rate': 2.5268660237622332e-05, 'epoch': 3.87}
 61%|██████    | 3051/5000 [14:49:37<10:22:47, 19.17s/it] 61%|██████    | 3052/5000 [14:49:56<10:23:26, 19.20s/it]                                                         {'loss': 17.4974, 'grad_norm': 19.0, 'learning_rate': 2.5246426538727856e-05, 'epoch': 3.88}
 61%|██████    | 3052/5000 [14:49:56<10:23:26, 19.20s/it] 61%|██████    | 3053/5000 [14:50:24<11:47:44, 21.81s/it]                                                         {'loss': 16.5687, 'grad_norm': 10.0625, 'learning_rate': 2.522419710637778e-05, 'epoch': 3.88}
 61%|██████    | 3053/5000 [14:50:24<11:47:44, 21.81s/it] 61%|██████    | 3054/5000 [14:50:37<10:27:43, 19.35s/it]                                                         {'loss': 18.6199, 'grad_norm': 38.25, 'learning_rate': 2.5201971950296002e-05, 'epoch': 3.88}
 61%|██████    | 3054/5000 [14:50:37<10:27:43, 19.35s/it] 61%|██████    | 3055/5000 [14:50:53<9:49:26, 18.18s/it]                                                         {'loss': 19.8018, 'grad_norm': 25.75, 'learning_rate': 2.5179751080204586e-05, 'epoch': 3.88}
 61%|██████    | 3055/5000 [14:50:53<9:49:26, 18.18s/it] 61%|██████    | 3056/5000 [14:51:08<9:17:17, 17.20s/it]                                                        {'loss': 18.0339, 'grad_norm': 14.1875, 'learning_rate': 2.5157534505823682e-05, 'epoch': 3.88}
 61%|██████    | 3056/5000 [14:51:08<9:17:17, 17.20s/it] 61%|██████    | 3057/5000 [14:51:21<8:35:57, 15.93s/it]                                                        {'loss': 18.626, 'grad_norm': 15.0625, 'learning_rate': 2.5135322236871564e-05, 'epoch': 3.88}
 61%|██████    | 3057/5000 [14:51:21<8:35:57, 15.93s/it] 61%|██████    | 3058/5000 [14:51:37<8:42:20, 16.14s/it]                                                        {'loss': 19.1153, 'grad_norm': 24.0, 'learning_rate': 2.5113114283064655e-05, 'epoch': 3.88}
 61%|██████    | 3058/5000 [14:51:37<8:42:20, 16.14s/it] 61%|██████    | 3059/5000 [14:51:51<8:17:47, 15.39s/it]                                                        {'loss': 18.1543, 'grad_norm': 28.5, 'learning_rate': 2.5090910654117454e-05, 'epoch': 3.88}
 61%|██████    | 3059/5000 [14:51:51<8:17:47, 15.39s/it] 61%|██████    | 3060/5000 [14:52:16<9:53:53, 18.37s/it]                                                        {'loss': 16.1338, 'grad_norm': 14.75, 'learning_rate': 2.5068711359742573e-05, 'epoch': 3.89}
 61%|██████    | 3060/5000 [14:52:16<9:53:53, 18.37s/it] 61%|██████    | 3061/5000 [14:52:46<11:41:55, 21.72s/it]                                                         {'loss': 16.9746, 'grad_norm': 10.625, 'learning_rate': 2.504651640965077e-05, 'epoch': 3.89}
 61%|██████    | 3061/5000 [14:52:46<11:41:55, 21.72s/it] 61%|██████    | 3062/5000 [14:53:03<10:59:13, 20.41s/it]                                                         {'loss': 18.5159, 'grad_norm': 12.0, 'learning_rate': 2.5024325813550835e-05, 'epoch': 3.89}
 61%|██████    | 3062/5000 [14:53:03<10:59:13, 20.41s/it] 61%|██████▏   | 3063/5000 [14:53:30<12:05:43, 22.48s/it]                                                         {'loss': 16.4752, 'grad_norm': 10.75, 'learning_rate': 2.5002139581149707e-05, 'epoch': 3.89}
 61%|██████▏   | 3063/5000 [14:53:30<12:05:43, 22.48s/it] 61%|██████▏   | 3064/5000 [14:53:46<10:55:25, 20.31s/it]                                                         {'loss': 18.8789, 'grad_norm': 14.375, 'learning_rate': 2.4979957722152393e-05, 'epoch': 3.89}
 61%|██████▏   | 3064/5000 [14:53:46<10:55:25, 20.31s/it] 61%|██████▏   | 3065/5000 [14:54:02<10:15:00, 19.07s/it]                                                         {'loss': 17.7367, 'grad_norm': 10.9375, 'learning_rate': 2.4957780246262004e-05, 'epoch': 3.89}
 61%|██████▏   | 3065/5000 [14:54:02<10:15:00, 19.07s/it] 61%|██████▏   | 3066/5000 [14:54:22<10:28:34, 19.50s/it]                                                         {'loss': 18.0309, 'grad_norm': 12.0625, 'learning_rate': 2.49356071631797e-05, 'epoch': 3.89}
 61%|██████▏   | 3066/5000 [14:54:22<10:28:34, 19.50s/it] 61%|██████▏   | 3067/5000 [14:54:41<10:18:27, 19.20s/it]                                                         {'loss': 17.7985, 'grad_norm': 13.25, 'learning_rate': 2.491343848260476e-05, 'epoch': 3.89}
 61%|██████▏   | 3067/5000 [14:54:41<10:18:27, 19.20s/it] 61%|██████▏   | 3068/5000 [14:54:58<9:59:25, 18.62s/it]                                                         {'loss': 20.2024, 'grad_norm': 23.25, 'learning_rate': 2.489127421423451e-05, 'epoch': 3.9}
 61%|██████▏   | 3068/5000 [14:54:58<9:59:25, 18.62s/it] 61%|██████▏   | 3069/5000 [14:55:24<11:08:25, 20.77s/it]                                                         {'loss': 16.9631, 'grad_norm': 18.875, 'learning_rate': 2.4869114367764356e-05, 'epoch': 3.9}
 61%|██████▏   | 3069/5000 [14:55:24<11:08:25, 20.77s/it] 61%|██████▏   | 3070/5000 [14:55:38<10:00:28, 18.67s/it]                                                         {'loss': 19.7967, 'grad_norm': 36.75, 'learning_rate': 2.4846958952887752e-05, 'epoch': 3.9}
 61%|██████▏   | 3070/5000 [14:55:38<10:00:28, 18.67s/it] 61%|██████▏   | 3071/5000 [14:55:51<9:06:09, 16.99s/it]                                                         {'loss': 18.4787, 'grad_norm': 11.875, 'learning_rate': 2.4824807979296257e-05, 'epoch': 3.9}
 61%|██████▏   | 3071/5000 [14:55:51<9:06:09, 16.99s/it] 61%|██████▏   | 3072/5000 [14:56:07<9:03:33, 16.92s/it]                                                        {'loss': 17.7844, 'grad_norm': 18.25, 'learning_rate': 2.4802661456679442e-05, 'epoch': 3.9}
 61%|██████▏   | 3072/5000 [14:56:07<9:03:33, 16.92s/it] 61%|██████▏   | 3073/5000 [14:56:23<8:54:04, 16.63s/it]                                                        {'loss': 19.1155, 'grad_norm': 19.125, 'learning_rate': 2.478051939472494e-05, 'epoch': 3.9}
 61%|██████▏   | 3073/5000 [14:56:23<8:54:04, 16.63s/it] 61%|██████▏   | 3074/5000 [14:56:37<8:24:02, 15.70s/it]                                                        {'loss': 19.3881, 'grad_norm': 15.6875, 'learning_rate': 2.4758381803118468e-05, 'epoch': 3.9}
 61%|██████▏   | 3074/5000 [14:56:37<8:24:02, 15.70s/it] 62%|██████▏   | 3075/5000 [14:56:58<9:13:06, 17.24s/it]                                                        {'loss': 18.3832, 'grad_norm': 14.875, 'learning_rate': 2.4736248691543736e-05, 'epoch': 3.9}
 62%|██████▏   | 3075/5000 [14:56:58<9:13:06, 17.24s/it] 62%|██████▏   | 3076/5000 [14:57:20<10:04:30, 18.85s/it]                                                         {'loss': 17.2036, 'grad_norm': 12.5, 'learning_rate': 2.4714120069682527e-05, 'epoch': 3.91}
 62%|██████▏   | 3076/5000 [14:57:20<10:04:30, 18.85s/it] 62%|██████▏   | 3077/5000 [14:57:49<11:42:40, 21.92s/it]                                                         {'loss': 16.6817, 'grad_norm': 13.0625, 'learning_rate': 2.4691995947214654e-05, 'epoch': 3.91}
 62%|██████▏   | 3077/5000 [14:57:49<11:42:40, 21.92s/it] 62%|██████▏   | 3078/5000 [14:58:06<10:47:51, 20.22s/it]                                                         {'loss': 18.1987, 'grad_norm': 18.5, 'learning_rate': 2.4669876333817968e-05, 'epoch': 3.91}
 62%|██████▏   | 3078/5000 [14:58:06<10:47:51, 20.22s/it] 62%|██████▏   | 3079/5000 [14:58:21<9:59:22, 18.72s/it]                                                         {'loss': 18.6822, 'grad_norm': 22.0, 'learning_rate': 2.464776123916832e-05, 'epoch': 3.91}
 62%|██████▏   | 3079/5000 [14:58:21<9:59:22, 18.72s/it] 62%|██████▏   | 3080/5000 [14:58:41<10:10:23, 19.07s/it]                                                         {'loss': 18.3806, 'grad_norm': 13.375, 'learning_rate': 2.462565067293962e-05, 'epoch': 3.91}
 62%|██████▏   | 3080/5000 [14:58:41<10:10:23, 19.07s/it] 62%|██████▏   | 3081/5000 [14:58:59<10:05:19, 18.93s/it]                                                         {'loss': 18.8809, 'grad_norm': 16.75, 'learning_rate': 2.4603544644803783e-05, 'epoch': 3.91}
 62%|██████▏   | 3081/5000 [14:58:59<10:05:19, 18.93s/it] 62%|██████▏   | 3082/5000 [14:59:21<10:28:35, 19.66s/it]                                                         {'loss': 23.7689, 'grad_norm': 60.25, 'learning_rate': 2.458144316443071e-05, 'epoch': 3.91}
 62%|██████▏   | 3082/5000 [14:59:21<10:28:35, 19.66s/it] 62%|██████▏   | 3083/5000 [14:59:38<10:06:46, 18.99s/it]                                                         {'loss': 17.5933, 'grad_norm': 8.5625, 'learning_rate': 2.455934624148837e-05, 'epoch': 3.91}
 62%|██████▏   | 3083/5000 [14:59:38<10:06:46, 18.99s/it] 62%|██████▏   | 3084/5000 [14:59:58<10:13:42, 19.22s/it]                                                         {'loss': 18.3288, 'grad_norm': 16.25, 'learning_rate': 2.4537253885642706e-05, 'epoch': 3.92}
 62%|██████▏   | 3084/5000 [14:59:58<10:13:42, 19.22s/it] 62%|██████▏   | 3085/5000 [15:00:24<11:14:25, 21.13s/it]                                                         {'loss': 18.0076, 'grad_norm': 18.625, 'learning_rate': 2.451516610655764e-05, 'epoch': 3.92}
 62%|██████▏   | 3085/5000 [15:00:24<11:14:25, 21.13s/it] 62%|██████▏   | 3086/5000 [15:00:44<11:11:42, 21.06s/it]                                                         {'loss': 15.5251, 'grad_norm': 6.9375, 'learning_rate': 2.4493082913895135e-05, 'epoch': 3.92}
 62%|██████▏   | 3086/5000 [15:00:44<11:11:42, 21.06s/it] 62%|██████▏   | 3087/5000 [15:00:59<10:13:25, 19.24s/it]                                                         {'loss': 18.1395, 'grad_norm': 14.375, 'learning_rate': 2.447100431731512e-05, 'epoch': 3.92}
 62%|██████▏   | 3087/5000 [15:00:59<10:13:25, 19.24s/it] 62%|██████▏   | 3088/5000 [15:01:18<10:07:03, 19.05s/it]                                                         {'loss': 18.0356, 'grad_norm': 19.25, 'learning_rate': 2.444893032647555e-05, 'epoch': 3.92}
 62%|██████▏   | 3088/5000 [15:01:18<10:07:03, 19.05s/it] 62%|██████▏   | 3089/5000 [15:01:35<9:49:29, 18.51s/it]                                                         {'loss': 18.0195, 'grad_norm': 20.0, 'learning_rate': 2.4426860951032295e-05, 'epoch': 3.92}
 62%|██████▏   | 3089/5000 [15:01:35<9:49:29, 18.51s/it] 62%|██████▏   | 3090/5000 [15:01:50<9:16:23, 17.48s/it]                                                        {'loss': 18.1406, 'grad_norm': 11.125, 'learning_rate': 2.440479620063929e-05, 'epoch': 3.92}
 62%|██████▏   | 3090/5000 [15:01:50<9:16:23, 17.48s/it] 62%|██████▏   | 3091/5000 [15:02:16<10:30:17, 19.81s/it]                                                         {'loss': 17.1105, 'grad_norm': 19.125, 'learning_rate': 2.438273608494839e-05, 'epoch': 3.93}
 62%|██████▏   | 3091/5000 [15:02:16<10:30:17, 19.81s/it] 62%|██████▏   | 3092/5000 [15:02:34<10:12:16, 19.25s/it]                                                         {'loss': 19.2087, 'grad_norm': 24.875, 'learning_rate': 2.436068061360942e-05, 'epoch': 3.93}
 62%|██████▏   | 3092/5000 [15:02:34<10:12:16, 19.25s/it] 62%|██████▏   | 3093/5000 [15:02:58<11:03:40, 20.88s/it]                                                         {'loss': 16.3781, 'grad_norm': 12.4375, 'learning_rate': 2.4338629796270233e-05, 'epoch': 3.93}
 62%|██████▏   | 3093/5000 [15:02:58<11:03:40, 20.88s/it] 62%|██████▏   | 3094/5000 [15:03:12<9:51:12, 18.61s/it]                                                         {'loss': 18.5199, 'grad_norm': 12.3125, 'learning_rate': 2.431658364257658e-05, 'epoch': 3.93}
 62%|██████▏   | 3094/5000 [15:03:12<9:51:12, 18.61s/it] 62%|██████▏   | 3095/5000 [15:03:27<9:23:56, 17.76s/it]                                                        {'loss': 18.1586, 'grad_norm': 10.5625, 'learning_rate': 2.429454216217219e-05, 'epoch': 3.93}
 62%|██████▏   | 3095/5000 [15:03:27<9:23:56, 17.76s/it] 62%|██████▏   | 3096/5000 [15:03:41<8:39:54, 16.38s/it]                                                        {'loss': 19.1283, 'grad_norm': 23.875, 'learning_rate': 2.4272505364698776e-05, 'epoch': 3.93}
 62%|██████▏   | 3096/5000 [15:03:41<8:39:54, 16.38s/it] 62%|██████▏   | 3097/5000 [15:03:56<8:29:57, 16.08s/it]                                                        {'loss': 19.6985, 'grad_norm': 23.0, 'learning_rate': 2.4250473259795973e-05, 'epoch': 3.93}
 62%|██████▏   | 3097/5000 [15:03:56<8:29:57, 16.08s/it] 62%|██████▏   | 3098/5000 [15:04:10<8:10:27, 15.47s/it]                                                        {'loss': 19.5107, 'grad_norm': 39.5, 'learning_rate': 2.4228445857101363e-05, 'epoch': 3.93}
 62%|██████▏   | 3098/5000 [15:04:10<8:10:27, 15.47s/it] 62%|██████▏   | 3099/5000 [15:04:29<8:40:39, 16.43s/it]                                                        {'loss': 18.1363, 'grad_norm': 25.25, 'learning_rate': 2.42064231662505e-05, 'epoch': 3.94}
 62%|██████▏   | 3099/5000 [15:04:29<8:40:39, 16.43s/it] 62%|██████▏   | 3100/5000 [15:04:45<8:36:32, 16.31s/it]                                                        {'loss': 17.0895, 'grad_norm': 16.625, 'learning_rate': 2.4184405196876842e-05, 'epoch': 3.94}
 62%|██████▏   | 3100/5000 [15:04:45<8:36:32, 16.31s/it] 62%|██████▏   | 3101/5000 [15:04:58<8:03:46, 15.28s/it]                                                        {'loss': 19.1678, 'grad_norm': 24.125, 'learning_rate': 2.4162391958611797e-05, 'epoch': 3.94}
 62%|██████▏   | 3101/5000 [15:04:58<8:03:46, 15.28s/it] 62%|██████▏   | 3102/5000 [15:05:13<8:02:37, 15.26s/it]                                                        {'loss': 19.402, 'grad_norm': 18.625, 'learning_rate': 2.414038346108471e-05, 'epoch': 3.94}
 62%|██████▏   | 3102/5000 [15:05:13<8:02:37, 15.26s/it] 62%|██████▏   | 3103/5000 [15:05:28<7:58:44, 15.14s/it]                                                        {'loss': 18.6378, 'grad_norm': 14.875, 'learning_rate': 2.411837971392285e-05, 'epoch': 3.94}
 62%|██████▏   | 3103/5000 [15:05:28<7:58:44, 15.14s/it] 62%|██████▏   | 3104/5000 [15:05:45<8:18:50, 15.79s/it]                                                        {'loss': 19.204, 'grad_norm': 14.5, 'learning_rate': 2.4096380726751396e-05, 'epoch': 3.94}
 62%|██████▏   | 3104/5000 [15:05:45<8:18:50, 15.79s/it] 62%|██████▏   | 3105/5000 [15:05:58<7:56:17, 15.08s/it]                                                        {'loss': 18.0861, 'grad_norm': 23.75, 'learning_rate': 2.407438650919344e-05, 'epoch': 3.94}
 62%|██████▏   | 3105/5000 [15:05:58<7:56:17, 15.08s/it] 62%|██████▏   | 3106/5000 [15:06:26<9:50:31, 18.71s/it]                                                        {'loss': 17.826, 'grad_norm': 18.25, 'learning_rate': 2.4052397070870032e-05, 'epoch': 3.94}
 62%|██████▏   | 3106/5000 [15:06:26<9:50:31, 18.71s/it] 62%|██████▏   | 3107/5000 [15:06:44<9:48:47, 18.66s/it]                                                        {'loss': 20.0839, 'grad_norm': 16.875, 'learning_rate': 2.4030412421400073e-05, 'epoch': 3.95}
 62%|██████▏   | 3107/5000 [15:06:44<9:48:47, 18.66s/it] 62%|██████▏   | 3108/5000 [15:06:58<9:01:24, 17.17s/it]                                                        {'loss': 18.5183, 'grad_norm': 12.75, 'learning_rate': 2.400843257040039e-05, 'epoch': 3.95}
 62%|██████▏   | 3108/5000 [15:06:58<9:01:24, 17.17s/it] 62%|██████▏   | 3109/5000 [15:07:22<10:08:57, 19.32s/it]                                                         {'loss': 19.1617, 'grad_norm': 39.0, 'learning_rate': 2.3986457527485747e-05, 'epoch': 3.95}
 62%|██████▏   | 3109/5000 [15:07:22<10:08:57, 19.32s/it] 62%|██████▏   | 3110/5000 [15:07:36<9:17:55, 17.71s/it]                                                         {'loss': 20.4384, 'grad_norm': 27.125, 'learning_rate': 2.3964487302268747e-05, 'epoch': 3.95}
 62%|██████▏   | 3110/5000 [15:07:36<9:17:55, 17.71s/it] 62%|██████▏   | 3111/5000 [15:07:53<9:08:16, 17.41s/it]                                                        {'loss': 16.8719, 'grad_norm': 16.125, 'learning_rate': 2.3942521904359924e-05, 'epoch': 3.95}
 62%|██████▏   | 3111/5000 [15:07:53<9:08:16, 17.41s/it] 62%|██████▏   | 3112/5000 [15:08:09<8:58:10, 17.10s/it]                                                        {'loss': 19.0104, 'grad_norm': 50.0, 'learning_rate': 2.392056134336769e-05, 'epoch': 3.95}
 62%|██████▏   | 3112/5000 [15:08:09<8:58:10, 17.10s/it] 62%|██████▏   | 3113/5000 [15:08:25<8:43:34, 16.65s/it]                                                        {'loss': 18.3836, 'grad_norm': 68.0, 'learning_rate': 2.3898605628898344e-05, 'epoch': 3.95}
 62%|██████▏   | 3113/5000 [15:08:25<8:43:34, 16.65s/it] 62%|██████▏   | 3114/5000 [15:08:48<9:49:27, 18.75s/it]                                                        {'loss': 19.9686, 'grad_norm': 20.375, 'learning_rate': 2.3876654770556042e-05, 'epoch': 3.95}
 62%|██████▏   | 3114/5000 [15:08:48<9:49:27, 18.75s/it] 62%|██████▏   | 3115/5000 [15:09:04<9:15:47, 17.69s/it]                                                        {'loss': 16.7705, 'grad_norm': 9.6875, 'learning_rate': 2.3854708777942864e-05, 'epoch': 3.96}
 62%|██████▏   | 3115/5000 [15:09:04<9:15:47, 17.69s/it] 62%|██████▏   | 3116/5000 [15:09:18<8:42:20, 16.64s/it]                                                        {'loss': 18.1097, 'grad_norm': 13.9375, 'learning_rate': 2.383276766065872e-05, 'epoch': 3.96}
 62%|██████▏   | 3116/5000 [15:09:18<8:42:20, 16.64s/it] 62%|██████▏   | 3117/5000 [15:09:35<8:48:18, 16.83s/it]                                                        {'loss': 17.8074, 'grad_norm': 10.8125, 'learning_rate': 2.3810831428301384e-05, 'epoch': 3.96}
 62%|██████▏   | 3117/5000 [15:09:35<8:48:18, 16.83s/it] 62%|██████▏   | 3118/5000 [15:09:50<8:31:58, 16.32s/it]                                                        {'loss': 17.3989, 'grad_norm': 11.9375, 'learning_rate': 2.378890009046653e-05, 'epoch': 3.96}
 62%|██████▏   | 3118/5000 [15:09:50<8:31:58, 16.32s/it] 62%|██████▏   | 3119/5000 [15:10:09<8:51:37, 16.96s/it]                                                        {'loss': 18.8081, 'grad_norm': 340.0, 'learning_rate': 2.376697365674767e-05, 'epoch': 3.96}
 62%|██████▏   | 3119/5000 [15:10:09<8:51:37, 16.96s/it] 62%|██████▏   | 3120/5000 [15:10:22<8:15:49, 15.82s/it]                                                        {'loss': 23.1278, 'grad_norm': 528.0, 'learning_rate': 2.3745052136736166e-05, 'epoch': 3.96}
 62%|██████▏   | 3120/5000 [15:10:22<8:15:49, 15.82s/it] 62%|██████▏   | 3121/5000 [15:10:35<7:51:05, 15.04s/it]                                                        {'loss': 18.4536, 'grad_norm': 27.875, 'learning_rate': 2.372313554002122e-05, 'epoch': 3.96}
 62%|██████▏   | 3121/5000 [15:10:35<7:51:05, 15.04s/it] 62%|██████▏   | 3122/5000 [15:11:00<9:26:50, 18.11s/it]                                                        {'loss': 17.7355, 'grad_norm': 15.625, 'learning_rate': 2.370122387618992e-05, 'epoch': 3.96}
 62%|██████▏   | 3122/5000 [15:11:00<9:26:50, 18.11s/it] 62%|██████▏   | 3123/5000 [15:11:15<8:55:39, 17.12s/it]                                                        {'loss': 17.5566, 'grad_norm': 13.0, 'learning_rate': 2.367931715482716e-05, 'epoch': 3.97}
 62%|██████▏   | 3123/5000 [15:11:15<8:55:39, 17.12s/it] 62%|██████▏   | 3124/5000 [15:11:35<9:23:04, 18.01s/it]                                                        {'loss': 17.8727, 'grad_norm': 18.25, 'learning_rate': 2.3657415385515688e-05, 'epoch': 3.97}
 62%|██████▏   | 3124/5000 [15:11:35<9:23:04, 18.01s/it] 62%|██████▎   | 3125/5000 [15:11:55<9:34:51, 18.40s/it]                                                        {'loss': 18.7341, 'grad_norm': 30.25, 'learning_rate': 2.363551857783608e-05, 'epoch': 3.97}
 62%|██████▎   | 3125/5000 [15:11:55<9:34:51, 18.40s/it] 63%|██████▎   | 3126/5000 [15:12:11<9:12:34, 17.69s/it]                                                        {'loss': 18.8576, 'grad_norm': 19.0, 'learning_rate': 2.3613626741366747e-05, 'epoch': 3.97}
 63%|██████▎   | 3126/5000 [15:12:11<9:12:34, 17.69s/it] 63%|██████▎   | 3127/5000 [15:12:28<9:08:37, 17.57s/it]                                                        {'loss': 18.3312, 'grad_norm': 11.25, 'learning_rate': 2.3591739885683907e-05, 'epoch': 3.97}
 63%|██████▎   | 3127/5000 [15:12:28<9:08:37, 17.57s/it] 63%|██████▎   | 3128/5000 [15:12:43<8:43:51, 16.79s/it]                                                        {'loss': 18.5611, 'grad_norm': 18.375, 'learning_rate': 2.3569858020361643e-05, 'epoch': 3.97}
 63%|██████▎   | 3128/5000 [15:12:43<8:43:51, 16.79s/it] 63%|██████▎   | 3129/5000 [15:12:59<8:36:34, 16.57s/it]                                                        {'loss': 20.2461, 'grad_norm': 19.375, 'learning_rate': 2.354798115497181e-05, 'epoch': 3.97}
 63%|██████▎   | 3129/5000 [15:12:59<8:36:34, 16.57s/it] 63%|██████▎   | 3130/5000 [15:13:15<8:30:12, 16.37s/it]                                                        {'loss': 19.3447, 'grad_norm': 16.875, 'learning_rate': 2.3526109299084077e-05, 'epoch': 3.97}
 63%|██████▎   | 3130/5000 [15:13:15<8:30:12, 16.37s/it] 63%|██████▎   | 3131/5000 [15:13:29<8:07:10, 15.64s/it]                                                        {'loss': 19.0648, 'grad_norm': 13.1875, 'learning_rate': 2.3504242462265964e-05, 'epoch': 3.98}
 63%|██████▎   | 3131/5000 [15:13:29<8:07:10, 15.64s/it] 63%|██████▎   | 3132/5000 [15:13:45<8:17:22, 15.98s/it]                                                        {'loss': 17.6061, 'grad_norm': 13.375, 'learning_rate': 2.3482380654082765e-05, 'epoch': 3.98}
 63%|██████▎   | 3132/5000 [15:13:45<8:17:22, 15.98s/it] 63%|██████▎   | 3133/5000 [15:13:59<7:50:11, 15.11s/it]                                                        {'loss': 20.0597, 'grad_norm': 26.0, 'learning_rate': 2.3460523884097547e-05, 'epoch': 3.98}
 63%|██████▎   | 3133/5000 [15:13:59<7:50:11, 15.11s/it] 63%|██████▎   | 3134/5000 [15:14:23<9:14:22, 17.83s/it]                                                        {'loss': 18.8645, 'grad_norm': 29.25, 'learning_rate': 2.3438672161871242e-05, 'epoch': 3.98}
 63%|██████▎   | 3134/5000 [15:14:23<9:14:22, 17.83s/it] 63%|██████▎   | 3135/5000 [15:14:38<8:50:37, 17.07s/it]                                                        {'loss': 19.307, 'grad_norm': 20.25, 'learning_rate': 2.3416825496962508e-05, 'epoch': 3.98}
 63%|██████▎   | 3135/5000 [15:14:38<8:50:37, 17.07s/it] 63%|██████▎   | 3136/5000 [15:14:57<9:06:47, 17.60s/it]                                                        {'loss': 18.0589, 'grad_norm': 19.375, 'learning_rate': 2.3394983898927836e-05, 'epoch': 3.98}
 63%|██████▎   | 3136/5000 [15:14:57<9:06:47, 17.60s/it] 63%|██████▎   | 3137/5000 [15:15:12<8:45:33, 16.93s/it]                                                        {'loss': 18.2408, 'grad_norm': 24.75, 'learning_rate': 2.3373147377321472e-05, 'epoch': 3.98}
 63%|██████▎   | 3137/5000 [15:15:12<8:45:33, 16.93s/it] 63%|██████▎   | 3138/5000 [15:15:37<9:56:58, 19.24s/it]                                                        {'loss': 19.4556, 'grad_norm': 31.0, 'learning_rate': 2.3351315941695468e-05, 'epoch': 3.98}
 63%|██████▎   | 3138/5000 [15:15:37<9:56:58, 19.24s/it] 63%|██████▎   | 3139/5000 [15:15:51<9:12:40, 17.82s/it]                                                        {'loss': 19.0503, 'grad_norm': 28.25, 'learning_rate': 2.3329489601599622e-05, 'epoch': 3.99}
 63%|██████▎   | 3139/5000 [15:15:51<9:12:40, 17.82s/it] 63%|██████▎   | 3140/5000 [15:16:08<8:57:33, 17.34s/it]                                                        {'loss': 16.457, 'grad_norm': 15.375, 'learning_rate': 2.3307668366581514e-05, 'epoch': 3.99}
 63%|██████▎   | 3140/5000 [15:16:08<8:57:33, 17.34s/it] 63%|██████▎   | 3141/5000 [15:16:33<10:09:45, 19.68s/it]                                                         {'loss': 18.5576, 'grad_norm': 13.8125, 'learning_rate': 2.3285852246186503e-05, 'epoch': 3.99}
 63%|██████▎   | 3141/5000 [15:16:33<10:09:45, 19.68s/it] 63%|██████▎   | 3142/5000 [15:16:48<9:25:53, 18.27s/it]                                                         {'loss': 19.7646, 'grad_norm': 35.5, 'learning_rate': 2.3264041249957703e-05, 'epoch': 3.99}
 63%|██████▎   | 3142/5000 [15:16:48<9:25:53, 18.27s/it] 63%|██████▎   | 3143/5000 [15:17:04<9:09:27, 17.75s/it]                                                        {'loss': 17.4757, 'grad_norm': 12.75, 'learning_rate': 2.324223538743596e-05, 'epoch': 3.99}
 63%|██████▎   | 3143/5000 [15:17:04<9:09:27, 17.75s/it] 63%|██████▎   | 3144/5000 [15:17:20<8:48:26, 17.08s/it]                                                        {'loss': 17.2923, 'grad_norm': 9.0, 'learning_rate': 2.3220434668159936e-05, 'epoch': 3.99}
 63%|██████▎   | 3144/5000 [15:17:20<8:48:26, 17.08s/it] 63%|██████▎   | 3145/5000 [15:17:46<10:15:02, 19.89s/it]                                                         {'loss': 16.7195, 'grad_norm': 12.4375, 'learning_rate': 2.3198639101665992e-05, 'epoch': 3.99}
 63%|██████▎   | 3145/5000 [15:17:46<10:15:02, 19.89s/it] 63%|██████▎   | 3146/5000 [15:17:59<9:11:10, 17.84s/it]                                                         {'loss': 19.0464, 'grad_norm': 12.1875, 'learning_rate': 2.317684869748823e-05, 'epoch': 3.99}
 63%|██████▎   | 3146/5000 [15:17:59<9:11:10, 17.84s/it] 63%|██████▎   | 3147/5000 [15:18:13<8:29:49, 16.51s/it]                                                        {'loss': 19.0913, 'grad_norm': 16.375, 'learning_rate': 2.3155063465158534e-05, 'epoch': 4.0}
 63%|██████▎   | 3147/5000 [15:18:13<8:29:49, 16.51s/it] 63%|██████▎   | 3148/5000 [15:18:30<8:32:26, 16.60s/it]                                                        {'loss': 16.8905, 'grad_norm': 15.375, 'learning_rate': 2.313328341420651e-05, 'epoch': 4.0}
 63%|██████▎   | 3148/5000 [15:18:30<8:32:26, 16.60s/it] 63%|██████▎   | 3149/5000 [15:18:50<9:05:13, 17.67s/it]                                                        {'loss': 19.2221, 'grad_norm': 15.8125, 'learning_rate': 2.311150855415946e-05, 'epoch': 4.0}
 63%|██████▎   | 3149/5000 [15:18:50<9:05:13, 17.67s/it] 63%|██████▎   | 3150/5000 [15:19:14<10:05:47, 19.65s/it]                                                         {'loss': 19.7243, 'grad_norm': 16.875, 'learning_rate': 2.308973889454249e-05, 'epoch': 4.0}
 63%|██████▎   | 3150/5000 [15:19:14<10:05:47, 19.65s/it] 63%|██████▎   | 3151/5000 [15:19:37<10:34:30, 20.59s/it]                                                         {'loss': 18.3726, 'grad_norm': 12.75, 'learning_rate': 2.306797444487837e-05, 'epoch': 4.0}
 63%|██████▎   | 3151/5000 [15:19:37<10:34:30, 20.59s/it] 63%|██████▎   | 3152/5000 [15:19:52<9:47:48, 19.08s/it]                                                         {'loss': 18.7702, 'grad_norm': 18.25, 'learning_rate': 2.3046215214687593e-05, 'epoch': 4.0}
 63%|██████▎   | 3152/5000 [15:19:52<9:47:48, 19.08s/it] 63%|██████▎   | 3153/5000 [15:20:08<9:19:51, 18.19s/it]                                                        {'loss': 17.2167, 'grad_norm': 9.375, 'learning_rate': 2.302446121348841e-05, 'epoch': 4.0}
 63%|██████▎   | 3153/5000 [15:20:08<9:19:51, 18.19s/it] 63%|██████▎   | 3154/5000 [15:20:23<8:46:00, 17.10s/it]                                                        {'loss': 19.8857, 'grad_norm': 25.0, 'learning_rate': 2.3002712450796754e-05, 'epoch': 4.01}
 63%|██████▎   | 3154/5000 [15:20:23<8:46:00, 17.10s/it] 63%|██████▎   | 3155/5000 [15:20:50<10:16:01, 20.03s/it]                                                         {'loss': 18.4038, 'grad_norm': 16.75, 'learning_rate': 2.298096893612627e-05, 'epoch': 4.01}
 63%|██████▎   | 3155/5000 [15:20:50<10:16:01, 20.03s/it] 63%|██████▎   | 3156/5000 [15:21:02<9:07:14, 17.81s/it]                                                         {'loss': 21.1435, 'grad_norm': 28.625, 'learning_rate': 2.2959230678988293e-05, 'epoch': 4.01}
 63%|██████▎   | 3156/5000 [15:21:02<9:07:14, 17.81s/it] 63%|██████▎   | 3157/5000 [15:21:16<8:30:11, 16.61s/it]                                                        {'loss': 18.7421, 'grad_norm': 11.75, 'learning_rate': 2.2937497688891906e-05, 'epoch': 4.01}
 63%|██████▎   | 3157/5000 [15:21:16<8:30:11, 16.61s/it] 63%|██████▎   | 3158/5000 [15:21:41<9:42:06, 18.96s/it]                                                        {'loss': 17.2608, 'grad_norm': 11.3125, 'learning_rate': 2.2915769975343836e-05, 'epoch': 4.01}
 63%|██████▎   | 3158/5000 [15:21:41<9:42:06, 18.96s/it] 63%|██████▎   | 3159/5000 [15:21:59<9:38:51, 18.87s/it]                                                        {'loss': 17.761, 'grad_norm': 8.75, 'learning_rate': 2.2894047547848537e-05, 'epoch': 4.01}
 63%|██████▎   | 3159/5000 [15:21:59<9:38:51, 18.87s/it] 63%|██████▎   | 3160/5000 [15:22:15<9:06:07, 17.81s/it]                                                        {'loss': 17.4501, 'grad_norm': 9.0, 'learning_rate': 2.2872330415908122e-05, 'epoch': 4.01}
 63%|██████▎   | 3160/5000 [15:22:15<9:06:07, 17.81s/it] 63%|██████▎   | 3161/5000 [15:22:28<8:26:23, 16.52s/it]                                                        {'loss': 18.6512, 'grad_norm': 13.125, 'learning_rate': 2.285061858902242e-05, 'epoch': 4.01}
 63%|██████▎   | 3161/5000 [15:22:28<8:26:23, 16.52s/it] 63%|██████▎   | 3162/5000 [15:22:53<9:46:26, 19.14s/it]                                                        {'loss': 19.491, 'grad_norm': 14.375, 'learning_rate': 2.2828912076688893e-05, 'epoch': 4.02}
 63%|██████▎   | 3162/5000 [15:22:53<9:46:26, 19.14s/it] 63%|██████▎   | 3163/5000 [15:23:08<9:04:21, 17.78s/it]                                                        {'loss': 18.0952, 'grad_norm': 16.5, 'learning_rate': 2.2807210888402746e-05, 'epoch': 4.02}
 63%|██████▎   | 3163/5000 [15:23:08<9:04:21, 17.78s/it] 63%|██████▎   | 3164/5000 [15:23:25<8:58:02, 17.58s/it]                                                        {'loss': 17.6582, 'grad_norm': 15.875, 'learning_rate': 2.2785515033656795e-05, 'epoch': 4.02}
 63%|██████▎   | 3164/5000 [15:23:25<8:58:02, 17.58s/it] 63%|██████▎   | 3165/5000 [15:23:42<8:54:39, 17.48s/it]                                                        {'loss': 17.8539, 'grad_norm': 15.6875, 'learning_rate': 2.2763824521941525e-05, 'epoch': 4.02}
 63%|██████▎   | 3165/5000 [15:23:42<8:54:39, 17.48s/it] 63%|██████▎   | 3166/5000 [15:23:56<8:22:24, 16.44s/it]                                                        {'loss': 18.6849, 'grad_norm': 14.5625, 'learning_rate': 2.2742139362745143e-05, 'epoch': 4.02}
 63%|██████▎   | 3166/5000 [15:23:56<8:22:24, 16.44s/it] 63%|██████▎   | 3167/5000 [15:24:10<7:56:13, 15.59s/it]                                                        {'loss': 19.7265, 'grad_norm': 41.5, 'learning_rate': 2.2720459565553448e-05, 'epoch': 4.02}
 63%|██████▎   | 3167/5000 [15:24:10<7:56:13, 15.59s/it] 63%|██████▎   | 3168/5000 [15:24:29<8:30:59, 16.74s/it]                                                        {'loss': 17.4194, 'grad_norm': 16.375, 'learning_rate': 2.269878513984992e-05, 'epoch': 4.02}
 63%|██████▎   | 3168/5000 [15:24:29<8:30:59, 16.74s/it] 63%|██████▎   | 3169/5000 [15:24:45<8:15:36, 16.24s/it]                                                        {'loss': 18.8041, 'grad_norm': 13.125, 'learning_rate': 2.2677116095115702e-05, 'epoch': 4.02}
 63%|██████▎   | 3169/5000 [15:24:45<8:15:36, 16.24s/it] 63%|██████▎   | 3170/5000 [15:24:59<7:54:48, 15.57s/it]                                                        {'loss': 18.4784, 'grad_norm': 13.0625, 'learning_rate': 2.2655452440829558e-05, 'epoch': 4.03}
 63%|██████▎   | 3170/5000 [15:24:59<7:54:48, 15.57s/it] 63%|██████▎   | 3171/5000 [15:25:14<7:51:02, 15.45s/it]                                                        {'loss': 17.5113, 'grad_norm': 8.0625, 'learning_rate': 2.2633794186467914e-05, 'epoch': 4.03}
 63%|██████▎   | 3171/5000 [15:25:14<7:51:02, 15.45s/it] 63%|██████▎   | 3172/5000 [15:25:29<7:46:32, 15.31s/it]                                                        {'loss': 18.8153, 'grad_norm': 21.25, 'learning_rate': 2.261214134150482e-05, 'epoch': 4.03}
 63%|██████▎   | 3172/5000 [15:25:29<7:46:32, 15.31s/it] 63%|██████▎   | 3173/5000 [15:25:44<7:45:57, 15.30s/it]                                                        {'loss': 18.7397, 'grad_norm': 21.5, 'learning_rate': 2.259049391541198e-05, 'epoch': 4.03}
 63%|██████▎   | 3173/5000 [15:25:44<7:45:57, 15.30s/it] 63%|██████▎   | 3174/5000 [15:26:09<9:16:42, 18.29s/it]                                                        {'loss': 18.4136, 'grad_norm': 14.75, 'learning_rate': 2.2568851917658703e-05, 'epoch': 4.03}
 63%|██████▎   | 3174/5000 [15:26:09<9:16:42, 18.29s/it] 64%|██████▎   | 3175/5000 [15:26:24<8:41:50, 17.16s/it]                                                        {'loss': 18.2007, 'grad_norm': 12.5625, 'learning_rate': 2.2547215357711918e-05, 'epoch': 4.03}
 64%|██████▎   | 3175/5000 [15:26:24<8:41:50, 17.16s/it] 64%|██████▎   | 3176/5000 [15:26:44<9:12:43, 18.18s/it]                                                        {'loss': 19.4642, 'grad_norm': 18.5, 'learning_rate': 2.2525584245036226e-05, 'epoch': 4.03}
 64%|██████▎   | 3176/5000 [15:26:44<9:12:43, 18.18s/it] 64%|██████▎   | 3177/5000 [15:27:09<10:14:00, 20.21s/it]                                                         {'loss': 18.1625, 'grad_norm': 11.25, 'learning_rate': 2.250395858909379e-05, 'epoch': 4.03}
 64%|██████▎   | 3177/5000 [15:27:09<10:14:00, 20.21s/it] 64%|██████▎   | 3178/5000 [15:27:26<9:41:17, 19.14s/it]                                                         {'loss': 17.2425, 'grad_norm': 12.375, 'learning_rate': 2.2482338399344397e-05, 'epoch': 4.04}
 64%|██████▎   | 3178/5000 [15:27:26<9:41:17, 19.14s/it] 64%|██████▎   | 3179/5000 [15:27:41<9:02:45, 17.88s/it]                                                        {'loss': 18.0991, 'grad_norm': 16.625, 'learning_rate': 2.246072368524548e-05, 'epoch': 4.04}
 64%|██████▎   | 3179/5000 [15:27:41<9:02:45, 17.88s/it] 64%|██████▎   | 3180/5000 [15:27:56<8:40:27, 17.16s/it]                                                        {'loss': 18.615, 'grad_norm': 19.875, 'learning_rate': 2.243911445625203e-05, 'epoch': 4.04}
 64%|██████▎   | 3180/5000 [15:27:56<8:40:27, 17.16s/it] 64%|██████▎   | 3181/5000 [15:28:11<8:14:08, 16.30s/it]                                                        {'loss': 18.9776, 'grad_norm': 15.9375, 'learning_rate': 2.241751072181665e-05, 'epoch': 4.04}
 64%|██████▎   | 3181/5000 [15:28:11<8:14:08, 16.30s/it] 64%|██████▎   | 3182/5000 [15:28:25<7:54:25, 15.66s/it]                                                        {'loss': 17.8698, 'grad_norm': 13.125, 'learning_rate': 2.2395912491389572e-05, 'epoch': 4.04}
 64%|██████▎   | 3182/5000 [15:28:25<7:54:25, 15.66s/it] 64%|██████▎   | 3183/5000 [15:28:45<8:38:00, 17.11s/it]                                                        {'loss': 18.8492, 'grad_norm': 13.875, 'learning_rate': 2.2374319774418584e-05, 'epoch': 4.04}
 64%|██████▎   | 3183/5000 [15:28:45<8:38:00, 17.11s/it] 64%|██████▎   | 3184/5000 [15:29:02<8:37:28, 17.10s/it]                                                        {'loss': 18.9257, 'grad_norm': 12.5625, 'learning_rate': 2.235273258034906e-05, 'epoch': 4.04}
 64%|██████▎   | 3184/5000 [15:29:02<8:37:28, 17.10s/it] 64%|██████▎   | 3185/5000 [15:29:15<7:55:25, 15.72s/it]                                                        {'loss': 21.0734, 'grad_norm': 19.875, 'learning_rate': 2.2331150918623993e-05, 'epoch': 4.04}
 64%|██████▎   | 3185/5000 [15:29:15<7:55:25, 15.72s/it] 64%|██████▎   | 3186/5000 [15:29:41<9:26:01, 18.72s/it]                                                        {'loss': 17.6939, 'grad_norm': 13.8125, 'learning_rate': 2.2309574798683937e-05, 'epoch': 4.05}
 64%|██████▎   | 3186/5000 [15:29:41<9:26:01, 18.72s/it] 64%|██████▎   | 3187/5000 [15:29:56<8:53:26, 17.65s/it]                                                        {'loss': 17.8192, 'grad_norm': 15.5, 'learning_rate': 2.228800422996699e-05, 'epoch': 4.05}
 64%|██████▎   | 3187/5000 [15:29:56<8:53:26, 17.65s/it] 64%|██████▍   | 3188/5000 [15:30:12<8:44:04, 17.35s/it]                                                        {'loss': 18.1198, 'grad_norm': 11.6875, 'learning_rate': 2.2266439221908887e-05, 'epoch': 4.05}
 64%|██████▍   | 3188/5000 [15:30:12<8:44:04, 17.35s/it] 64%|██████▍   | 3189/5000 [15:30:29<8:39:35, 17.21s/it]                                                        {'loss': 18.237, 'grad_norm': 43.25, 'learning_rate': 2.2244879783942884e-05, 'epoch': 4.05}
 64%|██████▍   | 3189/5000 [15:30:29<8:39:35, 17.21s/it] 64%|██████▍   | 3190/5000 [15:30:43<8:10:43, 16.27s/it]                                                        {'loss': 19.9057, 'grad_norm': 16.25, 'learning_rate': 2.2223325925499802e-05, 'epoch': 4.05}
 64%|██████▍   | 3190/5000 [15:30:43<8:10:43, 16.27s/it] 64%|██████▍   | 3191/5000 [15:31:02<8:29:08, 16.89s/it]                                                        {'loss': 17.2071, 'grad_norm': 15.5, 'learning_rate': 2.220177765600803e-05, 'epoch': 4.05}
 64%|██████▍   | 3191/5000 [15:31:02<8:29:08, 16.89s/it] 64%|██████▍   | 3192/5000 [15:31:19<8:31:41, 16.98s/it]                                                        {'loss': 17.2077, 'grad_norm': 14.375, 'learning_rate': 2.2180234984893527e-05, 'epoch': 4.05}
 64%|██████▍   | 3192/5000 [15:31:19<8:31:41, 16.98s/it] 64%|██████▍   | 3193/5000 [15:31:40<9:04:40, 18.09s/it]                                                        {'loss': 17.589, 'grad_norm': 18.75, 'learning_rate': 2.215869792157978e-05, 'epoch': 4.05}
 64%|██████▍   | 3193/5000 [15:31:40<9:04:40, 18.09s/it] 64%|██████▍   | 3194/5000 [15:32:04<9:59:19, 19.91s/it]                                                        {'loss': 18.7135, 'grad_norm': 11.8125, 'learning_rate': 2.213716647548783e-05, 'epoch': 4.06}
 64%|██████▍   | 3194/5000 [15:32:04<9:59:19, 19.91s/it] 64%|██████▍   | 3195/5000 [15:32:24<9:58:13, 19.89s/it]                                                        {'loss': 19.9928, 'grad_norm': 19.75, 'learning_rate': 2.2115640656036272e-05, 'epoch': 4.06}
 64%|██████▍   | 3195/5000 [15:32:24<9:58:13, 19.89s/it] 64%|██████▍   | 3196/5000 [15:32:37<8:55:37, 17.81s/it]                                                        {'loss': 20.1645, 'grad_norm': 31.5, 'learning_rate': 2.2094120472641227e-05, 'epoch': 4.06}
 64%|██████▍   | 3196/5000 [15:32:37<8:55:37, 17.81s/it] 64%|██████▍   | 3197/5000 [15:32:54<8:54:25, 17.78s/it]                                                        {'loss': 18.0837, 'grad_norm': 19.125, 'learning_rate': 2.2072605934716344e-05, 'epoch': 4.06}
 64%|██████▍   | 3197/5000 [15:32:54<8:54:25, 17.78s/it] 64%|██████▍   | 3198/5000 [15:33:10<8:33:32, 17.10s/it]                                                        {'loss': 18.3361, 'grad_norm': 14.5625, 'learning_rate': 2.205109705167283e-05, 'epoch': 4.06}
 64%|██████▍   | 3198/5000 [15:33:10<8:33:32, 17.10s/it] 64%|██████▍   | 3199/5000 [15:33:26<8:28:17, 16.93s/it]                                                        {'loss': 17.4989, 'grad_norm': 11.5, 'learning_rate': 2.20295938329194e-05, 'epoch': 4.06}
 64%|██████▍   | 3199/5000 [15:33:26<8:28:17, 16.93s/it] 64%|██████▍   | 3200/5000 [15:33:53<9:53:42, 19.79s/it]                                                        {'loss': 16.8834, 'grad_norm': 7.84375, 'learning_rate': 2.2008096287862266e-05, 'epoch': 4.06}
 64%|██████▍   | 3200/5000 [15:33:53<9:53:42, 19.79s/it] 64%|██████▍   | 3201/5000 [15:34:09<9:25:54, 18.87s/it]                                                        {'loss': 16.7423, 'grad_norm': 11.9375, 'learning_rate': 2.1986604425905223e-05, 'epoch': 4.06}
 64%|██████▍   | 3201/5000 [15:34:09<9:25:54, 18.87s/it] 64%|██████▍   | 3202/5000 [15:34:34<10:18:41, 20.65s/it]                                                         {'loss': 17.62, 'grad_norm': 10.5625, 'learning_rate': 2.196511825644952e-05, 'epoch': 4.07}
 64%|██████▍   | 3202/5000 [15:34:34<10:18:41, 20.65s/it] 64%|██████▍   | 3203/5000 [15:34:50<9:37:01, 19.27s/it]                                                         {'loss': 18.3151, 'grad_norm': 16.75, 'learning_rate': 2.194363778889393e-05, 'epoch': 4.07}
 64%|██████▍   | 3203/5000 [15:34:50<9:37:01, 19.27s/it] 64%|██████▍   | 3204/5000 [15:35:06<9:04:53, 18.20s/it]                                                        {'loss': 18.436, 'grad_norm': 20.125, 'learning_rate': 2.1922163032634764e-05, 'epoch': 4.07}
 64%|██████▍   | 3204/5000 [15:35:06<9:04:53, 18.20s/it] 64%|██████▍   | 3205/5000 [15:35:22<8:45:34, 17.57s/it]                                                        {'loss': 17.485, 'grad_norm': 10.4375, 'learning_rate': 2.190069399706579e-05, 'epoch': 4.07}
 64%|██████▍   | 3205/5000 [15:35:22<8:45:34, 17.57s/it] 64%|██████▍   | 3206/5000 [15:35:46<9:46:02, 19.60s/it]                                                        {'loss': 19.3193, 'grad_norm': 19.875, 'learning_rate': 2.18792306915783e-05, 'epoch': 4.07}
 64%|██████▍   | 3206/5000 [15:35:46<9:46:02, 19.60s/it] 64%|██████▍   | 3207/5000 [15:36:00<8:47:19, 17.65s/it]                                                        {'loss': 18.5232, 'grad_norm': 14.4375, 'learning_rate': 2.1857773125561066e-05, 'epoch': 4.07}
 64%|██████▍   | 3207/5000 [15:36:00<8:47:19, 17.65s/it] 64%|██████▍   | 3208/5000 [15:36:13<8:08:04, 16.34s/it]                                                        {'loss': 20.0992, 'grad_norm': 40.75, 'learning_rate': 2.1836321308400366e-05, 'epoch': 4.07}
 64%|██████▍   | 3208/5000 [15:36:13<8:08:04, 16.34s/it] 64%|██████▍   | 3209/5000 [15:36:27<7:52:21, 15.82s/it]                                                        {'loss': 17.8175, 'grad_norm': 13.5625, 'learning_rate': 2.1814875249479953e-05, 'epoch': 4.07}
 64%|██████▍   | 3209/5000 [15:36:27<7:52:21, 15.82s/it] 64%|██████▍   | 3210/5000 [15:36:51<9:01:52, 18.16s/it]                                                        {'loss': 17.2552, 'grad_norm': 16.625, 'learning_rate': 2.1793434958181048e-05, 'epoch': 4.08}
 64%|██████▍   | 3210/5000 [15:36:51<9:01:52, 18.16s/it] 64%|██████▍   | 3211/5000 [15:37:05<8:25:34, 16.96s/it]                                                        {'loss': 19.0405, 'grad_norm': 23.625, 'learning_rate': 2.1772000443882386e-05, 'epoch': 4.08}
 64%|██████▍   | 3211/5000 [15:37:05<8:25:34, 16.96s/it] 64%|██████▍   | 3212/5000 [15:37:30<9:37:12, 19.37s/it]                                                        {'loss': 16.8858, 'grad_norm': 7.96875, 'learning_rate': 2.175057171596014e-05, 'epoch': 4.08}
 64%|██████▍   | 3212/5000 [15:37:30<9:37:12, 19.37s/it] 64%|██████▍   | 3213/5000 [15:37:52<9:57:26, 20.06s/it]                                                        {'loss': 18.4648, 'grad_norm': 12.625, 'learning_rate': 2.1729148783787948e-05, 'epoch': 4.08}
 64%|██████▍   | 3213/5000 [15:37:52<9:57:26, 20.06s/it] 64%|██████▍   | 3214/5000 [15:38:20<11:06:57, 22.41s/it]                                                         {'loss': 16.8624, 'grad_norm': 8.0, 'learning_rate': 2.1707731656736964e-05, 'epoch': 4.08}
 64%|██████▍   | 3214/5000 [15:38:20<11:06:57, 22.41s/it] 64%|██████▍   | 3215/5000 [15:38:33<9:47:27, 19.75s/it]                                                         {'loss': 18.7471, 'grad_norm': 17.0, 'learning_rate': 2.168632034417574e-05, 'epoch': 4.08}
 64%|██████▍   | 3215/5000 [15:38:33<9:47:27, 19.75s/it] 64%|██████▍   | 3216/5000 [15:38:51<9:31:09, 19.21s/it]                                                        {'loss': 16.8382, 'grad_norm': 17.375, 'learning_rate': 2.166491485547031e-05, 'epoch': 4.08}
 64%|██████▍   | 3216/5000 [15:38:51<9:31:09, 19.21s/it] 64%|██████▍   | 3217/5000 [15:39:09<9:14:43, 18.67s/it]                                                        {'loss': 18.7067, 'grad_norm': 24.5, 'learning_rate': 2.1643515199984175e-05, 'epoch': 4.09}
 64%|██████▍   | 3217/5000 [15:39:09<9:14:43, 18.67s/it] 64%|██████▍   | 3218/5000 [15:39:24<8:47:44, 17.77s/it]                                                        {'loss': 18.5709, 'grad_norm': 17.25, 'learning_rate': 2.162212138707827e-05, 'epoch': 4.09}
 64%|██████▍   | 3218/5000 [15:39:24<8:47:44, 17.77s/it] 64%|██████▍   | 3219/5000 [15:39:40<8:26:09, 17.05s/it]                                                        {'loss': 17.873, 'grad_norm': 22.375, 'learning_rate': 2.1600733426110964e-05, 'epoch': 4.09}
 64%|██████▍   | 3219/5000 [15:39:40<8:26:09, 17.05s/it] 64%|██████▍   | 3220/5000 [15:39:55<8:07:13, 16.42s/it]                                                        {'loss': 19.0955, 'grad_norm': 16.75, 'learning_rate': 2.157935132643809e-05, 'epoch': 4.09}
 64%|██████▍   | 3220/5000 [15:39:55<8:07:13, 16.42s/it] 64%|██████▍   | 3221/5000 [15:40:14<8:34:22, 17.35s/it]                                                        {'loss': 18.2157, 'grad_norm': 12.5625, 'learning_rate': 2.155797509741291e-05, 'epoch': 4.09}
 64%|██████▍   | 3221/5000 [15:40:14<8:34:22, 17.35s/it] 64%|██████▍   | 3222/5000 [15:40:33<8:46:07, 17.75s/it]                                                        {'loss': 15.9483, 'grad_norm': 7.1875, 'learning_rate': 2.1536604748386085e-05, 'epoch': 4.09}
 64%|██████▍   | 3222/5000 [15:40:33<8:46:07, 17.75s/it] 64%|██████▍   | 3223/5000 [15:40:49<8:27:06, 17.12s/it]                                                        {'loss': 18.436, 'grad_norm': 12.5, 'learning_rate': 2.1515240288705762e-05, 'epoch': 4.09}
 64%|██████▍   | 3223/5000 [15:40:49<8:27:06, 17.12s/it] 64%|██████▍   | 3224/5000 [15:41:16<9:54:44, 20.09s/it]                                                        {'loss': 16.7119, 'grad_norm': 11.625, 'learning_rate': 2.1493881727717473e-05, 'epoch': 4.09}
 64%|██████▍   | 3224/5000 [15:41:16<9:54:44, 20.09s/it] 64%|██████▍   | 3225/5000 [15:41:30<9:07:50, 18.52s/it]                                                        {'loss': 19.3619, 'grad_norm': 16.875, 'learning_rate': 2.1472529074764177e-05, 'epoch': 4.1}
 64%|██████▍   | 3225/5000 [15:41:30<9:07:50, 18.52s/it] 65%|██████▍   | 3226/5000 [15:41:52<9:37:58, 19.55s/it]                                                        {'loss': 17.2836, 'grad_norm': 16.5, 'learning_rate': 2.145118233918623e-05, 'epoch': 4.1}
 65%|██████▍   | 3226/5000 [15:41:52<9:37:58, 19.55s/it] 65%|██████▍   | 3227/5000 [15:42:06<8:44:54, 17.76s/it]                                                        {'loss': 18.3607, 'grad_norm': 13.5, 'learning_rate': 2.1429841530321455e-05, 'epoch': 4.1}
 65%|██████▍   | 3227/5000 [15:42:06<8:44:54, 17.76s/it] 65%|██████▍   | 3228/5000 [15:42:21<8:19:38, 16.92s/it]                                                        {'loss': 18.0438, 'grad_norm': 21.875, 'learning_rate': 2.1408506657505026e-05, 'epoch': 4.1}
 65%|██████▍   | 3228/5000 [15:42:21<8:19:38, 16.92s/it] 65%|██████▍   | 3229/5000 [15:42:38<8:24:30, 17.09s/it]                                                        {'loss': 18.0485, 'grad_norm': 19.0, 'learning_rate': 2.1387177730069533e-05, 'epoch': 4.1}
 65%|██████▍   | 3229/5000 [15:42:38<8:24:30, 17.09s/it] 65%|██████▍   | 3230/5000 [15:42:55<8:23:27, 17.07s/it]                                                        {'loss': 16.9711, 'grad_norm': 15.75, 'learning_rate': 2.1365854757344998e-05, 'epoch': 4.1}
 65%|██████▍   | 3230/5000 [15:42:55<8:23:27, 17.07s/it] 65%|██████▍   | 3231/5000 [15:43:11<8:07:51, 16.55s/it]                                                        {'loss': 17.1281, 'grad_norm': 10.875, 'learning_rate': 2.134453774865879e-05, 'epoch': 4.1}
 65%|██████▍   | 3231/5000 [15:43:11<8:07:51, 16.55s/it] 65%|██████▍   | 3232/5000 [15:43:25<7:47:20, 15.86s/it]                                                        {'loss': 19.0697, 'grad_norm': 17.0, 'learning_rate': 2.1323226713335702e-05, 'epoch': 4.1}
 65%|██████▍   | 3232/5000 [15:43:25<7:47:20, 15.86s/it] 65%|██████▍   | 3233/5000 [15:43:38<7:20:25, 14.95s/it]                                                        {'loss': 20.2788, 'grad_norm': 21.625, 'learning_rate': 2.1301921660697914e-05, 'epoch': 4.11}
 65%|██████▍   | 3233/5000 [15:43:38<7:20:25, 14.95s/it] 65%|██████▍   | 3234/5000 [15:43:55<7:40:47, 15.66s/it]                                                        {'loss': 18.8856, 'grad_norm': 14.0625, 'learning_rate': 2.1280622600064975e-05, 'epoch': 4.11}
 65%|██████▍   | 3234/5000 [15:43:55<7:40:47, 15.66s/it] 65%|██████▍   | 3235/5000 [15:44:13<7:59:34, 16.30s/it]                                                        {'loss': 17.3364, 'grad_norm': 13.8125, 'learning_rate': 2.12593295407538e-05, 'epoch': 4.11}
 65%|██████▍   | 3235/5000 [15:44:13<7:59:34, 16.30s/it] 65%|██████▍   | 3236/5000 [15:44:33<8:29:44, 17.34s/it]                                                        {'loss': 17.3706, 'grad_norm': 11.6875, 'learning_rate': 2.1238042492078723e-05, 'epoch': 4.11}
 65%|██████▍   | 3236/5000 [15:44:33<8:29:44, 17.34s/it] 65%|██████▍   | 3237/5000 [15:44:47<7:59:45, 16.33s/it]                                                        {'loss': 17.9286, 'grad_norm': 21.5, 'learning_rate': 2.1216761463351413e-05, 'epoch': 4.11}
 65%|██████▍   | 3237/5000 [15:44:47<7:59:45, 16.33s/it] 65%|██████▍   | 3238/5000 [15:45:02<7:47:11, 15.91s/it]                                                        {'loss': 18.8163, 'grad_norm': 20.625, 'learning_rate': 2.1195486463880896e-05, 'epoch': 4.11}
 65%|██████▍   | 3238/5000 [15:45:02<7:47:11, 15.91s/it] 65%|██████▍   | 3239/5000 [15:45:22<8:25:16, 17.22s/it]                                                        {'loss': 17.2922, 'grad_norm': 12.0, 'learning_rate': 2.1174217502973613e-05, 'epoch': 4.11}
 65%|██████▍   | 3239/5000 [15:45:22<8:25:16, 17.22s/it] 65%|██████▍   | 3240/5000 [15:45:40<8:33:04, 17.49s/it]                                                        {'loss': 17.6279, 'grad_norm': 13.5, 'learning_rate': 2.1152954589933318e-05, 'epoch': 4.11}
 65%|██████▍   | 3240/5000 [15:45:40<8:33:04, 17.49s/it] 65%|██████▍   | 3241/5000 [15:45:58<8:34:32, 17.55s/it]                                                        {'loss': 17.44, 'grad_norm': 13.375, 'learning_rate': 2.1131697734061112e-05, 'epoch': 4.12}
 65%|██████▍   | 3241/5000 [15:45:58<8:34:32, 17.55s/it] 65%|██████▍   | 3242/5000 [15:46:14<8:27:33, 17.32s/it]                                                        {'loss': 17.4052, 'grad_norm': 15.1875, 'learning_rate': 2.1110446944655483e-05, 'epoch': 4.12}
 65%|██████▍   | 3242/5000 [15:46:14<8:27:33, 17.32s/it] 65%|██████▍   | 3243/5000 [15:46:31<8:22:30, 17.16s/it]                                                        {'loss': 16.6514, 'grad_norm': 10.125, 'learning_rate': 2.108920223101224e-05, 'epoch': 4.12}
 65%|██████▍   | 3243/5000 [15:46:31<8:22:30, 17.16s/it] 65%|██████▍   | 3244/5000 [15:46:48<8:16:19, 16.96s/it]                                                        {'loss': 17.862, 'grad_norm': 9.0625, 'learning_rate': 2.1067963602424565e-05, 'epoch': 4.12}
 65%|██████▍   | 3244/5000 [15:46:48<8:16:19, 16.96s/it] 65%|██████▍   | 3245/5000 [15:47:04<8:10:01, 16.75s/it]                                                        {'loss': 17.6473, 'grad_norm': 20.0, 'learning_rate': 2.104673106818292e-05, 'epoch': 4.12}
 65%|██████▍   | 3245/5000 [15:47:04<8:10:01, 16.75s/it] 65%|██████▍   | 3246/5000 [15:47:22<8:19:11, 17.08s/it]                                                        {'loss': 20.9096, 'grad_norm': 53.25, 'learning_rate': 2.102550463757517e-05, 'epoch': 4.12}
 65%|██████▍   | 3246/5000 [15:47:22<8:19:11, 17.08s/it] 65%|██████▍   | 3247/5000 [15:47:38<8:09:35, 16.76s/it]                                                        {'loss': 17.687, 'grad_norm': 15.1875, 'learning_rate': 2.1004284319886466e-05, 'epoch': 4.12}
 65%|██████▍   | 3247/5000 [15:47:38<8:09:35, 16.76s/it] 65%|██████▍   | 3248/5000 [15:47:51<7:33:41, 15.54s/it]                                                        {'loss': 18.0511, 'grad_norm': 13.25, 'learning_rate': 2.0983070124399277e-05, 'epoch': 4.12}
 65%|██████▍   | 3248/5000 [15:47:51<7:33:41, 15.54s/it] 65%|██████▍   | 3249/5000 [15:48:08<7:50:35, 16.13s/it]                                                        {'loss': 17.4802, 'grad_norm': 17.0, 'learning_rate': 2.0961862060393442e-05, 'epoch': 4.13}
 65%|██████▍   | 3249/5000 [15:48:08<7:50:35, 16.13s/it] 65%|██████▌   | 3250/5000 [15:48:27<8:17:55, 17.07s/it]                                                        {'loss': 18.8501, 'grad_norm': 16.5, 'learning_rate': 2.0940660137146074e-05, 'epoch': 4.13}
 65%|██████▌   | 3250/5000 [15:48:27<8:17:55, 17.07s/it] 65%|██████▌   | 3251/5000 [15:48:43<8:08:30, 16.76s/it]                                                        {'loss': 17.8853, 'grad_norm': 32.5, 'learning_rate': 2.0919464363931602e-05, 'epoch': 4.13}
 65%|██████▌   | 3251/5000 [15:48:43<8:08:30, 16.76s/it] 65%|██████▌   | 3252/5000 [15:49:01<8:13:29, 16.94s/it]                                                        {'loss': 17.6723, 'grad_norm': 14.9375, 'learning_rate': 2.0898274750021808e-05, 'epoch': 4.13}
 65%|██████▌   | 3252/5000 [15:49:01<8:13:29, 16.94s/it] 65%|██████▌   | 3253/5000 [15:49:15<7:53:49, 16.27s/it]                                                        {'loss': 19.2655, 'grad_norm': 58.75, 'learning_rate': 2.0877091304685733e-05, 'epoch': 4.13}
 65%|██████▌   | 3253/5000 [15:49:15<7:53:49, 16.27s/it] 65%|██████▌   | 3254/5000 [15:49:38<8:49:38, 18.20s/it]                                                        {'loss': 18.4911, 'grad_norm': 21.5, 'learning_rate': 2.0855914037189716e-05, 'epoch': 4.13}
 65%|██████▌   | 3254/5000 [15:49:38<8:49:38, 18.20s/it] 65%|██████▌   | 3255/5000 [15:50:02<9:42:08, 20.02s/it]                                                        {'loss': 18.8152, 'grad_norm': 22.125, 'learning_rate': 2.083474295679745e-05, 'epoch': 4.13}
 65%|██████▌   | 3255/5000 [15:50:02<9:42:08, 20.02s/it] 65%|██████▌   | 3256/5000 [15:50:18<8:59:12, 18.55s/it]                                                        {'loss': 19.0592, 'grad_norm': 26.25, 'learning_rate': 2.0813578072769857e-05, 'epoch': 4.13}
 65%|██████▌   | 3256/5000 [15:50:18<8:59:12, 18.55s/it] 65%|██████▌   | 3257/5000 [15:50:38<9:16:23, 19.15s/it]                                                        {'loss': 16.594, 'grad_norm': 11.4375, 'learning_rate': 2.079241939436519e-05, 'epoch': 4.14}
 65%|██████▌   | 3257/5000 [15:50:38<9:16:23, 19.15s/it] 65%|██████▌   | 3258/5000 [15:51:00<9:42:26, 20.06s/it]                                                        {'loss': 18.3209, 'grad_norm': 11.5, 'learning_rate': 2.077126693083897e-05, 'epoch': 4.14}
 65%|██████▌   | 3258/5000 [15:51:00<9:42:26, 20.06s/it] 65%|██████▌   | 3259/5000 [15:51:17<9:09:23, 18.93s/it]                                                        {'loss': 15.7969, 'grad_norm': 7.09375, 'learning_rate': 2.0750120691444004e-05, 'epoch': 4.14}
 65%|██████▌   | 3259/5000 [15:51:17<9:09:23, 18.93s/it] 65%|██████▌   | 3260/5000 [15:51:29<8:16:08, 17.11s/it]                                                        {'loss': 18.2745, 'grad_norm': 10.5625, 'learning_rate': 2.072898068543038e-05, 'epoch': 4.14}
 65%|██████▌   | 3260/5000 [15:51:29<8:16:08, 17.11s/it] 65%|██████▌   | 3261/5000 [15:51:47<8:20:06, 17.25s/it]                                                        {'loss': 18.4195, 'grad_norm': 10.4375, 'learning_rate': 2.0707846922045437e-05, 'epoch': 4.14}
 65%|██████▌   | 3261/5000 [15:51:47<8:20:06, 17.25s/it] 65%|██████▌   | 3262/5000 [15:51:59<7:37:58, 15.81s/it]                                                        {'loss': 18.5953, 'grad_norm': 12.3125, 'learning_rate': 2.0686719410533825e-05, 'epoch': 4.14}
 65%|██████▌   | 3262/5000 [15:51:59<7:37:58, 15.81s/it] 65%|██████▌   | 3263/5000 [15:52:15<7:33:05, 15.65s/it]                                                        {'loss': 19.1089, 'grad_norm': 13.875, 'learning_rate': 2.0665598160137425e-05, 'epoch': 4.14}
 65%|██████▌   | 3263/5000 [15:52:15<7:33:05, 15.65s/it] 65%|██████▌   | 3264/5000 [15:52:41<9:06:28, 18.89s/it]                                                        {'loss': 18.8635, 'grad_norm': 17.75, 'learning_rate': 2.0644483180095375e-05, 'epoch': 4.14}
 65%|██████▌   | 3264/5000 [15:52:41<9:06:28, 18.89s/it] 65%|██████▌   | 3265/5000 [15:52:55<8:22:15, 17.37s/it]                                                        {'loss': 18.2327, 'grad_norm': 16.375, 'learning_rate': 2.062337447964411e-05, 'epoch': 4.15}
 65%|██████▌   | 3265/5000 [15:52:55<8:22:15, 17.37s/it] 65%|██████▌   | 3266/5000 [15:53:13<8:29:45, 17.64s/it]                                                        {'loss': 18.0994, 'grad_norm': 18.25, 'learning_rate': 2.0602272068017275e-05, 'epoch': 4.15}
 65%|██████▌   | 3266/5000 [15:53:13<8:29:45, 17.64s/it] 65%|██████▌   | 3267/5000 [15:53:29<8:15:15, 17.15s/it]                                                        {'loss': 17.3752, 'grad_norm': 15.5625, 'learning_rate': 2.058117595444579e-05, 'epoch': 4.15}
 65%|██████▌   | 3267/5000 [15:53:29<8:15:15, 17.15s/it] 65%|██████▌   | 3268/5000 [15:54:05<10:58:13, 22.80s/it]                                                         {'loss': 17.062, 'grad_norm': 12.3125, 'learning_rate': 2.0560086148157807e-05, 'epoch': 4.15}
 65%|██████▌   | 3268/5000 [15:54:05<10:58:13, 22.80s/it] 65%|██████▌   | 3269/5000 [15:54:20<9:50:44, 20.48s/it]                                                         {'loss': 16.7875, 'grad_norm': 16.0, 'learning_rate': 2.0539002658378733e-05, 'epoch': 4.15}
 65%|██████▌   | 3269/5000 [15:54:20<9:50:44, 20.48s/it] 65%|██████▌   | 3270/5000 [15:54:38<9:23:05, 19.53s/it]                                                        {'loss': 17.1629, 'grad_norm': 9.25, 'learning_rate': 2.0517925494331187e-05, 'epoch': 4.15}
 65%|██████▌   | 3270/5000 [15:54:38<9:23:05, 19.53s/it] 65%|██████▌   | 3271/5000 [15:54:55<9:02:46, 18.84s/it]                                                        {'loss': 16.7848, 'grad_norm': 7.78125, 'learning_rate': 2.0496854665235056e-05, 'epoch': 4.15}
 65%|██████▌   | 3271/5000 [15:54:55<9:02:46, 18.84s/it] 65%|██████▌   | 3272/5000 [15:55:11<8:38:10, 17.99s/it]                                                        {'loss': 19.1973, 'grad_norm': 13.0, 'learning_rate': 2.0475790180307423e-05, 'epoch': 4.15}
 65%|██████▌   | 3272/5000 [15:55:11<8:38:10, 17.99s/it] 65%|██████▌   | 3273/5000 [15:55:36<9:36:30, 20.03s/it]                                                        {'loss': 17.6055, 'grad_norm': 8.125, 'learning_rate': 2.0454732048762603e-05, 'epoch': 4.16}
 65%|██████▌   | 3273/5000 [15:55:36<9:36:30, 20.03s/it] 65%|██████▌   | 3274/5000 [15:55:52<9:02:08, 18.85s/it]                                                        {'loss': 16.286, 'grad_norm': 7.4375, 'learning_rate': 2.043368027981216e-05, 'epoch': 4.16}
 65%|██████▌   | 3274/5000 [15:55:52<9:02:08, 18.85s/it] 66%|██████▌   | 3275/5000 [15:56:14<9:28:32, 19.78s/it]                                                        {'loss': 16.9775, 'grad_norm': 10.5, 'learning_rate': 2.041263488266484e-05, 'epoch': 4.16}
 66%|██████▌   | 3275/5000 [15:56:14<9:28:32, 19.78s/it] 66%|██████▌   | 3276/5000 [15:56:28<8:45:21, 18.28s/it]                                                        {'loss': 19.1264, 'grad_norm': 42.75, 'learning_rate': 2.039159586652662e-05, 'epoch': 4.16}
 66%|██████▌   | 3276/5000 [15:56:28<8:45:21, 18.28s/it] 66%|██████▌   | 3277/5000 [15:56:46<8:35:06, 17.94s/it]                                                        {'loss': 20.5998, 'grad_norm': 27.125, 'learning_rate': 2.0370563240600657e-05, 'epoch': 4.16}
 66%|██████▌   | 3277/5000 [15:56:46<8:35:06, 17.94s/it] 66%|██████▌   | 3278/5000 [15:57:00<8:08:31, 17.02s/it]                                                        {'loss': 19.4245, 'grad_norm': 14.25, 'learning_rate': 2.0349537014087368e-05, 'epoch': 4.16}
 66%|██████▌   | 3278/5000 [15:57:00<8:08:31, 17.02s/it] 66%|██████▌   | 3279/5000 [15:57:31<10:02:13, 21.00s/it]                                                         {'loss': 16.8844, 'grad_norm': 9.0625, 'learning_rate': 2.0328517196184327e-05, 'epoch': 4.16}
 66%|██████▌   | 3279/5000 [15:57:31<10:02:13, 21.00s/it] 66%|██████▌   | 3280/5000 [15:57:45<9:03:53, 18.97s/it]                                                         {'loss': 17.0037, 'grad_norm': 12.8125, 'learning_rate': 2.0307503796086304e-05, 'epoch': 4.17}
 66%|██████▌   | 3280/5000 [15:57:45<9:03:53, 18.97s/it] 66%|██████▌   | 3281/5000 [15:58:01<8:38:04, 18.08s/it]                                                        {'loss': 18.138, 'grad_norm': 24.875, 'learning_rate': 2.0286496822985298e-05, 'epoch': 4.17}
 66%|██████▌   | 3281/5000 [15:58:01<8:38:04, 18.08s/it] 66%|██████▌   | 3282/5000 [15:58:18<8:28:38, 17.76s/it]                                                        {'loss': 17.1238, 'grad_norm': 13.5, 'learning_rate': 2.026549628607046e-05, 'epoch': 4.17}
 66%|██████▌   | 3282/5000 [15:58:18<8:28:38, 17.76s/it] 66%|██████▌   | 3283/5000 [15:58:38<8:43:22, 18.29s/it]                                                        {'loss': 16.6918, 'grad_norm': 11.1875, 'learning_rate': 2.024450219452812e-05, 'epoch': 4.17}
 66%|██████▌   | 3283/5000 [15:58:38<8:43:22, 18.29s/it] 66%|██████▌   | 3284/5000 [15:58:53<8:21:26, 17.53s/it]                                                        {'loss': 18.8877, 'grad_norm': 13.5625, 'learning_rate': 2.022351455754184e-05, 'epoch': 4.17}
 66%|██████▌   | 3284/5000 [15:58:53<8:21:26, 17.53s/it] 66%|██████▌   | 3285/5000 [15:59:11<8:25:06, 17.67s/it]                                                        {'loss': 18.7409, 'grad_norm': 12.6875, 'learning_rate': 2.02025333842923e-05, 'epoch': 4.17}
 66%|██████▌   | 3285/5000 [15:59:11<8:25:06, 17.67s/it] 66%|██████▌   | 3286/5000 [15:59:43<10:25:10, 21.88s/it]                                                         {'loss': 18.1881, 'grad_norm': 10.0625, 'learning_rate': 2.0181558683957406e-05, 'epoch': 4.17}
 66%|██████▌   | 3286/5000 [15:59:43<10:25:10, 21.88s/it] 66%|██████▌   | 3287/5000 [15:59:59<9:37:11, 20.22s/it]                                                         {'loss': 18.39, 'grad_norm': 15.4375, 'learning_rate': 2.0160590465712173e-05, 'epoch': 4.17}
 66%|██████▌   | 3287/5000 [15:59:59<9:37:11, 20.22s/it] 66%|██████▌   | 3288/5000 [16:00:14<8:51:27, 18.63s/it]                                                        {'loss': 18.8054, 'grad_norm': 13.0, 'learning_rate': 2.0139628738728834e-05, 'epoch': 4.18}
 66%|██████▌   | 3288/5000 [16:00:14<8:51:27, 18.63s/it] 66%|██████▌   | 3289/5000 [16:00:40<9:54:03, 20.83s/it]                                                        {'loss': 17.7929, 'grad_norm': 16.125, 'learning_rate': 2.0118673512176744e-05, 'epoch': 4.18}
 66%|██████▌   | 3289/5000 [16:00:40<9:54:03, 20.83s/it] 66%|██████▌   | 3290/5000 [16:00:57<9:21:53, 19.72s/it]                                                        {'loss': 17.4316, 'grad_norm': 13.875, 'learning_rate': 2.0097724795222454e-05, 'epoch': 4.18}
 66%|██████▌   | 3290/5000 [16:00:57<9:21:53, 19.72s/it] 66%|██████▌   | 3291/5000 [16:01:21<9:58:28, 21.01s/it]                                                        {'loss': 19.9496, 'grad_norm': 25.0, 'learning_rate': 2.0076782597029626e-05, 'epoch': 4.18}
 66%|██████▌   | 3291/5000 [16:01:21<9:58:28, 21.01s/it] 66%|██████▌   | 3292/5000 [16:01:41<9:50:22, 20.74s/it]                                                        {'loss': 18.7013, 'grad_norm': 19.25, 'learning_rate': 2.0055846926759084e-05, 'epoch': 4.18}
 66%|██████▌   | 3292/5000 [16:01:42<9:50:22, 20.74s/it] 66%|██████▌   | 3293/5000 [16:02:06<10:21:01, 21.83s/it]                                                         {'loss': 18.913, 'grad_norm': 15.25, 'learning_rate': 2.003491779356882e-05, 'epoch': 4.18}
 66%|██████▌   | 3293/5000 [16:02:06<10:21:01, 21.83s/it] 66%|██████▌   | 3294/5000 [16:02:30<10:41:18, 22.55s/it]                                                         {'loss': 20.649, 'grad_norm': 23.25, 'learning_rate': 2.0013995206613943e-05, 'epoch': 4.18}
 66%|██████▌   | 3294/5000 [16:02:30<10:41:18, 22.55s/it] 66%|██████▌   | 3295/5000 [16:02:47<9:50:45, 20.79s/it]                                                         {'loss': 17.6367, 'grad_norm': 66.5, 'learning_rate': 1.9993079175046697e-05, 'epoch': 4.18}
 66%|██████▌   | 3295/5000 [16:02:47<9:50:45, 20.79s/it] 66%|██████▌   | 3296/5000 [16:03:02<9:02:42, 19.11s/it]                                                        {'loss': 18.9269, 'grad_norm': 25.375, 'learning_rate': 1.9972169708016453e-05, 'epoch': 4.19}
 66%|██████▌   | 3296/5000 [16:03:02<9:02:42, 19.11s/it] 66%|██████▌   | 3297/5000 [16:03:15<8:09:48, 17.26s/it]                                                        {'loss': 19.6129, 'grad_norm': 16.25, 'learning_rate': 1.9951266814669753e-05, 'epoch': 4.19}
 66%|██████▌   | 3297/5000 [16:03:15<8:09:48, 17.26s/it] 66%|██████▌   | 3298/5000 [16:03:28<7:36:34, 16.10s/it]                                                        {'loss': 18.0063, 'grad_norm': 12.0625, 'learning_rate': 1.9930370504150215e-05, 'epoch': 4.19}
 66%|██████▌   | 3298/5000 [16:03:28<7:36:34, 16.10s/it] 66%|██████▌   | 3299/5000 [16:03:46<7:46:56, 16.47s/it]                                                        {'loss': 20.9527, 'grad_norm': 19.75, 'learning_rate': 1.9909480785598578e-05, 'epoch': 4.19}
 66%|██████▌   | 3299/5000 [16:03:46<7:46:56, 16.47s/it] 66%|██████▌   | 3300/5000 [16:04:08<8:40:25, 18.37s/it]                                                        {'loss': 18.3396, 'grad_norm': 17.5, 'learning_rate': 1.988859766815275e-05, 'epoch': 4.19}
 66%|██████▌   | 3300/5000 [16:04:08<8:40:25, 18.37s/it] 66%|██████▌   | 3301/5000 [16:04:26<8:30:06, 18.01s/it]                                                        {'loss': 17.1863, 'grad_norm': 11.0, 'learning_rate': 1.9867721160947702e-05, 'epoch': 4.19}
 66%|██████▌   | 3301/5000 [16:04:26<8:30:06, 18.01s/it] 66%|██████▌   | 3302/5000 [16:04:44<8:36:25, 18.25s/it]                                                        {'loss': 18.1524, 'grad_norm': 12.0625, 'learning_rate': 1.98468512731155e-05, 'epoch': 4.19}
 66%|██████▌   | 3302/5000 [16:04:44<8:36:25, 18.25s/it] 66%|██████▌   | 3303/5000 [16:05:04<8:51:05, 18.78s/it]                                                        {'loss': 17.0058, 'grad_norm': 11.1875, 'learning_rate': 1.982598801378539e-05, 'epoch': 4.19}
 66%|██████▌   | 3303/5000 [16:05:04<8:51:05, 18.78s/it] 66%|██████▌   | 3304/5000 [16:05:27<9:18:53, 19.77s/it]                                                        {'loss': 17.6842, 'grad_norm': 22.5, 'learning_rate': 1.9805131392083643e-05, 'epoch': 4.2}
 66%|██████▌   | 3304/5000 [16:05:27<9:18:53, 19.77s/it] 66%|██████▌   | 3305/5000 [16:05:43<8:52:58, 18.87s/it]                                                        {'loss': 18.7156, 'grad_norm': 13.3125, 'learning_rate': 1.978428141713364e-05, 'epoch': 4.2}
 66%|██████▌   | 3305/5000 [16:05:43<8:52:58, 18.87s/it] 66%|██████▌   | 3306/5000 [16:06:11<10:08:28, 21.55s/it]                                                         {'loss': 17.1469, 'grad_norm': 12.75, 'learning_rate': 1.9763438098055907e-05, 'epoch': 4.2}
 66%|██████▌   | 3306/5000 [16:06:11<10:08:28, 21.55s/it] 66%|██████▌   | 3307/5000 [16:06:29<9:34:57, 20.38s/it]                                                         {'loss': 17.0033, 'grad_norm': 8.1875, 'learning_rate': 1.9742601443967995e-05, 'epoch': 4.2}
 66%|██████▌   | 3307/5000 [16:06:29<9:34:57, 20.38s/it] 66%|██████▌   | 3308/5000 [16:06:44<8:55:15, 18.98s/it]                                                        {'loss': 17.457, 'grad_norm': 30.5, 'learning_rate': 1.9721771463984556e-05, 'epoch': 4.2}
 66%|██████▌   | 3308/5000 [16:06:44<8:55:15, 18.98s/it] 66%|██████▌   | 3309/5000 [16:06:59<8:15:44, 17.59s/it]                                                        {'loss': 19.4311, 'grad_norm': 20.625, 'learning_rate': 1.9700948167217356e-05, 'epoch': 4.2}
 66%|██████▌   | 3309/5000 [16:06:59<8:15:44, 17.59s/it] 66%|██████▌   | 3310/5000 [16:07:13<7:46:28, 16.56s/it]                                                        {'loss': 18.5119, 'grad_norm': 19.0, 'learning_rate': 1.9680131562775185e-05, 'epoch': 4.2}
 66%|██████▌   | 3310/5000 [16:07:13<7:46:28, 16.56s/it] 66%|██████▌   | 3311/5000 [16:07:35<8:30:08, 18.12s/it]                                                        {'loss': 17.9174, 'grad_norm': 10.25, 'learning_rate': 1.9659321659763962e-05, 'epoch': 4.2}
 66%|██████▌   | 3311/5000 [16:07:35<8:30:08, 18.12s/it] 66%|██████▌   | 3312/5000 [16:07:53<8:27:08, 18.03s/it]                                                        {'loss': 18.1134, 'grad_norm': 19.0, 'learning_rate': 1.9638518467286614e-05, 'epoch': 4.21}
 66%|██████▌   | 3312/5000 [16:07:53<8:27:08, 18.03s/it] 66%|██████▋   | 3313/5000 [16:08:06<7:45:30, 16.56s/it]                                                        {'loss': 18.3319, 'grad_norm': 20.0, 'learning_rate': 1.96177219944432e-05, 'epoch': 4.21}
 66%|██████▋   | 3313/5000 [16:08:06<7:45:30, 16.56s/it] 66%|██████▋   | 3314/5000 [16:08:21<7:35:02, 16.19s/it]                                                        {'loss': 18.678, 'grad_norm': 15.75, 'learning_rate': 1.9596932250330785e-05, 'epoch': 4.21}
 66%|██████▋   | 3314/5000 [16:08:21<7:35:02, 16.19s/it] 66%|██████▋   | 3315/5000 [16:08:35<7:12:42, 15.41s/it]                                                        {'loss': 18.3906, 'grad_norm': 13.125, 'learning_rate': 1.9576149244043492e-05, 'epoch': 4.21}
 66%|██████▋   | 3315/5000 [16:08:35<7:12:42, 15.41s/it] 66%|██████▋   | 3316/5000 [16:08:51<7:21:01, 15.71s/it]                                                        {'loss': 16.5131, 'grad_norm': 15.0, 'learning_rate': 1.955537298467255e-05, 'epoch': 4.21}
 66%|██████▋   | 3316/5000 [16:08:51<7:21:01, 15.71s/it] 66%|██████▋   | 3317/5000 [16:09:08<7:29:56, 16.04s/it]                                                        {'loss': 19.4056, 'grad_norm': 23.375, 'learning_rate': 1.953460348130618e-05, 'epoch': 4.21}
 66%|██████▋   | 3317/5000 [16:09:08<7:29:56, 16.04s/it] 66%|██████▋   | 3318/5000 [16:09:34<8:51:29, 18.96s/it]                                                        {'loss': 17.1061, 'grad_norm': 7.78125, 'learning_rate': 1.9513840743029657e-05, 'epoch': 4.21}
 66%|██████▋   | 3318/5000 [16:09:34<8:51:29, 18.96s/it] 66%|██████▋   | 3319/5000 [16:09:55<9:10:25, 19.65s/it]                                                        {'loss': 18.1313, 'grad_norm': 16.125, 'learning_rate': 1.949308477892534e-05, 'epoch': 4.21}
 66%|██████▋   | 3319/5000 [16:09:55<9:10:25, 19.65s/it] 66%|██████▋   | 3320/5000 [16:10:12<8:45:33, 18.77s/it]                                                        {'loss': 16.7306, 'grad_norm': 9.5625, 'learning_rate': 1.947233559807257e-05, 'epoch': 4.22}
 66%|██████▋   | 3320/5000 [16:10:12<8:45:33, 18.77s/it] 66%|██████▋   | 3321/5000 [16:10:28<8:25:08, 18.05s/it]                                                        {'loss': 17.7763, 'grad_norm': 17.625, 'learning_rate': 1.945159320954773e-05, 'epoch': 4.22}
 66%|██████▋   | 3321/5000 [16:10:28<8:25:08, 18.05s/it] 66%|██████▋   | 3322/5000 [16:10:44<8:05:24, 17.36s/it]                                                        {'loss': 18.3545, 'grad_norm': 23.75, 'learning_rate': 1.9430857622424285e-05, 'epoch': 4.22}
 66%|██████▋   | 3322/5000 [16:10:44<8:05:24, 17.36s/it] 66%|██████▋   | 3323/5000 [16:10:56<7:25:54, 15.95s/it]                                                        {'loss': 19.0892, 'grad_norm': 12.5, 'learning_rate': 1.941012884577266e-05, 'epoch': 4.22}
 66%|██████▋   | 3323/5000 [16:10:56<7:25:54, 15.95s/it] 66%|██████▋   | 3324/5000 [16:11:12<7:23:21, 15.87s/it]                                                        {'loss': 16.7379, 'grad_norm': 8.5, 'learning_rate': 1.9389406888660318e-05, 'epoch': 4.22}
 66%|██████▋   | 3324/5000 [16:11:12<7:23:21, 15.87s/it] 66%|██████▋   | 3325/5000 [16:11:36<8:27:19, 18.17s/it]                                                        {'loss': 17.9002, 'grad_norm': 21.625, 'learning_rate': 1.9368691760151773e-05, 'epoch': 4.22}
 66%|██████▋   | 3325/5000 [16:11:36<8:27:19, 18.17s/it] 67%|██████▋   | 3326/5000 [16:12:01<9:30:37, 20.45s/it]                                                        {'loss': 18.0767, 'grad_norm': 12.875, 'learning_rate': 1.9347983469308513e-05, 'epoch': 4.22}
 67%|██████▋   | 3326/5000 [16:12:01<9:30:37, 20.45s/it] 67%|██████▋   | 3327/5000 [16:12:20<9:13:40, 19.86s/it]                                                        {'loss': 18.3877, 'grad_norm': 16.125, 'learning_rate': 1.932728202518903e-05, 'epoch': 4.22}
 67%|██████▋   | 3327/5000 [16:12:20<9:13:40, 19.86s/it] 67%|██████▋   | 3328/5000 [16:12:36<8:45:10, 18.85s/it]                                                        {'loss': 16.9649, 'grad_norm': 11.5625, 'learning_rate': 1.9306587436848877e-05, 'epoch': 4.23}
 67%|██████▋   | 3328/5000 [16:12:36<8:45:10, 18.85s/it] 67%|██████▋   | 3329/5000 [16:12:51<8:08:17, 17.53s/it]                                                        {'loss': 20.0331, 'grad_norm': 484.0, 'learning_rate': 1.928589971334054e-05, 'epoch': 4.23}
 67%|██████▋   | 3329/5000 [16:12:51<8:08:17, 17.53s/it] 67%|██████▋   | 3330/5000 [16:13:05<7:39:46, 16.52s/it]                                                        {'loss': 17.5902, 'grad_norm': 16.125, 'learning_rate': 1.9265218863713546e-05, 'epoch': 4.23}
 67%|██████▋   | 3330/5000 [16:13:05<7:39:46, 16.52s/it] 67%|██████▋   | 3331/5000 [16:13:19<7:16:08, 15.68s/it]                                                        {'loss': 18.59, 'grad_norm': 13.25, 'learning_rate': 1.9244544897014385e-05, 'epoch': 4.23}
 67%|██████▋   | 3331/5000 [16:13:19<7:16:08, 15.68s/it] 67%|██████▋   | 3332/5000 [16:13:36<7:26:43, 16.07s/it]                                                        {'loss': 17.1641, 'grad_norm': 10.9375, 'learning_rate': 1.9223877822286573e-05, 'epoch': 4.23}
 67%|██████▋   | 3332/5000 [16:13:36<7:26:43, 16.07s/it] 67%|██████▋   | 3333/5000 [16:13:56<8:01:36, 17.33s/it]                                                        {'loss': 19.0062, 'grad_norm': 10.5625, 'learning_rate': 1.9203217648570572e-05, 'epoch': 4.23}
 67%|██████▋   | 3333/5000 [16:13:56<8:01:36, 17.33s/it] 67%|██████▋   | 3334/5000 [16:14:12<7:54:03, 17.07s/it]                                                        {'loss': 18.0481, 'grad_norm': 11.9375, 'learning_rate': 1.918256438490386e-05, 'epoch': 4.23}
 67%|██████▋   | 3334/5000 [16:14:12<7:54:03, 17.07s/it] 67%|██████▋   | 3335/5000 [16:14:39<9:12:08, 19.90s/it]                                                        {'loss': 18.1078, 'grad_norm': 11.6875, 'learning_rate': 1.9161918040320865e-05, 'epoch': 4.23}
 67%|██████▋   | 3335/5000 [16:14:39<9:12:08, 19.90s/it] 67%|██████▋   | 3336/5000 [16:15:05<10:07:39, 21.91s/it]                                                         {'loss': 17.9828, 'grad_norm': 7.9375, 'learning_rate': 1.9141278623853016e-05, 'epoch': 4.24}
 67%|██████▋   | 3336/5000 [16:15:05<10:07:39, 21.91s/it] 67%|██████▋   | 3337/5000 [16:15:22<9:22:12, 20.28s/it]                                                         {'loss': 18.314, 'grad_norm': 15.375, 'learning_rate': 1.9120646144528663e-05, 'epoch': 4.24}
 67%|██████▋   | 3337/5000 [16:15:22<9:22:12, 20.28s/it] 67%|██████▋   | 3338/5000 [16:15:38<8:43:02, 18.88s/it]                                                        {'loss': 18.0938, 'grad_norm': 11.0, 'learning_rate': 1.9100020611373197e-05, 'epoch': 4.24}
 67%|██████▋   | 3338/5000 [16:15:38<8:43:02, 18.88s/it] 67%|██████▋   | 3339/5000 [16:15:59<9:00:45, 19.53s/it]                                                        {'loss': 20.5345, 'grad_norm': 50.75, 'learning_rate': 1.9079402033408895e-05, 'epoch': 4.24}
 67%|██████▋   | 3339/5000 [16:15:59<9:00:45, 19.53s/it] 67%|██████▋   | 3340/5000 [16:16:15<8:38:14, 18.73s/it]                                                        {'loss': 17.341, 'grad_norm': 13.3125, 'learning_rate': 1.9058790419655024e-05, 'epoch': 4.24}
 67%|██████▋   | 3340/5000 [16:16:15<8:38:14, 18.73s/it] 67%|██████▋   | 3341/5000 [16:16:32<8:23:19, 18.20s/it]                                                        {'loss': 17.4908, 'grad_norm': 13.25, 'learning_rate': 1.9038185779127823e-05, 'epoch': 4.24}
 67%|██████▋   | 3341/5000 [16:16:32<8:23:19, 18.20s/it] 67%|██████▋   | 3342/5000 [16:16:49<8:10:59, 17.77s/it]                                                        {'loss': 18.2798, 'grad_norm': 12.3125, 'learning_rate': 1.901758812084045e-05, 'epoch': 4.24}
 67%|██████▋   | 3342/5000 [16:16:49<8:10:59, 17.77s/it] 67%|██████▋   | 3343/5000 [16:17:05<7:51:15, 17.06s/it]                                                        {'loss': 17.9753, 'grad_norm': 13.5625, 'learning_rate': 1.899699745380301e-05, 'epoch': 4.25}
 67%|██████▋   | 3343/5000 [16:17:05<7:51:15, 17.06s/it] 67%|██████▋   | 3344/5000 [16:17:24<8:10:37, 17.78s/it]                                                        {'loss': 17.0784, 'grad_norm': 10.1875, 'learning_rate': 1.8976413787022586e-05, 'epoch': 4.25}
 67%|██████▋   | 3344/5000 [16:17:24<8:10:37, 17.78s/it] 67%|██████▋   | 3345/5000 [16:17:38<7:35:36, 16.52s/it]                                                        {'loss': 18.2153, 'grad_norm': 13.125, 'learning_rate': 1.895583712950316e-05, 'epoch': 4.25}
 67%|██████▋   | 3345/5000 [16:17:38<7:35:36, 16.52s/it] 67%|██████▋   | 3346/5000 [16:17:52<7:17:52, 15.88s/it]                                                        {'loss': 20.1788, 'grad_norm': 21.375, 'learning_rate': 1.8935267490245656e-05, 'epoch': 4.25}
 67%|██████▋   | 3346/5000 [16:17:52<7:17:52, 15.88s/it] 67%|██████▋   | 3347/5000 [16:18:08<7:17:11, 15.87s/it]                                                        {'loss': 17.6128, 'grad_norm': 13.9375, 'learning_rate': 1.8914704878247925e-05, 'epoch': 4.25}
 67%|██████▋   | 3347/5000 [16:18:08<7:17:11, 15.87s/it] 67%|██████▋   | 3348/5000 [16:18:23<7:08:12, 15.55s/it]                                                        {'loss': 16.8872, 'grad_norm': 13.0, 'learning_rate': 1.889414930250478e-05, 'epoch': 4.25}
 67%|██████▋   | 3348/5000 [16:18:23<7:08:12, 15.55s/it] 67%|██████▋   | 3349/5000 [16:18:40<7:24:37, 16.16s/it]                                                        {'loss': 17.1019, 'grad_norm': 7.65625, 'learning_rate': 1.8873600772007907e-05, 'epoch': 4.25}
 67%|██████▋   | 3349/5000 [16:18:40<7:24:37, 16.16s/it] 67%|██████▋   | 3350/5000 [16:19:05<8:31:07, 18.59s/it]                                                        {'loss': 18.2833, 'grad_norm': 12.5, 'learning_rate': 1.885305929574593e-05, 'epoch': 4.25}
 67%|██████▋   | 3350/5000 [16:19:05<8:31:07, 18.59s/it] 67%|██████▋   | 3351/5000 [16:19:19<8:00:55, 17.50s/it]                                                        {'loss': 18.6706, 'grad_norm': 12.75, 'learning_rate': 1.88325248827044e-05, 'epoch': 4.26}
 67%|██████▋   | 3351/5000 [16:19:19<8:00:55, 17.50s/it] 67%|██████▋   | 3352/5000 [16:19:35<7:41:42, 16.81s/it]                                                        {'loss': 17.5731, 'grad_norm': 32.75, 'learning_rate': 1.881199754186577e-05, 'epoch': 4.26}
 67%|██████▋   | 3352/5000 [16:19:35<7:41:42, 16.81s/it] 67%|██████▋   | 3353/5000 [16:19:54<8:02:37, 17.58s/it]                                                        {'loss': 17.8327, 'grad_norm': 14.875, 'learning_rate': 1.8791477282209374e-05, 'epoch': 4.26}
 67%|██████▋   | 3353/5000 [16:19:54<8:02:37, 17.58s/it] 67%|██████▋   | 3354/5000 [16:20:12<8:03:12, 17.61s/it]                                                        {'loss': 16.9518, 'grad_norm': 8.8125, 'learning_rate': 1.8770964112711498e-05, 'epoch': 4.26}
 67%|██████▋   | 3354/5000 [16:20:12<8:03:12, 17.61s/it] 67%|██████▋   | 3355/5000 [16:20:32<8:20:37, 18.26s/it]                                                        {'loss': 17.7954, 'grad_norm': 13.8125, 'learning_rate': 1.8750458042345282e-05, 'epoch': 4.26}
 67%|██████▋   | 3355/5000 [16:20:32<8:20:37, 18.26s/it] 67%|██████▋   | 3356/5000 [16:20:46<7:48:22, 17.09s/it]                                                        {'loss': 17.4924, 'grad_norm': 9.875, 'learning_rate': 1.8729959080080775e-05, 'epoch': 4.26}
 67%|██████▋   | 3356/5000 [16:20:46<7:48:22, 17.09s/it] 67%|██████▋   | 3357/5000 [16:21:01<7:33:07, 16.55s/it]                                                        {'loss': 18.1884, 'grad_norm': 24.125, 'learning_rate': 1.870946723488494e-05, 'epoch': 4.26}
 67%|██████▋   | 3357/5000 [16:21:01<7:33:07, 16.55s/it] 67%|██████▋   | 3358/5000 [16:21:24<8:25:09, 18.46s/it]                                                        {'loss': 16.0115, 'grad_norm': 9.625, 'learning_rate': 1.8688982515721574e-05, 'epoch': 4.26}
 67%|██████▋   | 3358/5000 [16:21:24<8:25:09, 18.46s/it] 67%|██████▋   | 3359/5000 [16:21:39<7:53:44, 17.32s/it]                                                        {'loss': 19.1479, 'grad_norm': 20.875, 'learning_rate': 1.8668504931551413e-05, 'epoch': 4.27}
 67%|██████▋   | 3359/5000 [16:21:39<7:53:44, 17.32s/it] 67%|██████▋   | 3360/5000 [16:21:54<7:36:13, 16.69s/it]                                                        {'loss': 20.6158, 'grad_norm': 55.0, 'learning_rate': 1.8648034491332057e-05, 'epoch': 4.27}
 67%|██████▋   | 3360/5000 [16:21:54<7:36:13, 16.69s/it] 67%|██████▋   | 3361/5000 [16:22:07<7:06:32, 15.61s/it]                                                        {'loss': 18.876, 'grad_norm': 15.3125, 'learning_rate': 1.8627571204017957e-05, 'epoch': 4.27}
 67%|██████▋   | 3361/5000 [16:22:07<7:06:32, 15.61s/it] 67%|██████▋   | 3362/5000 [16:22:30<8:06:18, 17.81s/it]                                                        {'loss': 17.9396, 'grad_norm': 20.125, 'learning_rate': 1.860711507856045e-05, 'epoch': 4.27}
 67%|██████▋   | 3362/5000 [16:22:30<8:06:18, 17.81s/it] 67%|██████▋   | 3363/5000 [16:22:45<7:46:28, 17.10s/it]                                                        {'loss': 18.5296, 'grad_norm': 18.125, 'learning_rate': 1.8586666123907733e-05, 'epoch': 4.27}
 67%|██████▋   | 3363/5000 [16:22:45<7:46:28, 17.10s/it] 67%|██████▋   | 3364/5000 [16:23:08<8:29:52, 18.70s/it]                                                        {'loss': 19.2072, 'grad_norm': 20.625, 'learning_rate': 1.856622434900489e-05, 'epoch': 4.27}
 67%|██████▋   | 3364/5000 [16:23:08<8:29:52, 18.70s/it] 67%|██████▋   | 3365/5000 [16:23:25<8:15:05, 18.17s/it]                                                        {'loss': 17.301, 'grad_norm': 16.375, 'learning_rate': 1.854578976279384e-05, 'epoch': 4.27}
 67%|██████▋   | 3365/5000 [16:23:25<8:15:05, 18.17s/it] 67%|██████▋   | 3366/5000 [16:23:44<8:22:08, 18.44s/it]                                                        {'loss': 17.8889, 'grad_norm': 13.4375, 'learning_rate': 1.852536237421336e-05, 'epoch': 4.27}
 67%|██████▋   | 3366/5000 [16:23:44<8:22:08, 18.44s/it] 67%|██████▋   | 3367/5000 [16:24:03<8:30:25, 18.75s/it]                                                        {'loss': 18.1549, 'grad_norm': 21.375, 'learning_rate': 1.8504942192199092e-05, 'epoch': 4.28}
 67%|██████▋   | 3367/5000 [16:24:03<8:30:25, 18.75s/it] 67%|██████▋   | 3368/5000 [16:24:17<7:51:19, 17.33s/it]                                                        {'loss': 19.8813, 'grad_norm': 63.5, 'learning_rate': 1.8484529225683518e-05, 'epoch': 4.28}
 67%|██████▋   | 3368/5000 [16:24:17<7:51:19, 17.33s/it] 67%|██████▋   | 3369/5000 [16:24:40<8:33:30, 18.89s/it]                                                        {'loss': 16.1282, 'grad_norm': 6.46875, 'learning_rate': 1.846412348359595e-05, 'epoch': 4.28}
 67%|██████▋   | 3369/5000 [16:24:40<8:33:30, 18.89s/it] 67%|██████▋   | 3370/5000 [16:24:55<8:00:28, 17.69s/it]                                                        {'loss': 19.2957, 'grad_norm': 14.5625, 'learning_rate': 1.8443724974862572e-05, 'epoch': 4.28}
 67%|██████▋   | 3370/5000 [16:24:55<8:00:28, 17.69s/it] 67%|██████▋   | 3371/5000 [16:25:10<7:41:48, 17.01s/it]                                                        {'loss': 18.1451, 'grad_norm': 9.5, 'learning_rate': 1.8423333708406377e-05, 'epoch': 4.28}
 67%|██████▋   | 3371/5000 [16:25:10<7:41:48, 17.01s/it] 67%|██████▋   | 3372/5000 [16:25:26<7:34:26, 16.75s/it]                                                        {'loss': 17.2503, 'grad_norm': 12.375, 'learning_rate': 1.8402949693147176e-05, 'epoch': 4.28}
 67%|██████▋   | 3372/5000 [16:25:26<7:34:26, 16.75s/it] 67%|██████▋   | 3373/5000 [16:25:41<7:20:21, 16.24s/it]                                                        {'loss': 16.7042, 'grad_norm': 10.5625, 'learning_rate': 1.8382572938001673e-05, 'epoch': 4.28}
 67%|██████▋   | 3373/5000 [16:25:41<7:20:21, 16.24s/it] 67%|██████▋   | 3374/5000 [16:25:58<7:20:39, 16.26s/it]                                                        {'loss': 17.5479, 'grad_norm': 13.5625, 'learning_rate': 1.8362203451883322e-05, 'epoch': 4.28}
 67%|██████▋   | 3374/5000 [16:25:58<7:20:39, 16.26s/it] 68%|██████▊   | 3375/5000 [16:26:13<7:14:38, 16.05s/it]                                                        {'loss': 17.7201, 'grad_norm': 19.875, 'learning_rate': 1.8341841243702424e-05, 'epoch': 4.29}
 68%|██████▊   | 3375/5000 [16:26:13<7:14:38, 16.05s/it] 68%|██████▊   | 3376/5000 [16:26:30<7:22:02, 16.33s/it]                                                        {'loss': 18.7838, 'grad_norm': 11.3125, 'learning_rate': 1.832148632236613e-05, 'epoch': 4.29}
 68%|██████▊   | 3376/5000 [16:26:30<7:22:02, 16.33s/it] 68%|██████▊   | 3377/5000 [16:26:44<7:02:51, 15.63s/it]                                                        {'loss': 17.8636, 'grad_norm': 37.25, 'learning_rate': 1.830113869677835e-05, 'epoch': 4.29}
 68%|██████▊   | 3377/5000 [16:26:44<7:02:51, 15.63s/it] 68%|██████▊   | 3378/5000 [16:26:58<6:49:36, 15.15s/it]                                                        {'loss': 18.686, 'grad_norm': 13.625, 'learning_rate': 1.8280798375839822e-05, 'epoch': 4.29}
 68%|██████▊   | 3378/5000 [16:26:58<6:49:36, 15.15s/it] 68%|██████▊   | 3379/5000 [16:27:25<8:22:08, 18.59s/it]                                                        {'loss': 17.4402, 'grad_norm': 14.25, 'learning_rate': 1.826046536844812e-05, 'epoch': 4.29}
 68%|██████▊   | 3379/5000 [16:27:25<8:22:08, 18.59s/it] 68%|██████▊   | 3380/5000 [16:27:45<8:31:59, 18.96s/it]                                                        {'loss': 16.9723, 'grad_norm': 13.8125, 'learning_rate': 1.824013968349756e-05, 'epoch': 4.29}
 68%|██████▊   | 3380/5000 [16:27:45<8:31:59, 18.96s/it] 68%|██████▊   | 3381/5000 [16:27:59<7:51:12, 17.46s/it]                                                        {'loss': 17.4727, 'grad_norm': 57.0, 'learning_rate': 1.8219821329879325e-05, 'epoch': 4.29}
 68%|██████▊   | 3381/5000 [16:27:59<7:51:12, 17.46s/it] 68%|██████▊   | 3382/5000 [16:28:15<7:38:40, 17.01s/it]                                                        {'loss': 17.5816, 'grad_norm': 13.375, 'learning_rate': 1.8199510316481313e-05, 'epoch': 4.29}
 68%|██████▊   | 3382/5000 [16:28:15<7:38:40, 17.01s/it] 68%|██████▊   | 3383/5000 [16:28:32<7:41:34, 17.13s/it]                                                        {'loss': 18.2624, 'grad_norm': 19.375, 'learning_rate': 1.8179206652188288e-05, 'epoch': 4.3}
 68%|██████▊   | 3383/5000 [16:28:32<7:41:34, 17.13s/it] 68%|██████▊   | 3384/5000 [16:28:58<8:49:39, 19.67s/it]                                                        {'loss': 17.1687, 'grad_norm': 10.6875, 'learning_rate': 1.815891034588174e-05, 'epoch': 4.3}
 68%|██████▊   | 3384/5000 [16:28:58<8:49:39, 19.67s/it] 68%|██████▊   | 3385/5000 [16:29:10<7:49:39, 17.45s/it]                                                        {'loss': 20.3683, 'grad_norm': 15.625, 'learning_rate': 1.8138621406439958e-05, 'epoch': 4.3}
 68%|██████▊   | 3385/5000 [16:29:10<7:49:39, 17.45s/it] 68%|██████▊   | 3386/5000 [16:29:23<7:11:18, 16.03s/it]                                                        {'loss': 19.9789, 'grad_norm': 16.125, 'learning_rate': 1.8118339842738035e-05, 'epoch': 4.3}
 68%|██████▊   | 3386/5000 [16:29:23<7:11:18, 16.03s/it] 68%|██████▊   | 3387/5000 [16:29:58<9:50:09, 21.95s/it]                                                        {'loss': 16.1226, 'grad_norm': 6.53125, 'learning_rate': 1.809806566364781e-05, 'epoch': 4.3}
 68%|██████▊   | 3387/5000 [16:29:58<9:50:09, 21.95s/it] 68%|██████▊   | 3388/5000 [16:30:25<10:24:20, 23.24s/it]                                                         {'loss': 17.1499, 'grad_norm': 14.8125, 'learning_rate': 1.807779887803787e-05, 'epoch': 4.3}
 68%|██████▊   | 3388/5000 [16:30:25<10:24:20, 23.24s/it] 68%|██████▊   | 3389/5000 [16:30:41<9:31:15, 21.28s/it]                                                         {'loss': 18.1726, 'grad_norm': 16.5, 'learning_rate': 1.8057539494773637e-05, 'epoch': 4.3}
 68%|██████▊   | 3389/5000 [16:30:41<9:31:15, 21.28s/it] 68%|██████▊   | 3390/5000 [16:30:59<9:00:28, 20.14s/it]                                                        {'loss': 17.3784, 'grad_norm': 13.25, 'learning_rate': 1.8037287522717234e-05, 'epoch': 4.3}
 68%|██████▊   | 3390/5000 [16:30:59<9:00:28, 20.14s/it] 68%|██████▊   | 3391/5000 [16:31:16<8:36:56, 19.28s/it]                                                        {'loss': 17.6546, 'grad_norm': 9.625, 'learning_rate': 1.8017042970727546e-05, 'epoch': 4.31}
 68%|██████▊   | 3391/5000 [16:31:16<8:36:56, 19.28s/it] 68%|██████▊   | 3392/5000 [16:31:33<8:19:17, 18.63s/it]                                                        {'loss': 17.0702, 'grad_norm': 12.375, 'learning_rate': 1.799680584766026e-05, 'epoch': 4.31}
 68%|██████▊   | 3392/5000 [16:31:33<8:19:17, 18.63s/it] 68%|██████▊   | 3393/5000 [16:31:46<7:34:53, 16.98s/it]                                                        {'loss': 19.3941, 'grad_norm': 20.875, 'learning_rate': 1.797657616236776e-05, 'epoch': 4.31}
 68%|██████▊   | 3393/5000 [16:31:46<7:34:53, 16.98s/it] 68%|██████▊   | 3394/5000 [16:32:04<7:38:42, 17.14s/it]                                                        {'loss': 17.6312, 'grad_norm': 16.75, 'learning_rate': 1.7956353923699198e-05, 'epoch': 4.31}
 68%|██████▊   | 3394/5000 [16:32:04<7:38:42, 17.14s/it] 68%|██████▊   | 3395/5000 [16:32:28<8:33:27, 19.19s/it]                                                        {'loss': 18.7148, 'grad_norm': 31.125, 'learning_rate': 1.7936139140500478e-05, 'epoch': 4.31}
 68%|██████▊   | 3395/5000 [16:32:28<8:33:27, 19.19s/it] 68%|██████▊   | 3396/5000 [16:32:51<9:05:32, 20.41s/it]                                                        {'loss': 18.1856, 'grad_norm': 15.1875, 'learning_rate': 1.791593182161423e-05, 'epoch': 4.31}
 68%|██████▊   | 3396/5000 [16:32:51<9:05:32, 20.41s/it] 68%|██████▊   | 3397/5000 [16:33:12<9:12:26, 20.68s/it]                                                        {'loss': 17.5934, 'grad_norm': 21.5, 'learning_rate': 1.789573197587982e-05, 'epoch': 4.31}
 68%|██████▊   | 3397/5000 [16:33:12<9:12:26, 20.68s/it] 68%|██████▊   | 3398/5000 [16:33:28<8:32:10, 19.18s/it]                                                        {'loss': 17.9786, 'grad_norm': 13.0, 'learning_rate': 1.7875539612133335e-05, 'epoch': 4.31}
 68%|██████▊   | 3398/5000 [16:33:28<8:32:10, 19.18s/it] 68%|██████▊   | 3399/5000 [16:33:53<9:20:41, 21.01s/it]                                                        {'loss': 19.3853, 'grad_norm': 27.875, 'learning_rate': 1.7855354739207624e-05, 'epoch': 4.32}
 68%|██████▊   | 3399/5000 [16:33:53<9:20:41, 21.01s/it] 68%|██████▊   | 3400/5000 [16:34:14<9:16:23, 20.86s/it]                                                        {'loss': 16.5299, 'grad_norm': 24.0, 'learning_rate': 1.7835177365932225e-05, 'epoch': 4.32}
 68%|██████▊   | 3400/5000 [16:34:14<9:16:23, 20.86s/it] 68%|██████▊   | 3401/5000 [16:34:31<8:43:46, 19.65s/it]                                                        {'loss': 17.8948, 'grad_norm': 32.75, 'learning_rate': 1.7815007501133393e-05, 'epoch': 4.32}
 68%|██████▊   | 3401/5000 [16:34:31<8:43:46, 19.65s/it] 68%|██████▊   | 3402/5000 [16:34:46<8:05:17, 18.22s/it]                                                        {'loss': 19.3039, 'grad_norm': 23.625, 'learning_rate': 1.779484515363414e-05, 'epoch': 4.32}
 68%|██████▊   | 3402/5000 [16:34:46<8:05:17, 18.22s/it] 68%|██████▊   | 3403/5000 [16:35:00<7:30:53, 16.94s/it]                                                        {'loss': 16.7392, 'grad_norm': 11.625, 'learning_rate': 1.7774690332254132e-05, 'epoch': 4.32}
 68%|██████▊   | 3403/5000 [16:35:00<7:30:53, 16.94s/it] 68%|██████▊   | 3404/5000 [16:35:18<7:42:14, 17.38s/it]                                                        {'loss': 18.1737, 'grad_norm': 12.125, 'learning_rate': 1.7754543045809797e-05, 'epoch': 4.32}
 68%|██████▊   | 3404/5000 [16:35:18<7:42:14, 17.38s/it] 68%|██████▊   | 3405/5000 [16:35:37<7:58:58, 18.02s/it]                                                        {'loss': 16.8799, 'grad_norm': 11.3125, 'learning_rate': 1.7734403303114226e-05, 'epoch': 4.32}
 68%|██████▊   | 3405/5000 [16:35:37<7:58:58, 18.02s/it] 68%|██████▊   | 3406/5000 [16:35:51<7:21:20, 16.61s/it]                                                        {'loss': 19.3451, 'grad_norm': 15.5, 'learning_rate': 1.7714271112977242e-05, 'epoch': 4.33}
 68%|██████▊   | 3406/5000 [16:35:51<7:21:20, 16.61s/it] 68%|██████▊   | 3407/5000 [16:36:16<8:28:50, 19.17s/it]                                                        {'loss': 17.6981, 'grad_norm': 7.8125, 'learning_rate': 1.7694146484205327e-05, 'epoch': 4.33}
 68%|██████▊   | 3407/5000 [16:36:16<8:28:50, 19.17s/it] 68%|██████▊   | 3408/5000 [16:36:36<8:36:12, 19.46s/it]                                                        {'loss': 19.7444, 'grad_norm': 20.75, 'learning_rate': 1.767402942560171e-05, 'epoch': 4.33}
 68%|██████▊   | 3408/5000 [16:36:36<8:36:12, 19.46s/it] 68%|██████▊   | 3409/5000 [16:36:55<8:29:58, 19.23s/it]                                                        {'loss': 18.1058, 'grad_norm': 21.125, 'learning_rate': 1.7653919945966256e-05, 'epoch': 4.33}
 68%|██████▊   | 3409/5000 [16:36:55<8:29:58, 19.23s/it] 68%|██████▊   | 3410/5000 [16:37:08<7:39:44, 17.35s/it]                                                        {'loss': 20.6027, 'grad_norm': 18.125, 'learning_rate': 1.763381805409552e-05, 'epoch': 4.33}
 68%|██████▊   | 3410/5000 [16:37:08<7:39:44, 17.35s/it] 68%|██████▊   | 3411/5000 [16:37:21<7:03:26, 15.99s/it]                                                        {'loss': 17.3356, 'grad_norm': 13.1875, 'learning_rate': 1.7613723758782783e-05, 'epoch': 4.33}
 68%|██████▊   | 3411/5000 [16:37:21<7:03:26, 15.99s/it] 68%|██████▊   | 3412/5000 [16:37:36<6:56:11, 15.73s/it]                                                        {'loss': 17.4012, 'grad_norm': 16.75, 'learning_rate': 1.759363706881796e-05, 'epoch': 4.33}
 68%|██████▊   | 3412/5000 [16:37:36<6:56:11, 15.73s/it] 68%|██████▊   | 3413/5000 [16:37:51<6:52:42, 15.60s/it]                                                        {'loss': 17.9176, 'grad_norm': 17.25, 'learning_rate': 1.7573557992987627e-05, 'epoch': 4.33}
 68%|██████▊   | 3413/5000 [16:37:51<6:52:42, 15.60s/it] 68%|██████▊   | 3414/5000 [16:38:12<7:34:02, 17.18s/it]                                                        {'loss': 18.0711, 'grad_norm': 16.0, 'learning_rate': 1.7553486540075094e-05, 'epoch': 4.34}
 68%|██████▊   | 3414/5000 [16:38:12<7:34:02, 17.18s/it] 68%|██████▊   | 3415/5000 [16:38:27<7:16:23, 16.52s/it]                                                        {'loss': 18.3887, 'grad_norm': 19.75, 'learning_rate': 1.753342271886028e-05, 'epoch': 4.34}
 68%|██████▊   | 3415/5000 [16:38:27<7:16:23, 16.52s/it] 68%|██████▊   | 3416/5000 [16:38:42<7:07:30, 16.19s/it]                                                        {'loss': 16.6813, 'grad_norm': 8.6875, 'learning_rate': 1.751336653811978e-05, 'epoch': 4.34}
 68%|██████▊   | 3416/5000 [16:38:42<7:07:30, 16.19s/it] 68%|██████▊   | 3417/5000 [16:38:57<6:59:36, 15.90s/it]                                                        {'loss': 19.4448, 'grad_norm': 16.75, 'learning_rate': 1.7493318006626837e-05, 'epoch': 4.34}
 68%|██████▊   | 3417/5000 [16:38:57<6:59:36, 15.90s/it] 68%|██████▊   | 3418/5000 [16:39:13<6:54:38, 15.73s/it]                                                        {'loss': 17.3676, 'grad_norm': 9.375, 'learning_rate': 1.7473277133151382e-05, 'epoch': 4.34}
 68%|██████▊   | 3418/5000 [16:39:13<6:54:38, 15.73s/it] 68%|██████▊   | 3419/5000 [16:39:36<7:50:56, 17.87s/it]                                                        {'loss': 19.221, 'grad_norm': 67.0, 'learning_rate': 1.745324392645997e-05, 'epoch': 4.34}
 68%|██████▊   | 3419/5000 [16:39:36<7:50:56, 17.87s/it] 68%|██████▊   | 3420/5000 [16:39:58<8:22:41, 19.09s/it]                                                        {'loss': 17.9477, 'grad_norm': 12.0625, 'learning_rate': 1.7433218395315782e-05, 'epoch': 4.34}
 68%|██████▊   | 3420/5000 [16:39:58<8:22:41, 19.09s/it] 68%|██████▊   | 3421/5000 [16:40:13<7:50:15, 17.87s/it]                                                        {'loss': 17.5195, 'grad_norm': 13.3125, 'learning_rate': 1.74132005484787e-05, 'epoch': 4.34}
 68%|██████▊   | 3421/5000 [16:40:13<7:50:15, 17.87s/it] 68%|██████▊   | 3422/5000 [16:40:26<7:16:46, 16.61s/it]                                                        {'loss': 18.7285, 'grad_norm': 20.375, 'learning_rate': 1.739319039470519e-05, 'epoch': 4.35}
 68%|██████▊   | 3422/5000 [16:40:26<7:16:46, 16.61s/it] 68%|██████▊   | 3423/5000 [16:40:39<6:44:58, 15.41s/it]                                                        {'loss': 18.5224, 'grad_norm': 36.5, 'learning_rate': 1.737318794274837e-05, 'epoch': 4.35}
 68%|██████▊   | 3423/5000 [16:40:39<6:44:58, 15.41s/it] 68%|██████▊   | 3424/5000 [16:40:57<7:09:35, 16.36s/it]                                                        {'loss': 17.9078, 'grad_norm': 12.0, 'learning_rate': 1.735319320135801e-05, 'epoch': 4.35}
 68%|██████▊   | 3424/5000 [16:40:57<7:09:35, 16.36s/it] 68%|██████▊   | 3425/5000 [16:41:21<8:05:16, 18.49s/it]                                                        {'loss': 16.9047, 'grad_norm': 20.0, 'learning_rate': 1.7333206179280478e-05, 'epoch': 4.35}
 68%|██████▊   | 3425/5000 [16:41:21<8:05:16, 18.49s/it] 69%|██████▊   | 3426/5000 [16:41:38<7:54:13, 18.08s/it]                                                        {'loss': 18.2131, 'grad_norm': 65.5, 'learning_rate': 1.7313226885258765e-05, 'epoch': 4.35}
 69%|██████▊   | 3426/5000 [16:41:38<7:54:13, 18.08s/it] 69%|██████▊   | 3427/5000 [16:41:55<7:47:44, 17.84s/it]                                                        {'loss': 18.2231, 'grad_norm': 21.625, 'learning_rate': 1.7293255328032523e-05, 'epoch': 4.35}
 69%|██████▊   | 3427/5000 [16:41:55<7:47:44, 17.84s/it] 69%|██████▊   | 3428/5000 [16:42:21<8:51:38, 20.29s/it]                                                        {'loss': 21.4929, 'grad_norm': 450.0, 'learning_rate': 1.727329151633795e-05, 'epoch': 4.35}
 69%|██████▊   | 3428/5000 [16:42:21<8:51:38, 20.29s/it] 69%|██████▊   | 3429/5000 [16:42:34<7:54:54, 18.14s/it]                                                        {'loss': 18.3768, 'grad_norm': 12.25, 'learning_rate': 1.725333545890793e-05, 'epoch': 4.35}
 69%|██████▊   | 3429/5000 [16:42:34<7:54:54, 18.14s/it] 69%|██████▊   | 3430/5000 [16:42:48<7:20:22, 16.83s/it]                                                        {'loss': 19.1389, 'grad_norm': 17.25, 'learning_rate': 1.7233387164471902e-05, 'epoch': 4.36}
 69%|██████▊   | 3430/5000 [16:42:48<7:20:22, 16.83s/it] 69%|██████▊   | 3431/5000 [16:43:04<7:08:25, 16.38s/it]                                                        {'loss': 19.1771, 'grad_norm': 12.875, 'learning_rate': 1.721344664175594e-05, 'epoch': 4.36}
 69%|██████▊   | 3431/5000 [16:43:04<7:08:25, 16.38s/it] 69%|██████▊   | 3432/5000 [16:43:22<7:22:57, 16.95s/it]                                                        {'loss': 17.5804, 'grad_norm': 16.875, 'learning_rate': 1.7193513899482707e-05, 'epoch': 4.36}
 69%|██████▊   | 3432/5000 [16:43:22<7:22:57, 16.95s/it] 69%|██████▊   | 3433/5000 [16:43:40<7:33:59, 17.38s/it]                                                        {'loss': 19.4728, 'grad_norm': 15.25, 'learning_rate': 1.7173588946371438e-05, 'epoch': 4.36}
 69%|██████▊   | 3433/5000 [16:43:40<7:33:59, 17.38s/it] 69%|██████▊   | 3434/5000 [16:44:03<8:19:50, 19.15s/it]                                                        {'loss': 17.0722, 'grad_norm': 7.78125, 'learning_rate': 1.715367179113801e-05, 'epoch': 4.36}
 69%|██████▊   | 3434/5000 [16:44:03<8:19:50, 19.15s/it] 69%|██████▊   | 3435/5000 [16:44:19<7:52:08, 18.10s/it]                                                        {'loss': 17.789, 'grad_norm': 10.4375, 'learning_rate': 1.7133762442494858e-05, 'epoch': 4.36}
 69%|██████▊   | 3435/5000 [16:44:19<7:52:08, 18.10s/it] 69%|██████▊   | 3436/5000 [16:44:32<7:10:42, 16.52s/it]                                                        {'loss': 19.7355, 'grad_norm': 41.5, 'learning_rate': 1.711386090915099e-05, 'epoch': 4.36}
 69%|██████▊   | 3436/5000 [16:44:32<7:10:42, 16.52s/it] 69%|██████▊   | 3437/5000 [16:44:49<7:15:26, 16.72s/it]                                                        {'loss': 19.3414, 'grad_norm': 140.0, 'learning_rate': 1.7093967199812036e-05, 'epoch': 4.36}
 69%|██████▊   | 3437/5000 [16:44:49<7:15:26, 16.72s/it] 69%|██████▉   | 3438/5000 [16:45:04<7:01:45, 16.20s/it]                                                        {'loss': 18.4957, 'grad_norm': 16.0, 'learning_rate': 1.7074081323180174e-05, 'epoch': 4.37}
 69%|██████▉   | 3438/5000 [16:45:04<7:01:45, 16.20s/it] 69%|██████▉   | 3439/5000 [16:45:22<7:11:28, 16.58s/it]                                                        {'loss': 16.7688, 'grad_norm': 24.875, 'learning_rate': 1.7054203287954144e-05, 'epoch': 4.37}
 69%|██████▉   | 3439/5000 [16:45:22<7:11:28, 16.58s/it] 69%|██████▉   | 3440/5000 [16:45:46<8:14:24, 19.02s/it]                                                        {'loss': 16.7542, 'grad_norm': 9.5625, 'learning_rate': 1.7034333102829306e-05, 'epoch': 4.37}
 69%|██████▉   | 3440/5000 [16:45:46<8:14:24, 19.02s/it] 69%|██████▉   | 3441/5000 [16:46:04<8:02:29, 18.57s/it]                                                        {'loss': 17.1708, 'grad_norm': 19.0, 'learning_rate': 1.701447077649753e-05, 'epoch': 4.37}
 69%|██████▉   | 3441/5000 [16:46:04<8:02:29, 18.57s/it] 69%|██████▉   | 3442/5000 [16:46:21<7:51:46, 18.17s/it]                                                        {'loss': 19.0655, 'grad_norm': 19.125, 'learning_rate': 1.699461631764727e-05, 'epoch': 4.37}
 69%|██████▉   | 3442/5000 [16:46:21<7:51:46, 18.17s/it] 69%|██████▉   | 3443/5000 [16:46:35<7:19:06, 16.92s/it]                                                        {'loss': 18.504, 'grad_norm': 14.6875, 'learning_rate': 1.6974769734963564e-05, 'epoch': 4.37}
 69%|██████▉   | 3443/5000 [16:46:35<7:19:06, 16.92s/it] 69%|██████▉   | 3444/5000 [16:46:51<7:08:07, 16.51s/it]                                                        {'loss': 16.529, 'grad_norm': 9.5, 'learning_rate': 1.6954931037127965e-05, 'epoch': 4.37}
 69%|██████▉   | 3444/5000 [16:46:51<7:08:07, 16.51s/it] 69%|██████▉   | 3445/5000 [16:47:06<7:01:09, 16.25s/it]                                                        {'loss': 19.5471, 'grad_norm': 19.375, 'learning_rate': 1.6935100232818586e-05, 'epoch': 4.37}
 69%|██████▉   | 3445/5000 [16:47:06<7:01:09, 16.25s/it] 69%|██████▉   | 3446/5000 [16:47:33<8:23:46, 19.45s/it]                                                        {'loss': 17.8547, 'grad_norm': 11.9375, 'learning_rate': 1.6915277330710114e-05, 'epoch': 4.38}
 69%|██████▉   | 3446/5000 [16:47:33<8:23:46, 19.45s/it] 69%|██████▉   | 3447/5000 [16:47:47<7:41:27, 17.83s/it]                                                        {'loss': 19.3749, 'grad_norm': 10.375, 'learning_rate': 1.689546233947375e-05, 'epoch': 4.38}
 69%|██████▉   | 3447/5000 [16:47:47<7:41:27, 17.83s/it] 69%|██████▉   | 3448/5000 [16:48:02<7:18:04, 16.94s/it]                                                        {'loss': 19.3872, 'grad_norm': 17.375, 'learning_rate': 1.6875655267777234e-05, 'epoch': 4.38}
 69%|██████▉   | 3448/5000 [16:48:02<7:18:04, 16.94s/it] 69%|██████▉   | 3449/5000 [16:48:16<6:51:35, 15.92s/it]                                                        {'loss': 18.4746, 'grad_norm': 10.375, 'learning_rate': 1.6855856124284875e-05, 'epoch': 4.38}
 69%|██████▉   | 3449/5000 [16:48:16<6:51:35, 15.92s/it] 69%|██████▉   | 3450/5000 [16:48:32<6:57:57, 16.18s/it]                                                        {'loss': 17.6189, 'grad_norm': 12.375, 'learning_rate': 1.6836064917657478e-05, 'epoch': 4.38}
 69%|██████▉   | 3450/5000 [16:48:32<6:57:57, 16.18s/it] 69%|██████▉   | 3451/5000 [16:48:48<6:56:01, 16.11s/it]                                                        {'loss': 16.1837, 'grad_norm': 8.875, 'learning_rate': 1.6816281656552376e-05, 'epoch': 4.38}
 69%|██████▉   | 3451/5000 [16:48:48<6:56:01, 16.11s/it] 69%|██████▉   | 3452/5000 [16:49:01<6:30:29, 15.14s/it]                                                        {'loss': 18.7274, 'grad_norm': 11.0, 'learning_rate': 1.6796506349623468e-05, 'epoch': 4.38}
 69%|██████▉   | 3452/5000 [16:49:01<6:30:29, 15.14s/it] 69%|██████▉   | 3453/5000 [16:49:26<7:43:37, 17.98s/it]                                                        {'loss': 18.3365, 'grad_norm': 19.375, 'learning_rate': 1.677673900552112e-05, 'epoch': 4.38}
 69%|██████▉   | 3453/5000 [16:49:26<7:43:37, 17.98s/it] 69%|██████▉   | 3454/5000 [16:49:51<8:40:07, 20.19s/it]                                                        {'loss': 19.7179, 'grad_norm': 17.375, 'learning_rate': 1.675697963289226e-05, 'epoch': 4.39}
 69%|██████▉   | 3454/5000 [16:49:51<8:40:07, 20.19s/it] 69%|██████▉   | 3455/5000 [16:50:05<7:50:17, 18.26s/it]                                                        {'loss': 19.1274, 'grad_norm': 14.6875, 'learning_rate': 1.6737228240380286e-05, 'epoch': 4.39}
 69%|██████▉   | 3455/5000 [16:50:05<7:50:17, 18.26s/it] 69%|██████▉   | 3456/5000 [16:50:22<7:39:53, 17.87s/it]                                                        {'loss': 17.4859, 'grad_norm': 11.6875, 'learning_rate': 1.6717484836625155e-05, 'epoch': 4.39}
 69%|██████▉   | 3456/5000 [16:50:22<7:39:53, 17.87s/it] 69%|██████▉   | 3457/5000 [16:50:42<7:54:36, 18.46s/it]                                                        {'loss': 17.6313, 'grad_norm': 9.6875, 'learning_rate': 1.669774943026329e-05, 'epoch': 4.39}
 69%|██████▉   | 3457/5000 [16:50:42<7:54:36, 18.46s/it] 69%|██████▉   | 3458/5000 [16:50:55<7:17:57, 17.04s/it]                                                        {'loss': 18.4239, 'grad_norm': 13.875, 'learning_rate': 1.6678022029927614e-05, 'epoch': 4.39}
 69%|██████▉   | 3458/5000 [16:50:55<7:17:57, 17.04s/it] 69%|██████▉   | 3459/5000 [16:51:10<7:01:39, 16.42s/it]                                                        {'loss': 16.7031, 'grad_norm': 10.625, 'learning_rate': 1.665830264424759e-05, 'epoch': 4.39}
 69%|██████▉   | 3459/5000 [16:51:10<7:01:39, 16.42s/it] 69%|██████▉   | 3460/5000 [16:51:26<6:55:27, 16.19s/it]                                                        {'loss': 18.434, 'grad_norm': 13.125, 'learning_rate': 1.663859128184913e-05, 'epoch': 4.39}
 69%|██████▉   | 3460/5000 [16:51:26<6:55:27, 16.19s/it] 69%|██████▉   | 3461/5000 [16:51:40<6:40:34, 15.62s/it]                                                        {'loss': 19.2403, 'grad_norm': 14.6875, 'learning_rate': 1.6618887951354645e-05, 'epoch': 4.39}
 69%|██████▉   | 3461/5000 [16:51:40<6:40:34, 15.62s/it] 69%|██████▉   | 3462/5000 [16:51:58<6:54:40, 16.18s/it]                                                        {'loss': 18.2313, 'grad_norm': 12.6875, 'learning_rate': 1.6599192661383064e-05, 'epoch': 4.4}
 69%|██████▉   | 3462/5000 [16:51:58<6:54:40, 16.18s/it] 69%|██████▉   | 3463/5000 [16:52:26<8:23:59, 19.67s/it]                                                        {'loss': 18.4682, 'grad_norm': 12.9375, 'learning_rate': 1.6579505420549764e-05, 'epoch': 4.4}
 69%|██████▉   | 3463/5000 [16:52:26<8:23:59, 19.67s/it] 69%|██████▉   | 3464/5000 [16:52:44<8:11:21, 19.19s/it]                                                        {'loss': 19.1907, 'grad_norm': 15.4375, 'learning_rate': 1.6559826237466603e-05, 'epoch': 4.4}
 69%|██████▉   | 3464/5000 [16:52:44<8:11:21, 19.19s/it] 69%|██████▉   | 3465/5000 [16:53:03<8:08:09, 19.08s/it]                                                        {'loss': 18.1567, 'grad_norm': 13.625, 'learning_rate': 1.6540155120741943e-05, 'epoch': 4.4}
 69%|██████▉   | 3465/5000 [16:53:03<8:08:09, 19.08s/it] 69%|██████▉   | 3466/5000 [16:53:34<9:41:11, 22.73s/it]                                                        {'loss': 17.8941, 'grad_norm': 17.25, 'learning_rate': 1.6520492078980595e-05, 'epoch': 4.4}
 69%|██████▉   | 3466/5000 [16:53:34<9:41:11, 22.73s/it] 69%|██████▉   | 3467/5000 [16:53:46<8:22:18, 19.66s/it]                                                        {'loss': 19.2686, 'grad_norm': 32.5, 'learning_rate': 1.6500837120783833e-05, 'epoch': 4.4}
 69%|██████▉   | 3467/5000 [16:53:46<8:22:18, 19.66s/it] 69%|██████▉   | 3468/5000 [16:53:59<7:29:28, 17.60s/it]                                                        {'loss': 18.756, 'grad_norm': 17.0, 'learning_rate': 1.6481190254749397e-05, 'epoch': 4.4}
 69%|██████▉   | 3468/5000 [16:53:59<7:29:28, 17.60s/it] 69%|██████▉   | 3469/5000 [16:54:12<6:53:15, 16.20s/it]                                                        {'loss': 18.9484, 'grad_norm': 12.6875, 'learning_rate': 1.6461551489471515e-05, 'epoch': 4.41}
 69%|██████▉   | 3469/5000 [16:54:12<6:53:15, 16.20s/it] 69%|██████▉   | 3470/5000 [16:54:37<7:59:06, 18.79s/it]                                                        {'loss': 17.3164, 'grad_norm': 13.375, 'learning_rate': 1.644192083354084e-05, 'epoch': 4.41}
 69%|██████▉   | 3470/5000 [16:54:37<7:59:06, 18.79s/it] 69%|██████▉   | 3471/5000 [16:54:49<7:09:36, 16.86s/it]                                                        {'loss': 20.1174, 'grad_norm': 27.5, 'learning_rate': 1.6422298295544478e-05, 'epoch': 4.41}
 69%|██████▉   | 3471/5000 [16:54:49<7:09:36, 16.86s/it] 69%|██████▉   | 3472/5000 [16:55:04<6:53:32, 16.24s/it]                                                        {'loss': 17.683, 'grad_norm': 15.625, 'learning_rate': 1.640268388406601e-05, 'epoch': 4.41}
 69%|██████▉   | 3472/5000 [16:55:04<6:53:32, 16.24s/it] 69%|██████▉   | 3473/5000 [16:55:23<7:11:56, 16.97s/it]                                                        {'loss': 19.2263, 'grad_norm': 11.8125, 'learning_rate': 1.6383077607685444e-05, 'epoch': 4.41}
 69%|██████▉   | 3473/5000 [16:55:23<7:11:56, 16.97s/it] 69%|██████▉   | 3474/5000 [16:55:49<8:18:50, 19.61s/it]                                                        {'loss': 18.4072, 'grad_norm': 11.625, 'learning_rate': 1.6363479474979206e-05, 'epoch': 4.41}
 69%|██████▉   | 3474/5000 [16:55:49<8:18:50, 19.61s/it] 70%|██████▉   | 3475/5000 [16:56:04<7:49:44, 18.48s/it]                                                        {'loss': 17.8773, 'grad_norm': 13.25, 'learning_rate': 1.6343889494520224e-05, 'epoch': 4.41}
 70%|██████▉   | 3475/5000 [16:56:04<7:49:44, 18.48s/it] 70%|██████▉   | 3476/5000 [16:56:22<7:41:56, 18.19s/it]                                                        {'loss': 17.7851, 'grad_norm': 13.375, 'learning_rate': 1.6324307674877788e-05, 'epoch': 4.41}
 70%|██████▉   | 3476/5000 [16:56:22<7:41:56, 18.19s/it] 70%|██████▉   | 3477/5000 [16:56:39<7:31:01, 17.77s/it]                                                        {'loss': 18.3933, 'grad_norm': 24.375, 'learning_rate': 1.6304734024617677e-05, 'epoch': 4.42}
 70%|██████▉   | 3477/5000 [16:56:39<7:31:01, 17.77s/it] 70%|██████▉   | 3478/5000 [16:56:55<7:17:29, 17.25s/it]                                                        {'loss': 17.965, 'grad_norm': 12.75, 'learning_rate': 1.6285168552302047e-05, 'epoch': 4.42}
 70%|██████▉   | 3478/5000 [16:56:55<7:17:29, 17.25s/it] 70%|██████▉   | 3479/5000 [16:57:10<6:59:53, 16.56s/it]                                                        {'loss': 17.6086, 'grad_norm': 11.1875, 'learning_rate': 1.6265611266489527e-05, 'epoch': 4.42}
 70%|██████▉   | 3479/5000 [16:57:10<6:59:53, 16.56s/it] 70%|██████▉   | 3480/5000 [16:57:24<6:39:26, 15.77s/it]                                                        {'loss': 19.281, 'grad_norm': 14.125, 'learning_rate': 1.6246062175735108e-05, 'epoch': 4.42}
 70%|██████▉   | 3480/5000 [16:57:24<6:39:26, 15.77s/it] 70%|██████▉   | 3481/5000 [16:57:53<8:23:52, 19.90s/it]                                                        {'loss': 16.2252, 'grad_norm': 10.625, 'learning_rate': 1.6226521288590256e-05, 'epoch': 4.42}
 70%|██████▉   | 3481/5000 [16:57:53<8:23:52, 19.90s/it] 70%|██████▉   | 3482/5000 [16:58:10<7:58:10, 18.90s/it]                                                        {'loss': 16.8726, 'grad_norm': 21.375, 'learning_rate': 1.6206988613602803e-05, 'epoch': 4.42}
 70%|██████▉   | 3482/5000 [16:58:10<7:58:10, 18.90s/it] 70%|██████▉   | 3483/5000 [16:58:23<7:17:30, 17.30s/it]                                                        {'loss': 18.4572, 'grad_norm': 24.375, 'learning_rate': 1.6187464159317e-05, 'epoch': 4.42}
 70%|██████▉   | 3483/5000 [16:58:23<7:17:30, 17.30s/it] 70%|██████▉   | 3484/5000 [16:58:47<8:04:47, 19.19s/it]                                                        {'loss': 16.5558, 'grad_norm': 38.0, 'learning_rate': 1.6167947934273522e-05, 'epoch': 4.42}
 70%|██████▉   | 3484/5000 [16:58:47<8:04:47, 19.19s/it] 70%|██████▉   | 3485/5000 [16:59:02<7:34:17, 17.99s/it]                                                        {'loss': 17.4625, 'grad_norm': 11.4375, 'learning_rate': 1.6148439947009418e-05, 'epoch': 4.43}
 70%|██████▉   | 3485/5000 [16:59:02<7:34:17, 17.99s/it] 70%|██████▉   | 3486/5000 [16:59:19<7:28:18, 17.77s/it]                                                        {'loss': 18.3357, 'grad_norm': 23.75, 'learning_rate': 1.612894020605815e-05, 'epoch': 4.43}
 70%|██████▉   | 3486/5000 [16:59:19<7:28:18, 17.77s/it] 70%|██████▉   | 3487/5000 [16:59:37<7:24:20, 17.62s/it]                                                        {'loss': 20.3151, 'grad_norm': 18.625, 'learning_rate': 1.6109448719949548e-05, 'epoch': 4.43}
 70%|██████▉   | 3487/5000 [16:59:37<7:24:20, 17.62s/it] 70%|██████▉   | 3488/5000 [16:59:54<7:25:13, 17.67s/it]                                                        {'loss': 17.8827, 'grad_norm': 12.1875, 'learning_rate': 1.608996549720988e-05, 'epoch': 4.43}
 70%|██████▉   | 3488/5000 [16:59:54<7:25:13, 17.67s/it] 70%|██████▉   | 3489/5000 [17:00:09<7:01:13, 16.73s/it]                                                        {'loss': 17.8309, 'grad_norm': 10.6875, 'learning_rate': 1.6070490546361754e-05, 'epoch': 4.43}
 70%|██████▉   | 3489/5000 [17:00:09<7:01:13, 16.73s/it] 70%|██████▉   | 3490/5000 [17:00:28<7:19:00, 17.44s/it]                                                        {'loss': 18.4641, 'grad_norm': 11.9375, 'learning_rate': 1.605102387592417e-05, 'epoch': 4.43}
 70%|██████▉   | 3490/5000 [17:00:28<7:19:00, 17.44s/it] 70%|██████▉   | 3491/5000 [17:00:43<6:58:15, 16.63s/it]                                                        {'loss': 19.7728, 'grad_norm': 19.625, 'learning_rate': 1.6031565494412523e-05, 'epoch': 4.43}
 70%|██████▉   | 3491/5000 [17:00:43<6:58:15, 16.63s/it] 70%|██████▉   | 3492/5000 [17:00:57<6:37:14, 15.81s/it]                                                        {'loss': 19.0695, 'grad_norm': 16.0, 'learning_rate': 1.601211541033857e-05, 'epoch': 4.43}
 70%|██████▉   | 3492/5000 [17:00:57<6:37:14, 15.81s/it] 70%|██████▉   | 3493/5000 [17:01:11<6:23:39, 15.28s/it]                                                        {'loss': 18.1022, 'grad_norm': 11.9375, 'learning_rate': 1.599267363221042e-05, 'epoch': 4.44}
 70%|██████▉   | 3493/5000 [17:01:11<6:23:39, 15.28s/it] 70%|██████▉   | 3494/5000 [17:01:27<6:35:23, 15.75s/it]                                                        {'loss': 16.7704, 'grad_norm': 9.4375, 'learning_rate': 1.597324016853259e-05, 'epoch': 4.44}
 70%|██████▉   | 3494/5000 [17:01:27<6:35:23, 15.75s/it] 70%|██████▉   | 3495/5000 [17:01:43<6:36:29, 15.81s/it]                                                        {'loss': 17.6798, 'grad_norm': 14.0625, 'learning_rate': 1.595381502780593e-05, 'epoch': 4.44}
 70%|██████▉   | 3495/5000 [17:01:43<6:36:29, 15.81s/it] 70%|██████▉   | 3496/5000 [17:02:01<6:50:41, 16.38s/it]                                                        {'loss': 16.375, 'grad_norm': 22.125, 'learning_rate': 1.593439821852764e-05, 'epoch': 4.44}
 70%|██████▉   | 3496/5000 [17:02:01<6:50:41, 16.38s/it] 70%|██████▉   | 3497/5000 [17:02:14<6:27:06, 15.45s/it]                                                        {'loss': 20.2062, 'grad_norm': 25.125, 'learning_rate': 1.591498974919132e-05, 'epoch': 4.44}
 70%|██████▉   | 3497/5000 [17:02:14<6:27:06, 15.45s/it] 70%|██████▉   | 3498/5000 [17:02:28<6:13:00, 14.90s/it]                                                        {'loss': 18.1552, 'grad_norm': 15.5625, 'learning_rate': 1.5895589628286882e-05, 'epoch': 4.44}
 70%|██████▉   | 3498/5000 [17:02:28<6:13:00, 14.90s/it] 70%|██████▉   | 3499/5000 [17:02:42<6:03:11, 14.52s/it]                                                        {'loss': 18.0197, 'grad_norm': 10.75, 'learning_rate': 1.5876197864300582e-05, 'epoch': 4.44}
 70%|██████▉   | 3499/5000 [17:02:42<6:03:11, 14.52s/it] 70%|███████   | 3500/5000 [17:02:54<5:48:37, 13.94s/it]                                                        {'loss': 17.7693, 'grad_norm': 12.5, 'learning_rate': 1.5856814465715064e-05, 'epoch': 4.44}
 70%|███████   | 3500/5000 [17:02:54<5:48:37, 13.94s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:23,  4.46s/it][A
  3%|▎         | 3/88 [00:16<08:06,  5.72s/it][A
  5%|▍         | 4/88 [00:20<07:15,  5.19s/it][A
  6%|▌         | 5/88 [00:24<06:30,  4.71s/it][A
  7%|▋         | 6/88 [00:29<06:47,  4.97s/it][A
  8%|▊         | 7/88 [00:33<06:11,  4.58s/it][A
  9%|▉         | 8/88 [00:37<05:47,  4.34s/it][A
 10%|█         | 9/88 [00:41<05:21,  4.07s/it][A
 11%|█▏        | 10/88 [00:44<04:50,  3.73s/it][A
 12%|█▎        | 11/88 [00:46<04:27,  3.48s/it][A
 14%|█▎        | 12/88 [00:49<03:57,  3.13s/it][A
 15%|█▍        | 13/88 [00:51<03:41,  2.95s/it][A
 16%|█▌        | 14/88 [00:57<04:36,  3.74s/it][A
 17%|█▋        | 15/88 [01:02<05:11,  4.26s/it][A
 18%|█▊        | 16/88 [01:06<04:47,  4.00s/it][A
 19%|█▉        | 17/88 [01:11<05:14,  4.43s/it][A
 20%|██        | 18/88 [01:14<04:44,  4.06s/it][A
 22%|██▏       | 19/88 [01:19<04:44,  4.13s/it][A
 23%|██▎       | 20/88 [01:22<04:34,  4.03s/it][A
 24%|██▍       | 21/88 [01:26<04:14,  3.81s/it][A
 25%|██▌       | 22/88 [01:30<04:12,  3.82s/it][A
 26%|██▌       | 23/88 [01:32<03:45,  3.47s/it][A
 27%|██▋       | 24/88 [01:41<05:21,  5.03s/it][A
 28%|██▊       | 25/88 [01:44<04:47,  4.56s/it][A
 30%|██▉       | 26/88 [01:51<05:18,  5.14s/it][A
 31%|███       | 27/88 [01:55<04:58,  4.89s/it][A
 32%|███▏      | 28/88 [02:04<06:02,  6.05s/it][A
 33%|███▎      | 29/88 [02:09<05:42,  5.80s/it][A
 34%|███▍      | 30/88 [02:14<05:25,  5.62s/it][A
 35%|███▌      | 31/88 [02:18<04:54,  5.17s/it][A
 36%|███▋      | 32/88 [02:28<05:55,  6.35s/it][A
 38%|███▊      | 33/88 [02:32<05:21,  5.85s/it][A
 39%|███▊      | 34/88 [02:41<06:02,  6.72s/it][A
 40%|███▉      | 35/88 [02:44<04:57,  5.61s/it][A
 41%|████      | 36/88 [02:49<04:45,  5.49s/it][A
 42%|████▏     | 37/88 [02:53<04:12,  4.94s/it][A
 43%|████▎     | 38/88 [02:57<03:56,  4.74s/it][A
 44%|████▍     | 39/88 [03:00<03:28,  4.26s/it][A
 45%|████▌     | 40/88 [03:07<03:58,  4.97s/it][A
 47%|████▋     | 41/88 [03:17<04:59,  6.36s/it][A
 48%|████▊     | 42/88 [03:20<04:13,  5.52s/it][A
 49%|████▉     | 43/88 [03:29<04:51,  6.47s/it][A
 50%|█████     | 44/88 [03:32<04:02,  5.51s/it][A
 51%|█████     | 45/88 [03:37<03:44,  5.23s/it][A
 52%|█████▏    | 46/88 [03:40<03:22,  4.83s/it][A
 53%|█████▎    | 47/88 [03:43<02:44,  4.01s/it][A
 55%|█████▍    | 48/88 [03:45<02:20,  3.50s/it][A
 56%|█████▌    | 49/88 [03:50<02:34,  3.96s/it][A
 57%|█████▋    | 50/88 [03:54<02:32,  4.01s/it][A
 58%|█████▊    | 51/88 [03:58<02:30,  4.06s/it][A
 59%|█████▉    | 52/88 [04:04<02:45,  4.60s/it][A
 60%|██████    | 53/88 [04:09<02:45,  4.73s/it][A
 61%|██████▏   | 54/88 [04:19<03:29,  6.17s/it][A
 62%|██████▎   | 55/88 [04:24<03:11,  5.80s/it][A
 64%|██████▎   | 56/88 [04:26<02:34,  4.82s/it][A
 65%|██████▍   | 57/88 [04:30<02:21,  4.55s/it][A
 66%|██████▌   | 58/88 [04:39<02:55,  5.85s/it][A
 67%|██████▋   | 59/88 [04:44<02:40,  5.55s/it][A
 68%|██████▊   | 60/88 [04:47<02:15,  4.86s/it][A
 69%|██████▉   | 61/88 [04:51<02:04,  4.62s/it][A
 70%|███████   | 62/88 [04:54<01:47,  4.14s/it][A
 72%|███████▏  | 63/88 [05:00<01:53,  4.55s/it][A
 73%|███████▎  | 64/88 [05:04<01:46,  4.42s/it][A
 74%|███████▍  | 65/88 [05:08<01:41,  4.39s/it][A
 75%|███████▌  | 66/88 [05:12<01:32,  4.20s/it][A
 76%|███████▌  | 67/88 [05:15<01:21,  3.89s/it][A
 77%|███████▋  | 68/88 [05:20<01:23,  4.19s/it][A
 78%|███████▊  | 69/88 [05:26<01:31,  4.83s/it][A
 80%|███████▉  | 70/88 [05:30<01:22,  4.57s/it][A
 81%|████████  | 71/88 [05:34<01:13,  4.34s/it][A
 82%|████████▏ | 72/88 [05:38<01:07,  4.21s/it][A
 83%|████████▎ | 73/88 [05:41<00:57,  3.87s/it][A
 84%|████████▍ | 74/88 [05:44<00:49,  3.55s/it][A
 85%|████████▌ | 75/88 [05:49<00:51,  3.95s/it][A
 86%|████████▋ | 76/88 [05:52<00:47,  3.93s/it][A
 88%|████████▊ | 77/88 [06:00<00:53,  4.89s/it][A
 89%|████████▊ | 78/88 [06:04<00:45,  4.59s/it][A
 90%|████████▉ | 79/88 [06:08<00:41,  4.62s/it][A
 91%|█████████ | 80/88 [06:11<00:33,  4.14s/it][A
 92%|█████████▏| 81/88 [06:16<00:30,  4.40s/it][A
 93%|█████████▎| 82/88 [06:20<00:25,  4.26s/it][A
 94%|█████████▍| 83/88 [06:24<00:20,  4.16s/it][A
 95%|█████████▌| 84/88 [06:28<00:16,  4.07s/it][A
 97%|█████████▋| 85/88 [06:31<00:11,  3.73s/it][A
 98%|█████████▊| 86/88 [06:34<00:06,  3.47s/it][A
 99%|█████████▉| 87/88 [06:38<00:03,  3.65s/it][A
100%|██████████| 88/88 [06:42<00:00,  3.71s/it][A                                                        
                                               [A{'eval_loss': 17.619455337524414, 'eval_runtime': 406.0974, 'eval_samples_per_second': 6.895, 'eval_steps_per_second': 0.217, 'epoch': 4.44}
 70%|███████   | 3500/5000 [17:09:40<5:48:37, 13.94s/it]
100%|██████████| 88/88 [06:42<00:00,  3.71s/it][A
                                               [A2024-06-14 02:45:18,032 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-14 02:45:28,451 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 70%|███████   | 3501/5000 [17:10:09<58:18:41, 140.04s/it]                                                          {'loss': 18.8753, 'grad_norm': 17.875, 'learning_rate': 1.5837439441009258e-05, 'epoch': 4.45}
 70%|███████   | 3501/5000 [17:10:09<58:18:41, 140.04s/it] 70%|███████   | 3502/5000 [17:10:21<42:24:01, 101.90s/it]                                                          {'loss': 19.3254, 'grad_norm': 18.875, 'learning_rate': 1.5818072798658486e-05, 'epoch': 4.45}
 70%|███████   | 3502/5000 [17:10:21<42:24:01, 101.90s/it] 70%|███████   | 3503/5000 [17:10:36<31:30:16, 75.76s/it]                                                          {'loss': 19.2742, 'grad_norm': 12.9375, 'learning_rate': 1.579871454713435e-05, 'epoch': 4.45}
 70%|███████   | 3503/5000 [17:10:36<31:30:16, 75.76s/it] 70%|███████   | 3504/5000 [17:10:54<24:13:08, 58.28s/it]                                                         {'loss': 21.7906, 'grad_norm': 39.25, 'learning_rate': 1.5779364694904825e-05, 'epoch': 4.45}
 70%|███████   | 3504/5000 [17:10:54<24:13:08, 58.28s/it] 70%|███████   | 3505/5000 [17:11:08<18:43:22, 45.09s/it]                                                         {'loss': 19.2594, 'grad_norm': 27.5, 'learning_rate': 1.5760023250434193e-05, 'epoch': 4.45}
 70%|███████   | 3505/5000 [17:11:08<18:43:22, 45.09s/it] 70%|███████   | 3506/5000 [17:11:36<16:33:43, 39.91s/it]                                                         {'loss': 18.5881, 'grad_norm': 12.8125, 'learning_rate': 1.5740690222183036e-05, 'epoch': 4.45}
 70%|███████   | 3506/5000 [17:11:36<16:33:43, 39.91s/it] 70%|███████   | 3507/5000 [17:11:52<13:36:00, 32.79s/it]                                                         {'loss': 19.2136, 'grad_norm': 16.75, 'learning_rate': 1.572136561860831e-05, 'epoch': 4.45}
 70%|███████   | 3507/5000 [17:11:52<13:36:00, 32.79s/it] 70%|███████   | 3508/5000 [17:12:06<11:14:46, 27.14s/it]                                                         {'loss': 18.8931, 'grad_norm': 17.375, 'learning_rate': 1.5702049448163236e-05, 'epoch': 4.45}
 70%|███████   | 3508/5000 [17:12:06<11:14:46, 27.14s/it] 70%|███████   | 3509/5000 [17:12:21<9:42:53, 23.46s/it]                                                         {'loss': 19.7133, 'grad_norm': 14.75, 'learning_rate': 1.5682741719297353e-05, 'epoch': 4.46}
 70%|███████   | 3509/5000 [17:12:21<9:42:53, 23.46s/it] 70%|███████   | 3510/5000 [17:12:35<8:36:54, 20.82s/it]                                                        {'loss': 17.8003, 'grad_norm': 9.625, 'learning_rate': 1.5663442440456538e-05, 'epoch': 4.46}
 70%|███████   | 3510/5000 [17:12:35<8:36:54, 20.82s/it] 70%|███████   | 3511/5000 [17:12:53<8:13:47, 19.90s/it]                                                        {'loss': 17.4668, 'grad_norm': 9.4375, 'learning_rate': 1.5644151620082945e-05, 'epoch': 4.46}
 70%|███████   | 3511/5000 [17:12:53<8:13:47, 19.90s/it] 70%|███████   | 3512/5000 [17:13:18<8:52:32, 21.47s/it]                                                        {'loss': 18.3938, 'grad_norm': 37.25, 'learning_rate': 1.5624869266615016e-05, 'epoch': 4.46}
 70%|███████   | 3512/5000 [17:13:18<8:52:32, 21.47s/it] 70%|███████   | 3513/5000 [17:13:35<8:17:42, 20.08s/it]                                                        {'loss': 17.839, 'grad_norm': 14.6875, 'learning_rate': 1.560559538848754e-05, 'epoch': 4.46}
 70%|███████   | 3513/5000 [17:13:35<8:17:42, 20.08s/it] 70%|███████   | 3514/5000 [17:13:50<7:35:47, 18.40s/it]                                                        {'loss': 17.6827, 'grad_norm': 8.875, 'learning_rate': 1.5586329994131548e-05, 'epoch': 4.46}
 70%|███████   | 3514/5000 [17:13:50<7:35:47, 18.40s/it] 70%|███████   | 3515/5000 [17:14:10<7:50:14, 19.00s/it]                                                        {'loss': 17.8889, 'grad_norm': 13.5, 'learning_rate': 1.5567073091974363e-05, 'epoch': 4.46}
 70%|███████   | 3515/5000 [17:14:10<7:50:14, 19.00s/it] 70%|███████   | 3516/5000 [17:14:36<8:43:21, 21.16s/it]                                                        {'loss': 18.2691, 'grad_norm': 13.625, 'learning_rate': 1.554782469043964e-05, 'epoch': 4.46}
 70%|███████   | 3516/5000 [17:14:36<8:43:21, 21.16s/it] 70%|███████   | 3517/5000 [17:14:52<8:04:33, 19.60s/it]                                                        {'loss': 18.7575, 'grad_norm': 12.25, 'learning_rate': 1.5528584797947268e-05, 'epoch': 4.47}
 70%|███████   | 3517/5000 [17:14:52<8:04:33, 19.60s/it] 70%|███████   | 3518/5000 [17:15:07<7:31:28, 18.28s/it]                                                        {'loss': 16.9265, 'grad_norm': 12.4375, 'learning_rate': 1.5509353422913433e-05, 'epoch': 4.47}
 70%|███████   | 3518/5000 [17:15:07<7:31:28, 18.28s/it] 70%|███████   | 3519/5000 [17:15:37<8:53:49, 21.63s/it]                                                        {'loss': 16.4441, 'grad_norm': 7.53125, 'learning_rate': 1.5490130573750572e-05, 'epoch': 4.47}
 70%|███████   | 3519/5000 [17:15:37<8:53:49, 21.63s/it] 70%|███████   | 3520/5000 [17:15:53<8:14:44, 20.06s/it]                                                        {'loss': 19.1525, 'grad_norm': 12.75, 'learning_rate': 1.547091625886744e-05, 'epoch': 4.47}
 70%|███████   | 3520/5000 [17:15:53<8:14:44, 20.06s/it] 70%|███████   | 3521/5000 [17:16:19<8:56:07, 21.75s/it]                                                        {'loss': 17.4563, 'grad_norm': 12.5625, 'learning_rate': 1.545171048666902e-05, 'epoch': 4.47}
 70%|███████   | 3521/5000 [17:16:19<8:56:07, 21.75s/it] 70%|███████   | 3522/5000 [17:16:34<8:06:34, 19.75s/it]                                                        {'loss': 17.1677, 'grad_norm': 15.875, 'learning_rate': 1.5432513265556563e-05, 'epoch': 4.47}
 70%|███████   | 3522/5000 [17:16:34<8:06:34, 19.75s/it] 70%|███████   | 3523/5000 [17:16:51<7:42:01, 18.77s/it]                                                        {'loss': 17.0211, 'grad_norm': 13.8125, 'learning_rate': 1.5413324603927596e-05, 'epoch': 4.47}
 70%|███████   | 3523/5000 [17:16:51<7:42:01, 18.77s/it] 70%|███████   | 3524/5000 [17:17:17<8:40:03, 21.14s/it]                                                        {'loss': 17.3199, 'grad_norm': 14.125, 'learning_rate': 1.5394144510175882e-05, 'epoch': 4.47}
 70%|███████   | 3524/5000 [17:17:17<8:40:03, 21.14s/it] 70%|███████   | 3525/5000 [17:17:35<8:15:22, 20.15s/it]                                                        {'loss': 17.4009, 'grad_norm': 11.375, 'learning_rate': 1.5374972992691458e-05, 'epoch': 4.48}
 70%|███████   | 3525/5000 [17:17:35<8:15:22, 20.15s/it] 71%|███████   | 3526/5000 [17:17:51<7:43:38, 18.87s/it]                                                        {'loss': 17.4827, 'grad_norm': 11.0625, 'learning_rate': 1.535581005986058e-05, 'epoch': 4.48}
 71%|███████   | 3526/5000 [17:17:51<7:43:38, 18.87s/it] 71%|███████   | 3527/5000 [17:18:05<7:09:36, 17.50s/it]                                                        {'loss': 18.5184, 'grad_norm': 24.625, 'learning_rate': 1.533665572006579e-05, 'epoch': 4.48}
 71%|███████   | 3527/5000 [17:18:05<7:09:36, 17.50s/it] 71%|███████   | 3528/5000 [17:18:25<7:25:13, 18.15s/it]                                                        {'loss': 19.6727, 'grad_norm': 17.25, 'learning_rate': 1.5317509981685817e-05, 'epoch': 4.48}
 71%|███████   | 3528/5000 [17:18:25<7:25:13, 18.15s/it] 71%|███████   | 3529/5000 [17:18:39<6:57:00, 17.01s/it]                                                        {'loss': 20.0133, 'grad_norm': 16.625, 'learning_rate': 1.5298372853095693e-05, 'epoch': 4.48}
 71%|███████   | 3529/5000 [17:18:39<6:57:00, 17.01s/it] 71%|███████   | 3530/5000 [17:18:52<6:27:33, 15.82s/it]                                                        {'loss': 19.6565, 'grad_norm': 38.5, 'learning_rate': 1.527924434266662e-05, 'epoch': 4.48}
 71%|███████   | 3530/5000 [17:18:52<6:27:33, 15.82s/it] 71%|███████   | 3531/5000 [17:19:16<7:28:02, 18.30s/it]                                                        {'loss': 18.0343, 'grad_norm': 9.875, 'learning_rate': 1.526012445876606e-05, 'epoch': 4.48}
 71%|███████   | 3531/5000 [17:19:16<7:28:02, 18.30s/it] 71%|███████   | 3532/5000 [17:19:41<8:10:41, 20.06s/it]                                                        {'loss': 18.8184, 'grad_norm': 13.5, 'learning_rate': 1.524101320975772e-05, 'epoch': 4.49}
 71%|███████   | 3532/5000 [17:19:41<8:10:41, 20.06s/it] 71%|███████   | 3533/5000 [17:19:59<7:55:59, 19.47s/it]                                                        {'loss': 17.8718, 'grad_norm': 19.125, 'learning_rate': 1.5221910604001493e-05, 'epoch': 4.49}
 71%|███████   | 3533/5000 [17:19:59<7:55:59, 19.47s/it] 71%|███████   | 3534/5000 [17:20:16<7:38:41, 18.77s/it]                                                        {'loss': 17.0625, 'grad_norm': 9.25, 'learning_rate': 1.5202816649853501e-05, 'epoch': 4.49}
 71%|███████   | 3534/5000 [17:20:16<7:38:41, 18.77s/it] 71%|███████   | 3535/5000 [17:20:29<7:00:23, 17.22s/it]                                                        {'loss': 18.0378, 'grad_norm': 11.125, 'learning_rate': 1.5183731355666112e-05, 'epoch': 4.49}
 71%|███████   | 3535/5000 [17:20:29<7:00:23, 17.22s/it] 71%|███████   | 3536/5000 [17:21:04<9:04:21, 22.31s/it]                                                        {'loss': 16.784, 'grad_norm': 11.0, 'learning_rate': 1.5164654729787866e-05, 'epoch': 4.49}
 71%|███████   | 3536/5000 [17:21:04<9:04:21, 22.31s/it] 71%|███████   | 3537/5000 [17:21:19<8:15:50, 20.34s/it]                                                        {'loss': 17.781, 'grad_norm': 12.25, 'learning_rate': 1.5145586780563529e-05, 'epoch': 4.49}
 71%|███████   | 3537/5000 [17:21:19<8:15:50, 20.34s/it] 71%|███████   | 3538/5000 [17:21:35<7:42:48, 18.99s/it]                                                        {'loss': 17.5593, 'grad_norm': 15.4375, 'learning_rate': 1.5126527516334062e-05, 'epoch': 4.49}
 71%|███████   | 3538/5000 [17:21:35<7:42:48, 18.99s/it] 71%|███████   | 3539/5000 [17:21:53<7:37:17, 18.78s/it]                                                        {'loss': 16.5811, 'grad_norm': 9.6875, 'learning_rate': 1.5107476945436653e-05, 'epoch': 4.49}
 71%|███████   | 3539/5000 [17:21:53<7:37:17, 18.78s/it] 71%|███████   | 3540/5000 [17:22:07<7:02:06, 17.35s/it]                                                        {'loss': 19.2489, 'grad_norm': 11.0625, 'learning_rate': 1.5088435076204661e-05, 'epoch': 4.5}
 71%|███████   | 3540/5000 [17:22:07<7:02:06, 17.35s/it] 71%|███████   | 3541/5000 [17:22:26<7:11:16, 17.74s/it]                                                        {'loss': 15.6926, 'grad_norm': 8.6875, 'learning_rate': 1.506940191696763e-05, 'epoch': 4.5}
 71%|███████   | 3541/5000 [17:22:26<7:11:16, 17.74s/it] 71%|███████   | 3542/5000 [17:22:44<7:10:26, 17.71s/it]                                                        {'loss': 17.9103, 'grad_norm': 13.375, 'learning_rate': 1.5050377476051335e-05, 'epoch': 4.5}
 71%|███████   | 3542/5000 [17:22:44<7:10:26, 17.71s/it] 71%|███████   | 3543/5000 [17:23:01<7:05:04, 17.50s/it]                                                        {'loss': 17.9668, 'grad_norm': 10.125, 'learning_rate': 1.5031361761777709e-05, 'epoch': 4.5}
 71%|███████   | 3543/5000 [17:23:01<7:05:04, 17.50s/it] 71%|███████   | 3544/5000 [17:23:15<6:38:04, 16.40s/it]                                                        {'loss': 16.9877, 'grad_norm': 8.125, 'learning_rate': 1.501235478246485e-05, 'epoch': 4.5}
 71%|███████   | 3544/5000 [17:23:15<6:38:04, 16.40s/it] 71%|███████   | 3545/5000 [17:23:36<7:11:58, 17.81s/it]                                                        {'loss': 18.1976, 'grad_norm': 11.5, 'learning_rate': 1.4993356546427084e-05, 'epoch': 4.5}
 71%|███████   | 3545/5000 [17:23:36<7:11:58, 17.81s/it] 71%|███████   | 3546/5000 [17:23:51<6:54:42, 17.11s/it]                                                        {'loss': 16.6728, 'grad_norm': 15.0, 'learning_rate': 1.4974367061974878e-05, 'epoch': 4.5}
 71%|███████   | 3546/5000 [17:23:51<6:54:42, 17.11s/it] 71%|███████   | 3547/5000 [17:24:05<6:27:43, 16.01s/it]                                                        {'loss': 18.3567, 'grad_norm': 29.625, 'learning_rate': 1.4955386337414854e-05, 'epoch': 4.5}
 71%|███████   | 3547/5000 [17:24:05<6:27:43, 16.01s/it] 71%|███████   | 3548/5000 [17:24:19<6:16:13, 15.55s/it]                                                        {'loss': 17.468, 'grad_norm': 11.0625, 'learning_rate': 1.493641438104986e-05, 'epoch': 4.51}
 71%|███████   | 3548/5000 [17:24:19<6:16:13, 15.55s/it] 71%|███████   | 3549/5000 [17:24:41<6:59:17, 17.34s/it]                                                        {'loss': 19.3115, 'grad_norm': 31.75, 'learning_rate': 1.4917451201178844e-05, 'epoch': 4.51}
 71%|███████   | 3549/5000 [17:24:41<6:59:17, 17.34s/it] 71%|███████   | 3550/5000 [17:24:55<6:40:19, 16.57s/it]                                                        {'loss': 18.5562, 'grad_norm': 13.625, 'learning_rate': 1.4898496806096974e-05, 'epoch': 4.51}
 71%|███████   | 3550/5000 [17:24:55<6:40:19, 16.57s/it] 71%|███████   | 3551/5000 [17:25:10<6:27:30, 16.05s/it]                                                        {'loss': 18.8541, 'grad_norm': 13.0625, 'learning_rate': 1.4879551204095518e-05, 'epoch': 4.51}
 71%|███████   | 3551/5000 [17:25:10<6:27:30, 16.05s/it] 71%|███████   | 3552/5000 [17:25:28<6:37:21, 16.47s/it]                                                        {'loss': 17.7289, 'grad_norm': 15.3125, 'learning_rate': 1.4860614403461953e-05, 'epoch': 4.51}
 71%|███████   | 3552/5000 [17:25:28<6:37:21, 16.47s/it] 71%|███████   | 3553/5000 [17:25:41<6:16:23, 15.61s/it]                                                        {'loss': 18.3418, 'grad_norm': 13.25, 'learning_rate': 1.4841686412479867e-05, 'epoch': 4.51}
 71%|███████   | 3553/5000 [17:25:41<6:16:23, 15.61s/it] 71%|███████   | 3554/5000 [17:25:56<6:08:10, 15.28s/it]                                                        {'loss': 20.3061, 'grad_norm': 18.5, 'learning_rate': 1.4822767239428993e-05, 'epoch': 4.51}
 71%|███████   | 3554/5000 [17:25:56<6:08:10, 15.28s/it] 71%|███████   | 3555/5000 [17:26:18<6:58:52, 17.39s/it]                                                        {'loss': 18.2734, 'grad_norm': 12.75, 'learning_rate': 1.4803856892585246e-05, 'epoch': 4.51}
 71%|███████   | 3555/5000 [17:26:18<6:58:52, 17.39s/it] 71%|███████   | 3556/5000 [17:26:33<6:38:52, 16.57s/it]                                                        {'loss': 17.9946, 'grad_norm': 18.75, 'learning_rate': 1.4784955380220639e-05, 'epoch': 4.52}
 71%|███████   | 3556/5000 [17:26:33<6:38:52, 16.57s/it] 71%|███████   | 3557/5000 [17:26:49<6:37:36, 16.53s/it]                                                        {'loss': 18.2784, 'grad_norm': 15.4375, 'learning_rate': 1.4766062710603326e-05, 'epoch': 4.52}
 71%|███████   | 3557/5000 [17:26:49<6:37:36, 16.53s/it] 71%|███████   | 3558/5000 [17:27:04<6:25:01, 16.02s/it]                                                        {'loss': 18.8268, 'grad_norm': 29.0, 'learning_rate': 1.4747178891997625e-05, 'epoch': 4.52}
 71%|███████   | 3558/5000 [17:27:04<6:25:01, 16.02s/it] 71%|███████   | 3559/5000 [17:27:22<6:40:00, 16.66s/it]                                                        {'loss': 19.0911, 'grad_norm': 20.375, 'learning_rate': 1.4728303932663945e-05, 'epoch': 4.52}
 71%|███████   | 3559/5000 [17:27:22<6:40:00, 16.66s/it] 71%|███████   | 3560/5000 [17:27:38<6:31:17, 16.30s/it]                                                        {'loss': 19.5545, 'grad_norm': 19.375, 'learning_rate': 1.470943784085883e-05, 'epoch': 4.52}
 71%|███████   | 3560/5000 [17:27:38<6:31:17, 16.30s/it] 71%|███████   | 3561/5000 [17:28:04<7:40:36, 19.21s/it]                                                        {'loss': 16.7014, 'grad_norm': 11.0625, 'learning_rate': 1.4690580624834972e-05, 'epoch': 4.52}
 71%|███████   | 3561/5000 [17:28:04<7:40:36, 19.21s/it] 71%|███████   | 3562/5000 [17:28:19<7:09:24, 17.92s/it]                                                        {'loss': 19.0929, 'grad_norm': 20.625, 'learning_rate': 1.4671732292841141e-05, 'epoch': 4.52}
 71%|███████   | 3562/5000 [17:28:19<7:09:24, 17.92s/it] 71%|███████▏  | 3563/5000 [17:28:35<6:57:58, 17.45s/it]                                                        {'loss': 16.9331, 'grad_norm': 11.4375, 'learning_rate': 1.4652892853122226e-05, 'epoch': 4.52}
 71%|███████▏  | 3563/5000 [17:28:35<6:57:58, 17.45s/it] 71%|███████▏  | 3564/5000 [17:28:58<7:41:12, 19.27s/it]                                                        {'loss': 16.9571, 'grad_norm': 9.8125, 'learning_rate': 1.4634062313919266e-05, 'epoch': 4.53}
 71%|███████▏  | 3564/5000 [17:28:58<7:41:12, 19.27s/it] 71%|███████▏  | 3565/5000 [17:29:12<6:58:01, 17.48s/it]                                                        {'loss': 18.438, 'grad_norm': 19.125, 'learning_rate': 1.4615240683469365e-05, 'epoch': 4.53}
 71%|███████▏  | 3565/5000 [17:29:12<6:58:01, 17.48s/it] 71%|███████▏  | 3566/5000 [17:29:26<6:38:05, 16.66s/it]                                                        {'loss': 18.5406, 'grad_norm': 24.625, 'learning_rate': 1.4596427970005731e-05, 'epoch': 4.53}
 71%|███████▏  | 3566/5000 [17:29:26<6:38:05, 16.66s/it] 71%|███████▏  | 3567/5000 [17:29:53<7:49:11, 19.64s/it]                                                        {'loss': 17.3159, 'grad_norm': 9.25, 'learning_rate': 1.4577624181757712e-05, 'epoch': 4.53}
 71%|███████▏  | 3567/5000 [17:29:53<7:49:11, 19.64s/it] 71%|███████▏  | 3568/5000 [17:30:08<7:16:31, 18.29s/it]                                                        {'loss': 20.2046, 'grad_norm': 15.1875, 'learning_rate': 1.4558829326950704e-05, 'epoch': 4.53}
 71%|███████▏  | 3568/5000 [17:30:08<7:16:31, 18.29s/it] 71%|███████▏  | 3569/5000 [17:30:42<9:09:07, 23.02s/it]                                                        {'loss': 17.7132, 'grad_norm': 11.125, 'learning_rate': 1.454004341380621e-05, 'epoch': 4.53}
 71%|███████▏  | 3569/5000 [17:30:42<9:09:07, 23.02s/it] 71%|███████▏  | 3570/5000 [17:31:00<8:34:24, 21.58s/it]                                                        {'loss': 18.9282, 'grad_norm': 13.25, 'learning_rate': 1.4521266450541846e-05, 'epoch': 4.53}
 71%|███████▏  | 3570/5000 [17:31:00<8:34:24, 21.58s/it] 71%|███████▏  | 3571/5000 [17:31:16<7:52:19, 19.83s/it]                                                        {'loss': 18.7247, 'grad_norm': 13.0, 'learning_rate': 1.4502498445371287e-05, 'epoch': 4.53}
 71%|███████▏  | 3571/5000 [17:31:16<7:52:19, 19.83s/it] 71%|███████▏  | 3572/5000 [17:31:29<7:00:17, 17.66s/it]                                                        {'loss': 18.8308, 'grad_norm': 18.875, 'learning_rate': 1.4483739406504276e-05, 'epoch': 4.54}
 71%|███████▏  | 3572/5000 [17:31:29<7:00:17, 17.66s/it] 71%|███████▏  | 3573/5000 [17:31:43<6:35:05, 16.61s/it]                                                        {'loss': 17.5671, 'grad_norm': 8.9375, 'learning_rate': 1.4464989342146687e-05, 'epoch': 4.54}
 71%|███████▏  | 3573/5000 [17:31:43<6:35:05, 16.61s/it] 71%|███████▏  | 3574/5000 [17:32:00<6:37:12, 16.71s/it]                                                        {'loss': 16.0343, 'grad_norm': 8.8125, 'learning_rate': 1.4446248260500404e-05, 'epoch': 4.54}
 71%|███████▏  | 3574/5000 [17:32:00<6:37:12, 16.71s/it] 72%|███████▏  | 3575/5000 [17:32:16<6:30:38, 16.45s/it]                                                        {'loss': 18.1063, 'grad_norm': 11.5625, 'learning_rate': 1.4427516169763444e-05, 'epoch': 4.54}
 72%|███████▏  | 3575/5000 [17:32:16<6:30:38, 16.45s/it] 72%|███████▏  | 3576/5000 [17:32:43<7:44:45, 19.58s/it]                                                        {'loss': 17.1988, 'grad_norm': 10.125, 'learning_rate': 1.4408793078129825e-05, 'epoch': 4.54}
 72%|███████▏  | 3576/5000 [17:32:43<7:44:45, 19.58s/it] 72%|███████▏  | 3577/5000 [17:33:00<7:26:10, 18.81s/it]                                                        {'loss': 17.3973, 'grad_norm': 14.1875, 'learning_rate': 1.4390078993789699e-05, 'epoch': 4.54}
 72%|███████▏  | 3577/5000 [17:33:00<7:26:10, 18.81s/it] 72%|███████▏  | 3578/5000 [17:33:12<6:42:56, 17.00s/it]                                                        {'loss': 20.0799, 'grad_norm': 19.125, 'learning_rate': 1.4371373924929225e-05, 'epoch': 4.54}
 72%|███████▏  | 3578/5000 [17:33:12<6:42:56, 17.00s/it] 72%|███████▏  | 3579/5000 [17:33:26<6:16:27, 15.90s/it]                                                        {'loss': 16.5876, 'grad_norm': 35.25, 'learning_rate': 1.4352677879730619e-05, 'epoch': 4.54}
 72%|███████▏  | 3579/5000 [17:33:26<6:16:27, 15.90s/it] 72%|███████▏  | 3580/5000 [17:33:43<6:26:26, 16.33s/it]                                                        {'loss': 17.1152, 'grad_norm': 26.625, 'learning_rate': 1.43339908663722e-05, 'epoch': 4.55}
 72%|███████▏  | 3580/5000 [17:33:43<6:26:26, 16.33s/it] 72%|███████▏  | 3581/5000 [17:34:00<6:32:39, 16.60s/it]                                                        {'loss': 19.4194, 'grad_norm': 17.25, 'learning_rate': 1.4315312893028286e-05, 'epoch': 4.55}
 72%|███████▏  | 3581/5000 [17:34:00<6:32:39, 16.60s/it] 72%|███████▏  | 3582/5000 [17:34:15<6:19:10, 16.04s/it]                                                        {'loss': 17.6534, 'grad_norm': 73.5, 'learning_rate': 1.4296643967869243e-05, 'epoch': 4.55}
 72%|███████▏  | 3582/5000 [17:34:15<6:19:10, 16.04s/it] 72%|███████▏  | 3583/5000 [17:34:31<6:15:10, 15.89s/it]                                                        {'loss': 16.9769, 'grad_norm': 12.4375, 'learning_rate': 1.4277984099061519e-05, 'epoch': 4.55}
 72%|███████▏  | 3583/5000 [17:34:31<6:15:10, 15.89s/it] 72%|███████▏  | 3584/5000 [17:34:45<6:05:25, 15.48s/it]                                                        {'loss': 17.9637, 'grad_norm': 11.6875, 'learning_rate': 1.4259333294767562e-05, 'epoch': 4.55}
 72%|███████▏  | 3584/5000 [17:34:45<6:05:25, 15.48s/it] 72%|███████▏  | 3585/5000 [17:35:00<5:59:56, 15.26s/it]                                                        {'loss': 18.2872, 'grad_norm': 15.4375, 'learning_rate': 1.4240691563145856e-05, 'epoch': 4.55}
 72%|███████▏  | 3585/5000 [17:35:00<5:59:56, 15.26s/it] 72%|███████▏  | 3586/5000 [17:35:16<6:02:20, 15.37s/it]                                                        {'loss': 17.2942, 'grad_norm': 10.3125, 'learning_rate': 1.4222058912350955e-05, 'epoch': 4.55}
 72%|███████▏  | 3586/5000 [17:35:16<6:02:20, 15.37s/it] 72%|███████▏  | 3587/5000 [17:35:30<5:53:53, 15.03s/it]                                                        {'loss': 18.3703, 'grad_norm': 21.625, 'learning_rate': 1.4203435350533395e-05, 'epoch': 4.55}
 72%|███████▏  | 3587/5000 [17:35:30<5:53:53, 15.03s/it] 72%|███████▏  | 3588/5000 [17:35:48<6:19:05, 16.11s/it]                                                        {'loss': 16.0837, 'grad_norm': 7.9375, 'learning_rate': 1.418482088583976e-05, 'epoch': 4.56}
 72%|███████▏  | 3588/5000 [17:35:48<6:19:05, 16.11s/it] 72%|███████▏  | 3589/5000 [17:36:01<5:51:29, 14.95s/it]                                                        {'loss': 20.2678, 'grad_norm': 27.75, 'learning_rate': 1.4166215526412633e-05, 'epoch': 4.56}
 72%|███████▏  | 3589/5000 [17:36:01<5:51:29, 14.95s/it] 72%|███████▏  | 3590/5000 [17:36:18<6:05:09, 15.54s/it]                                                        {'loss': 18.584, 'grad_norm': 17.0, 'learning_rate': 1.4147619280390657e-05, 'epoch': 4.56}
 72%|███████▏  | 3590/5000 [17:36:18<6:05:09, 15.54s/it] 72%|███████▏  | 3591/5000 [17:36:34<6:10:40, 15.78s/it]                                                        {'loss': 19.0187, 'grad_norm': 19.25, 'learning_rate': 1.4129032155908451e-05, 'epoch': 4.56}
 72%|███████▏  | 3591/5000 [17:36:34<6:10:40, 15.78s/it] 72%|███████▏  | 3592/5000 [17:36:51<6:21:19, 16.25s/it]                                                        {'loss': 16.6151, 'grad_norm': 9.5, 'learning_rate': 1.4110454161096638e-05, 'epoch': 4.56}
 72%|███████▏  | 3592/5000 [17:36:51<6:21:19, 16.25s/it] 72%|███████▏  | 3593/5000 [17:37:07<6:19:06, 16.17s/it]                                                        {'loss': 19.1252, 'grad_norm': 16.875, 'learning_rate': 1.4091885304081894e-05, 'epoch': 4.56}
 72%|███████▏  | 3593/5000 [17:37:07<6:19:06, 16.17s/it] 72%|███████▏  | 3594/5000 [17:37:22<6:07:42, 15.69s/it]                                                        {'loss': 17.6978, 'grad_norm': 15.3125, 'learning_rate': 1.4073325592986851e-05, 'epoch': 4.56}
 72%|███████▏  | 3594/5000 [17:37:22<6:07:42, 15.69s/it] 72%|███████▏  | 3595/5000 [17:37:39<6:19:12, 16.19s/it]                                                        {'loss': 17.0118, 'grad_norm': 11.8125, 'learning_rate': 1.4054775035930146e-05, 'epoch': 4.57}
 72%|███████▏  | 3595/5000 [17:37:39<6:19:12, 16.19s/it] 72%|███████▏  | 3596/5000 [17:37:57<6:33:28, 16.82s/it]                                                        {'loss': 17.7189, 'grad_norm': 8.75, 'learning_rate': 1.4036233641026452e-05, 'epoch': 4.57}
 72%|███████▏  | 3596/5000 [17:37:57<6:33:28, 16.82s/it] 72%|███████▏  | 3597/5000 [17:38:12<6:14:31, 16.02s/it]                                                        {'loss': 19.1124, 'grad_norm': 31.625, 'learning_rate': 1.4017701416386373e-05, 'epoch': 4.57}
 72%|███████▏  | 3597/5000 [17:38:12<6:14:31, 16.02s/it] 72%|███████▏  | 3598/5000 [17:38:31<6:36:43, 16.98s/it]                                                        {'loss': 17.3867, 'grad_norm': 10.5, 'learning_rate': 1.3999178370116565e-05, 'epoch': 4.57}
 72%|███████▏  | 3598/5000 [17:38:31<6:36:43, 16.98s/it] 72%|███████▏  | 3599/5000 [17:38:47<6:30:52, 16.74s/it]                                                        {'loss': 16.6813, 'grad_norm': 10.625, 'learning_rate': 1.3980664510319607e-05, 'epoch': 4.57}
 72%|███████▏  | 3599/5000 [17:38:47<6:30:52, 16.74s/it] 72%|███████▏  | 3600/5000 [17:39:03<6:28:23, 16.65s/it]                                                        {'loss': 18.921, 'grad_norm': 17.625, 'learning_rate': 1.396215984509412e-05, 'epoch': 4.57}
 72%|███████▏  | 3600/5000 [17:39:03<6:28:23, 16.65s/it] 72%|███████▏  | 3601/5000 [17:39:22<6:43:46, 17.32s/it]                                                        {'loss': 16.7576, 'grad_norm': 8.0625, 'learning_rate': 1.3943664382534648e-05, 'epoch': 4.57}
 72%|███████▏  | 3601/5000 [17:39:22<6:43:46, 17.32s/it] 72%|███████▏  | 3602/5000 [17:39:38<6:31:57, 16.82s/it]                                                        {'loss': 18.5219, 'grad_norm': 10.25, 'learning_rate': 1.3925178130731756e-05, 'epoch': 4.57}
 72%|███████▏  | 3602/5000 [17:39:38<6:31:57, 16.82s/it] 72%|███████▏  | 3603/5000 [17:39:52<6:14:25, 16.08s/it]                                                        {'loss': 18.7478, 'grad_norm': 10.3125, 'learning_rate': 1.390670109777195e-05, 'epoch': 4.58}
 72%|███████▏  | 3603/5000 [17:39:52<6:14:25, 16.08s/it] 72%|███████▏  | 3604/5000 [17:40:08<6:11:31, 15.97s/it]                                                        {'loss': 17.176, 'grad_norm': 11.25, 'learning_rate': 1.3888233291737697e-05, 'epoch': 4.58}
 72%|███████▏  | 3604/5000 [17:40:08<6:11:31, 15.97s/it] 72%|███████▏  | 3605/5000 [17:40:24<6:08:46, 15.86s/it]                                                        {'loss': 19.3163, 'grad_norm': 25.625, 'learning_rate': 1.3869774720707471e-05, 'epoch': 4.58}
 72%|███████▏  | 3605/5000 [17:40:24<6:08:46, 15.86s/it] 72%|███████▏  | 3606/5000 [17:40:39<6:01:54, 15.58s/it]                                                        {'loss': 19.4966, 'grad_norm': 89.5, 'learning_rate': 1.3851325392755661e-05, 'epoch': 4.58}
 72%|███████▏  | 3606/5000 [17:40:39<6:01:54, 15.58s/it] 72%|███████▏  | 3607/5000 [17:40:53<5:56:24, 15.35s/it]                                                        {'loss': 18.5699, 'grad_norm': 62.0, 'learning_rate': 1.3832885315952632e-05, 'epoch': 4.58}
 72%|███████▏  | 3607/5000 [17:40:53<5:56:24, 15.35s/it] 72%|███████▏  | 3608/5000 [17:41:14<6:31:47, 16.89s/it]                                                        {'loss': 16.6875, 'grad_norm': 53.0, 'learning_rate': 1.381445449836469e-05, 'epoch': 4.58}
 72%|███████▏  | 3608/5000 [17:41:14<6:31:47, 16.89s/it] 72%|███████▏  | 3609/5000 [17:41:28<6:10:02, 15.96s/it]                                                        {'loss': 18.9725, 'grad_norm': 23.625, 'learning_rate': 1.3796032948054119e-05, 'epoch': 4.58}
 72%|███████▏  | 3609/5000 [17:41:28<6:10:02, 15.96s/it] 72%|███████▏  | 3610/5000 [17:41:44<6:13:34, 16.13s/it]                                                        {'loss': 18.5659, 'grad_norm': 15.9375, 'learning_rate': 1.3777620673079124e-05, 'epoch': 4.58}
 72%|███████▏  | 3610/5000 [17:41:44<6:13:34, 16.13s/it] 72%|███████▏  | 3611/5000 [17:41:59<6:06:36, 15.84s/it]                                                        {'loss': 63.8081, 'grad_norm': 250.0, 'learning_rate': 1.3759217681493848e-05, 'epoch': 4.59}
 72%|███████▏  | 3611/5000 [17:41:59<6:06:36, 15.84s/it] 72%|███████▏  | 3612/5000 [17:42:25<7:15:39, 18.83s/it]                                                        {'loss': 18.1864, 'grad_norm': 12.0625, 'learning_rate': 1.3740823981348409e-05, 'epoch': 4.59}
 72%|███████▏  | 3612/5000 [17:42:25<7:15:39, 18.83s/it] 72%|███████▏  | 3613/5000 [17:42:43<7:10:18, 18.61s/it]                                                        {'loss': 16.9275, 'grad_norm': 12.375, 'learning_rate': 1.3722439580688818e-05, 'epoch': 4.59}
 72%|███████▏  | 3613/5000 [17:42:43<7:10:18, 18.61s/it] 72%|███████▏  | 3614/5000 [17:43:00<7:00:23, 18.20s/it]                                                        {'loss': 18.285, 'grad_norm': 20.875, 'learning_rate': 1.3704064487557033e-05, 'epoch': 4.59}
 72%|███████▏  | 3614/5000 [17:43:00<7:00:23, 18.20s/it] 72%|███████▏  | 3615/5000 [17:43:16<6:41:15, 17.38s/it]                                                        {'loss': 18.2217, 'grad_norm': 18.375, 'learning_rate': 1.3685698709990961e-05, 'epoch': 4.59}
 72%|███████▏  | 3615/5000 [17:43:16<6:41:15, 17.38s/it] 72%|███████▏  | 3616/5000 [17:43:41<7:36:22, 19.79s/it]                                                        {'loss': 17.326, 'grad_norm': 12.75, 'learning_rate': 1.366734225602441e-05, 'epoch': 4.59}
 72%|███████▏  | 3616/5000 [17:43:41<7:36:22, 19.79s/it] 72%|███████▏  | 3617/5000 [17:44:02<7:39:22, 19.93s/it]                                                        {'loss': 17.9203, 'grad_norm': 10.875, 'learning_rate': 1.3648995133687102e-05, 'epoch': 4.59}
 72%|███████▏  | 3617/5000 [17:44:02<7:39:22, 19.93s/it] 72%|███████▏  | 3618/5000 [17:44:26<8:09:11, 21.24s/it]                                                        {'loss': 19.8255, 'grad_norm': 15.9375, 'learning_rate': 1.3630657351004716e-05, 'epoch': 4.59}
 72%|███████▏  | 3618/5000 [17:44:26<8:09:11, 21.24s/it] 72%|███████▏  | 3619/5000 [17:44:41<7:23:23, 19.26s/it]                                                        {'loss': 16.9846, 'grad_norm': 19.0, 'learning_rate': 1.361232891599881e-05, 'epoch': 4.6}
 72%|███████▏  | 3619/5000 [17:44:41<7:23:23, 19.26s/it] 72%|███████▏  | 3620/5000 [17:45:07<8:15:05, 21.53s/it]                                                        {'loss': 17.9359, 'grad_norm': 26.125, 'learning_rate': 1.3594009836686852e-05, 'epoch': 4.6}
 72%|███████▏  | 3620/5000 [17:45:07<8:15:05, 21.53s/it] 72%|███████▏  | 3621/5000 [17:45:23<7:31:18, 19.64s/it]                                                        {'loss': 16.5406, 'grad_norm': 13.9375, 'learning_rate': 1.3575700121082253e-05, 'epoch': 4.6}
 72%|███████▏  | 3621/5000 [17:45:23<7:31:18, 19.64s/it] 72%|███████▏  | 3622/5000 [17:45:49<8:19:07, 21.73s/it]                                                        {'loss': 16.9923, 'grad_norm': 14.0, 'learning_rate': 1.355739977719428e-05, 'epoch': 4.6}
 72%|███████▏  | 3622/5000 [17:45:49<8:19:07, 21.73s/it] 72%|███████▏  | 3623/5000 [17:46:10<8:09:39, 21.34s/it]                                                        {'loss': 17.8141, 'grad_norm': 13.0625, 'learning_rate': 1.3539108813028151e-05, 'epoch': 4.6}
 72%|███████▏  | 3623/5000 [17:46:10<8:09:39, 21.34s/it] 72%|███████▏  | 3624/5000 [17:46:27<7:42:21, 20.16s/it]                                                        {'loss': 17.8775, 'grad_norm': 11.9375, 'learning_rate': 1.3520827236584932e-05, 'epoch': 4.6}
 72%|███████▏  | 3624/5000 [17:46:27<7:42:21, 20.16s/it] 72%|███████▎  | 3625/5000 [17:46:42<7:09:04, 18.72s/it]                                                        {'loss': 19.3469, 'grad_norm': 12.9375, 'learning_rate': 1.3502555055861625e-05, 'epoch': 4.6}
 72%|███████▎  | 3625/5000 [17:46:42<7:09:04, 18.72s/it] 73%|███████▎  | 3626/5000 [17:47:09<8:05:54, 21.22s/it]                                                        {'loss': 17.2049, 'grad_norm': 10.6875, 'learning_rate': 1.3484292278851095e-05, 'epoch': 4.6}
 73%|███████▎  | 3626/5000 [17:47:09<8:05:54, 21.22s/it] 73%|███████▎  | 3627/5000 [17:47:25<7:27:16, 19.55s/it]                                                        {'loss': 17.9013, 'grad_norm': 18.5, 'learning_rate': 1.3466038913542088e-05, 'epoch': 4.61}
 73%|███████▎  | 3627/5000 [17:47:25<7:27:16, 19.55s/it] 73%|███████▎  | 3628/5000 [17:47:42<7:09:34, 18.79s/it]                                                        {'loss': 17.3418, 'grad_norm': 13.0, 'learning_rate': 1.3447794967919267e-05, 'epoch': 4.61}
 73%|███████▎  | 3628/5000 [17:47:42<7:09:34, 18.79s/it] 73%|███████▎  | 3629/5000 [17:48:01<7:07:00, 18.69s/it]                                                        {'loss': 15.8879, 'grad_norm': 12.375, 'learning_rate': 1.342956044996315e-05, 'epoch': 4.61}
 73%|███████▎  | 3629/5000 [17:48:01<7:07:00, 18.69s/it] 73%|███████▎  | 3630/5000 [17:48:25<7:42:42, 20.26s/it]                                                        {'loss': 17.1318, 'grad_norm': 13.1875, 'learning_rate': 1.3411335367650113e-05, 'epoch': 4.61}
 73%|███████▎  | 3630/5000 [17:48:25<7:42:42, 20.26s/it] 73%|███████▎  | 3631/5000 [17:48:38<6:54:19, 18.16s/it]                                                        {'loss': 19.8207, 'grad_norm': 19.0, 'learning_rate': 1.3393119728952454e-05, 'epoch': 4.61}
 73%|███████▎  | 3631/5000 [17:48:38<6:54:19, 18.16s/it] 73%|███████▎  | 3632/5000 [17:48:54<6:39:37, 17.53s/it]                                                        {'loss': 17.3346, 'grad_norm': 14.0, 'learning_rate': 1.3374913541838301e-05, 'epoch': 4.61}
 73%|███████▎  | 3632/5000 [17:48:54<6:39:37, 17.53s/it] 73%|███████▎  | 3633/5000 [17:49:08<6:19:01, 16.64s/it]                                                        {'loss': 18.1983, 'grad_norm': 13.625, 'learning_rate': 1.3356716814271648e-05, 'epoch': 4.61}
 73%|███████▎  | 3633/5000 [17:49:08<6:19:01, 16.64s/it] 73%|███████▎  | 3634/5000 [17:49:25<6:19:43, 16.68s/it]                                                        {'loss': 19.3133, 'grad_norm': 18.75, 'learning_rate': 1.3338529554212386e-05, 'epoch': 4.61}
 73%|███████▎  | 3634/5000 [17:49:25<6:19:43, 16.68s/it] 73%|███████▎  | 3635/5000 [17:49:51<7:19:00, 19.30s/it]                                                        {'loss': 17.0991, 'grad_norm': 13.9375, 'learning_rate': 1.3320351769616223e-05, 'epoch': 4.62}
 73%|███████▎  | 3635/5000 [17:49:51<7:19:00, 19.30s/it] 73%|███████▎  | 3636/5000 [17:50:10<7:19:31, 19.33s/it]                                                        {'loss': 17.5476, 'grad_norm': 10.0, 'learning_rate': 1.3302183468434732e-05, 'epoch': 4.62}
 73%|███████▎  | 3636/5000 [17:50:10<7:19:31, 19.33s/it] 73%|███████▎  | 3637/5000 [17:50:25<6:50:29, 18.07s/it]                                                        {'loss': 18.6521, 'grad_norm': 17.0, 'learning_rate': 1.3284024658615367e-05, 'epoch': 4.62}
 73%|███████▎  | 3637/5000 [17:50:25<6:50:29, 18.07s/it] 73%|███████▎  | 3638/5000 [17:50:40<6:31:23, 17.24s/it]                                                        {'loss': 16.6904, 'grad_norm': 12.1875, 'learning_rate': 1.3265875348101398e-05, 'epoch': 4.62}
 73%|███████▎  | 3638/5000 [17:50:40<6:31:23, 17.24s/it] 73%|███████▎  | 3639/5000 [17:50:55<6:15:00, 16.53s/it]                                                        {'loss': 18.7407, 'grad_norm': 57.75, 'learning_rate': 1.3247735544831938e-05, 'epoch': 4.62}
 73%|███████▎  | 3639/5000 [17:50:55<6:15:00, 16.53s/it] 73%|███████▎  | 3640/5000 [17:51:09<5:54:29, 15.64s/it]                                                        {'loss': 18.2139, 'grad_norm': 13.75, 'learning_rate': 1.322960525674198e-05, 'epoch': 4.62}
 73%|███████▎  | 3640/5000 [17:51:09<5:54:29, 15.64s/it] 73%|███████▎  | 3641/5000 [17:51:27<6:14:05, 16.52s/it]                                                        {'loss': 17.8915, 'grad_norm': 34.5, 'learning_rate': 1.3211484491762311e-05, 'epoch': 4.62}
 73%|███████▎  | 3641/5000 [17:51:27<6:14:05, 16.52s/it] 73%|███████▎  | 3642/5000 [17:51:41<5:56:13, 15.74s/it]                                                        {'loss': 18.8586, 'grad_norm': 19.625, 'learning_rate': 1.3193373257819568e-05, 'epoch': 4.62}
 73%|███████▎  | 3642/5000 [17:51:41<5:56:13, 15.74s/it] 73%|███████▎  | 3643/5000 [17:51:59<6:09:12, 16.32s/it]                                                        {'loss': 16.6818, 'grad_norm': 10.75, 'learning_rate': 1.3175271562836215e-05, 'epoch': 4.63}
 73%|███████▎  | 3643/5000 [17:51:59<6:09:12, 16.32s/it] 73%|███████▎  | 3644/5000 [17:52:14<5:58:32, 15.86s/it]                                                        {'loss': 16.6107, 'grad_norm': 14.75, 'learning_rate': 1.315717941473057e-05, 'epoch': 4.63}
 73%|███████▎  | 3644/5000 [17:52:14<5:58:32, 15.86s/it] 73%|███████▎  | 3645/5000 [17:52:28<5:49:44, 15.49s/it]                                                        {'loss': 17.7607, 'grad_norm': 10.375, 'learning_rate': 1.3139096821416724e-05, 'epoch': 4.63}
 73%|███████▎  | 3645/5000 [17:52:28<5:49:44, 15.49s/it] 73%|███████▎  | 3646/5000 [17:52:46<6:00:35, 15.98s/it]                                                        {'loss': 19.2209, 'grad_norm': 19.625, 'learning_rate': 1.3121023790804647e-05, 'epoch': 4.63}
 73%|███████▎  | 3646/5000 [17:52:46<6:00:35, 15.98s/it] 73%|███████▎  | 3647/5000 [17:53:01<5:53:52, 15.69s/it]                                                        {'loss': 18.1047, 'grad_norm': 14.625, 'learning_rate': 1.310296033080007e-05, 'epoch': 4.63}
 73%|███████▎  | 3647/5000 [17:53:01<5:53:52, 15.69s/it] 73%|███████▎  | 3648/5000 [17:53:25<6:50:46, 18.23s/it]                                                        {'loss': 18.756, 'grad_norm': 16.5, 'learning_rate': 1.308490644930459e-05, 'epoch': 4.63}
 73%|███████▎  | 3648/5000 [17:53:25<6:50:46, 18.23s/it] 73%|███████▎  | 3649/5000 [17:53:39<6:26:51, 17.18s/it]                                                        {'loss': 19.1986, 'grad_norm': 322.0, 'learning_rate': 1.3066862154215565e-05, 'epoch': 4.63}
 73%|███████▎  | 3649/5000 [17:53:39<6:26:51, 17.18s/it] 73%|███████▎  | 3650/5000 [17:53:52<5:58:23, 15.93s/it]                                                        {'loss': 19.2562, 'grad_norm': 40.0, 'learning_rate': 1.3048827453426203e-05, 'epoch': 4.63}
 73%|███████▎  | 3650/5000 [17:53:52<5:58:23, 15.93s/it] 73%|███████▎  | 3651/5000 [17:54:06<5:41:11, 15.18s/it]                                                        {'loss': 16.9428, 'grad_norm': 18.875, 'learning_rate': 1.3030802354825486e-05, 'epoch': 4.64}
 73%|███████▎  | 3651/5000 [17:54:06<5:41:11, 15.18s/it] 73%|███████▎  | 3652/5000 [17:54:30<6:43:26, 17.96s/it]                                                        {'loss': 17.4606, 'grad_norm': 58.0, 'learning_rate': 1.3012786866298188e-05, 'epoch': 4.64}
 73%|███████▎  | 3652/5000 [17:54:30<6:43:26, 17.96s/it] 73%|███████▎  | 3653/5000 [17:54:49<6:50:56, 18.31s/it]                                                        {'loss': 16.6607, 'grad_norm': 38.5, 'learning_rate': 1.299478099572493e-05, 'epoch': 4.64}
 73%|███████▎  | 3653/5000 [17:54:49<6:50:56, 18.31s/it] 73%|███████▎  | 3654/5000 [17:55:07<6:48:24, 18.21s/it]                                                        {'loss': 17.1962, 'grad_norm': 54.75, 'learning_rate': 1.2976784750982064e-05, 'epoch': 4.64}
 73%|███████▎  | 3654/5000 [17:55:07<6:48:24, 18.21s/it] 73%|███████▎  | 3655/5000 [17:55:21<6:19:25, 16.93s/it]                                                        {'loss': 17.8915, 'grad_norm': 10.1875, 'learning_rate': 1.2958798139941764e-05, 'epoch': 4.64}
 73%|███████▎  | 3655/5000 [17:55:21<6:19:25, 16.93s/it] 73%|███████▎  | 3656/5000 [17:55:38<6:19:51, 16.96s/it]                                                        {'loss': 17.4521, 'grad_norm': 11.4375, 'learning_rate': 1.2940821170471997e-05, 'epoch': 4.64}
 73%|███████▎  | 3656/5000 [17:55:38<6:19:51, 16.96s/it] 73%|███████▎  | 3657/5000 [17:55:55<6:17:55, 16.88s/it]                                                        {'loss': 17.3516, 'grad_norm': 10.4375, 'learning_rate': 1.2922853850436488e-05, 'epoch': 4.64}
 73%|███████▎  | 3657/5000 [17:55:55<6:17:55, 16.88s/it] 73%|███████▎  | 3658/5000 [17:56:20<7:08:10, 19.14s/it]                                                        {'loss': 17.8069, 'grad_norm': 11.6875, 'learning_rate': 1.290489618769476e-05, 'epoch': 4.65}
 73%|███████▎  | 3658/5000 [17:56:20<7:08:10, 19.14s/it] 73%|███████▎  | 3659/5000 [17:56:47<8:01:25, 21.54s/it]                                                        {'loss': 17.3083, 'grad_norm': 9.8125, 'learning_rate': 1.2886948190102088e-05, 'epoch': 4.65}
 73%|███████▎  | 3659/5000 [17:56:47<8:01:25, 21.54s/it] 73%|███████▎  | 3660/5000 [17:57:04<7:31:08, 20.20s/it]                                                        {'loss': 17.6501, 'grad_norm': 11.9375, 'learning_rate': 1.2869009865509564e-05, 'epoch': 4.65}
 73%|███████▎  | 3660/5000 [17:57:04<7:31:08, 20.20s/it] 73%|███████▎  | 3661/5000 [17:57:20<7:05:59, 19.09s/it]                                                        {'loss': 20.1828, 'grad_norm': 15.625, 'learning_rate': 1.2851081221764004e-05, 'epoch': 4.65}
 73%|███████▎  | 3661/5000 [17:57:20<7:05:59, 19.09s/it] 73%|███████▎  | 3662/5000 [17:57:36<6:42:40, 18.06s/it]                                                        {'loss': 19.2962, 'grad_norm': 16.0, 'learning_rate': 1.2833162266707997e-05, 'epoch': 4.65}
 73%|███████▎  | 3662/5000 [17:57:36<6:42:40, 18.06s/it] 73%|███████▎  | 3663/5000 [17:57:51<6:22:08, 17.15s/it]                                                        {'loss': 19.2902, 'grad_norm': 45.25, 'learning_rate': 1.2815253008179921e-05, 'epoch': 4.65}
 73%|███████▎  | 3663/5000 [17:57:51<6:22:08, 17.15s/it] 73%|███████▎  | 3664/5000 [17:58:15<7:07:04, 19.18s/it]                                                        {'loss': 19.4403, 'grad_norm': 13.8125, 'learning_rate': 1.2797353454013892e-05, 'epoch': 4.65}
 73%|███████▎  | 3664/5000 [17:58:15<7:07:04, 19.18s/it] 73%|███████▎  | 3665/5000 [17:58:33<6:59:18, 18.85s/it]                                                        {'loss': 17.8008, 'grad_norm': 15.0625, 'learning_rate': 1.2779463612039766e-05, 'epoch': 4.65}
 73%|███████▎  | 3665/5000 [17:58:33<6:59:18, 18.85s/it] 73%|███████▎  | 3666/5000 [17:58:51<6:52:11, 18.54s/it]                                                        {'loss': 18.5614, 'grad_norm': 18.125, 'learning_rate': 1.2761583490083189e-05, 'epoch': 4.66}
 73%|███████▎  | 3666/5000 [17:58:51<6:52:11, 18.54s/it] 73%|███████▎  | 3667/5000 [17:59:08<6:46:13, 18.28s/it]                                                        {'loss': 17.1701, 'grad_norm': 9.3125, 'learning_rate': 1.2743713095965525e-05, 'epoch': 4.66}
 73%|███████▎  | 3667/5000 [17:59:08<6:46:13, 18.28s/it] 73%|███████▎  | 3668/5000 [17:59:25<6:33:32, 17.73s/it]                                                        {'loss': 17.8895, 'grad_norm': 14.3125, 'learning_rate': 1.2725852437503876e-05, 'epoch': 4.66}
 73%|███████▎  | 3668/5000 [17:59:25<6:33:32, 17.73s/it] 73%|███████▎  | 3669/5000 [17:59:50<7:25:26, 20.08s/it]                                                        {'loss': 16.8421, 'grad_norm': 8.0625, 'learning_rate': 1.270800152251112e-05, 'epoch': 4.66}
 73%|███████▎  | 3669/5000 [17:59:50<7:25:26, 20.08s/it] 73%|███████▎  | 3670/5000 [18:00:12<7:36:09, 20.58s/it]                                                        {'loss': 18.723, 'grad_norm': 47.25, 'learning_rate': 1.2690160358795858e-05, 'epoch': 4.66}
 73%|███████▎  | 3670/5000 [18:00:12<7:36:09, 20.58s/it] 73%|███████▎  | 3671/5000 [18:00:29<7:11:35, 19.48s/it]                                                        {'loss': 17.4887, 'grad_norm': 22.375, 'learning_rate': 1.2672328954162404e-05, 'epoch': 4.66}
 73%|███████▎  | 3671/5000 [18:00:29<7:11:35, 19.48s/it] 73%|███████▎  | 3672/5000 [18:00:43<6:34:15, 17.81s/it]                                                        {'loss': 18.417, 'grad_norm': 19.75, 'learning_rate': 1.2654507316410838e-05, 'epoch': 4.66}
 73%|███████▎  | 3672/5000 [18:00:43<6:34:15, 17.81s/it] 73%|███████▎  | 3673/5000 [18:01:01<6:35:23, 17.88s/it]                                                        {'loss': 17.5697, 'grad_norm': 17.75, 'learning_rate': 1.2636695453336943e-05, 'epoch': 4.66}
 73%|███████▎  | 3673/5000 [18:01:01<6:35:23, 17.88s/it] 73%|███████▎  | 3674/5000 [18:01:20<6:42:53, 18.23s/it]                                                        {'loss': 19.1213, 'grad_norm': 20.0, 'learning_rate': 1.2618893372732228e-05, 'epoch': 4.67}
 73%|███████▎  | 3674/5000 [18:01:20<6:42:53, 18.23s/it] 74%|███████▎  | 3675/5000 [18:01:37<6:36:12, 17.94s/it]                                                        {'loss': 17.1656, 'grad_norm': 10.4375, 'learning_rate': 1.2601101082383917e-05, 'epoch': 4.67}
 74%|███████▎  | 3675/5000 [18:01:37<6:36:12, 17.94s/it] 74%|███████▎  | 3676/5000 [18:01:53<6:20:51, 17.26s/it]                                                        {'loss': 17.718, 'grad_norm': 13.8125, 'learning_rate': 1.258331859007499e-05, 'epoch': 4.67}
 74%|███████▎  | 3676/5000 [18:01:53<6:20:51, 17.26s/it] 74%|███████▎  | 3677/5000 [18:02:09<6:15:24, 17.03s/it]                                                        {'loss': 18.3513, 'grad_norm': 10.875, 'learning_rate': 1.2565545903584099e-05, 'epoch': 4.67}
 74%|███████▎  | 3677/5000 [18:02:09<6:15:24, 17.03s/it] 74%|███████▎  | 3678/5000 [18:02:45<8:17:31, 22.58s/it]                                                        {'loss': 17.0631, 'grad_norm': 13.3125, 'learning_rate': 1.2547783030685609e-05, 'epoch': 4.67}
 74%|███████▎  | 3678/5000 [18:02:45<8:17:31, 22.58s/it] 74%|███████▎  | 3679/5000 [18:03:11<8:39:37, 23.60s/it]                                                        {'loss': 18.2118, 'grad_norm': 11.375, 'learning_rate': 1.2530029979149626e-05, 'epoch': 4.67}
 74%|███████▎  | 3679/5000 [18:03:11<8:39:37, 23.60s/it] 74%|███████▎  | 3680/5000 [18:03:38<9:04:06, 24.73s/it]                                                        {'loss': 16.4957, 'grad_norm': 8.875, 'learning_rate': 1.2512286756741933e-05, 'epoch': 4.67}
 74%|███████▎  | 3680/5000 [18:03:38<9:04:06, 24.73s/it] 74%|███████▎  | 3681/5000 [18:03:52<7:49:38, 21.36s/it]                                                        {'loss': 18.6348, 'grad_norm': 11.625, 'learning_rate': 1.2494553371223995e-05, 'epoch': 4.67}
 74%|███████▎  | 3681/5000 [18:03:52<7:49:38, 21.36s/it] 74%|███████▎  | 3682/5000 [18:04:07<7:05:26, 19.37s/it]                                                        {'loss': 19.7101, 'grad_norm': 16.75, 'learning_rate': 1.2476829830353032e-05, 'epoch': 4.68}
 74%|███████▎  | 3682/5000 [18:04:07<7:05:26, 19.37s/it] 74%|███████▎  | 3683/5000 [18:04:21<6:33:56, 17.95s/it]                                                        {'loss': 18.3321, 'grad_norm': 13.8125, 'learning_rate': 1.2459116141881904e-05, 'epoch': 4.68}
 74%|███████▎  | 3683/5000 [18:04:21<6:33:56, 17.95s/it] 74%|███████▎  | 3684/5000 [18:04:46<7:16:58, 19.92s/it]                                                        {'loss': 18.3415, 'grad_norm': 11.0, 'learning_rate': 1.2441412313559168e-05, 'epoch': 4.68}
 74%|███████▎  | 3684/5000 [18:04:46<7:16:58, 19.92s/it] 74%|███████▎  | 3685/5000 [18:05:02<6:54:57, 18.93s/it]                                                        {'loss': 18.3072, 'grad_norm': 18.125, 'learning_rate': 1.2423718353129108e-05, 'epoch': 4.68}
 74%|███████▎  | 3685/5000 [18:05:02<6:54:57, 18.93s/it] 74%|███████▎  | 3686/5000 [18:05:17<6:25:01, 17.58s/it]                                                        {'loss': 20.1422, 'grad_norm': 20.0, 'learning_rate': 1.2406034268331653e-05, 'epoch': 4.68}
 74%|███████▎  | 3686/5000 [18:05:17<6:25:01, 17.58s/it] 74%|███████▎  | 3687/5000 [18:05:43<7:21:04, 20.16s/it]                                                        {'loss': 16.4518, 'grad_norm': 6.375, 'learning_rate': 1.2388360066902408e-05, 'epoch': 4.68}
 74%|███████▎  | 3687/5000 [18:05:43<7:21:04, 20.16s/it] 74%|███████▍  | 3688/5000 [18:05:59<6:55:44, 19.01s/it]                                                        {'loss': 17.1216, 'grad_norm': 10.875, 'learning_rate': 1.237069575657269e-05, 'epoch': 4.68}
 74%|███████▍  | 3688/5000 [18:05:59<6:55:44, 19.01s/it] 74%|███████▍  | 3689/5000 [18:06:13<6:19:26, 17.37s/it]                                                        {'loss': 18.219, 'grad_norm': 12.1875, 'learning_rate': 1.2353041345069458e-05, 'epoch': 4.68}
 74%|███████▍  | 3689/5000 [18:06:13<6:19:26, 17.37s/it] 74%|███████▍  | 3690/5000 [18:06:42<7:39:17, 21.04s/it]                                                        {'loss': 19.6297, 'grad_norm': 19.75, 'learning_rate': 1.233539684011534e-05, 'epoch': 4.69}
 74%|███████▍  | 3690/5000 [18:06:42<7:39:17, 21.04s/it] 74%|███████▍  | 3691/5000 [18:06:55<6:43:22, 18.49s/it]                                                        {'loss': 19.2115, 'grad_norm': 20.875, 'learning_rate': 1.2317762249428667e-05, 'epoch': 4.69}
 74%|███████▍  | 3691/5000 [18:06:55<6:43:22, 18.49s/it] 74%|███████▍  | 3692/5000 [18:07:11<6:29:08, 17.85s/it]                                                        {'loss': 17.0171, 'grad_norm': 41.25, 'learning_rate': 1.2300137580723385e-05, 'epoch': 4.69}
 74%|███████▍  | 3692/5000 [18:07:11<6:29:08, 17.85s/it] 74%|███████▍  | 3693/5000 [18:07:32<6:46:39, 18.67s/it]                                                        {'loss': 17.5204, 'grad_norm': 11.1875, 'learning_rate': 1.2282522841709145e-05, 'epoch': 4.69}
 74%|███████▍  | 3693/5000 [18:07:32<6:46:39, 18.67s/it] 74%|███████▍  | 3694/5000 [18:07:54<7:10:40, 19.79s/it]                                                        {'loss': 16.6249, 'grad_norm': 8.4375, 'learning_rate': 1.2264918040091205e-05, 'epoch': 4.69}
 74%|███████▍  | 3694/5000 [18:07:54<7:10:40, 19.79s/it] 74%|███████▍  | 3695/5000 [18:08:09<6:36:44, 18.24s/it]                                                        {'loss': 18.5102, 'grad_norm': 10.8125, 'learning_rate': 1.2247323183570536e-05, 'epoch': 4.69}
 74%|███████▍  | 3695/5000 [18:08:09<6:36:44, 18.24s/it] 74%|███████▍  | 3696/5000 [18:08:32<7:09:31, 19.76s/it]                                                        {'loss': 18.1493, 'grad_norm': 25.375, 'learning_rate': 1.2229738279843707e-05, 'epoch': 4.69}
 74%|███████▍  | 3696/5000 [18:08:32<7:09:31, 19.76s/it] 74%|███████▍  | 3697/5000 [18:08:59<7:52:53, 21.78s/it]                                                        {'loss': 19.0403, 'grad_norm': 1344.0, 'learning_rate': 1.2212163336602941e-05, 'epoch': 4.69}
 74%|███████▍  | 3697/5000 [18:08:59<7:52:53, 21.78s/it] 74%|███████▍  | 3698/5000 [18:09:14<7:11:21, 19.88s/it]                                                        {'loss': 18.1095, 'grad_norm': 11.3125, 'learning_rate': 1.2194598361536142e-05, 'epoch': 4.7}
 74%|███████▍  | 3698/5000 [18:09:14<7:11:21, 19.88s/it] 74%|███████▍  | 3699/5000 [18:09:31<6:51:43, 18.99s/it]                                                        {'loss': 18.021, 'grad_norm': 15.125, 'learning_rate': 1.2177043362326813e-05, 'epoch': 4.7}
 74%|███████▍  | 3699/5000 [18:09:31<6:51:43, 18.99s/it] 74%|███████▍  | 3700/5000 [18:09:48<6:38:15, 18.38s/it]                                                        {'loss': 17.5859, 'grad_norm': 9.6875, 'learning_rate': 1.2159498346654094e-05, 'epoch': 4.7}
 74%|███████▍  | 3700/5000 [18:09:48<6:38:15, 18.38s/it] 74%|███████▍  | 3701/5000 [18:10:06<6:35:04, 18.25s/it]                                                        {'loss': 17.4782, 'grad_norm': 10.5625, 'learning_rate': 1.21419633221928e-05, 'epoch': 4.7}
 74%|███████▍  | 3701/5000 [18:10:06<6:35:04, 18.25s/it] 74%|███████▍  | 3702/5000 [18:10:21<6:11:11, 17.16s/it]                                                        {'loss': 18.3564, 'grad_norm': 16.625, 'learning_rate': 1.2124438296613328e-05, 'epoch': 4.7}
 74%|███████▍  | 3702/5000 [18:10:21<6:11:11, 17.16s/it] 74%|███████▍  | 3703/5000 [18:10:35<5:54:29, 16.40s/it]                                                        {'loss': 26.9255, 'grad_norm': 2024.0, 'learning_rate': 1.2106923277581708e-05, 'epoch': 4.7}
 74%|███████▍  | 3703/5000 [18:10:35<5:54:29, 16.40s/it] 74%|███████▍  | 3704/5000 [18:10:55<6:15:38, 17.39s/it]                                                        {'loss': 17.3483, 'grad_norm': 10.8125, 'learning_rate': 1.2089418272759634e-05, 'epoch': 4.7}
 74%|███████▍  | 3704/5000 [18:10:55<6:15:38, 17.39s/it] 74%|███████▍  | 3705/5000 [18:11:19<6:58:19, 19.38s/it]                                                        {'loss': 17.4514, 'grad_norm': 9.6875, 'learning_rate': 1.2071923289804373e-05, 'epoch': 4.7}
 74%|███████▍  | 3705/5000 [18:11:19<6:58:19, 19.38s/it] 74%|███████▍  | 3706/5000 [18:11:43<7:28:16, 20.79s/it]                                                        {'loss': 15.8916, 'grad_norm': 7.75, 'learning_rate': 1.2054438336368818e-05, 'epoch': 4.71}
 74%|███████▍  | 3706/5000 [18:11:43<7:28:16, 20.79s/it] 74%|███████▍  | 3707/5000 [18:11:58<6:53:25, 19.18s/it]                                                        {'loss': 18.4207, 'grad_norm': 12.0, 'learning_rate': 1.20369634201015e-05, 'epoch': 4.71}
 74%|███████▍  | 3707/5000 [18:11:58<6:53:25, 19.18s/it] 74%|███████▍  | 3708/5000 [18:12:12<6:19:03, 17.60s/it]                                                        {'loss': 18.3565, 'grad_norm': 9.5, 'learning_rate': 1.2019498548646528e-05, 'epoch': 4.71}
 74%|███████▍  | 3708/5000 [18:12:12<6:19:03, 17.60s/it] 74%|███████▍  | 3709/5000 [18:12:26<5:49:54, 16.26s/it]                                                        {'loss': 18.2507, 'grad_norm': 13.25, 'learning_rate': 1.2002043729643636e-05, 'epoch': 4.71}
 74%|███████▍  | 3709/5000 [18:12:26<5:49:54, 16.26s/it] 74%|███████▍  | 3710/5000 [18:12:41<5:41:51, 15.90s/it]                                                        {'loss': 17.9801, 'grad_norm': 11.9375, 'learning_rate': 1.1984598970728143e-05, 'epoch': 4.71}
 74%|███████▍  | 3710/5000 [18:12:41<5:41:51, 15.90s/it] 74%|███████▍  | 3711/5000 [18:13:02<6:19:15, 17.65s/it]                                                        {'loss': 18.2024, 'grad_norm': 12.5625, 'learning_rate': 1.1967164279530996e-05, 'epoch': 4.71}
 74%|███████▍  | 3711/5000 [18:13:02<6:19:15, 17.65s/it] 74%|███████▍  | 3712/5000 [18:13:30<7:24:08, 20.69s/it]                                                        {'loss': 17.3924, 'grad_norm': 47.75, 'learning_rate': 1.1949739663678719e-05, 'epoch': 4.71}
 74%|███████▍  | 3712/5000 [18:13:30<7:24:08, 20.69s/it] 74%|███████▍  | 3713/5000 [18:13:45<6:43:21, 18.80s/it]                                                        {'loss': 18.9697, 'grad_norm': 29.875, 'learning_rate': 1.1932325130793408e-05, 'epoch': 4.71}
 74%|███████▍  | 3713/5000 [18:13:45<6:43:21, 18.80s/it] 74%|███████▍  | 3714/5000 [18:14:00<6:19:34, 17.71s/it]                                                        {'loss': 18.4156, 'grad_norm': 23.5, 'learning_rate': 1.1914920688492803e-05, 'epoch': 4.72}
 74%|███████▍  | 3714/5000 [18:14:00<6:19:34, 17.71s/it] 74%|███████▍  | 3715/5000 [18:14:16<6:10:18, 17.29s/it]                                                        {'loss': 18.5333, 'grad_norm': 10.3125, 'learning_rate': 1.1897526344390173e-05, 'epoch': 4.72}
 74%|███████▍  | 3715/5000 [18:14:16<6:10:18, 17.29s/it] 74%|███████▍  | 3716/5000 [18:14:33<6:07:25, 17.17s/it]                                                        {'loss': 18.4954, 'grad_norm': 13.875, 'learning_rate': 1.1880142106094417e-05, 'epoch': 4.72}
 74%|███████▍  | 3716/5000 [18:14:33<6:07:25, 17.17s/it] 74%|███████▍  | 3717/5000 [18:14:46<5:40:44, 15.93s/it]                                                        {'loss': 17.5679, 'grad_norm': 13.625, 'learning_rate': 1.1862767981209973e-05, 'epoch': 4.72}
 74%|███████▍  | 3717/5000 [18:14:46<5:40:44, 15.93s/it] 74%|███████▍  | 3718/5000 [18:15:02<5:42:33, 16.03s/it]                                                        {'loss': 17.2118, 'grad_norm': 14.25, 'learning_rate': 1.1845403977336887e-05, 'epoch': 4.72}
 74%|███████▍  | 3718/5000 [18:15:02<5:42:33, 16.03s/it] 74%|███████▍  | 3719/5000 [18:15:15<5:21:29, 15.06s/it]                                                        {'loss': 18.2681, 'grad_norm': 18.75, 'learning_rate': 1.1828050102070751e-05, 'epoch': 4.72}
 74%|███████▍  | 3719/5000 [18:15:15<5:21:29, 15.06s/it] 74%|███████▍  | 3720/5000 [18:15:31<5:30:28, 15.49s/it]                                                        {'loss': 18.7745, 'grad_norm': 16.125, 'learning_rate': 1.181070636300276e-05, 'epoch': 4.72}
 74%|███████▍  | 3720/5000 [18:15:31<5:30:28, 15.49s/it] 74%|███████▍  | 3721/5000 [18:15:46<5:25:44, 15.28s/it]                                                        {'loss': 16.7823, 'grad_norm': 11.25, 'learning_rate': 1.1793372767719638e-05, 'epoch': 4.73}
 74%|███████▍  | 3721/5000 [18:15:46<5:25:44, 15.28s/it] 74%|███████▍  | 3722/5000 [18:16:01<5:24:05, 15.22s/it]                                                        {'loss': 18.926, 'grad_norm': 19.75, 'learning_rate': 1.1776049323803683e-05, 'epoch': 4.73}
 74%|███████▍  | 3722/5000 [18:16:01<5:24:05, 15.22s/it] 74%|███████▍  | 3723/5000 [18:16:15<5:13:31, 14.73s/it]                                                        {'loss': 18.2486, 'grad_norm': 13.6875, 'learning_rate': 1.175873603883278e-05, 'epoch': 4.73}
 74%|███████▍  | 3723/5000 [18:16:15<5:13:31, 14.73s/it] 74%|███████▍  | 3724/5000 [18:16:28<5:01:52, 14.19s/it]                                                        {'loss': 18.7118, 'grad_norm': 17.5, 'learning_rate': 1.1741432920380334e-05, 'epoch': 4.73}
 74%|███████▍  | 3724/5000 [18:16:28<5:01:52, 14.19s/it] 74%|███████▍  | 3725/5000 [18:16:41<4:51:52, 13.74s/it]                                                        {'loss': 19.8874, 'grad_norm': 17.125, 'learning_rate': 1.1724139976015306e-05, 'epoch': 4.73}
 74%|███████▍  | 3725/5000 [18:16:41<4:51:52, 13.74s/it] 75%|███████▍  | 3726/5000 [18:16:55<4:58:37, 14.06s/it]                                                        {'loss': 17.5879, 'grad_norm': 10.6875, 'learning_rate': 1.170685721330224e-05, 'epoch': 4.73}
 75%|███████▍  | 3726/5000 [18:16:55<4:58:37, 14.06s/it] 75%|███████▍  | 3727/5000 [18:17:11<5:08:58, 14.56s/it]                                                        {'loss': 18.2661, 'grad_norm': 17.625, 'learning_rate': 1.1689584639801194e-05, 'epoch': 4.73}
 75%|███████▍  | 3727/5000 [18:17:11<5:08:58, 14.56s/it] 75%|███████▍  | 3728/5000 [18:17:26<5:13:00, 14.76s/it]                                                        {'loss': 16.1389, 'grad_norm': 12.4375, 'learning_rate': 1.1672322263067781e-05, 'epoch': 4.73}
 75%|███████▍  | 3728/5000 [18:17:26<5:13:00, 14.76s/it] 75%|███████▍  | 3729/5000 [18:17:51<6:12:44, 17.60s/it]                                                        {'loss': 19.1467, 'grad_norm': 12.0625, 'learning_rate': 1.1655070090653131e-05, 'epoch': 4.74}
 75%|███████▍  | 3729/5000 [18:17:51<6:12:44, 17.60s/it] 75%|███████▍  | 3730/5000 [18:18:05<5:55:18, 16.79s/it]                                                        {'loss': 18.7331, 'grad_norm': 10.5, 'learning_rate': 1.1637828130103963e-05, 'epoch': 4.74}
 75%|███████▍  | 3730/5000 [18:18:05<5:55:18, 16.79s/it] 75%|███████▍  | 3731/5000 [18:18:20<5:40:56, 16.12s/it]                                                        {'loss': 17.9834, 'grad_norm': 21.0, 'learning_rate': 1.1620596388962477e-05, 'epoch': 4.74}
 75%|███████▍  | 3731/5000 [18:18:20<5:40:56, 16.12s/it] 75%|███████▍  | 3732/5000 [18:18:33<5:23:01, 15.28s/it]                                                        {'loss': 17.2255, 'grad_norm': 10.375, 'learning_rate': 1.1603374874766414e-05, 'epoch': 4.74}
 75%|███████▍  | 3732/5000 [18:18:33<5:23:01, 15.28s/it] 75%|███████▍  | 3733/5000 [18:18:51<5:40:27, 16.12s/it]                                                        {'loss': 17.2533, 'grad_norm': 11.125, 'learning_rate': 1.1586163595049075e-05, 'epoch': 4.74}
 75%|███████▍  | 3733/5000 [18:18:51<5:40:27, 16.12s/it] 75%|███████▍  | 3734/5000 [18:19:07<5:36:41, 15.96s/it]                                                        {'loss': 18.8329, 'grad_norm': 15.875, 'learning_rate': 1.1568962557339246e-05, 'epoch': 4.74}
 75%|███████▍  | 3734/5000 [18:19:07<5:36:41, 15.96s/it] 75%|███████▍  | 3735/5000 [18:19:29<6:16:41, 17.87s/it]                                                        {'loss': 18.9801, 'grad_norm': 10.8125, 'learning_rate': 1.1551771769161226e-05, 'epoch': 4.74}
 75%|███████▍  | 3735/5000 [18:19:29<6:16:41, 17.87s/it] 75%|███████▍  | 3736/5000 [18:19:50<6:33:07, 18.66s/it]                                                        {'loss': 19.2279, 'grad_norm': 60.0, 'learning_rate': 1.1534591238034881e-05, 'epoch': 4.74}
 75%|███████▍  | 3736/5000 [18:19:50<6:33:07, 18.66s/it] 75%|███████▍  | 3737/5000 [18:20:05<6:11:01, 17.63s/it]                                                        {'loss': 19.4925, 'grad_norm': 25.5, 'learning_rate': 1.1517420971475547e-05, 'epoch': 4.75}
 75%|███████▍  | 3737/5000 [18:20:05<6:11:01, 17.63s/it] 75%|███████▍  | 3738/5000 [18:20:22<6:04:12, 17.32s/it]                                                        {'loss': 18.2501, 'grad_norm': 11.5625, 'learning_rate': 1.1500260976994069e-05, 'epoch': 4.75}
 75%|███████▍  | 3738/5000 [18:20:22<6:04:12, 17.32s/it] 75%|███████▍  | 3739/5000 [18:20:36<5:46:20, 16.48s/it]                                                        {'loss': 18.4202, 'grad_norm': 12.9375, 'learning_rate': 1.1483111262096833e-05, 'epoch': 4.75}
 75%|███████▍  | 3739/5000 [18:20:36<5:46:20, 16.48s/it] 75%|███████▍  | 3740/5000 [18:20:58<6:20:30, 18.12s/it]                                                        {'loss': 17.849, 'grad_norm': 16.5, 'learning_rate': 1.1465971834285687e-05, 'epoch': 4.75}
 75%|███████▍  | 3740/5000 [18:20:58<6:20:30, 18.12s/it] 75%|███████▍  | 3741/5000 [18:21:14<6:05:56, 17.44s/it]                                                        {'loss': 17.1647, 'grad_norm': 13.6875, 'learning_rate': 1.1448842701058016e-05, 'epoch': 4.75}
 75%|███████▍  | 3741/5000 [18:21:14<6:05:56, 17.44s/it] 75%|███████▍  | 3742/5000 [18:21:27<5:37:32, 16.10s/it]                                                        {'loss': 20.1135, 'grad_norm': 24.375, 'learning_rate': 1.1431723869906673e-05, 'epoch': 4.75}
 75%|███████▍  | 3742/5000 [18:21:27<5:37:32, 16.10s/it] 75%|███████▍  | 3743/5000 [18:21:45<5:52:07, 16.81s/it]                                                        {'loss': 16.6272, 'grad_norm': 10.3125, 'learning_rate': 1.1414615348320024e-05, 'epoch': 4.75}
 75%|███████▍  | 3743/5000 [18:21:45<5:52:07, 16.81s/it] 75%|███████▍  | 3744/5000 [18:22:10<6:40:14, 19.12s/it]                                                        {'loss': 17.0203, 'grad_norm': 9.0, 'learning_rate': 1.1397517143781922e-05, 'epoch': 4.75}
 75%|███████▍  | 3744/5000 [18:22:10<6:40:14, 19.12s/it] 75%|███████▍  | 3745/5000 [18:22:25<6:16:50, 18.02s/it]                                                        {'loss': 18.0823, 'grad_norm': 13.9375, 'learning_rate': 1.1380429263771679e-05, 'epoch': 4.76}
 75%|███████▍  | 3745/5000 [18:22:25<6:16:50, 18.02s/it] 75%|███████▍  | 3746/5000 [18:22:42<6:09:53, 17.70s/it]                                                        {'loss': 18.893, 'grad_norm': 16.5, 'learning_rate': 1.1363351715764143e-05, 'epoch': 4.76}
 75%|███████▍  | 3746/5000 [18:22:42<6:09:53, 17.70s/it] 75%|███████▍  | 3747/5000 [18:23:05<6:40:51, 19.19s/it]                                                        {'loss': 17.1075, 'grad_norm': 12.125, 'learning_rate': 1.1346284507229603e-05, 'epoch': 4.76}
 75%|███████▍  | 3747/5000 [18:23:05<6:40:51, 19.19s/it] 75%|███████▍  | 3748/5000 [18:23:21<6:19:36, 18.19s/it]                                                        {'loss': 18.6425, 'grad_norm': 15.8125, 'learning_rate': 1.132922764563382e-05, 'epoch': 4.76}
 75%|███████▍  | 3748/5000 [18:23:21<6:19:36, 18.19s/it] 75%|███████▍  | 3749/5000 [18:23:34<5:49:33, 16.77s/it]                                                        {'loss': 18.3683, 'grad_norm': 29.375, 'learning_rate': 1.1312181138438073e-05, 'epoch': 4.76}
 75%|███████▍  | 3749/5000 [18:23:34<5:49:33, 16.77s/it] 75%|███████▌  | 3750/5000 [18:23:59<6:38:29, 19.13s/it]                                                        {'loss': 17.0965, 'grad_norm': 11.4375, 'learning_rate': 1.1295144993099068e-05, 'epoch': 4.76}
 75%|███████▌  | 3750/5000 [18:23:59<6:38:29, 19.13s/it] 75%|███████▌  | 3751/5000 [18:24:13<6:04:00, 17.49s/it]                                                        {'loss': 18.8198, 'grad_norm': 13.0625, 'learning_rate': 1.1278119217068981e-05, 'epoch': 4.76}
 75%|███████▌  | 3751/5000 [18:24:13<6:04:00, 17.49s/it] 75%|███████▌  | 3752/5000 [18:24:27<5:42:39, 16.47s/it]                                                        {'loss': 19.6588, 'grad_norm': 19.375, 'learning_rate': 1.1261103817795494e-05, 'epoch': 4.76}
 75%|███████▌  | 3752/5000 [18:24:27<5:42:39, 16.47s/it] 75%|███████▌  | 3753/5000 [18:24:42<5:32:15, 15.99s/it]                                                        {'loss': 17.4223, 'grad_norm': 12.8125, 'learning_rate': 1.1244098802721702e-05, 'epoch': 4.77}
 75%|███████▌  | 3753/5000 [18:24:42<5:32:15, 15.99s/it] 75%|███████▌  | 3754/5000 [18:24:59<5:42:54, 16.51s/it]                                                        {'loss': 16.9275, 'grad_norm': 8.5625, 'learning_rate': 1.122710417928617e-05, 'epoch': 4.77}
 75%|███████▌  | 3754/5000 [18:24:59<5:42:54, 16.51s/it] 75%|███████▌  | 3755/5000 [18:25:18<5:57:22, 17.22s/it]                                                        {'loss': 17.8469, 'grad_norm': 9.75, 'learning_rate': 1.1210119954922946e-05, 'epoch': 4.77}
 75%|███████▌  | 3755/5000 [18:25:18<5:57:22, 17.22s/it] 75%|███████▌  | 3756/5000 [18:25:32<5:35:41, 16.19s/it]                                                        {'loss': 20.1635, 'grad_norm': 15.75, 'learning_rate': 1.119314613706149e-05, 'epoch': 4.77}
 75%|███████▌  | 3756/5000 [18:25:32<5:35:41, 16.19s/it] 75%|███████▌  | 3757/5000 [18:25:49<5:40:16, 16.42s/it]                                                        {'loss': 17.1555, 'grad_norm': 14.5, 'learning_rate': 1.1176182733126725e-05, 'epoch': 4.77}
 75%|███████▌  | 3757/5000 [18:25:49<5:40:16, 16.42s/it] 75%|███████▌  | 3758/5000 [18:26:01<5:13:49, 15.16s/it]                                                        {'loss': 19.8497, 'grad_norm': 16.5, 'learning_rate': 1.1159229750539033e-05, 'epoch': 4.77}
 75%|███████▌  | 3758/5000 [18:26:01<5:13:49, 15.16s/it] 75%|███████▌  | 3759/5000 [18:26:19<5:32:00, 16.05s/it]                                                        {'loss': 17.3984, 'grad_norm': 10.1875, 'learning_rate': 1.1142287196714218e-05, 'epoch': 4.77}
 75%|███████▌  | 3759/5000 [18:26:19<5:32:00, 16.05s/it] 75%|███████▌  | 3760/5000 [18:26:37<5:43:15, 16.61s/it]                                                        {'loss': 17.2873, 'grad_norm': 12.5, 'learning_rate': 1.1125355079063521e-05, 'epoch': 4.77}
 75%|███████▌  | 3760/5000 [18:26:37<5:43:15, 16.61s/it] 75%|███████▌  | 3761/5000 [18:26:54<5:43:47, 16.65s/it]                                                        {'loss': 16.5263, 'grad_norm': 8.0, 'learning_rate': 1.1108433404993639e-05, 'epoch': 4.78}
 75%|███████▌  | 3761/5000 [18:26:54<5:43:47, 16.65s/it] 75%|███████▌  | 3762/5000 [18:27:13<6:00:07, 17.45s/it]                                                        {'loss': 17.1479, 'grad_norm': 11.8125, 'learning_rate': 1.1091522181906684e-05, 'epoch': 4.78}
 75%|███████▌  | 3762/5000 [18:27:13<6:00:07, 17.45s/it] 75%|███████▌  | 3763/5000 [18:27:28<5:46:18, 16.80s/it]                                                        {'loss': 18.4715, 'grad_norm': 13.125, 'learning_rate': 1.1074621417200186e-05, 'epoch': 4.78}
 75%|███████▌  | 3763/5000 [18:27:28<5:46:18, 16.80s/it] 75%|███████▌  | 3764/5000 [18:27:43<5:30:38, 16.05s/it]                                                        {'loss': 18.6372, 'grad_norm': 13.5625, 'learning_rate': 1.105773111826713e-05, 'epoch': 4.78}
 75%|███████▌  | 3764/5000 [18:27:43<5:30:38, 16.05s/it] 75%|███████▌  | 3765/5000 [18:27:57<5:17:04, 15.40s/it]                                                        {'loss': 19.1859, 'grad_norm': 16.875, 'learning_rate': 1.1040851292495893e-05, 'epoch': 4.78}
 75%|███████▌  | 3765/5000 [18:27:57<5:17:04, 15.40s/it] 75%|███████▌  | 3766/5000 [18:28:20<6:04:35, 17.73s/it]                                                        {'loss': 17.3106, 'grad_norm': 7.09375, 'learning_rate': 1.1023981947270304e-05, 'epoch': 4.78}
 75%|███████▌  | 3766/5000 [18:28:20<6:04:35, 17.73s/it] 75%|███████▌  | 3767/5000 [18:28:35<5:50:13, 17.04s/it]                                                        {'loss': 19.2622, 'grad_norm': 14.9375, 'learning_rate': 1.1007123089969564e-05, 'epoch': 4.78}
 75%|███████▌  | 3767/5000 [18:28:35<5:50:13, 17.04s/it] 75%|███████▌  | 3768/5000 [18:28:55<6:04:44, 17.76s/it]                                                        {'loss': 17.5695, 'grad_norm': 11.625, 'learning_rate': 1.0990274727968329e-05, 'epoch': 4.78}
 75%|███████▌  | 3768/5000 [18:28:55<6:04:44, 17.76s/it] 75%|███████▌  | 3769/5000 [18:29:10<5:51:45, 17.15s/it]                                                        {'loss': 17.472, 'grad_norm': 9.375, 'learning_rate': 1.097343686863664e-05, 'epoch': 4.79}
 75%|███████▌  | 3769/5000 [18:29:10<5:51:45, 17.15s/it] 75%|███████▌  | 3770/5000 [18:29:26<5:41:12, 16.64s/it]                                                        {'loss': 17.8388, 'grad_norm': 15.875, 'learning_rate': 1.095660951933994e-05, 'epoch': 4.79}
 75%|███████▌  | 3770/5000 [18:29:26<5:41:12, 16.64s/it] 75%|███████▌  | 3771/5000 [18:29:44<5:50:30, 17.11s/it]                                                        {'loss': 18.4823, 'grad_norm': 10.8125, 'learning_rate': 1.0939792687439095e-05, 'epoch': 4.79}
 75%|███████▌  | 3771/5000 [18:29:44<5:50:30, 17.11s/it] 75%|███████▌  | 3772/5000 [18:30:09<6:40:07, 19.55s/it]                                                        {'loss': 19.6345, 'grad_norm': 15.6875, 'learning_rate': 1.0922986380290351e-05, 'epoch': 4.79}
 75%|███████▌  | 3772/5000 [18:30:09<6:40:07, 19.55s/it] 75%|███████▌  | 3773/5000 [18:30:25<6:13:56, 18.29s/it]                                                        {'loss': 19.3316, 'grad_norm': 15.625, 'learning_rate': 1.0906190605245356e-05, 'epoch': 4.79}
 75%|███████▌  | 3773/5000 [18:30:25<6:13:56, 18.29s/it] 75%|███████▌  | 3774/5000 [18:30:46<6:34:28, 19.31s/it]                                                        {'loss': 20.2815, 'grad_norm': 20.0, 'learning_rate': 1.0889405369651161e-05, 'epoch': 4.79}
 75%|███████▌  | 3774/5000 [18:30:46<6:34:28, 19.31s/it] 76%|███████▌  | 3775/5000 [18:31:01<6:05:12, 17.89s/it]                                                        {'loss': 18.0546, 'grad_norm': 11.0625, 'learning_rate': 1.0872630680850196e-05, 'epoch': 4.79}
 76%|███████▌  | 3775/5000 [18:31:01<6:05:12, 17.89s/it] 76%|███████▌  | 3776/5000 [18:31:16<5:46:33, 16.99s/it]                                                        {'loss': 19.4306, 'grad_norm': 122.5, 'learning_rate': 1.085586654618027e-05, 'epoch': 4.79}
 76%|███████▌  | 3776/5000 [18:31:16<5:46:33, 16.99s/it] 76%|███████▌  | 3777/5000 [18:31:28<5:18:22, 15.62s/it]                                                        {'loss': 19.3281, 'grad_norm': 14.4375, 'learning_rate': 1.0839112972974597e-05, 'epoch': 4.8}
 76%|███████▌  | 3777/5000 [18:31:28<5:18:22, 15.62s/it] 76%|███████▌  | 3778/5000 [18:31:53<6:12:09, 18.27s/it]                                                        {'loss': 18.1297, 'grad_norm': 11.125, 'learning_rate': 1.0822369968561762e-05, 'epoch': 4.8}
 76%|███████▌  | 3778/5000 [18:31:53<6:12:09, 18.27s/it] 76%|███████▌  | 3779/5000 [18:32:06<5:42:09, 16.81s/it]                                                        {'loss': 19.011, 'grad_norm': 27.0, 'learning_rate': 1.0805637540265711e-05, 'epoch': 4.8}
 76%|███████▌  | 3779/5000 [18:32:06<5:42:09, 16.81s/it] 76%|███████▌  | 3780/5000 [18:32:21<5:32:59, 16.38s/it]                                                        {'loss': 16.7036, 'grad_norm': 9.3125, 'learning_rate': 1.0788915695405775e-05, 'epoch': 4.8}
 76%|███████▌  | 3780/5000 [18:32:21<5:32:59, 16.38s/it] 76%|███████▌  | 3781/5000 [18:32:34<5:09:43, 15.24s/it]                                                        {'loss': 20.2757, 'grad_norm': 18.0, 'learning_rate': 1.0772204441296674e-05, 'epoch': 4.8}
 76%|███████▌  | 3781/5000 [18:32:34<5:09:43, 15.24s/it] 76%|███████▌  | 3782/5000 [18:32:50<5:14:02, 15.47s/it]                                                        {'loss': 18.2076, 'grad_norm': 15.875, 'learning_rate': 1.0755503785248467e-05, 'epoch': 4.8}
 76%|███████▌  | 3782/5000 [18:32:50<5:14:02, 15.47s/it] 76%|███████▌  | 3783/5000 [18:33:13<5:57:19, 17.62s/it]                                                        {'loss': 17.294, 'grad_norm': 12.3125, 'learning_rate': 1.0738813734566579e-05, 'epoch': 4.8}
 76%|███████▌  | 3783/5000 [18:33:13<5:57:19, 17.62s/it] 76%|███████▌  | 3784/5000 [18:33:33<6:13:53, 18.45s/it]                                                        {'loss': 17.6242, 'grad_norm': 13.6875, 'learning_rate': 1.072213429655183e-05, 'epoch': 4.81}
 76%|███████▌  | 3784/5000 [18:33:33<6:13:53, 18.45s/it] 76%|███████▌  | 3785/5000 [18:34:00<7:05:33, 21.01s/it]                                                        {'loss': 17.3032, 'grad_norm': 17.25, 'learning_rate': 1.0705465478500359e-05, 'epoch': 4.81}
 76%|███████▌  | 3785/5000 [18:34:00<7:05:33, 21.01s/it] 76%|███████▌  | 3786/5000 [18:34:19<6:53:08, 20.42s/it]                                                        {'loss': 18.0298, 'grad_norm': 25.0, 'learning_rate': 1.068880728770366e-05, 'epoch': 4.81}
 76%|███████▌  | 3786/5000 [18:34:19<6:53:08, 20.42s/it] 76%|███████▌  | 3787/5000 [18:34:39<6:47:00, 20.13s/it]                                                        {'loss': 17.4341, 'grad_norm': 280.0, 'learning_rate': 1.0672159731448617e-05, 'epoch': 4.81}
 76%|███████▌  | 3787/5000 [18:34:39<6:47:00, 20.13s/it] 76%|███████▌  | 3788/5000 [18:34:56<6:28:34, 19.24s/it]                                                        {'loss': 18.0291, 'grad_norm': 36.25, 'learning_rate': 1.0655522817017417e-05, 'epoch': 4.81}
 76%|███████▌  | 3788/5000 [18:34:56<6:28:34, 19.24s/it] 76%|███████▌  | 3789/5000 [18:35:11<6:02:17, 17.95s/it]                                                        {'loss': 18.0187, 'grad_norm': 20.0, 'learning_rate': 1.0638896551687627e-05, 'epoch': 4.81}
 76%|███████▌  | 3789/5000 [18:35:11<6:02:17, 17.95s/it] 76%|███████▌  | 3790/5000 [18:35:38<6:58:12, 20.74s/it]                                                        {'loss': 15.8287, 'grad_norm': 13.625, 'learning_rate': 1.062228094273212e-05, 'epoch': 4.81}
 76%|███████▌  | 3790/5000 [18:35:38<6:58:12, 20.74s/it] 76%|███████▌  | 3791/5000 [18:35:53<6:26:32, 19.18s/it]                                                        {'loss': 18.1037, 'grad_norm': 10.75, 'learning_rate': 1.0605675997419152e-05, 'epoch': 4.81}
 76%|███████▌  | 3791/5000 [18:35:53<6:26:32, 19.18s/it] 76%|███████▌  | 3792/5000 [18:36:08<5:59:02, 17.83s/it]                                                        {'loss': 17.5298, 'grad_norm': 10.0, 'learning_rate': 1.058908172301227e-05, 'epoch': 4.82}
 76%|███████▌  | 3792/5000 [18:36:08<5:59:02, 17.83s/it] 76%|███████▌  | 3793/5000 [18:36:28<6:10:32, 18.42s/it]                                                        {'loss': 17.494, 'grad_norm': 18.0, 'learning_rate': 1.0572498126770394e-05, 'epoch': 4.82}
 76%|███████▌  | 3793/5000 [18:36:28<6:10:32, 18.42s/it] 76%|███████▌  | 3794/5000 [18:36:42<5:42:38, 17.05s/it]                                                        {'loss': 18.3921, 'grad_norm': 19.25, 'learning_rate': 1.0555925215947736e-05, 'epoch': 4.82}
 76%|███████▌  | 3794/5000 [18:36:42<5:42:38, 17.05s/it] 76%|███████▌  | 3795/5000 [18:36:55<5:21:28, 16.01s/it]                                                        {'loss': 17.5998, 'grad_norm': 12.8125, 'learning_rate': 1.0539362997793845e-05, 'epoch': 4.82}
 76%|███████▌  | 3795/5000 [18:36:55<5:21:28, 16.01s/it] 76%|███████▌  | 3796/5000 [18:37:20<6:13:22, 18.61s/it]                                                        {'loss': 18.846, 'grad_norm': 14.0, 'learning_rate': 1.0522811479553623e-05, 'epoch': 4.82}
 76%|███████▌  | 3796/5000 [18:37:20<6:13:22, 18.61s/it] 76%|███████▌  | 3797/5000 [18:37:37<6:01:15, 18.02s/it]                                                        {'loss': 43.1181, 'grad_norm': 1496.0, 'learning_rate': 1.0506270668467248e-05, 'epoch': 4.82}
 76%|███████▌  | 3797/5000 [18:37:37<6:01:15, 18.02s/it] 76%|███████▌  | 3798/5000 [18:37:56<6:08:30, 18.39s/it]                                                        {'loss': 17.1262, 'grad_norm': 22.75, 'learning_rate': 1.0489740571770235e-05, 'epoch': 4.82}
 76%|███████▌  | 3798/5000 [18:37:56<6:08:30, 18.39s/it] 76%|███████▌  | 3799/5000 [18:38:13<5:58:17, 17.90s/it]                                                        {'loss': 17.8129, 'grad_norm': 14.4375, 'learning_rate': 1.0473221196693397e-05, 'epoch': 4.82}
 76%|███████▌  | 3799/5000 [18:38:13<5:58:17, 17.90s/it] 76%|███████▌  | 3800/5000 [18:38:29<5:50:45, 17.54s/it]                                                        {'loss': 15.5653, 'grad_norm': 10.375, 'learning_rate': 1.0456712550462898e-05, 'epoch': 4.83}
 76%|███████▌  | 3800/5000 [18:38:29<5:50:45, 17.54s/it] 76%|███████▌  | 3801/5000 [18:38:46<5:44:59, 17.26s/it]                                                        {'loss': 17.9548, 'grad_norm': 14.625, 'learning_rate': 1.0440214640300167e-05, 'epoch': 4.83}
 76%|███████▌  | 3801/5000 [18:38:46<5:44:59, 17.26s/it] 76%|███████▌  | 3802/5000 [18:39:05<5:57:28, 17.90s/it]                                                        {'loss': 18.1248, 'grad_norm': 14.0, 'learning_rate': 1.042372747342194e-05, 'epoch': 4.83}
 76%|███████▌  | 3802/5000 [18:39:05<5:57:28, 17.90s/it] 76%|███████▌  | 3803/5000 [18:39:24<6:01:03, 18.10s/it]                                                        {'loss': 17.471, 'grad_norm': 13.3125, 'learning_rate': 1.0407251057040282e-05, 'epoch': 4.83}
 76%|███████▌  | 3803/5000 [18:39:24<6:01:03, 18.10s/it] 76%|███████▌  | 3804/5000 [18:39:49<6:40:17, 20.08s/it]                                                        {'loss': 17.2999, 'grad_norm': 11.875, 'learning_rate': 1.0390785398362529e-05, 'epoch': 4.83}
 76%|███████▌  | 3804/5000 [18:39:49<6:40:17, 20.08s/it] 76%|███████▌  | 3805/5000 [18:40:02<5:59:03, 18.03s/it]                                                        {'loss': 19.0148, 'grad_norm': 18.125, 'learning_rate': 1.0374330504591307e-05, 'epoch': 4.83}
 76%|███████▌  | 3805/5000 [18:40:02<5:59:03, 18.03s/it] 76%|███████▌  | 3806/5000 [18:40:17<5:40:46, 17.12s/it]                                                        {'loss': 17.8069, 'grad_norm': 28.625, 'learning_rate': 1.0357886382924572e-05, 'epoch': 4.83}
 76%|███████▌  | 3806/5000 [18:40:17<5:40:46, 17.12s/it] 76%|███████▌  | 3807/5000 [18:40:30<5:14:54, 15.84s/it]                                                        {'loss': 18.1943, 'grad_norm': 11.4375, 'learning_rate': 1.0341453040555525e-05, 'epoch': 4.83}
 76%|███████▌  | 3807/5000 [18:40:30<5:14:54, 15.84s/it] 76%|███████▌  | 3808/5000 [18:40:44<5:06:58, 15.45s/it]                                                        {'loss': 18.6714, 'grad_norm': 11.0, 'learning_rate': 1.0325030484672663e-05, 'epoch': 4.84}
 76%|███████▌  | 3808/5000 [18:40:44<5:06:58, 15.45s/it] 76%|███████▌  | 3809/5000 [18:40:57<4:51:42, 14.70s/it]                                                        {'loss': 19.9143, 'grad_norm': 18.875, 'learning_rate': 1.0308618722459785e-05, 'epoch': 4.84}
 76%|███████▌  | 3809/5000 [18:40:57<4:51:42, 14.70s/it] 76%|███████▌  | 3810/5000 [18:41:11<4:48:32, 14.55s/it]                                                        {'loss': 18.1703, 'grad_norm': 12.875, 'learning_rate': 1.0292217761095948e-05, 'epoch': 4.84}
 76%|███████▌  | 3810/5000 [18:41:11<4:48:32, 14.55s/it] 76%|███████▌  | 3811/5000 [18:41:28<4:59:09, 15.10s/it]                                                        {'loss': 16.661, 'grad_norm': 6.8125, 'learning_rate': 1.0275827607755474e-05, 'epoch': 4.84}
 76%|███████▌  | 3811/5000 [18:41:28<4:59:09, 15.10s/it] 76%|███████▌  | 3812/5000 [18:41:54<6:04:24, 18.40s/it]                                                        {'loss': 16.6788, 'grad_norm': 9.875, 'learning_rate': 1.0259448269608002e-05, 'epoch': 4.84}
 76%|███████▌  | 3812/5000 [18:41:54<6:04:24, 18.40s/it] 76%|███████▋  | 3813/5000 [18:42:11<5:54:13, 17.91s/it]                                                        {'loss': 17.1725, 'grad_norm': 16.375, 'learning_rate': 1.0243079753818384e-05, 'epoch': 4.84}
 76%|███████▋  | 3813/5000 [18:42:11<5:54:13, 17.91s/it] 76%|███████▋  | 3814/5000 [18:42:28<5:49:12, 17.67s/it]                                                        {'loss': 18.3454, 'grad_norm': 13.4375, 'learning_rate': 1.0226722067546783e-05, 'epoch': 4.84}
 76%|███████▋  | 3814/5000 [18:42:28<5:49:12, 17.67s/it] 76%|███████▋  | 3815/5000 [18:42:43<5:31:46, 16.80s/it]                                                        {'loss': 17.4414, 'grad_norm': 12.5, 'learning_rate': 1.021037521794859e-05, 'epoch': 4.84}
 76%|███████▋  | 3815/5000 [18:42:43<5:31:46, 16.80s/it] 76%|███████▋  | 3816/5000 [18:42:57<5:14:53, 15.96s/it]                                                        {'loss': 18.4547, 'grad_norm': 11.25, 'learning_rate': 1.0194039212174499e-05, 'epoch': 4.85}
 76%|███████▋  | 3816/5000 [18:42:57<5:14:53, 15.96s/it] 76%|███████▋  | 3817/5000 [18:43:11<5:03:56, 15.42s/it]                                                        {'loss': 18.2445, 'grad_norm': 11.5625, 'learning_rate': 1.0177714057370415e-05, 'epoch': 4.85}
 76%|███████▋  | 3817/5000 [18:43:11<5:03:56, 15.42s/it] 76%|███████▋  | 3818/5000 [18:43:26<5:03:59, 15.43s/it]                                                        {'loss': 17.5993, 'grad_norm': 14.0625, 'learning_rate': 1.0161399760677504e-05, 'epoch': 4.85}
 76%|███████▋  | 3818/5000 [18:43:26<5:03:59, 15.43s/it] 76%|███████▋  | 3819/5000 [18:43:42<5:07:57, 15.65s/it]                                                        {'loss': 17.1239, 'grad_norm': 10.75, 'learning_rate': 1.0145096329232223e-05, 'epoch': 4.85}
 76%|███████▋  | 3819/5000 [18:43:42<5:07:57, 15.65s/it] 76%|███████▋  | 3820/5000 [18:43:57<5:01:16, 15.32s/it]                                                        {'loss': 18.1044, 'grad_norm': 12.625, 'learning_rate': 1.0128803770166229e-05, 'epoch': 4.85}
 76%|███████▋  | 3820/5000 [18:43:57<5:01:16, 15.32s/it] 76%|███████▋  | 3821/5000 [18:44:17<5:29:36, 16.77s/it]                                                        {'loss': 17.8002, 'grad_norm': 11.1875, 'learning_rate': 1.0112522090606432e-05, 'epoch': 4.85}
 76%|███████▋  | 3821/5000 [18:44:17<5:29:36, 16.77s/it] 76%|███████▋  | 3822/5000 [18:44:44<6:28:31, 19.79s/it]                                                        {'loss': 17.0395, 'grad_norm': 10.8125, 'learning_rate': 1.0096251297675012e-05, 'epoch': 4.85}
 76%|███████▋  | 3822/5000 [18:44:44<6:28:31, 19.79s/it] 76%|███████▋  | 3823/5000 [18:44:59<5:58:22, 18.27s/it]                                                        {'loss': 18.984, 'grad_norm': 12.25, 'learning_rate': 1.0079991398489361e-05, 'epoch': 4.85}
 76%|███████▋  | 3823/5000 [18:44:59<5:58:22, 18.27s/it] 76%|███████▋  | 3824/5000 [18:45:16<5:50:30, 17.88s/it]                                                        {'loss': 17.518, 'grad_norm': 14.8125, 'learning_rate': 1.0063742400162096e-05, 'epoch': 4.86}
 76%|███████▋  | 3824/5000 [18:45:16<5:50:30, 17.88s/it] 76%|███████▋  | 3825/5000 [18:45:29<5:26:04, 16.65s/it]                                                        {'loss': 18.7681, 'grad_norm': 11.625, 'learning_rate': 1.0047504309801104e-05, 'epoch': 4.86}
 76%|███████▋  | 3825/5000 [18:45:29<5:26:04, 16.65s/it] 77%|███████▋  | 3826/5000 [18:45:46<5:23:22, 16.53s/it]                                                        {'loss': 19.409, 'grad_norm': 15.3125, 'learning_rate': 1.0031277134509466e-05, 'epoch': 4.86}
 77%|███████▋  | 3826/5000 [18:45:46<5:23:22, 16.53s/it] 77%|███████▋  | 3827/5000 [18:46:07<5:48:47, 17.84s/it]                                                        {'loss': 16.9462, 'grad_norm': 5.65625, 'learning_rate': 1.0015060881385489e-05, 'epoch': 4.86}
 77%|███████▋  | 3827/5000 [18:46:07<5:48:47, 17.84s/it] 77%|███████▋  | 3828/5000 [18:46:29<6:15:36, 19.23s/it]                                                        {'loss': 17.814, 'grad_norm': 12.0625, 'learning_rate': 9.998855557522733e-06, 'epoch': 4.86}
 77%|███████▋  | 3828/5000 [18:46:29<6:15:36, 19.23s/it] 77%|███████▋  | 3829/5000 [18:46:45<5:57:22, 18.31s/it]                                                        {'loss': 18.8154, 'grad_norm': 14.6875, 'learning_rate': 9.982661170009951e-06, 'epoch': 4.86}
 77%|███████▋  | 3829/5000 [18:46:45<5:57:22, 18.31s/it] 77%|███████▋  | 3830/5000 [18:46:59<5:32:18, 17.04s/it]                                                        {'loss': 18.1332, 'grad_norm': 12.3125, 'learning_rate': 9.966477725931114e-06, 'epoch': 4.86}
 77%|███████▋  | 3830/5000 [18:46:59<5:32:18, 17.04s/it] 77%|███████▋  | 3831/5000 [18:47:14<5:20:51, 16.47s/it]                                                        {'loss': 19.2696, 'grad_norm': 15.875, 'learning_rate': 9.950305232365402e-06, 'epoch': 4.86}
 77%|███████▋  | 3831/5000 [18:47:14<5:20:51, 16.47s/it] 77%|███████▋  | 3832/5000 [18:47:35<5:47:30, 17.85s/it]                                                        {'loss': 17.4058, 'grad_norm': 17.625, 'learning_rate': 9.934143696387233e-06, 'epoch': 4.87}
 77%|███████▋  | 3832/5000 [18:47:35<5:47:30, 17.85s/it] 77%|███████▋  | 3833/5000 [18:48:03<6:43:47, 20.76s/it]                                                        {'loss': 17.0779, 'grad_norm': 10.9375, 'learning_rate': 9.917993125066203e-06, 'epoch': 4.87}
 77%|███████▋  | 3833/5000 [18:48:03<6:43:47, 20.76s/it] 77%|███████▋  | 3834/5000 [18:48:20<6:19:24, 19.52s/it]                                                        {'loss': 17.6213, 'grad_norm': 8.9375, 'learning_rate': 9.901853525467108e-06, 'epoch': 4.87}
 77%|███████▋  | 3834/5000 [18:48:20<6:19:24, 19.52s/it] 77%|███████▋  | 3835/5000 [18:48:37<6:05:38, 18.83s/it]                                                        {'loss': 16.631, 'grad_norm': 13.875, 'learning_rate': 9.885724904649978e-06, 'epoch': 4.87}
 77%|███████▋  | 3835/5000 [18:48:37<6:05:38, 18.83s/it] 77%|███████▋  | 3836/5000 [18:48:52<5:45:26, 17.81s/it]                                                        {'loss': 17.3544, 'grad_norm': 8.0625, 'learning_rate': 9.869607269670003e-06, 'epoch': 4.87}
 77%|███████▋  | 3836/5000 [18:48:52<5:45:26, 17.81s/it] 77%|███████▋  | 3837/5000 [18:49:14<6:05:33, 18.86s/it]                                                        {'loss': 17.0163, 'grad_norm': 9.9375, 'learning_rate': 9.853500627577598e-06, 'epoch': 4.87}
 77%|███████▋  | 3837/5000 [18:49:14<6:05:33, 18.86s/it] 77%|███████▋  | 3838/5000 [18:49:30<5:53:21, 18.25s/it]                                                        {'loss': 17.444, 'grad_norm': 14.125, 'learning_rate': 9.837404985418342e-06, 'epoch': 4.87}
 77%|███████▋  | 3838/5000 [18:49:30<5:53:21, 18.25s/it] 77%|███████▋  | 3839/5000 [18:49:50<5:58:37, 18.53s/it]                                                        {'loss': 18.4463, 'grad_norm': 11.875, 'learning_rate': 9.821320350233033e-06, 'epoch': 4.87}
 77%|███████▋  | 3839/5000 [18:49:50<5:58:37, 18.53s/it] 77%|███████▋  | 3840/5000 [18:50:09<6:01:02, 18.67s/it]                                                        {'loss': 18.8706, 'grad_norm': 23.875, 'learning_rate': 9.805246729057612e-06, 'epoch': 4.88}
 77%|███████▋  | 3840/5000 [18:50:09<6:01:02, 18.67s/it] 77%|███████▋  | 3841/5000 [18:50:29<6:09:21, 19.12s/it]                                                        {'loss': 19.4749, 'grad_norm': 13.75, 'learning_rate': 9.789184128923257e-06, 'epoch': 4.88}
 77%|███████▋  | 3841/5000 [18:50:29<6:09:21, 19.12s/it] 77%|███████▋  | 3842/5000 [18:50:41<5:31:04, 17.15s/it]                                                        {'loss': 18.7396, 'grad_norm': 19.25, 'learning_rate': 9.773132556856277e-06, 'epoch': 4.88}
 77%|███████▋  | 3842/5000 [18:50:41<5:31:04, 17.15s/it] 77%|███████▋  | 3843/5000 [18:50:57<5:22:53, 16.74s/it]                                                        {'loss': 18.0376, 'grad_norm': 15.75, 'learning_rate': 9.757092019878166e-06, 'epoch': 4.88}
 77%|███████▋  | 3843/5000 [18:50:57<5:22:53, 16.74s/it] 77%|███████▋  | 3844/5000 [18:51:11<5:06:47, 15.92s/it]                                                        {'loss': 19.1493, 'grad_norm': 13.6875, 'learning_rate': 9.741062525005625e-06, 'epoch': 4.88}
 77%|███████▋  | 3844/5000 [18:51:11<5:06:47, 15.92s/it] 77%|███████▋  | 3845/5000 [18:51:36<6:00:00, 18.70s/it]                                                        {'loss': 16.8979, 'grad_norm': 12.25, 'learning_rate': 9.725044079250484e-06, 'epoch': 4.88}
 77%|███████▋  | 3845/5000 [18:51:36<6:00:00, 18.70s/it] 77%|███████▋  | 3846/5000 [18:51:52<5:39:44, 17.66s/it]                                                        {'loss': 18.5579, 'grad_norm': 12.625, 'learning_rate': 9.709036689619746e-06, 'epoch': 4.88}
 77%|███████▋  | 3846/5000 [18:51:52<5:39:44, 17.66s/it] 77%|███████▋  | 3847/5000 [18:52:09<5:39:56, 17.69s/it]                                                        {'loss': 17.1013, 'grad_norm': 21.75, 'learning_rate': 9.693040363115613e-06, 'epoch': 4.89}
 77%|███████▋  | 3847/5000 [18:52:09<5:39:56, 17.69s/it] 77%|███████▋  | 3848/5000 [18:52:24<5:22:54, 16.82s/it]                                                        {'loss': 18.4647, 'grad_norm': 8.75, 'learning_rate': 9.677055106735402e-06, 'epoch': 4.89}
 77%|███████▋  | 3848/5000 [18:52:24<5:22:54, 16.82s/it] 77%|███████▋  | 3849/5000 [18:52:40<5:20:06, 16.69s/it]                                                        {'loss': 16.6235, 'grad_norm': 11.625, 'learning_rate': 9.661080927471616e-06, 'epoch': 4.89}
 77%|███████▋  | 3849/5000 [18:52:40<5:20:06, 16.69s/it] 77%|███████▋  | 3850/5000 [18:52:59<5:28:40, 17.15s/it]                                                        {'loss': 18.3875, 'grad_norm': 14.625, 'learning_rate': 9.645117832311886e-06, 'epoch': 4.89}
 77%|███████▋  | 3850/5000 [18:52:59<5:28:40, 17.15s/it] 77%|███████▋  | 3851/5000 [18:53:20<5:52:34, 18.41s/it]                                                        {'loss': 18.7902, 'grad_norm': 12.8125, 'learning_rate': 9.629165828239038e-06, 'epoch': 4.89}
 77%|███████▋  | 3851/5000 [18:53:20<5:52:34, 18.41s/it] 77%|███████▋  | 3852/5000 [18:53:38<5:47:34, 18.17s/it]                                                        {'loss': 17.4631, 'grad_norm': 11.5, 'learning_rate': 9.613224922231013e-06, 'epoch': 4.89}
 77%|███████▋  | 3852/5000 [18:53:38<5:47:34, 18.17s/it] 77%|███████▋  | 3853/5000 [18:53:53<5:32:16, 17.38s/it]                                                        {'loss': 19.6728, 'grad_norm': 23.0, 'learning_rate': 9.597295121260884e-06, 'epoch': 4.89}
 77%|███████▋  | 3853/5000 [18:53:53<5:32:16, 17.38s/it] 77%|███████▋  | 3854/5000 [18:54:13<5:43:17, 17.97s/it]                                                        {'loss': 18.1806, 'grad_norm': 15.625, 'learning_rate': 9.581376432296918e-06, 'epoch': 4.89}
 77%|███████▋  | 3854/5000 [18:54:13<5:43:17, 17.97s/it] 77%|███████▋  | 3855/5000 [18:54:28<5:31:23, 17.37s/it]                                                        {'loss': 18.3534, 'grad_norm': 13.3125, 'learning_rate': 9.565468862302476e-06, 'epoch': 4.9}
 77%|███████▋  | 3855/5000 [18:54:28<5:31:23, 17.37s/it] 77%|███████▋  | 3856/5000 [18:54:42<5:08:55, 16.20s/it]                                                        {'loss': 18.242, 'grad_norm': 16.5, 'learning_rate': 9.549572418236058e-06, 'epoch': 4.9}
 77%|███████▋  | 3856/5000 [18:54:42<5:08:55, 16.20s/it] 77%|███████▋  | 3857/5000 [18:54:57<5:02:57, 15.90s/it]                                                        {'loss': 17.5583, 'grad_norm': 10.1875, 'learning_rate': 9.533687107051332e-06, 'epoch': 4.9}
 77%|███████▋  | 3857/5000 [18:54:57<5:02:57, 15.90s/it] 77%|███████▋  | 3858/5000 [18:55:11<4:50:17, 15.25s/it]                                                        {'loss': 19.6353, 'grad_norm': 15.875, 'learning_rate': 9.517812935697058e-06, 'epoch': 4.9}
 77%|███████▋  | 3858/5000 [18:55:11<4:50:17, 15.25s/it] 77%|███████▋  | 3859/5000 [18:55:26<4:51:01, 15.30s/it]                                                        {'loss': 17.3904, 'grad_norm': 12.4375, 'learning_rate': 9.501949911117133e-06, 'epoch': 4.9}
 77%|███████▋  | 3859/5000 [18:55:26<4:51:01, 15.30s/it] 77%|███████▋  | 3860/5000 [18:55:53<5:53:31, 18.61s/it]                                                        {'loss': 16.6792, 'grad_norm': 15.6875, 'learning_rate': 9.486098040250603e-06, 'epoch': 4.9}
 77%|███████▋  | 3860/5000 [18:55:53<5:53:31, 18.61s/it] 77%|███████▋  | 3861/5000 [18:56:08<5:37:18, 17.77s/it]                                                        {'loss': 17.9154, 'grad_norm': 15.75, 'learning_rate': 9.470257330031589e-06, 'epoch': 4.9}
 77%|███████▋  | 3861/5000 [18:56:08<5:37:18, 17.77s/it] 77%|███████▋  | 3862/5000 [18:56:22<5:13:11, 16.51s/it]                                                        {'loss': 17.4089, 'grad_norm': 11.0625, 'learning_rate': 9.454427787389384e-06, 'epoch': 4.9}
 77%|███████▋  | 3862/5000 [18:56:22<5:13:11, 16.51s/it] 77%|███████▋  | 3863/5000 [18:56:42<5:29:40, 17.40s/it]                                                        {'loss': 18.2167, 'grad_norm': 11.875, 'learning_rate': 9.438609419248338e-06, 'epoch': 4.91}
 77%|███████▋  | 3863/5000 [18:56:42<5:29:40, 17.40s/it] 77%|███████▋  | 3864/5000 [18:56:54<5:03:40, 16.04s/it]                                                        {'loss': 19.9948, 'grad_norm': 20.125, 'learning_rate': 9.422802232527974e-06, 'epoch': 4.91}
 77%|███████▋  | 3864/5000 [18:56:54<5:03:40, 16.04s/it] 77%|███████▋  | 3865/5000 [18:57:12<5:10:05, 16.39s/it]                                                        {'loss': 19.2749, 'grad_norm': 52.5, 'learning_rate': 9.407006234142873e-06, 'epoch': 4.91}
 77%|███████▋  | 3865/5000 [18:57:12<5:10:05, 16.39s/it] 77%|███████▋  | 3866/5000 [18:57:33<5:40:11, 18.00s/it]                                                        {'loss': 17.0761, 'grad_norm': 11.25, 'learning_rate': 9.391221431002735e-06, 'epoch': 4.91}
 77%|███████▋  | 3866/5000 [18:57:33<5:40:11, 18.00s/it] 77%|███████▋  | 3867/5000 [18:57:47<5:17:31, 16.82s/it]                                                        {'loss': 17.916, 'grad_norm': 9.875, 'learning_rate': 9.375447830012391e-06, 'epoch': 4.91}
 77%|███████▋  | 3867/5000 [18:57:47<5:17:31, 16.82s/it] 77%|███████▋  | 3868/5000 [18:58:07<5:35:16, 17.77s/it]                                                        {'loss': 17.9544, 'grad_norm': 17.125, 'learning_rate': 9.359685438071734e-06, 'epoch': 4.91}
 77%|███████▋  | 3868/5000 [18:58:07<5:35:16, 17.77s/it] 77%|███████▋  | 3869/5000 [18:58:23<5:22:47, 17.12s/it]                                                        {'loss': 18.4802, 'grad_norm': 12.625, 'learning_rate': 9.34393426207577e-06, 'epoch': 4.91}
 77%|███████▋  | 3869/5000 [18:58:23<5:22:47, 17.12s/it] 77%|███████▋  | 3870/5000 [18:58:39<5:14:59, 16.73s/it]                                                        {'loss': 18.5914, 'grad_norm': 14.5, 'learning_rate': 9.328194308914608e-06, 'epoch': 4.91}
 77%|███████▋  | 3870/5000 [18:58:39<5:14:59, 16.73s/it] 77%|███████▋  | 3871/5000 [18:58:56<5:16:55, 16.84s/it]                                                        {'loss': 19.064, 'grad_norm': 22.375, 'learning_rate': 9.312465585473432e-06, 'epoch': 4.92}
 77%|███████▋  | 3871/5000 [18:58:56<5:16:55, 16.84s/it] 77%|███████▋  | 3872/5000 [18:59:13<5:18:24, 16.94s/it]                                                        {'loss': 17.258, 'grad_norm': 13.5, 'learning_rate': 9.296748098632513e-06, 'epoch': 4.92}
 77%|███████▋  | 3872/5000 [18:59:13<5:18:24, 16.94s/it] 77%|███████▋  | 3873/5000 [18:59:26<4:53:16, 15.61s/it]                                                        {'loss': 19.6104, 'grad_norm': 18.125, 'learning_rate': 9.28104185526723e-06, 'epoch': 4.92}
 77%|███████▋  | 3873/5000 [18:59:26<4:53:16, 15.61s/it] 77%|███████▋  | 3874/5000 [18:59:57<6:23:35, 20.44s/it]                                                        {'loss': 18.2782, 'grad_norm': 18.5, 'learning_rate': 9.265346862248019e-06, 'epoch': 4.92}
 77%|███████▋  | 3874/5000 [18:59:57<6:23:35, 20.44s/it] 78%|███████▊  | 3875/5000 [19:00:12<5:52:57, 18.82s/it]                                                        {'loss': 17.7384, 'grad_norm': 11.25, 'learning_rate': 9.249663126440394e-06, 'epoch': 4.92}
 78%|███████▊  | 3875/5000 [19:00:12<5:52:57, 18.82s/it] 78%|███████▊  | 3876/5000 [19:00:30<5:47:26, 18.55s/it]                                                        {'loss': 16.7995, 'grad_norm': 10.0625, 'learning_rate': 9.233990654704971e-06, 'epoch': 4.92}
 78%|███████▊  | 3876/5000 [19:00:30<5:47:26, 18.55s/it] 78%|███████▊  | 3877/5000 [19:00:45<5:24:30, 17.34s/it]                                                        {'loss': 17.8448, 'grad_norm': 10.625, 'learning_rate': 9.218329453897415e-06, 'epoch': 4.92}
 78%|███████▊  | 3877/5000 [19:00:45<5:24:30, 17.34s/it] 78%|███████▊  | 3878/5000 [19:00:58<5:00:20, 16.06s/it]                                                        {'loss': 19.0375, 'grad_norm': 125.5, 'learning_rate': 9.202679530868452e-06, 'epoch': 4.92}
 78%|███████▊  | 3878/5000 [19:00:58<5:00:20, 16.06s/it] 78%|███████▊  | 3879/5000 [19:01:23<5:50:54, 18.78s/it]                                                        {'loss': 19.3893, 'grad_norm': 20.25, 'learning_rate': 9.187040892463915e-06, 'epoch': 4.93}
 78%|███████▊  | 3879/5000 [19:01:23<5:50:54, 18.78s/it] 78%|███████▊  | 3880/5000 [19:01:42<5:52:35, 18.89s/it]                                                        {'loss': 19.0586, 'grad_norm': 20.0, 'learning_rate': 9.171413545524669e-06, 'epoch': 4.93}
 78%|███████▊  | 3880/5000 [19:01:42<5:52:35, 18.89s/it] 78%|███████▊  | 3881/5000 [19:01:55<5:18:36, 17.08s/it]                                                        {'loss': 18.9794, 'grad_norm': 13.75, 'learning_rate': 9.155797496886627e-06, 'epoch': 4.93}
 78%|███████▊  | 3881/5000 [19:01:55<5:18:36, 17.08s/it] 78%|███████▊  | 3882/5000 [19:02:19<5:56:37, 19.14s/it]                                                        {'loss': 19.3709, 'grad_norm': 19.75, 'learning_rate': 9.140192753380808e-06, 'epoch': 4.93}
 78%|███████▊  | 3882/5000 [19:02:19<5:56:37, 19.14s/it] 78%|███████▊  | 3883/5000 [19:02:34<5:34:13, 17.95s/it]                                                        {'loss': 17.9272, 'grad_norm': 8.9375, 'learning_rate': 9.124599321833241e-06, 'epoch': 4.93}
 78%|███████▊  | 3883/5000 [19:02:34<5:34:13, 17.95s/it] 78%|███████▊  | 3884/5000 [19:02:48<5:11:44, 16.76s/it]                                                        {'loss': 18.3653, 'grad_norm': 17.875, 'learning_rate': 9.109017209065014e-06, 'epoch': 4.93}
 78%|███████▊  | 3884/5000 [19:02:48<5:11:44, 16.76s/it] 78%|███████▊  | 3885/5000 [19:03:04<5:05:18, 16.43s/it]                                                        {'loss': 18.5655, 'grad_norm': 9.875, 'learning_rate': 9.093446421892292e-06, 'epoch': 4.93}
 78%|███████▊  | 3885/5000 [19:03:04<5:05:18, 16.43s/it] 78%|███████▊  | 3886/5000 [19:03:18<4:50:31, 15.65s/it]                                                        {'loss': 20.478, 'grad_norm': 20.25, 'learning_rate': 9.077886967126251e-06, 'epoch': 4.93}
 78%|███████▊  | 3886/5000 [19:03:18<4:50:31, 15.65s/it] 78%|███████▊  | 3887/5000 [19:03:34<4:53:42, 15.83s/it]                                                        {'loss': 18.379, 'grad_norm': 12.125, 'learning_rate': 9.062338851573142e-06, 'epoch': 4.94}
 78%|███████▊  | 3887/5000 [19:03:34<4:53:42, 15.83s/it] 78%|███████▊  | 3888/5000 [19:03:52<5:07:03, 16.57s/it]                                                        {'loss': 18.9824, 'grad_norm': 12.6875, 'learning_rate': 9.046802082034218e-06, 'epoch': 4.94}
 78%|███████▊  | 3888/5000 [19:03:52<5:07:03, 16.57s/it] 78%|███████▊  | 3889/5000 [19:04:11<5:20:44, 17.32s/it]                                                        {'loss': 15.8706, 'grad_norm': 7.1875, 'learning_rate': 9.031276665305808e-06, 'epoch': 4.94}
 78%|███████▊  | 3889/5000 [19:04:11<5:20:44, 17.32s/it] 78%|███████▊  | 3890/5000 [19:04:30<5:30:36, 17.87s/it]                                                        {'loss': 18.2635, 'grad_norm': 12.875, 'learning_rate': 9.015762608179255e-06, 'epoch': 4.94}
 78%|███████▊  | 3890/5000 [19:04:30<5:30:36, 17.87s/it] 78%|███████▊  | 3891/5000 [19:04:46<5:20:39, 17.35s/it]                                                        {'loss': 19.9919, 'grad_norm': 22.625, 'learning_rate': 9.000259917440911e-06, 'epoch': 4.94}
 78%|███████▊  | 3891/5000 [19:04:46<5:20:39, 17.35s/it] 78%|███████▊  | 3892/5000 [19:05:03<5:13:11, 16.96s/it]                                                        {'loss': 18.6196, 'grad_norm': 17.5, 'learning_rate': 8.984768599872214e-06, 'epoch': 4.94}
 78%|███████▊  | 3892/5000 [19:05:03<5:13:11, 16.96s/it] 78%|███████▊  | 3893/5000 [19:05:18<5:05:51, 16.58s/it]                                                        {'loss': 16.884, 'grad_norm': 12.875, 'learning_rate': 8.969288662249579e-06, 'epoch': 4.94}
 78%|███████▊  | 3893/5000 [19:05:18<5:05:51, 16.58s/it] 78%|███████▊  | 3894/5000 [19:05:40<5:33:56, 18.12s/it]                                                        {'loss': 18.2075, 'grad_norm': 9.6875, 'learning_rate': 8.953820111344434e-06, 'epoch': 4.94}
 78%|███████▊  | 3894/5000 [19:05:40<5:33:56, 18.12s/it] 78%|███████▊  | 3895/5000 [19:05:53<5:06:16, 16.63s/it]                                                        {'loss': 18.5405, 'grad_norm': 13.125, 'learning_rate': 8.938362953923288e-06, 'epoch': 4.95}
 78%|███████▊  | 3895/5000 [19:05:53<5:06:16, 16.63s/it] 78%|███████▊  | 3896/5000 [19:06:06<4:46:57, 15.60s/it]                                                        {'loss': 18.1457, 'grad_norm': 12.75, 'learning_rate': 8.922917196747602e-06, 'epoch': 4.95}
 78%|███████▊  | 3896/5000 [19:06:06<4:46:57, 15.60s/it] 78%|███████▊  | 3897/5000 [19:06:22<4:48:59, 15.72s/it]                                                        {'loss': 18.576, 'grad_norm': 20.25, 'learning_rate': 8.907482846573872e-06, 'epoch': 4.95}
 78%|███████▊  | 3897/5000 [19:06:22<4:48:59, 15.72s/it] 78%|███████▊  | 3898/5000 [19:06:39<4:52:13, 15.91s/it]                                                        {'loss': 16.7187, 'grad_norm': 8.375, 'learning_rate': 8.892059910153626e-06, 'epoch': 4.95}
 78%|███████▊  | 3898/5000 [19:06:39<4:52:13, 15.91s/it] 78%|███████▊  | 3899/5000 [19:06:56<4:58:38, 16.27s/it]                                                        {'loss': 17.0407, 'grad_norm': 8.8125, 'learning_rate': 8.876648394233371e-06, 'epoch': 4.95}
 78%|███████▊  | 3899/5000 [19:06:56<4:58:38, 16.27s/it] 78%|███████▊  | 3900/5000 [19:07:12<4:58:11, 16.26s/it]                                                        {'loss': 17.1316, 'grad_norm': 8.25, 'learning_rate': 8.861248305554624e-06, 'epoch': 4.95}
 78%|███████▊  | 3900/5000 [19:07:12<4:58:11, 16.26s/it] 78%|███████▊  | 3901/5000 [19:07:39<5:58:14, 19.56s/it]                                                        {'loss': 16.9045, 'grad_norm': 9.4375, 'learning_rate': 8.845859650853898e-06, 'epoch': 4.95}
 78%|███████▊  | 3901/5000 [19:07:39<5:58:14, 19.56s/it] 78%|███████▊  | 3902/5000 [19:07:58<5:55:47, 19.44s/it]                                                        {'loss': 16.1121, 'grad_norm': 8.875, 'learning_rate': 8.830482436862737e-06, 'epoch': 4.95}
 78%|███████▊  | 3902/5000 [19:07:58<5:55:47, 19.44s/it] 78%|███████▊  | 3903/5000 [19:08:31<7:05:55, 23.30s/it]                                                        {'loss': 18.2654, 'grad_norm': 13.8125, 'learning_rate': 8.815116670307648e-06, 'epoch': 4.96}
 78%|███████▊  | 3903/5000 [19:08:31<7:05:55, 23.30s/it] 78%|███████▊  | 3904/5000 [19:08:47<6:25:56, 21.13s/it]                                                        {'loss': 17.3027, 'grad_norm': 8.5625, 'learning_rate': 8.799762357910122e-06, 'epoch': 4.96}
 78%|███████▊  | 3904/5000 [19:08:47<6:25:56, 21.13s/it] 78%|███████▊  | 3905/5000 [19:09:02<5:52:08, 19.30s/it]                                                        {'loss': 18.1324, 'grad_norm': 12.375, 'learning_rate': 8.784419506386683e-06, 'epoch': 4.96}
 78%|███████▊  | 3905/5000 [19:09:02<5:52:08, 19.30s/it] 78%|███████▊  | 3906/5000 [19:09:20<5:43:17, 18.83s/it]                                                        {'loss': 21.2362, 'grad_norm': 478.0, 'learning_rate': 8.7690881224488e-06, 'epoch': 4.96}
 78%|███████▊  | 3906/5000 [19:09:20<5:43:17, 18.83s/it] 78%|███████▊  | 3907/5000 [19:09:35<5:24:44, 17.83s/it]                                                        {'loss': 17.0896, 'grad_norm': 10.1875, 'learning_rate': 8.753768212802937e-06, 'epoch': 4.96}
 78%|███████▊  | 3907/5000 [19:09:35<5:24:44, 17.83s/it] 78%|███████▊  | 3908/5000 [19:09:49<5:05:37, 16.79s/it]                                                        {'loss': 19.8789, 'grad_norm': 71.0, 'learning_rate': 8.738459784150562e-06, 'epoch': 4.96}
 78%|███████▊  | 3908/5000 [19:09:49<5:05:37, 16.79s/it] 78%|███████▊  | 3909/5000 [19:10:04<4:52:32, 16.09s/it]                                                        {'loss': 17.3179, 'grad_norm': 8.3125, 'learning_rate': 8.723162843188077e-06, 'epoch': 4.96}
 78%|███████▊  | 3909/5000 [19:10:04<4:52:32, 16.09s/it] 78%|███████▊  | 3910/5000 [19:10:19<4:45:13, 15.70s/it]                                                        {'loss': 18.4571, 'grad_norm': 22.0, 'learning_rate': 8.707877396606912e-06, 'epoch': 4.97}
 78%|███████▊  | 3910/5000 [19:10:19<4:45:13, 15.70s/it] 78%|███████▊  | 3911/5000 [19:10:32<4:34:41, 15.13s/it]                                                        {'loss': 18.0222, 'grad_norm': 10.5625, 'learning_rate': 8.692603451093412e-06, 'epoch': 4.97}
 78%|███████▊  | 3911/5000 [19:10:32<4:34:41, 15.13s/it] 78%|███████▊  | 3912/5000 [19:10:50<4:47:19, 15.84s/it]                                                        {'loss': 18.0883, 'grad_norm': 12.625, 'learning_rate': 8.677341013328942e-06, 'epoch': 4.97}
 78%|███████▊  | 3912/5000 [19:10:50<4:47:19, 15.84s/it] 78%|███████▊  | 3913/5000 [19:11:04<4:37:25, 15.31s/it]                                                        {'loss': 19.3156, 'grad_norm': 13.4375, 'learning_rate': 8.662090089989797e-06, 'epoch': 4.97}
 78%|███████▊  | 3913/5000 [19:11:04<4:37:25, 15.31s/it] 78%|███████▊  | 3914/5000 [19:11:21<4:45:25, 15.77s/it]                                                        {'loss': 17.3468, 'grad_norm': 15.9375, 'learning_rate': 8.64685068774726e-06, 'epoch': 4.97}
 78%|███████▊  | 3914/5000 [19:11:21<4:45:25, 15.77s/it] 78%|███████▊  | 3915/5000 [19:11:37<4:44:41, 15.74s/it]                                                        {'loss': 17.6377, 'grad_norm': 14.125, 'learning_rate': 8.631622813267558e-06, 'epoch': 4.97}
 78%|███████▊  | 3915/5000 [19:11:37<4:44:41, 15.74s/it] 78%|███████▊  | 3916/5000 [19:11:56<5:05:43, 16.92s/it]                                                        {'loss': 17.2065, 'grad_norm': 17.875, 'learning_rate': 8.616406473211872e-06, 'epoch': 4.97}
 78%|███████▊  | 3916/5000 [19:11:56<5:05:43, 16.92s/it] 78%|███████▊  | 3917/5000 [19:12:14<5:08:33, 17.10s/it]                                                        {'loss': 17.6156, 'grad_norm': 10.75, 'learning_rate': 8.601201674236364e-06, 'epoch': 4.97}
 78%|███████▊  | 3917/5000 [19:12:14<5:08:33, 17.10s/it] 78%|███████▊  | 3918/5000 [19:12:40<6:00:19, 19.98s/it]                                                        {'loss': 17.8058, 'grad_norm': 13.4375, 'learning_rate': 8.58600842299212e-06, 'epoch': 4.98}
 78%|███████▊  | 3918/5000 [19:12:40<6:00:19, 19.98s/it] 78%|███████▊  | 3919/5000 [19:12:55<5:30:37, 18.35s/it]                                                        {'loss': 18.9943, 'grad_norm': 13.875, 'learning_rate': 8.570826726125183e-06, 'epoch': 4.98}
 78%|███████▊  | 3919/5000 [19:12:55<5:30:37, 18.35s/it] 78%|███████▊  | 3920/5000 [19:13:15<5:37:38, 18.76s/it]                                                        {'loss': 17.0312, 'grad_norm': 8.5, 'learning_rate': 8.555656590276537e-06, 'epoch': 4.98}
 78%|███████▊  | 3920/5000 [19:13:15<5:37:38, 18.76s/it] 78%|███████▊  | 3921/5000 [19:13:41<6:18:35, 21.05s/it]                                                        {'loss': 16.3265, 'grad_norm': 5.78125, 'learning_rate': 8.54049802208213e-06, 'epoch': 4.98}
 78%|███████▊  | 3921/5000 [19:13:41<6:18:35, 21.05s/it] 78%|███████▊  | 3922/5000 [19:13:59<6:00:33, 20.07s/it]                                                        {'loss': 16.5914, 'grad_norm': 11.0625, 'learning_rate': 8.525351028172826e-06, 'epoch': 4.98}
 78%|███████▊  | 3922/5000 [19:13:59<6:00:33, 20.07s/it] 78%|███████▊  | 3923/5000 [19:14:18<5:53:03, 19.67s/it]                                                        {'loss': 16.2871, 'grad_norm': 9.9375, 'learning_rate': 8.510215615174425e-06, 'epoch': 4.98}
 78%|███████▊  | 3923/5000 [19:14:18<5:53:03, 19.67s/it] 78%|███████▊  | 3924/5000 [19:14:40<6:09:02, 20.58s/it]                                                        {'loss': 16.4555, 'grad_norm': 7.21875, 'learning_rate': 8.495091789707693e-06, 'epoch': 4.98}
 78%|███████▊  | 3924/5000 [19:14:40<6:09:02, 20.58s/it] 78%|███████▊  | 3925/5000 [19:14:53<5:28:50, 18.35s/it]                                                        {'loss': 20.3452, 'grad_norm': 23.5, 'learning_rate': 8.47997955838829e-06, 'epoch': 4.98}
 78%|███████▊  | 3925/5000 [19:14:53<5:28:50, 18.35s/it] 79%|███████▊  | 3926/5000 [19:15:07<5:04:22, 17.00s/it]                                                        {'loss': 18.5319, 'grad_norm': 13.875, 'learning_rate': 8.464878927826818e-06, 'epoch': 4.99}
 79%|███████▊  | 3926/5000 [19:15:07<5:04:22, 17.00s/it] 79%|███████▊  | 3927/5000 [19:15:24<5:04:20, 17.02s/it]                                                        {'loss': 15.9563, 'grad_norm': 9.3125, 'learning_rate': 8.449789904628818e-06, 'epoch': 4.99}
 79%|███████▊  | 3927/5000 [19:15:24<5:04:20, 17.02s/it] 79%|███████▊  | 3928/5000 [19:15:39<4:51:19, 16.31s/it]                                                        {'loss': 17.269, 'grad_norm': 10.125, 'learning_rate': 8.434712495394734e-06, 'epoch': 4.99}
 79%|███████▊  | 3928/5000 [19:15:39<4:51:19, 16.31s/it] 79%|███████▊  | 3929/5000 [19:15:53<4:36:44, 15.50s/it]                                                        {'loss': 18.4904, 'grad_norm': 11.6875, 'learning_rate': 8.419646706719928e-06, 'epoch': 4.99}
 79%|███████▊  | 3929/5000 [19:15:53<4:36:44, 15.50s/it] 79%|███████▊  | 3930/5000 [19:16:08<4:37:57, 15.59s/it]                                                        {'loss': 17.8374, 'grad_norm': 12.1875, 'learning_rate': 8.404592545194704e-06, 'epoch': 4.99}
 79%|███████▊  | 3930/5000 [19:16:08<4:37:57, 15.59s/it] 79%|███████▊  | 3931/5000 [19:16:24<4:37:13, 15.56s/it]                                                        {'loss': 19.5478, 'grad_norm': 22.25, 'learning_rate': 8.38955001740426e-06, 'epoch': 4.99}
 79%|███████▊  | 3931/5000 [19:16:24<4:37:13, 15.56s/it] 79%|███████▊  | 3932/5000 [19:16:51<5:37:44, 18.97s/it]                                                        {'loss': 17.138, 'grad_norm': 10.0, 'learning_rate': 8.374519129928696e-06, 'epoch': 4.99}
 79%|███████▊  | 3932/5000 [19:16:51<5:37:44, 18.97s/it] 79%|███████▊  | 3933/5000 [19:17:06<5:14:32, 17.69s/it]                                                        {'loss': 18.5524, 'grad_norm': 20.75, 'learning_rate': 8.359499889343048e-06, 'epoch': 4.99}
 79%|███████▊  | 3933/5000 [19:17:06<5:14:32, 17.69s/it] 79%|███████▊  | 3934/5000 [19:17:22<5:07:01, 17.28s/it]                                                        {'loss': 20.7267, 'grad_norm': 18.5, 'learning_rate': 8.344492302217228e-06, 'epoch': 5.0}
 79%|███████▊  | 3934/5000 [19:17:22<5:07:01, 17.28s/it] 79%|███████▊  | 3935/5000 [19:17:37<4:55:46, 16.66s/it]                                                        {'loss': 17.6114, 'grad_norm': 12.9375, 'learning_rate': 8.329496375116082e-06, 'epoch': 5.0}
 79%|███████▊  | 3935/5000 [19:17:37<4:55:46, 16.66s/it] 79%|███████▊  | 3936/5000 [19:17:53<4:53:19, 16.54s/it]                                                        {'loss': 16.5919, 'grad_norm': 10.0625, 'learning_rate': 8.31451211459932e-06, 'epoch': 5.0}
 79%|███████▊  | 3936/5000 [19:17:53<4:53:19, 16.54s/it] 79%|███████▊  | 3937/5000 [19:18:08<4:43:03, 15.98s/it]                                                        {'loss': 18.2014, 'grad_norm': 11.5, 'learning_rate': 8.299539527221586e-06, 'epoch': 5.0}
 79%|███████▊  | 3937/5000 [19:18:08<4:43:03, 15.98s/it] 79%|███████▉  | 3938/5000 [19:18:27<4:57:37, 16.81s/it]                                                        {'loss': 17.9646, 'grad_norm': 9.0, 'learning_rate': 8.284578619532386e-06, 'epoch': 5.0}
 79%|███████▉  | 3938/5000 [19:18:27<4:57:37, 16.81s/it] 79%|███████▉  | 3939/5000 [19:18:42<4:50:13, 16.41s/it]                                                        {'loss': 16.9456, 'grad_norm': 13.125, 'learning_rate': 8.269629398076121e-06, 'epoch': 5.0}
 79%|███████▉  | 3939/5000 [19:18:42<4:50:13, 16.41s/it] 79%|███████▉  | 3940/5000 [19:18:56<4:36:34, 15.66s/it]                                                        {'loss': 20.2865, 'grad_norm': 18.25, 'learning_rate': 8.25469186939211e-06, 'epoch': 5.0}
 79%|███████▉  | 3940/5000 [19:18:56<4:36:34, 15.66s/it] 79%|███████▉  | 3941/5000 [19:19:12<4:39:49, 15.85s/it]                                                        {'loss': 18.6972, 'grad_norm': 17.5, 'learning_rate': 8.239766040014523e-06, 'epoch': 5.0}
 79%|███████▉  | 3941/5000 [19:19:12<4:39:49, 15.85s/it] 79%|███████▉  | 3942/5000 [19:19:26<4:28:32, 15.23s/it]                                                        {'loss': 19.4508, 'grad_norm': 15.6875, 'learning_rate': 8.224851916472418e-06, 'epoch': 5.01}
 79%|███████▉  | 3942/5000 [19:19:26<4:28:32, 15.23s/it] 79%|███████▉  | 3943/5000 [19:19:51<5:19:07, 18.11s/it]                                                        {'loss': 16.4217, 'grad_norm': 9.6875, 'learning_rate': 8.209949505289754e-06, 'epoch': 5.01}
 79%|███████▉  | 3943/5000 [19:19:51<5:19:07, 18.11s/it] 79%|███████▉  | 3944/5000 [19:20:12<5:35:19, 19.05s/it]                                                        {'loss': 15.9662, 'grad_norm': 11.625, 'learning_rate': 8.195058812985344e-06, 'epoch': 5.01}
 79%|███████▉  | 3944/5000 [19:20:12<5:35:19, 19.05s/it] 79%|███████▉  | 3945/5000 [19:20:37<6:03:15, 20.66s/it]                                                        {'loss': 18.4336, 'grad_norm': 14.5625, 'learning_rate': 8.180179846072877e-06, 'epoch': 5.01}
 79%|███████▉  | 3945/5000 [19:20:37<6:03:15, 20.66s/it] 79%|███████▉  | 3946/5000 [19:20:50<5:23:51, 18.44s/it]                                                        {'loss': 17.0211, 'grad_norm': 12.625, 'learning_rate': 8.165312611060931e-06, 'epoch': 5.01}
 79%|███████▉  | 3946/5000 [19:20:50<5:23:51, 18.44s/it] 79%|███████▉  | 3947/5000 [19:21:20<6:23:33, 21.86s/it]                                                        {'loss': 16.3499, 'grad_norm': 9.9375, 'learning_rate': 8.15045711445294e-06, 'epoch': 5.01}
 79%|███████▉  | 3947/5000 [19:21:20<6:23:33, 21.86s/it] 79%|███████▉  | 3948/5000 [19:21:38<6:01:54, 20.64s/it]                                                        {'loss': 16.8693, 'grad_norm': 8.5, 'learning_rate': 8.135613362747183e-06, 'epoch': 5.01}
 79%|███████▉  | 3948/5000 [19:21:38<6:01:54, 20.64s/it] 79%|███████▉  | 3949/5000 [19:21:56<5:51:02, 20.04s/it]                                                        {'loss': 17.7291, 'grad_norm': 9.9375, 'learning_rate': 8.120781362436846e-06, 'epoch': 5.01}
 79%|███████▉  | 3949/5000 [19:21:56<5:51:02, 20.04s/it] 79%|███████▉  | 3950/5000 [19:22:15<5:43:38, 19.64s/it]                                                        {'loss': 18.4246, 'grad_norm': 10.0625, 'learning_rate': 8.10596112000994e-06, 'epoch': 5.02}
 79%|███████▉  | 3950/5000 [19:22:15<5:43:38, 19.64s/it] 79%|███████▉  | 3951/5000 [19:22:31<5:22:44, 18.46s/it]                                                        {'loss': 17.7951, 'grad_norm': 10.0, 'learning_rate': 8.09115264194933e-06, 'epoch': 5.02}
 79%|███████▉  | 3951/5000 [19:22:31<5:22:44, 18.46s/it] 79%|███████▉  | 3952/5000 [19:22:45<4:58:43, 17.10s/it]                                                        {'loss': 18.7514, 'grad_norm': 12.625, 'learning_rate': 8.076355934732771e-06, 'epoch': 5.02}
 79%|███████▉  | 3952/5000 [19:22:45<4:58:43, 17.10s/it] 79%|███████▉  | 3953/5000 [19:22:59<4:44:19, 16.29s/it]                                                        {'loss': 17.8059, 'grad_norm': 11.5, 'learning_rate': 8.061571004832836e-06, 'epoch': 5.02}
 79%|███████▉  | 3953/5000 [19:22:59<4:44:19, 16.29s/it] 79%|███████▉  | 3954/5000 [19:23:16<4:46:26, 16.43s/it]                                                        {'loss': 17.4468, 'grad_norm': 10.5, 'learning_rate': 8.046797858716948e-06, 'epoch': 5.02}
 79%|███████▉  | 3954/5000 [19:23:16<4:46:26, 16.43s/it] 79%|███████▉  | 3955/5000 [19:23:30<4:37:06, 15.91s/it]                                                        {'loss': 18.7422, 'grad_norm': 12.1875, 'learning_rate': 8.03203650284738e-06, 'epoch': 5.02}
 79%|███████▉  | 3955/5000 [19:23:30<4:37:06, 15.91s/it] 79%|███████▉  | 3956/5000 [19:23:45<4:27:52, 15.40s/it]                                                        {'loss': 17.5006, 'grad_norm': 8.875, 'learning_rate': 8.017286943681267e-06, 'epoch': 5.02}
 79%|███████▉  | 3956/5000 [19:23:45<4:27:52, 15.40s/it] 79%|███████▉  | 3957/5000 [19:23:57<4:13:50, 14.60s/it]                                                        {'loss': 18.8314, 'grad_norm': 12.75, 'learning_rate': 8.002549187670555e-06, 'epoch': 5.02}
 79%|███████▉  | 3957/5000 [19:23:57<4:13:50, 14.60s/it] 79%|███████▉  | 3958/5000 [19:24:14<4:22:25, 15.11s/it]                                                        {'loss': 18.5035, 'grad_norm': 15.375, 'learning_rate': 7.987823241262039e-06, 'epoch': 5.03}
 79%|███████▉  | 3958/5000 [19:24:14<4:22:25, 15.11s/it] 79%|███████▉  | 3959/5000 [19:24:30<4:30:22, 15.58s/it]                                                        {'loss': 17.3013, 'grad_norm': 7.625, 'learning_rate': 7.973109110897361e-06, 'epoch': 5.03}
 79%|███████▉  | 3959/5000 [19:24:30<4:30:22, 15.58s/it] 79%|███████▉  | 3960/5000 [19:24:47<4:33:12, 15.76s/it]                                                        {'loss': 17.4276, 'grad_norm': 10.0, 'learning_rate': 7.958406803012975e-06, 'epoch': 5.03}
 79%|███████▉  | 3960/5000 [19:24:47<4:33:12, 15.76s/it] 79%|███████▉  | 3961/5000 [19:25:01<4:24:29, 15.27s/it]                                                        {'loss': 17.7158, 'grad_norm': 14.625, 'learning_rate': 7.943716324040157e-06, 'epoch': 5.03}
 79%|███████▉  | 3961/5000 [19:25:01<4:24:29, 15.27s/it] 79%|███████▉  | 3962/5000 [19:25:25<5:13:29, 18.12s/it]                                                        {'loss': 16.1122, 'grad_norm': 8.6875, 'learning_rate': 7.929037680405045e-06, 'epoch': 5.03}
 79%|███████▉  | 3962/5000 [19:25:25<5:13:29, 18.12s/it] 79%|███████▉  | 3963/5000 [19:25:54<6:06:22, 21.20s/it]                                                        {'loss': 17.1906, 'grad_norm': 41.75, 'learning_rate': 7.914370878528563e-06, 'epoch': 5.03}
 79%|███████▉  | 3963/5000 [19:25:54<6:06:22, 21.20s/it] 79%|███████▉  | 3964/5000 [19:26:13<5:53:23, 20.47s/it]                                                        {'loss': 17.2079, 'grad_norm': 10.0, 'learning_rate': 7.899715924826457e-06, 'epoch': 5.03}
 79%|███████▉  | 3964/5000 [19:26:13<5:53:23, 20.47s/it] 79%|███████▉  | 3965/5000 [19:26:31<5:43:56, 19.94s/it]                                                        {'loss': 16.0347, 'grad_norm': 7.3125, 'learning_rate': 7.885072825709328e-06, 'epoch': 5.03}
 79%|███████▉  | 3965/5000 [19:26:31<5:43:56, 19.94s/it] 79%|███████▉  | 3966/5000 [19:26:59<6:21:05, 22.11s/it]                                                        {'loss': 19.2389, 'grad_norm': 15.75, 'learning_rate': 7.87044158758255e-06, 'epoch': 5.04}
 79%|███████▉  | 3966/5000 [19:26:59<6:21:05, 22.11s/it] 79%|███████▉  | 3967/5000 [19:27:14<5:46:36, 20.13s/it]                                                        {'loss': 17.2335, 'grad_norm': 11.75, 'learning_rate': 7.85582221684631e-06, 'epoch': 5.04}
 79%|███████▉  | 3967/5000 [19:27:14<5:46:36, 20.13s/it] 79%|███████▉  | 3968/5000 [19:27:28<5:16:22, 18.39s/it]                                                        {'loss': 18.047, 'grad_norm': 11.25, 'learning_rate': 7.841214719895647e-06, 'epoch': 5.04}
 79%|███████▉  | 3968/5000 [19:27:28<5:16:22, 18.39s/it] 79%|███████▉  | 3969/5000 [19:27:46<5:10:32, 18.07s/it]                                                        {'loss': 18.5635, 'grad_norm': 8.5625, 'learning_rate': 7.82661910312036e-06, 'epoch': 5.04}
 79%|███████▉  | 3969/5000 [19:27:46<5:10:32, 18.07s/it] 79%|███████▉  | 3970/5000 [19:28:01<4:55:40, 17.22s/it]                                                        {'loss': 17.9049, 'grad_norm': 13.3125, 'learning_rate': 7.812035372905069e-06, 'epoch': 5.04}
 79%|███████▉  | 3970/5000 [19:28:01<4:55:40, 17.22s/it] 79%|███████▉  | 3971/5000 [19:28:27<5:41:47, 19.93s/it]                                                        {'loss': 18.4941, 'grad_norm': 22.5, 'learning_rate': 7.797463535629187e-06, 'epoch': 5.04}
 79%|███████▉  | 3971/5000 [19:28:27<5:41:47, 19.93s/it] 79%|███████▉  | 3972/5000 [19:28:54<6:16:46, 21.99s/it]                                                        {'loss': 17.6807, 'grad_norm': 8.75, 'learning_rate': 7.782903597666942e-06, 'epoch': 5.04}
 79%|███████▉  | 3972/5000 [19:28:54<6:16:46, 21.99s/it] 79%|███████▉  | 3973/5000 [19:29:08<5:33:38, 19.49s/it]                                                        {'loss': 18.553, 'grad_norm': 18.75, 'learning_rate': 7.768355565387345e-06, 'epoch': 5.05}
 79%|███████▉  | 3973/5000 [19:29:08<5:33:38, 19.49s/it] 79%|███████▉  | 3974/5000 [19:29:26<5:25:55, 19.06s/it]                                                        {'loss': 17.7305, 'grad_norm': 14.375, 'learning_rate': 7.753819445154183e-06, 'epoch': 5.05}
 79%|███████▉  | 3974/5000 [19:29:26<5:25:55, 19.06s/it] 80%|███████▉  | 3975/5000 [19:29:43<5:16:31, 18.53s/it]                                                        {'loss': 17.1592, 'grad_norm': 8.625, 'learning_rate': 7.739295243326067e-06, 'epoch': 5.05}
 80%|███████▉  | 3975/5000 [19:29:43<5:16:31, 18.53s/it] 80%|███████▉  | 3976/5000 [19:29:59<5:02:52, 17.75s/it]                                                        {'loss': 17.8643, 'grad_norm': 13.0, 'learning_rate': 7.724782966256377e-06, 'epoch': 5.05}
 80%|███████▉  | 3976/5000 [19:29:59<5:02:52, 17.75s/it] 80%|███████▉  | 3977/5000 [19:30:23<5:33:34, 19.56s/it]                                                        {'loss': 17.3813, 'grad_norm': 34.75, 'learning_rate': 7.710282620293254e-06, 'epoch': 5.05}
 80%|███████▉  | 3977/5000 [19:30:23<5:33:34, 19.56s/it] 80%|███████▉  | 3978/5000 [19:30:38<5:10:17, 18.22s/it]                                                        {'loss': 16.5543, 'grad_norm': 13.5, 'learning_rate': 7.69579421177967e-06, 'epoch': 5.05}
 80%|███████▉  | 3978/5000 [19:30:38<5:10:17, 18.22s/it] 80%|███████▉  | 3979/5000 [19:30:58<5:21:15, 18.88s/it]                                                        {'loss': 18.4217, 'grad_norm': 25.0, 'learning_rate': 7.681317747053326e-06, 'epoch': 5.05}
 80%|███████▉  | 3979/5000 [19:30:58<5:21:15, 18.88s/it] 80%|███████▉  | 3980/5000 [19:31:15<5:12:50, 18.40s/it]                                                        {'loss': 16.6111, 'grad_norm': 10.0625, 'learning_rate': 7.666853232446736e-06, 'epoch': 5.05}
 80%|███████▉  | 3980/5000 [19:31:15<5:12:50, 18.40s/it] 80%|███████▉  | 3981/5000 [19:31:31<4:57:22, 17.51s/it]                                                        {'loss': 16.8968, 'grad_norm': 11.1875, 'learning_rate': 7.652400674287154e-06, 'epoch': 5.06}
 80%|███████▉  | 3981/5000 [19:31:31<4:57:22, 17.51s/it] 80%|███████▉  | 3982/5000 [19:31:48<4:56:02, 17.45s/it]                                                        {'loss': 18.3756, 'grad_norm': 14.375, 'learning_rate': 7.637960078896634e-06, 'epoch': 5.06}
 80%|███████▉  | 3982/5000 [19:31:48<4:56:02, 17.45s/it] 80%|███████▉  | 3983/5000 [19:32:05<4:51:30, 17.20s/it]                                                        {'loss': 16.4356, 'grad_norm': 13.75, 'learning_rate': 7.6235314525919686e-06, 'epoch': 5.06}
 80%|███████▉  | 3983/5000 [19:32:05<4:51:30, 17.20s/it] 80%|███████▉  | 3984/5000 [19:32:18<4:30:17, 15.96s/it]                                                        {'loss': 18.1737, 'grad_norm': 11.3125, 'learning_rate': 7.6091148016847486e-06, 'epoch': 5.06}
 80%|███████▉  | 3984/5000 [19:32:18<4:30:17, 15.96s/it] 80%|███████▉  | 3985/5000 [19:32:35<4:35:04, 16.26s/it]                                                        {'loss': 18.4522, 'grad_norm': 12.9375, 'learning_rate': 7.594710132481294e-06, 'epoch': 5.06}
 80%|███████▉  | 3985/5000 [19:32:35<4:35:04, 16.26s/it] 80%|███████▉  | 3986/5000 [19:32:55<4:52:47, 17.33s/it]                                                        {'loss': 17.9751, 'grad_norm': 14.9375, 'learning_rate': 7.5803174512827e-06, 'epoch': 5.06}
 80%|███████▉  | 3986/5000 [19:32:55<4:52:47, 17.33s/it] 80%|███████▉  | 3987/5000 [19:33:22<5:41:12, 20.21s/it]                                                        {'loss': 16.4265, 'grad_norm': 6.4375, 'learning_rate': 7.565936764384802e-06, 'epoch': 5.06}
 80%|███████▉  | 3987/5000 [19:33:22<5:41:12, 20.21s/it] 80%|███████▉  | 3988/5000 [19:33:36<5:12:00, 18.50s/it]                                                        {'loss': 18.1726, 'grad_norm': 13.75, 'learning_rate': 7.551568078078224e-06, 'epoch': 5.06}
 80%|███████▉  | 3988/5000 [19:33:36<5:12:00, 18.50s/it] 80%|███████▉  | 3989/5000 [19:34:01<5:44:17, 20.43s/it]                                                        {'loss': 16.8964, 'grad_norm': 12.625, 'learning_rate': 7.537211398648302e-06, 'epoch': 5.07}
 80%|███████▉  | 3989/5000 [19:34:01<5:44:17, 20.43s/it] 80%|███████▉  | 3990/5000 [19:34:17<5:21:28, 19.10s/it]                                                        {'loss': 16.5109, 'grad_norm': 8.625, 'learning_rate': 7.522866732375129e-06, 'epoch': 5.07}
 80%|███████▉  | 3990/5000 [19:34:17<5:21:28, 19.10s/it] 80%|███████▉  | 3991/5000 [19:34:32<4:58:46, 17.77s/it]                                                        {'loss': 18.1789, 'grad_norm': 14.875, 'learning_rate': 7.508534085533569e-06, 'epoch': 5.07}
 80%|███████▉  | 3991/5000 [19:34:32<4:58:46, 17.77s/it] 80%|███████▉  | 3992/5000 [19:34:46<4:39:54, 16.66s/it]                                                        {'loss': 19.6884, 'grad_norm': 15.0625, 'learning_rate': 7.4942134643932e-06, 'epoch': 5.07}
 80%|███████▉  | 3992/5000 [19:34:46<4:39:54, 16.66s/it] 80%|███████▉  | 3993/5000 [19:35:04<4:46:48, 17.09s/it]                                                        {'loss': 17.2441, 'grad_norm': 11.5625, 'learning_rate': 7.479904875218337e-06, 'epoch': 5.07}
 80%|███████▉  | 3993/5000 [19:35:04<4:46:48, 17.09s/it] 80%|███████▉  | 3994/5000 [19:35:28<5:20:37, 19.12s/it]                                                        {'loss': 18.8967, 'grad_norm': 12.3125, 'learning_rate': 7.465608324268062e-06, 'epoch': 5.07}
 80%|███████▉  | 3994/5000 [19:35:28<5:20:37, 19.12s/it] 80%|███████▉  | 3995/5000 [19:35:50<5:34:58, 20.00s/it]                                                        {'loss': 17.1758, 'grad_norm': 11.625, 'learning_rate': 7.451323817796162e-06, 'epoch': 5.07}
 80%|███████▉  | 3995/5000 [19:35:50<5:34:58, 20.00s/it] 80%|███████▉  | 3996/5000 [19:36:14<5:58:11, 21.41s/it]                                                        {'loss': 17.9345, 'grad_norm': 14.0, 'learning_rate': 7.437051362051159e-06, 'epoch': 5.07}
 80%|███████▉  | 3996/5000 [19:36:14<5:58:11, 21.41s/it] 80%|███████▉  | 3997/5000 [19:36:38<6:06:25, 21.92s/it]                                                        {'loss': 17.6376, 'grad_norm': 22.5, 'learning_rate': 7.422790963276325e-06, 'epoch': 5.08}
 80%|███████▉  | 3997/5000 [19:36:38<6:06:25, 21.92s/it] 80%|███████▉  | 3998/5000 [19:36:55<5:44:22, 20.62s/it]                                                        {'loss': 30.6005, 'grad_norm': 186.0, 'learning_rate': 7.408542627709636e-06, 'epoch': 5.08}
 80%|███████▉  | 3998/5000 [19:36:55<5:44:22, 20.62s/it] 80%|███████▉  | 3999/5000 [19:37:10<5:12:51, 18.75s/it]                                                        {'loss': 17.8498, 'grad_norm': 16.5, 'learning_rate': 7.39430636158379e-06, 'epoch': 5.08}
 80%|███████▉  | 3999/5000 [19:37:10<5:12:51, 18.75s/it] 80%|████████  | 4000/5000 [19:37:27<5:05:48, 18.35s/it]                                                        {'loss': 20.0797, 'grad_norm': 21.5, 'learning_rate': 7.380082171126228e-06, 'epoch': 5.08}
 80%|████████  | 4000/5000 [19:37:27<5:05:48, 18.35s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:18,  4.40s/it][A
  3%|▎         | 3/88 [00:16<08:03,  5.69s/it][A
  5%|▍         | 4/88 [00:20<07:14,  5.17s/it][A
  6%|▌         | 5/88 [00:24<06:29,  4.69s/it][A
  7%|▋         | 6/88 [00:29<06:41,  4.90s/it][A
  8%|▊         | 7/88 [00:33<06:12,  4.60s/it][A
  9%|▉         | 8/88 [00:37<05:46,  4.33s/it][A
 10%|█         | 9/88 [00:40<05:21,  4.06s/it][A
 11%|█▏        | 10/88 [00:43<04:50,  3.72s/it][A
 12%|█▎        | 11/88 [00:46<04:26,  3.47s/it][A
 14%|█▎        | 12/88 [00:49<03:57,  3.12s/it][A
 15%|█▍        | 13/88 [00:51<03:40,  2.95s/it][A
 16%|█▌        | 14/88 [00:57<04:36,  3.73s/it][A
 17%|█▋        | 15/88 [01:02<05:10,  4.26s/it][A
 18%|█▊        | 16/88 [01:06<04:47,  3.99s/it][A
 19%|█▉        | 17/88 [01:11<05:12,  4.40s/it][A
 20%|██        | 18/88 [01:14<04:42,  4.04s/it][A
 22%|██▏       | 19/88 [01:18<04:43,  4.11s/it][A
 23%|██▎       | 20/88 [01:22<04:33,  4.02s/it][A
 24%|██▍       | 21/88 [01:25<04:14,  3.80s/it][A
 25%|██▌       | 22/88 [01:29<04:11,  3.81s/it][A
 26%|██▌       | 23/88 [01:32<03:45,  3.46s/it][A
 27%|██▋       | 24/88 [01:41<05:20,  5.01s/it][A
 28%|██▊       | 25/88 [01:44<04:46,  4.55s/it][A
 30%|██▉       | 26/88 [01:50<05:17,  5.12s/it][A
 31%|███       | 27/88 [01:55<04:57,  4.88s/it][A
 32%|███▏      | 28/88 [02:04<06:03,  6.05s/it][A
 33%|███▎      | 29/88 [02:09<05:42,  5.80s/it][A
 34%|███▍      | 30/88 [02:14<05:26,  5.62s/it][A
 35%|███▌      | 31/88 [02:18<04:54,  5.17s/it][A
 36%|███▋      | 32/88 [02:27<05:54,  6.33s/it][A
 38%|███▊      | 33/88 [02:32<05:20,  5.83s/it][A
 39%|███▊      | 34/88 [02:41<06:02,  6.72s/it][A
 40%|███▉      | 35/88 [02:44<04:56,  5.59s/it][A
 41%|████      | 36/88 [02:49<04:44,  5.48s/it][A
 42%|████▏     | 37/88 [02:52<04:11,  4.94s/it][A
 43%|████▎     | 38/88 [02:57<03:55,  4.71s/it][A
 44%|████▍     | 39/88 [03:00<03:27,  4.23s/it][A
 45%|████▌     | 40/88 [03:06<03:57,  4.95s/it][A
 47%|████▋     | 41/88 [03:16<04:59,  6.37s/it][A
 48%|████▊     | 42/88 [03:20<04:13,  5.52s/it][A
 49%|████▉     | 43/88 [03:28<04:50,  6.47s/it][A
 50%|█████     | 44/88 [03:32<04:02,  5.52s/it][A
 51%|█████     | 45/88 [03:36<03:44,  5.23s/it][A
 52%|█████▏    | 46/88 [03:40<03:23,  4.84s/it][A
 53%|█████▎    | 47/88 [03:42<02:44,  4.01s/it][A
 55%|█████▍    | 48/88 [03:44<02:20,  3.50s/it][A
 56%|█████▌    | 49/88 [03:49<02:34,  3.96s/it][A
 57%|█████▋    | 50/88 [03:54<02:32,  4.01s/it][A
 58%|█████▊    | 51/88 [03:58<02:30,  4.06s/it][A
 59%|█████▉    | 52/88 [04:04<02:44,  4.58s/it][A
 60%|██████    | 53/88 [04:09<02:44,  4.71s/it][A
 61%|██████▏   | 54/88 [04:18<03:30,  6.18s/it][A
 62%|██████▎   | 55/88 [04:23<03:11,  5.80s/it][A
 64%|██████▎   | 56/88 [04:26<02:34,  4.83s/it][A
 65%|██████▍   | 57/88 [04:30<02:21,  4.56s/it][A
 66%|██████▌   | 58/88 [04:38<02:55,  5.84s/it][A
 67%|██████▋   | 59/88 [04:43<02:40,  5.55s/it][A
 68%|██████▊   | 60/88 [04:47<02:16,  4.86s/it][A
 69%|██████▉   | 61/88 [04:51<02:04,  4.62s/it][A
 70%|███████   | 62/88 [04:54<01:47,  4.14s/it][A
 72%|███████▏  | 63/88 [04:59<01:53,  4.55s/it][A
 73%|███████▎  | 64/88 [05:03<01:46,  4.42s/it][A
 74%|███████▍  | 65/88 [05:08<01:40,  4.39s/it][A
 75%|███████▌  | 66/88 [05:11<01:32,  4.20s/it][A
 76%|███████▌  | 67/88 [05:14<01:21,  3.86s/it][A
 77%|███████▋  | 68/88 [05:19<01:23,  4.18s/it][A
 78%|███████▊  | 69/88 [05:26<01:31,  4.82s/it][A
 80%|███████▉  | 70/88 [05:30<01:22,  4.57s/it][A
 81%|████████  | 71/88 [05:33<01:13,  4.34s/it][A
 82%|████████▏ | 72/88 [05:37<01:07,  4.21s/it][A
 83%|████████▎ | 73/88 [05:40<00:57,  3.86s/it][A
 84%|████████▍ | 74/88 [05:43<00:49,  3.54s/it][A
 85%|████████▌ | 75/88 [05:48<00:50,  3.91s/it][A
 86%|████████▋ | 76/88 [05:52<00:46,  3.89s/it][A
 88%|████████▊ | 77/88 [05:59<00:53,  4.86s/it][A
 89%|████████▊ | 78/88 [06:03<00:45,  4.57s/it][A
 90%|████████▉ | 79/88 [06:08<00:41,  4.61s/it][A
 91%|█████████ | 80/88 [06:10<00:32,  4.11s/it][A
 92%|█████████▏| 81/88 [06:15<00:30,  4.38s/it][A
 93%|█████████▎| 82/88 [06:19<00:25,  4.24s/it][A
 94%|█████████▍| 83/88 [06:23<00:20,  4.11s/it][A
 95%|█████████▌| 84/88 [06:27<00:16,  4.04s/it][A
 97%|█████████▋| 85/88 [06:30<00:11,  3.72s/it][A
 98%|█████████▊| 86/88 [06:33<00:06,  3.46s/it][A
 99%|█████████▉| 87/88 [06:37<00:03,  3.65s/it][A
100%|██████████| 88/88 [06:41<00:00,  3.71s/it][A                                                        
                                               [A{'eval_loss': 17.517189025878906, 'eval_runtime': 405.2571, 'eval_samples_per_second': 6.909, 'eval_steps_per_second': 0.217, 'epoch': 5.08}
 80%|████████  | 4000/5000 [19:44:12<5:05:48, 18.35s/it]
100%|██████████| 88/88 [06:41<00:00,  3.71s/it][A
                                               [A2024-06-14 05:19:50,117 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-14 05:20:00,764 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 80%|████████  | 4001/5000 [19:44:44<39:57:41, 144.01s/it]                                                          {'loss': 18.5794, 'grad_norm': 12.4375, 'learning_rate': 7.3658700625590884e-06, 'epoch': 5.08}
 80%|████████  | 4001/5000 [19:44:44<39:57:41, 144.01s/it] 80%|████████  | 4002/5000 [19:45:08<29:54:06, 107.86s/it]                                                          {'loss': 18.1248, 'grad_norm': 12.8125, 'learning_rate': 7.351670042099216e-06, 'epoch': 5.08}
 80%|████████  | 4002/5000 [19:45:08<29:54:06, 107.86s/it] 80%|████████  | 4003/5000 [19:45:21<22:00:08, 79.45s/it]                                                          {'loss': 18.8708, 'grad_norm': 15.5625, 'learning_rate': 7.3374821159582095e-06, 'epoch': 5.08}
 80%|████████  | 4003/5000 [19:45:21<22:00:08, 79.45s/it] 80%|████████  | 4004/5000 [19:45:44<17:16:09, 62.42s/it]                                                         {'loss': 17.061, 'grad_norm': 8.3125, 'learning_rate': 7.323306290342321e-06, 'epoch': 5.08}
 80%|████████  | 4004/5000 [19:45:44<17:16:09, 62.42s/it] 80%|████████  | 4005/5000 [19:46:02<13:38:17, 49.34s/it]                                                         {'loss': 16.7798, 'grad_norm': 6.5625, 'learning_rate': 7.309142571452565e-06, 'epoch': 5.09}
 80%|████████  | 4005/5000 [19:46:02<13:38:17, 49.34s/it] 80%|████████  | 4006/5000 [19:46:23<11:14:10, 40.69s/it]                                                         {'loss': 17.2436, 'grad_norm': 8.5, 'learning_rate': 7.294990965484612e-06, 'epoch': 5.09}
 80%|████████  | 4006/5000 [19:46:23<11:14:10, 40.69s/it] 80%|████████  | 4007/5000 [19:46:38<9:08:31, 33.14s/it]                                                         {'loss': 16.7478, 'grad_norm': 9.6875, 'learning_rate': 7.2808514786288716e-06, 'epoch': 5.09}
 80%|████████  | 4007/5000 [19:46:38<9:08:31, 33.14s/it] 80%|████████  | 4008/5000 [19:47:01<8:15:04, 29.94s/it]                                                        {'loss': 18.0193, 'grad_norm': 12.0, 'learning_rate': 7.266724117070431e-06, 'epoch': 5.09}
 80%|████████  | 4008/5000 [19:47:01<8:15:04, 29.94s/it] 80%|████████  | 4009/5000 [19:47:18<7:08:30, 25.94s/it]                                                        {'loss': 18.2721, 'grad_norm': 9.9375, 'learning_rate': 7.252608886989065e-06, 'epoch': 5.09}
 80%|████████  | 4009/5000 [19:47:18<7:08:30, 25.94s/it] 80%|████████  | 4010/5000 [19:47:35<6:26:30, 23.42s/it]                                                        {'loss': 17.6746, 'grad_norm': 18.125, 'learning_rate': 7.2385057945592745e-06, 'epoch': 5.09}
 80%|████████  | 4010/5000 [19:47:35<6:26:30, 23.42s/it] 80%|████████  | 4011/5000 [19:47:52<5:55:21, 21.56s/it]                                                        {'loss': 16.397, 'grad_norm': 11.0, 'learning_rate': 7.2244148459502255e-06, 'epoch': 5.09}
 80%|████████  | 4011/5000 [19:47:52<5:55:21, 21.56s/it] 80%|████████  | 4012/5000 [19:48:08<5:27:38, 19.90s/it]                                                        {'loss': 17.0191, 'grad_norm': 10.75, 'learning_rate': 7.210336047325761e-06, 'epoch': 5.09}
 80%|████████  | 4012/5000 [19:48:08<5:27:38, 19.90s/it] 80%|████████  | 4013/5000 [19:48:24<5:06:57, 18.66s/it]                                                        {'loss': 18.3914, 'grad_norm': 24.5, 'learning_rate': 7.1962694048444465e-06, 'epoch': 5.1}
 80%|████████  | 4013/5000 [19:48:24<5:06:57, 18.66s/it] 80%|████████  | 4014/5000 [19:48:44<5:11:04, 18.93s/it]                                                        {'loss': 16.8426, 'grad_norm': 6.25, 'learning_rate': 7.182214924659507e-06, 'epoch': 5.1}
 80%|████████  | 4014/5000 [19:48:44<5:11:04, 18.93s/it] 80%|████████  | 4015/5000 [19:48:59<4:53:40, 17.89s/it]                                                        {'loss': 17.8556, 'grad_norm': 17.5, 'learning_rate': 7.1681726129188304e-06, 'epoch': 5.1}
 80%|████████  | 4015/5000 [19:48:59<4:53:40, 17.89s/it] 80%|████████  | 4016/5000 [19:49:15<4:44:23, 17.34s/it]                                                        {'loss': 18.7242, 'grad_norm': 106.0, 'learning_rate': 7.1541424757650265e-06, 'epoch': 5.1}
 80%|████████  | 4016/5000 [19:49:15<4:44:23, 17.34s/it] 80%|████████  | 4017/5000 [19:49:34<4:49:15, 17.66s/it]                                                        {'loss': 17.7658, 'grad_norm': 9.6875, 'learning_rate': 7.140124519335344e-06, 'epoch': 5.1}
 80%|████████  | 4017/5000 [19:49:34<4:49:15, 17.66s/it] 80%|████████  | 4018/5000 [19:49:53<4:56:28, 18.11s/it]                                                        {'loss': 16.8549, 'grad_norm': 11.0, 'learning_rate': 7.1261187497617016e-06, 'epoch': 5.1}
 80%|████████  | 4018/5000 [19:49:53<4:56:28, 18.11s/it] 80%|████████  | 4019/5000 [19:50:11<4:59:11, 18.30s/it]                                                        {'loss': 16.8967, 'grad_norm': 10.8125, 'learning_rate': 7.1121251731707184e-06, 'epoch': 5.1}
 80%|████████  | 4019/5000 [19:50:11<4:59:11, 18.30s/it] 80%|████████  | 4020/5000 [19:50:26<4:38:41, 17.06s/it]                                                        {'loss': 18.3395, 'grad_norm': 14.25, 'learning_rate': 7.098143795683648e-06, 'epoch': 5.1}
 80%|████████  | 4020/5000 [19:50:26<4:38:41, 17.06s/it] 80%|████████  | 4021/5000 [19:50:42<4:34:36, 16.83s/it]                                                        {'loss': 17.9892, 'grad_norm': 19.25, 'learning_rate': 7.084174623416427e-06, 'epoch': 5.11}
 80%|████████  | 4021/5000 [19:50:42<4:34:36, 16.83s/it] 80%|████████  | 4022/5000 [19:51:04<4:57:51, 18.27s/it]                                                        {'loss': 17.5653, 'grad_norm': 16.375, 'learning_rate': 7.0702176624796294e-06, 'epoch': 5.11}
 80%|████████  | 4022/5000 [19:51:04<4:57:51, 18.27s/it] 80%|████████  | 4023/5000 [19:51:30<5:36:42, 20.68s/it]                                                        {'loss': 17.8361, 'grad_norm': 14.125, 'learning_rate': 7.056272918978525e-06, 'epoch': 5.11}
 80%|████████  | 4023/5000 [19:51:30<5:36:42, 20.68s/it] 80%|████████  | 4024/5000 [19:51:50<5:34:21, 20.56s/it]                                                        {'loss': 18.4958, 'grad_norm': 12.4375, 'learning_rate': 7.042340399013012e-06, 'epoch': 5.11}
 80%|████████  | 4024/5000 [19:51:50<5:34:21, 20.56s/it] 80%|████████  | 4025/5000 [19:52:03<4:57:49, 18.33s/it]                                                        {'loss': 18.7235, 'grad_norm': 18.125, 'learning_rate': 7.028420108677635e-06, 'epoch': 5.11}
 80%|████████  | 4025/5000 [19:52:03<4:57:49, 18.33s/it] 81%|████████  | 4026/5000 [19:52:19<4:42:45, 17.42s/it]                                                        {'loss': 18.0486, 'grad_norm': 14.0625, 'learning_rate': 7.014512054061621e-06, 'epoch': 5.11}
 81%|████████  | 4026/5000 [19:52:19<4:42:45, 17.42s/it] 81%|████████  | 4027/5000 [19:52:37<4:47:30, 17.73s/it]                                                        {'loss': 17.3296, 'grad_norm': 8.375, 'learning_rate': 7.000616241248812e-06, 'epoch': 5.11}
 81%|████████  | 4027/5000 [19:52:37<4:47:30, 17.73s/it] 81%|████████  | 4028/5000 [19:52:53<4:40:16, 17.30s/it]                                                        {'loss': 21.153, 'grad_norm': 17.375, 'learning_rate': 6.98673267631772e-06, 'epoch': 5.11}
 81%|████████  | 4028/5000 [19:52:53<4:40:16, 17.30s/it] 81%|████████  | 4029/5000 [19:53:16<5:05:47, 18.90s/it]                                                        {'loss': 19.0747, 'grad_norm': 19.875, 'learning_rate': 6.97286136534147e-06, 'epoch': 5.12}
 81%|████████  | 4029/5000 [19:53:16<5:05:47, 18.90s/it] 81%|████████  | 4030/5000 [19:53:37<5:13:47, 19.41s/it]                                                        {'loss': 16.8847, 'grad_norm': 7.84375, 'learning_rate': 6.959002314387868e-06, 'epoch': 5.12}
 81%|████████  | 4030/5000 [19:53:37<5:13:47, 19.41s/it] 81%|████████  | 4031/5000 [19:53:52<4:56:42, 18.37s/it]                                                        {'loss': 18.6699, 'grad_norm': 12.75, 'learning_rate': 6.945155529519313e-06, 'epoch': 5.12}
 81%|████████  | 4031/5000 [19:53:52<4:56:42, 18.37s/it] 81%|████████  | 4032/5000 [19:54:08<4:41:53, 17.47s/it]                                                        {'loss': 19.256, 'grad_norm': 27.125, 'learning_rate': 6.9313210167928805e-06, 'epoch': 5.12}
 81%|████████  | 4032/5000 [19:54:08<4:41:53, 17.47s/it] 81%|████████  | 4033/5000 [19:54:25<4:41:31, 17.47s/it]                                                        {'loss': 18.0559, 'grad_norm': 12.6875, 'learning_rate': 6.917498782260241e-06, 'epoch': 5.12}
 81%|████████  | 4033/5000 [19:54:25<4:41:31, 17.47s/it] 81%|████████  | 4034/5000 [19:54:52<5:24:51, 20.18s/it]                                                        {'loss': 18.3633, 'grad_norm': 11.125, 'learning_rate': 6.903688831967705e-06, 'epoch': 5.12}
 81%|████████  | 4034/5000 [19:54:52<5:24:51, 20.18s/it] 81%|████████  | 4035/5000 [19:55:05<4:53:11, 18.23s/it]                                                        {'loss': 19.0227, 'grad_norm': 12.5, 'learning_rate': 6.88989117195623e-06, 'epoch': 5.12}
 81%|████████  | 4035/5000 [19:55:05<4:53:11, 18.23s/it] 81%|████████  | 4036/5000 [19:55:18<4:27:24, 16.64s/it]                                                        {'loss': 18.9459, 'grad_norm': 13.0625, 'learning_rate': 6.876105808261371e-06, 'epoch': 5.13}
 81%|████████  | 4036/5000 [19:55:18<4:27:24, 16.64s/it] 81%|████████  | 4037/5000 [19:55:36<4:29:09, 16.77s/it]                                                        {'loss': 17.9709, 'grad_norm': 13.9375, 'learning_rate': 6.86233274691331e-06, 'epoch': 5.13}
 81%|████████  | 4037/5000 [19:55:36<4:29:09, 16.77s/it] 81%|████████  | 4038/5000 [19:55:52<4:27:04, 16.66s/it]                                                        {'loss': 17.3688, 'grad_norm': 9.8125, 'learning_rate': 6.848571993936867e-06, 'epoch': 5.13}
 81%|████████  | 4038/5000 [19:55:52<4:27:04, 16.66s/it] 81%|████████  | 4039/5000 [19:56:07<4:21:37, 16.33s/it]                                                        {'loss': 17.2774, 'grad_norm': 7.8125, 'learning_rate': 6.834823555351451e-06, 'epoch': 5.13}
 81%|████████  | 4039/5000 [19:56:07<4:21:37, 16.33s/it] 81%|████████  | 4040/5000 [19:56:23<4:18:55, 16.18s/it]                                                        {'loss': 19.5355, 'grad_norm': 14.0, 'learning_rate': 6.821087437171103e-06, 'epoch': 5.13}
 81%|████████  | 4040/5000 [19:56:23<4:18:55, 16.18s/it] 81%|████████  | 4041/5000 [19:56:40<4:22:35, 16.43s/it]                                                        {'loss': 18.2439, 'grad_norm': 13.5625, 'learning_rate': 6.8073636454044536e-06, 'epoch': 5.13}
 81%|████████  | 4041/5000 [19:56:40<4:22:35, 16.43s/it] 81%|████████  | 4042/5000 [19:57:00<4:36:50, 17.34s/it]                                                        {'loss': 18.4055, 'grad_norm': 12.625, 'learning_rate': 6.793652186054776e-06, 'epoch': 5.13}
 81%|████████  | 4042/5000 [19:57:00<4:36:50, 17.34s/it] 81%|████████  | 4043/5000 [19:57:28<5:26:28, 20.47s/it]                                                        {'loss': 18.5911, 'grad_norm': 15.4375, 'learning_rate': 6.779953065119918e-06, 'epoch': 5.13}
 81%|████████  | 4043/5000 [19:57:28<5:26:28, 20.47s/it] 81%|████████  | 4044/5000 [19:57:44<5:06:00, 19.21s/it]                                                        {'loss': 18.31, 'grad_norm': 10.6875, 'learning_rate': 6.766266288592329e-06, 'epoch': 5.14}
 81%|████████  | 4044/5000 [19:57:44<5:06:00, 19.21s/it] 81%|████████  | 4045/5000 [19:58:00<4:50:33, 18.26s/it]                                                        {'loss': 17.1502, 'grad_norm': 14.8125, 'learning_rate': 6.752591862459095e-06, 'epoch': 5.14}
 81%|████████  | 4045/5000 [19:58:00<4:50:33, 18.26s/it] 81%|████████  | 4046/5000 [19:58:24<5:20:24, 20.15s/it]                                                        {'loss': 16.8356, 'grad_norm': 7.3125, 'learning_rate': 6.73892979270186e-06, 'epoch': 5.14}
 81%|████████  | 4046/5000 [19:58:24<5:20:24, 20.15s/it] 81%|████████  | 4047/5000 [19:58:42<5:06:17, 19.28s/it]                                                        {'loss': 18.5698, 'grad_norm': 11.3125, 'learning_rate': 6.7252800852968676e-06, 'epoch': 5.14}
 81%|████████  | 4047/5000 [19:58:42<5:06:17, 19.28s/it] 81%|████████  | 4048/5000 [19:58:57<4:45:04, 17.97s/it]                                                        {'loss': 19.4912, 'grad_norm': 12.4375, 'learning_rate': 6.7116427462149855e-06, 'epoch': 5.14}
 81%|████████  | 4048/5000 [19:58:57<4:45:04, 17.97s/it] 81%|████████  | 4049/5000 [19:59:24<5:28:32, 20.73s/it]                                                        {'loss': 16.9745, 'grad_norm': 10.1875, 'learning_rate': 6.69801778142164e-06, 'epoch': 5.14}
 81%|████████  | 4049/5000 [19:59:24<5:28:32, 20.73s/it] 81%|████████  | 4050/5000 [19:59:40<5:05:23, 19.29s/it]                                                        {'loss': 17.9287, 'grad_norm': 11.375, 'learning_rate': 6.684405196876842e-06, 'epoch': 5.14}
 81%|████████  | 4050/5000 [19:59:40<5:05:23, 19.29s/it] 81%|████████  | 4051/5000 [19:59:56<4:51:17, 18.42s/it]                                                        {'loss': 17.6602, 'grad_norm': 11.8125, 'learning_rate': 6.670804998535217e-06, 'epoch': 5.14}
 81%|████████  | 4051/5000 [19:59:56<4:51:17, 18.42s/it] 81%|████████  | 4052/5000 [20:00:09<4:26:43, 16.88s/it]                                                        {'loss': 18.4987, 'grad_norm': 14.6875, 'learning_rate': 6.657217192345936e-06, 'epoch': 5.15}
 81%|████████  | 4052/5000 [20:00:09<4:26:43, 16.88s/it] 81%|████████  | 4053/5000 [20:00:28<4:34:58, 17.42s/it]                                                        {'loss': 17.5258, 'grad_norm': 22.5, 'learning_rate': 6.643641784252787e-06, 'epoch': 5.15}
 81%|████████  | 4053/5000 [20:00:28<4:34:58, 17.42s/it] 81%|████████  | 4054/5000 [20:00:55<5:19:24, 20.26s/it]                                                        {'loss': 17.4461, 'grad_norm': 27.625, 'learning_rate': 6.630078780194094e-06, 'epoch': 5.15}
 81%|████████  | 4054/5000 [20:00:55<5:19:24, 20.26s/it] 81%|████████  | 4055/5000 [20:01:21<5:48:46, 22.14s/it]                                                        {'loss': 15.9754, 'grad_norm': 9.5, 'learning_rate': 6.616528186102799e-06, 'epoch': 5.15}
 81%|████████  | 4055/5000 [20:01:21<5:48:46, 22.14s/it] 81%|████████  | 4056/5000 [20:01:46<6:00:16, 22.90s/it]                                                        {'loss': 18.2333, 'grad_norm': 56.75, 'learning_rate': 6.602990007906375e-06, 'epoch': 5.15}
 81%|████████  | 4056/5000 [20:01:46<6:00:16, 22.90s/it] 81%|████████  | 4057/5000 [20:02:06<5:44:25, 21.91s/it]                                                        {'loss': 17.8445, 'grad_norm': 13.875, 'learning_rate': 6.589464251526878e-06, 'epoch': 5.15}
 81%|████████  | 4057/5000 [20:02:06<5:44:25, 21.91s/it] 81%|████████  | 4058/5000 [20:02:31<6:00:59, 22.99s/it]                                                        {'loss': 17.7598, 'grad_norm': 11.25, 'learning_rate': 6.575950922880948e-06, 'epoch': 5.15}
 81%|████████  | 4058/5000 [20:02:31<6:00:59, 22.99s/it] 81%|████████  | 4059/5000 [20:03:01<6:32:17, 25.01s/it]                                                        {'loss': 16.1741, 'grad_norm': 11.8125, 'learning_rate': 6.562450027879767e-06, 'epoch': 5.15}
 81%|████████  | 4059/5000 [20:03:01<6:32:17, 25.01s/it] 81%|████████  | 4060/5000 [20:03:15<5:40:00, 21.70s/it]                                                        {'loss': 18.2004, 'grad_norm': 12.125, 'learning_rate': 6.548961572429076e-06, 'epoch': 5.16}
 81%|████████  | 4060/5000 [20:03:15<5:40:00, 21.70s/it] 81%|████████  | 4061/5000 [20:03:34<5:28:09, 20.97s/it]                                                        {'loss': 16.8176, 'grad_norm': 11.3125, 'learning_rate': 6.5354855624292e-06, 'epoch': 5.16}
 81%|████████  | 4061/5000 [20:03:34<5:28:09, 20.97s/it] 81%|████████  | 4062/5000 [20:03:52<5:11:25, 19.92s/it]                                                        {'loss': 17.2747, 'grad_norm': 16.625, 'learning_rate': 6.522022003775e-06, 'epoch': 5.16}
 81%|████████  | 4062/5000 [20:03:52<5:11:25, 19.92s/it] 81%|████████▏ | 4063/5000 [20:04:12<5:14:25, 20.13s/it]                                                        {'loss': 17.9881, 'grad_norm': 9.375, 'learning_rate': 6.508570902355879e-06, 'epoch': 5.16}
 81%|████████▏ | 4063/5000 [20:04:12<5:14:25, 20.13s/it] 81%|████████▏ | 4064/5000 [20:04:31<5:07:45, 19.73s/it]                                                        {'loss': 18.7543, 'grad_norm': 11.9375, 'learning_rate': 6.495132264055827e-06, 'epoch': 5.16}
 81%|████████▏ | 4064/5000 [20:04:31<5:07:45, 19.73s/it] 81%|████████▏ | 4065/5000 [20:04:46<4:46:54, 18.41s/it]                                                        {'loss': 17.3122, 'grad_norm': 11.0, 'learning_rate': 6.481706094753349e-06, 'epoch': 5.16}
 81%|████████▏ | 4065/5000 [20:04:46<4:46:54, 18.41s/it] 81%|████████▏ | 4066/5000 [20:05:15<5:31:54, 21.32s/it]                                                        {'loss': 17.5713, 'grad_norm': 13.5625, 'learning_rate': 6.4682924003215015e-06, 'epoch': 5.16}
 81%|████████▏ | 4066/5000 [20:05:15<5:31:54, 21.32s/it] 81%|████████▏ | 4067/5000 [20:05:34<5:24:00, 20.84s/it]                                                        {'loss': 16.7411, 'grad_norm': 13.8125, 'learning_rate': 6.454891186627907e-06, 'epoch': 5.16}
 81%|████████▏ | 4067/5000 [20:05:34<5:24:00, 20.84s/it] 81%|████████▏ | 4068/5000 [20:05:49<4:53:30, 18.90s/it]                                                        {'loss': 17.2919, 'grad_norm': 8.875, 'learning_rate': 6.4415024595347e-06, 'epoch': 5.17}
 81%|████████▏ | 4068/5000 [20:05:49<4:53:30, 18.90s/it] 81%|████████▏ | 4069/5000 [20:06:13<5:20:25, 20.65s/it]                                                        {'loss': 16.3789, 'grad_norm': 9.0, 'learning_rate': 6.428126224898562e-06, 'epoch': 5.17}
 81%|████████▏ | 4069/5000 [20:06:13<5:20:25, 20.65s/it] 81%|████████▏ | 4070/5000 [20:06:40<5:47:37, 22.43s/it]                                                        {'loss': 16.7936, 'grad_norm': 10.875, 'learning_rate': 6.414762488570726e-06, 'epoch': 5.17}
 81%|████████▏ | 4070/5000 [20:06:40<5:47:37, 22.43s/it] 81%|████████▏ | 4071/5000 [20:06:58<5:27:17, 21.14s/it]                                                        {'loss': 17.4647, 'grad_norm': 8.125, 'learning_rate': 6.401411256396935e-06, 'epoch': 5.17}
 81%|████████▏ | 4071/5000 [20:06:58<5:27:17, 21.14s/it] 81%|████████▏ | 4072/5000 [20:07:17<5:16:14, 20.45s/it]                                                        {'loss': 17.431, 'grad_norm': 13.1875, 'learning_rate': 6.388072534217462e-06, 'epoch': 5.17}
 81%|████████▏ | 4072/5000 [20:07:17<5:16:14, 20.45s/it] 81%|████████▏ | 4073/5000 [20:07:37<5:13:03, 20.26s/it]                                                        {'loss': 17.3018, 'grad_norm': 8.8125, 'learning_rate': 6.374746327867141e-06, 'epoch': 5.17}
 81%|████████▏ | 4073/5000 [20:07:37<5:13:03, 20.26s/it] 81%|████████▏ | 4074/5000 [20:07:50<4:38:17, 18.03s/it]                                                        {'loss': 18.0046, 'grad_norm': 14.0625, 'learning_rate': 6.361432643175292e-06, 'epoch': 5.17}
 81%|████████▏ | 4074/5000 [20:07:50<4:38:17, 18.03s/it] 82%|████████▏ | 4075/5000 [20:08:09<4:42:59, 18.36s/it]                                                        {'loss': 17.2727, 'grad_norm': 7.46875, 'learning_rate': 6.3481314859657675e-06, 'epoch': 5.17}
 82%|████████▏ | 4075/5000 [20:08:09<4:42:59, 18.36s/it] 82%|████████▏ | 4076/5000 [20:08:32<5:05:31, 19.84s/it]                                                        {'loss': 18.5904, 'grad_norm': 12.25, 'learning_rate': 6.334842862056959e-06, 'epoch': 5.18}
 82%|████████▏ | 4076/5000 [20:08:32<5:05:31, 19.84s/it] 82%|████████▏ | 4077/5000 [20:08:57<5:27:03, 21.26s/it]                                                        {'loss': 17.7764, 'grad_norm': 11.9375, 'learning_rate': 6.321566777261748e-06, 'epoch': 5.18}
 82%|████████▏ | 4077/5000 [20:08:57<5:27:03, 21.26s/it] 82%|████████▏ | 4078/5000 [20:09:21<5:42:42, 22.30s/it]                                                        {'loss': 20.2816, 'grad_norm': 22.875, 'learning_rate': 6.308303237387565e-06, 'epoch': 5.18}
 82%|████████▏ | 4078/5000 [20:09:21<5:42:42, 22.30s/it] 82%|████████▏ | 4079/5000 [20:09:35<5:01:23, 19.63s/it]                                                        {'loss': 18.0903, 'grad_norm': 10.5, 'learning_rate': 6.295052248236314e-06, 'epoch': 5.18}
 82%|████████▏ | 4079/5000 [20:09:35<5:01:23, 19.63s/it] 82%|████████▏ | 4080/5000 [20:09:51<4:46:19, 18.67s/it]                                                        {'loss': 17.13, 'grad_norm': 10.25, 'learning_rate': 6.281813815604443e-06, 'epoch': 5.18}
 82%|████████▏ | 4080/5000 [20:09:51<4:46:19, 18.67s/it] 82%|████████▏ | 4081/5000 [20:10:04<4:21:05, 17.05s/it]                                                        {'loss': 17.6681, 'grad_norm': 10.1875, 'learning_rate': 6.268587945282883e-06, 'epoch': 5.18}
 82%|████████▏ | 4081/5000 [20:10:04<4:21:05, 17.05s/it] 82%|████████▏ | 4082/5000 [20:10:29<4:54:25, 19.24s/it]                                                        {'loss': 19.2957, 'grad_norm': 12.375, 'learning_rate': 6.255374643057072e-06, 'epoch': 5.18}
 82%|████████▏ | 4082/5000 [20:10:29<4:54:25, 19.24s/it] 82%|████████▏ | 4083/5000 [20:10:44<4:35:00, 17.99s/it]                                                        {'loss': 16.8766, 'grad_norm': 10.0, 'learning_rate': 6.242173914706974e-06, 'epoch': 5.18}
 82%|████████▏ | 4083/5000 [20:10:44<4:35:00, 17.99s/it] 82%|████████▏ | 4084/5000 [20:11:01<4:30:16, 17.70s/it]                                                        {'loss': 17.2783, 'grad_norm': 10.5, 'learning_rate': 6.228985766007027e-06, 'epoch': 5.19}
 82%|████████▏ | 4084/5000 [20:11:01<4:30:16, 17.70s/it] 82%|████████▏ | 4085/5000 [20:11:20<4:34:57, 18.03s/it]                                                        {'loss': 17.8354, 'grad_norm': 9.625, 'learning_rate': 6.215810202726168e-06, 'epoch': 5.19}
 82%|████████▏ | 4085/5000 [20:11:20<4:34:57, 18.03s/it] 82%|████████▏ | 4086/5000 [20:11:34<4:16:32, 16.84s/it]                                                        {'loss': 18.7933, 'grad_norm': 15.9375, 'learning_rate': 6.2026472306278565e-06, 'epoch': 5.19}
 82%|████████▏ | 4086/5000 [20:11:34<4:16:32, 16.84s/it] 82%|████████▏ | 4087/5000 [20:11:49<4:11:28, 16.53s/it]                                                        {'loss': 17.8428, 'grad_norm': 11.0, 'learning_rate': 6.189496855470007e-06, 'epoch': 5.19}
 82%|████████▏ | 4087/5000 [20:11:49<4:11:28, 16.53s/it] 82%|████████▏ | 4088/5000 [20:12:07<4:17:34, 16.95s/it]                                                        {'loss': 17.3293, 'grad_norm': 10.375, 'learning_rate': 6.176359083005039e-06, 'epoch': 5.19}
 82%|████████▏ | 4088/5000 [20:12:07<4:17:34, 16.95s/it] 82%|████████▏ | 4089/5000 [20:12:22<4:07:10, 16.28s/it]                                                        {'loss': 17.9483, 'grad_norm': 9.5, 'learning_rate': 6.163233918979871e-06, 'epoch': 5.19}
 82%|████████▏ | 4089/5000 [20:12:22<4:07:10, 16.28s/it] 82%|████████▏ | 4090/5000 [20:12:35<3:53:16, 15.38s/it]                                                        {'loss': 18.3913, 'grad_norm': 10.9375, 'learning_rate': 6.150121369135895e-06, 'epoch': 5.19}
 82%|████████▏ | 4090/5000 [20:12:35<3:53:16, 15.38s/it] 82%|████████▏ | 4091/5000 [20:12:56<4:14:42, 16.81s/it]                                                        {'loss': 27.038, 'grad_norm': 68.5, 'learning_rate': 6.137021439208979e-06, 'epoch': 5.19}
 82%|████████▏ | 4091/5000 [20:12:56<4:14:42, 16.81s/it] 82%|████████▏ | 4092/5000 [20:13:14<4:19:53, 17.17s/it]                                                        {'loss': 17.101, 'grad_norm': 9.5, 'learning_rate': 6.123934134929471e-06, 'epoch': 5.2}
 82%|████████▏ | 4092/5000 [20:13:14<4:19:53, 17.17s/it] 82%|████████▏ | 4093/5000 [20:13:29<4:12:16, 16.69s/it]                                                        {'loss': 16.9754, 'grad_norm': 10.1875, 'learning_rate': 6.110859462022221e-06, 'epoch': 5.2}
 82%|████████▏ | 4093/5000 [20:13:29<4:12:16, 16.69s/it] 82%|████████▏ | 4094/5000 [20:13:46<4:14:24, 16.85s/it]                                                        {'loss': 17.5525, 'grad_norm': 9.6875, 'learning_rate': 6.097797426206528e-06, 'epoch': 5.2}
 82%|████████▏ | 4094/5000 [20:13:46<4:14:24, 16.85s/it] 82%|████████▏ | 4095/5000 [20:14:00<3:58:02, 15.78s/it]                                                        {'loss': 18.3563, 'grad_norm': 15.0, 'learning_rate': 6.084748033196156e-06, 'epoch': 5.2}
 82%|████████▏ | 4095/5000 [20:14:00<3:58:02, 15.78s/it] 82%|████████▏ | 4096/5000 [20:14:17<4:03:16, 16.15s/it]                                                        {'loss': 17.7553, 'grad_norm': 9.5, 'learning_rate': 6.071711288699379e-06, 'epoch': 5.2}
 82%|████████▏ | 4096/5000 [20:14:17<4:03:16, 16.15s/it] 82%|████████▏ | 4097/5000 [20:14:31<3:53:58, 15.55s/it]                                                        {'loss': 17.4154, 'grad_norm': 13.4375, 'learning_rate': 6.0586871984188905e-06, 'epoch': 5.2}
 82%|████████▏ | 4097/5000 [20:14:31<3:53:58, 15.55s/it] 82%|████████▏ | 4098/5000 [20:14:45<3:45:19, 14.99s/it]                                                        {'loss': 19.1109, 'grad_norm': 12.25, 'learning_rate': 6.045675768051875e-06, 'epoch': 5.2}
 82%|████████▏ | 4098/5000 [20:14:45<3:45:19, 14.99s/it] 82%|████████▏ | 4099/5000 [20:15:02<3:56:18, 15.74s/it]                                                        {'loss': 17.085, 'grad_norm': 11.125, 'learning_rate': 6.032677003289982e-06, 'epoch': 5.21}
 82%|████████▏ | 4099/5000 [20:15:02<3:56:18, 15.74s/it] 82%|████████▏ | 4100/5000 [20:15:15<3:45:44, 15.05s/it]                                                        {'loss': 18.4941, 'grad_norm': 10.0, 'learning_rate': 6.019690909819298e-06, 'epoch': 5.21}
 82%|████████▏ | 4100/5000 [20:15:15<3:45:44, 15.05s/it] 82%|████████▏ | 4101/5000 [20:15:31<3:48:56, 15.28s/it]                                                        {'loss': 19.0284, 'grad_norm': 15.1875, 'learning_rate': 6.006717493320399e-06, 'epoch': 5.21}
 82%|████████▏ | 4101/5000 [20:15:31<3:48:56, 15.28s/it] 82%|████████▏ | 4102/5000 [20:15:46<3:44:55, 15.03s/it]                                                        {'loss': 18.3075, 'grad_norm': 16.625, 'learning_rate': 5.993756759468285e-06, 'epoch': 5.21}
 82%|████████▏ | 4102/5000 [20:15:46<3:44:55, 15.03s/it] 82%|████████▏ | 4103/5000 [20:16:01<3:43:56, 14.98s/it]                                                        {'loss': 18.8655, 'grad_norm': 28.25, 'learning_rate': 5.980808713932431e-06, 'epoch': 5.21}
 82%|████████▏ | 4103/5000 [20:16:01<3:43:56, 14.98s/it] 82%|████████▏ | 4104/5000 [20:16:14<3:38:37, 14.64s/it]                                                        {'loss': 19.3679, 'grad_norm': 16.75, 'learning_rate': 5.967873362376742e-06, 'epoch': 5.21}
 82%|████████▏ | 4104/5000 [20:16:14<3:38:37, 14.64s/it] 82%|████████▏ | 4105/5000 [20:16:30<3:44:48, 15.07s/it]                                                        {'loss': 16.4338, 'grad_norm': 11.0, 'learning_rate': 5.9549507104595915e-06, 'epoch': 5.21}
 82%|████████▏ | 4105/5000 [20:16:30<3:44:48, 15.07s/it] 82%|████████▏ | 4106/5000 [20:16:47<3:53:04, 15.64s/it]                                                        {'loss': 18.6267, 'grad_norm': 13.375, 'learning_rate': 5.942040763833783e-06, 'epoch': 5.21}
 82%|████████▏ | 4106/5000 [20:16:47<3:53:04, 15.64s/it] 82%|████████▏ | 4107/5000 [20:17:12<4:30:42, 18.19s/it]                                                        {'loss': 18.2015, 'grad_norm': 16.5, 'learning_rate': 5.929143528146552e-06, 'epoch': 5.22}
 82%|████████▏ | 4107/5000 [20:17:12<4:30:42, 18.19s/it] 82%|████████▏ | 4108/5000 [20:17:28<4:24:05, 17.76s/it]                                                        {'loss': 17.8821, 'grad_norm': 10.125, 'learning_rate': 5.916259009039611e-06, 'epoch': 5.22}
 82%|████████▏ | 4108/5000 [20:17:28<4:24:05, 17.76s/it] 82%|████████▏ | 4109/5000 [20:17:48<4:33:29, 18.42s/it]                                                        {'loss': 18.0496, 'grad_norm': 10.9375, 'learning_rate': 5.903387212149069e-06, 'epoch': 5.22}
 82%|████████▏ | 4109/5000 [20:17:48<4:33:29, 18.42s/it] 82%|████████▏ | 4110/5000 [20:18:04<4:19:08, 17.47s/it]                                                        {'loss': 17.6101, 'grad_norm': 11.5, 'learning_rate': 5.890528143105497e-06, 'epoch': 5.22}
 82%|████████▏ | 4110/5000 [20:18:04<4:19:08, 17.47s/it] 82%|████████▏ | 4111/5000 [20:18:19<4:08:24, 16.77s/it]                                                        {'loss': 17.4524, 'grad_norm': 14.8125, 'learning_rate': 5.877681807533872e-06, 'epoch': 5.22}
 82%|████████▏ | 4111/5000 [20:18:19<4:08:24, 16.77s/it] 82%|████████▏ | 4112/5000 [20:18:33<3:55:54, 15.94s/it]                                                        {'loss': 18.3973, 'grad_norm': 17.25, 'learning_rate': 5.8648482110536416e-06, 'epoch': 5.22}
 82%|████████▏ | 4112/5000 [20:18:33<3:55:54, 15.94s/it] 82%|████████▏ | 4113/5000 [20:18:49<3:59:22, 16.19s/it]                                                        {'loss': 16.1807, 'grad_norm': 8.375, 'learning_rate': 5.852027359278645e-06, 'epoch': 5.22}
 82%|████████▏ | 4113/5000 [20:18:49<3:59:22, 16.19s/it] 82%|████████▏ | 4114/5000 [20:19:09<4:14:31, 17.24s/it]                                                        {'loss': 17.9734, 'grad_norm': 11.25, 'learning_rate': 5.8392192578171486e-06, 'epoch': 5.22}
 82%|████████▏ | 4114/5000 [20:19:09<4:14:31, 17.24s/it] 82%|████████▏ | 4115/5000 [20:19:35<4:53:07, 19.87s/it]                                                        {'loss': 16.1618, 'grad_norm': 10.0, 'learning_rate': 5.826423912271872e-06, 'epoch': 5.23}
 82%|████████▏ | 4115/5000 [20:19:35<4:53:07, 19.87s/it] 82%|████████▏ | 4116/5000 [20:19:51<4:33:51, 18.59s/it]                                                        {'loss': 17.5308, 'grad_norm': 10.1875, 'learning_rate': 5.813641328239922e-06, 'epoch': 5.23}
 82%|████████▏ | 4116/5000 [20:19:51<4:33:51, 18.59s/it] 82%|████████▏ | 4117/5000 [20:20:07<4:23:32, 17.91s/it]                                                        {'loss': 18.3095, 'grad_norm': 11.5, 'learning_rate': 5.800871511312823e-06, 'epoch': 5.23}
 82%|████████▏ | 4117/5000 [20:20:07<4:23:32, 17.91s/it] 82%|████████▏ | 4118/5000 [20:20:32<4:52:38, 19.91s/it]                                                        {'loss': 18.6351, 'grad_norm': 13.875, 'learning_rate': 5.788114467076554e-06, 'epoch': 5.23}
 82%|████████▏ | 4118/5000 [20:20:32<4:52:38, 19.91s/it] 82%|████████▏ | 4119/5000 [20:20:49<4:40:14, 19.09s/it]                                                        {'loss': 18.2046, 'grad_norm': 45.0, 'learning_rate': 5.7753702011114615e-06, 'epoch': 5.23}
 82%|████████▏ | 4119/5000 [20:20:49<4:40:14, 19.09s/it] 82%|████████▏ | 4120/5000 [20:21:06<4:29:21, 18.37s/it]                                                        {'loss': 17.4234, 'grad_norm': 10.25, 'learning_rate': 5.762638718992318e-06, 'epoch': 5.23}
 82%|████████▏ | 4120/5000 [20:21:06<4:29:21, 18.37s/it] 82%|████████▏ | 4121/5000 [20:21:21<4:14:14, 17.35s/it]                                                        {'loss': 19.1378, 'grad_norm': 11.5, 'learning_rate': 5.749920026288322e-06, 'epoch': 5.23}
 82%|████████▏ | 4121/5000 [20:21:21<4:14:14, 17.35s/it] 82%|████████▏ | 4122/5000 [20:21:36<4:05:22, 16.77s/it]                                                        {'loss': 17.8359, 'grad_norm': 14.1875, 'learning_rate': 5.7372141285630545e-06, 'epoch': 5.23}
 82%|████████▏ | 4122/5000 [20:21:36<4:05:22, 16.77s/it] 82%|████████▏ | 4123/5000 [20:21:52<4:03:18, 16.65s/it]                                                        {'loss': 18.3712, 'grad_norm': 17.875, 'learning_rate': 5.724521031374492e-06, 'epoch': 5.24}
 82%|████████▏ | 4123/5000 [20:21:52<4:03:18, 16.65s/it] 82%|████████▏ | 4124/5000 [20:22:08<4:00:31, 16.47s/it]                                                        {'loss': 19.2676, 'grad_norm': 16.25, 'learning_rate': 5.7118407402750495e-06, 'epoch': 5.24}
 82%|████████▏ | 4124/5000 [20:22:08<4:00:31, 16.47s/it] 82%|████████▎ | 4125/5000 [20:22:24<3:54:57, 16.11s/it]                                                        {'loss': 17.7651, 'grad_norm': 11.0625, 'learning_rate': 5.6991732608115e-06, 'epoch': 5.24}
 82%|████████▎ | 4125/5000 [20:22:24<3:54:57, 16.11s/it] 83%|████████▎ | 4126/5000 [20:22:39<3:51:04, 15.86s/it]                                                        {'loss': 18.1954, 'grad_norm': 12.625, 'learning_rate': 5.6865185985250414e-06, 'epoch': 5.24}
 83%|████████▎ | 4126/5000 [20:22:39<3:51:04, 15.86s/it] 83%|████████▎ | 4127/5000 [20:23:05<4:37:34, 19.08s/it]                                                        {'loss': 17.5398, 'grad_norm': 10.375, 'learning_rate': 5.673876758951243e-06, 'epoch': 5.24}
 83%|████████▎ | 4127/5000 [20:23:05<4:37:34, 19.08s/it] 83%|████████▎ | 4128/5000 [20:23:20<4:16:38, 17.66s/it]                                                        {'loss': 18.0908, 'grad_norm': 10.5, 'learning_rate': 5.661247747620085e-06, 'epoch': 5.24}
 83%|████████▎ | 4128/5000 [20:23:20<4:16:38, 17.66s/it] 83%|████████▎ | 4129/5000 [20:23:35<4:05:28, 16.91s/it]                                                        {'loss': 18.4151, 'grad_norm': 9.625, 'learning_rate': 5.648631570055921e-06, 'epoch': 5.24}
 83%|████████▎ | 4129/5000 [20:23:35<4:05:28, 16.91s/it] 83%|████████▎ | 4130/5000 [20:23:55<4:16:39, 17.70s/it]                                                        {'loss': 16.8344, 'grad_norm': 8.0625, 'learning_rate': 5.636028231777489e-06, 'epoch': 5.24}
 83%|████████▎ | 4130/5000 [20:23:55<4:16:39, 17.70s/it] 83%|████████▎ | 4131/5000 [20:24:13<4:20:59, 18.02s/it]                                                        {'loss': 17.9476, 'grad_norm': 10.4375, 'learning_rate': 5.6234377382979325e-06, 'epoch': 5.25}
 83%|████████▎ | 4131/5000 [20:24:13<4:20:59, 18.02s/it] 83%|████████▎ | 4132/5000 [20:24:37<4:46:18, 19.79s/it]                                                        {'loss': 17.1985, 'grad_norm': 8.875, 'learning_rate': 5.610860095124752e-06, 'epoch': 5.25}
 83%|████████▎ | 4132/5000 [20:24:37<4:46:18, 19.79s/it] 83%|████████▎ | 4133/5000 [20:24:54<4:34:07, 18.97s/it]                                                        {'loss': 17.3314, 'grad_norm': 15.5, 'learning_rate': 5.598295307759826e-06, 'epoch': 5.25}
 83%|████████▎ | 4133/5000 [20:24:54<4:34:07, 18.97s/it] 83%|████████▎ | 4134/5000 [20:25:10<4:21:51, 18.14s/it]                                                        {'loss': 17.7615, 'grad_norm': 8.6875, 'learning_rate': 5.5857433816994394e-06, 'epoch': 5.25}
 83%|████████▎ | 4134/5000 [20:25:10<4:21:51, 18.14s/it] 83%|████████▎ | 4135/5000 [20:25:31<4:32:47, 18.92s/it]                                                        {'loss': 17.7513, 'grad_norm': 14.0625, 'learning_rate': 5.5732043224342235e-06, 'epoch': 5.25}
 83%|████████▎ | 4135/5000 [20:25:31<4:32:47, 18.92s/it] 83%|████████▎ | 4136/5000 [20:25:49<4:27:08, 18.55s/it]                                                        {'loss': 16.4423, 'grad_norm': 7.5, 'learning_rate': 5.560678135449174e-06, 'epoch': 5.25}
 83%|████████▎ | 4136/5000 [20:25:49<4:27:08, 18.55s/it] 83%|████████▎ | 4137/5000 [20:26:05<4:16:45, 17.85s/it]                                                        {'loss': 18.0972, 'grad_norm': 9.125, 'learning_rate': 5.5481648262236935e-06, 'epoch': 5.25}
 83%|████████▎ | 4137/5000 [20:26:05<4:16:45, 17.85s/it] 83%|████████▎ | 4138/5000 [20:26:19<3:57:47, 16.55s/it]                                                        {'loss': 18.291, 'grad_norm': 13.6875, 'learning_rate': 5.535664400231522e-06, 'epoch': 5.25}
 83%|████████▎ | 4138/5000 [20:26:19<3:57:47, 16.55s/it] 83%|████████▎ | 4139/5000 [20:26:32<3:45:21, 15.70s/it]                                                        {'loss': 18.775, 'grad_norm': 11.75, 'learning_rate': 5.523176862940756e-06, 'epoch': 5.26}
 83%|████████▎ | 4139/5000 [20:26:32<3:45:21, 15.70s/it] 83%|████████▎ | 4140/5000 [20:26:48<3:43:51, 15.62s/it]                                                        {'loss': 17.3527, 'grad_norm': 16.375, 'learning_rate': 5.510702219813887e-06, 'epoch': 5.26}
 83%|████████▎ | 4140/5000 [20:26:48<3:43:51, 15.62s/it] 83%|████████▎ | 4141/5000 [20:27:06<3:56:08, 16.49s/it]                                                        {'loss': 16.8495, 'grad_norm': 8.0625, 'learning_rate': 5.49824047630774e-06, 'epoch': 5.26}
 83%|████████▎ | 4141/5000 [20:27:06<3:56:08, 16.49s/it] 83%|████████▎ | 4142/5000 [20:27:23<3:54:55, 16.43s/it]                                                        {'loss': 19.0797, 'grad_norm': 13.75, 'learning_rate': 5.485791637873509e-06, 'epoch': 5.26}
 83%|████████▎ | 4142/5000 [20:27:23<3:54:55, 16.43s/it] 83%|████████▎ | 4143/5000 [20:27:40<4:00:10, 16.81s/it]                                                        {'loss': 17.1579, 'grad_norm': 8.5, 'learning_rate': 5.473355709956722e-06, 'epoch': 5.26}
 83%|████████▎ | 4143/5000 [20:27:40<4:00:10, 16.81s/it] 83%|████████▎ | 4144/5000 [20:27:55<3:51:55, 16.26s/it]                                                        {'loss': 17.7527, 'grad_norm': 10.375, 'learning_rate': 5.460932697997299e-06, 'epoch': 5.26}
 83%|████████▎ | 4144/5000 [20:27:55<3:51:55, 16.26s/it] 83%|████████▎ | 4145/5000 [20:28:18<4:19:40, 18.22s/it]                                                        {'loss': 16.7447, 'grad_norm': 7.03125, 'learning_rate': 5.4485226074294795e-06, 'epoch': 5.26}
 83%|████████▎ | 4145/5000 [20:28:18<4:19:40, 18.22s/it] 83%|████████▎ | 4146/5000 [20:28:35<4:11:46, 17.69s/it]                                                        {'loss': 17.0928, 'grad_norm': 13.0625, 'learning_rate': 5.4361254436818475e-06, 'epoch': 5.26}
 83%|████████▎ | 4146/5000 [20:28:35<4:11:46, 17.69s/it] 83%|████████▎ | 4147/5000 [20:28:52<4:10:03, 17.59s/it]                                                        {'loss': 17.1813, 'grad_norm': 7.46875, 'learning_rate': 5.423741212177363e-06, 'epoch': 5.27}
 83%|████████▎ | 4147/5000 [20:28:52<4:10:03, 17.59s/it] 83%|████████▎ | 4148/5000 [20:29:09<4:09:25, 17.57s/it]                                                        {'loss': 16.7595, 'grad_norm': 12.25, 'learning_rate': 5.411369918333293e-06, 'epoch': 5.27}
 83%|████████▎ | 4148/5000 [20:29:09<4:09:25, 17.57s/it] 83%|████████▎ | 4149/5000 [20:29:27<4:09:20, 17.58s/it]                                                        {'loss': 17.6383, 'grad_norm': 9.3125, 'learning_rate': 5.399011567561276e-06, 'epoch': 5.27}
 83%|████████▎ | 4149/5000 [20:29:27<4:09:20, 17.58s/it] 83%|████████▎ | 4150/5000 [20:29:44<4:07:02, 17.44s/it]                                                        {'loss': 18.4829, 'grad_norm': 13.25, 'learning_rate': 5.386666165267256e-06, 'epoch': 5.27}
 83%|████████▎ | 4150/5000 [20:29:44<4:07:02, 17.44s/it] 83%|████████▎ | 4151/5000 [20:30:04<4:17:15, 18.18s/it]                                                        {'loss': 16.5177, 'grad_norm': 10.0, 'learning_rate': 5.374333716851555e-06, 'epoch': 5.27}
 83%|████████▎ | 4151/5000 [20:30:04<4:17:15, 18.18s/it] 83%|████████▎ | 4152/5000 [20:30:21<4:11:52, 17.82s/it]                                                        {'loss': 17.2177, 'grad_norm': 7.65625, 'learning_rate': 5.3620142277087825e-06, 'epoch': 5.27}
 83%|████████▎ | 4152/5000 [20:30:21<4:11:52, 17.82s/it] 83%|████████▎ | 4153/5000 [20:30:38<4:09:02, 17.64s/it]                                                        {'loss': 18.5751, 'grad_norm': 12.8125, 'learning_rate': 5.349707703227918e-06, 'epoch': 5.27}
 83%|████████▎ | 4153/5000 [20:30:38<4:09:02, 17.64s/it] 83%|████████▎ | 4154/5000 [20:30:52<3:52:00, 16.45s/it]                                                        {'loss': 17.0962, 'grad_norm': 8.6875, 'learning_rate': 5.337414148792248e-06, 'epoch': 5.27}
 83%|████████▎ | 4154/5000 [20:30:52<3:52:00, 16.45s/it] 83%|████████▎ | 4155/5000 [20:31:05<3:38:43, 15.53s/it]                                                        {'loss': 17.8231, 'grad_norm': 12.75, 'learning_rate': 5.325133569779386e-06, 'epoch': 5.28}
 83%|████████▎ | 4155/5000 [20:31:05<3:38:43, 15.53s/it] 83%|████████▎ | 4156/5000 [20:31:33<4:27:56, 19.05s/it]                                                        {'loss': 17.2493, 'grad_norm': 9.375, 'learning_rate': 5.3128659715612795e-06, 'epoch': 5.28}
 83%|████████▎ | 4156/5000 [20:31:33<4:27:56, 19.05s/it] 83%|████████▎ | 4157/5000 [20:31:51<4:26:07, 18.94s/it]                                                        {'loss': 17.7625, 'grad_norm': 13.1875, 'learning_rate': 5.3006113595041954e-06, 'epoch': 5.28}
 83%|████████▎ | 4157/5000 [20:31:51<4:26:07, 18.94s/it] 83%|████████▎ | 4158/5000 [20:32:06<4:10:15, 17.83s/it]                                                        {'loss': 17.0259, 'grad_norm': 20.0, 'learning_rate': 5.288369738968704e-06, 'epoch': 5.28}
 83%|████████▎ | 4158/5000 [20:32:06<4:10:15, 17.83s/it] 83%|████████▎ | 4159/5000 [20:32:33<4:47:22, 20.50s/it]                                                        {'loss': 18.9396, 'grad_norm': 10.9375, 'learning_rate': 5.276141115309727e-06, 'epoch': 5.28}
 83%|████████▎ | 4159/5000 [20:32:33<4:47:22, 20.50s/it] 83%|████████▎ | 4160/5000 [20:32:49<4:27:54, 19.14s/it]                                                        {'loss': 18.0735, 'grad_norm': 12.75, 'learning_rate': 5.263925493876464e-06, 'epoch': 5.28}
 83%|████████▎ | 4160/5000 [20:32:49<4:27:54, 19.14s/it] 83%|████████▎ | 4161/5000 [20:33:07<4:23:56, 18.87s/it]                                                        {'loss': 17.733, 'grad_norm': 18.25, 'learning_rate': 5.251722880012449e-06, 'epoch': 5.28}
 83%|████████▎ | 4161/5000 [20:33:07<4:23:56, 18.87s/it] 83%|████████▎ | 4162/5000 [20:33:24<4:12:51, 18.10s/it]                                                        {'loss': 17.5244, 'grad_norm': 10.0, 'learning_rate': 5.239533279055514e-06, 'epoch': 5.29}
 83%|████████▎ | 4162/5000 [20:33:24<4:12:51, 18.10s/it] 83%|████████▎ | 4163/5000 [20:33:39<3:59:52, 17.20s/it]                                                        {'loss': 19.1867, 'grad_norm': 9.8125, 'learning_rate': 5.227356696337813e-06, 'epoch': 5.29}
 83%|████████▎ | 4163/5000 [20:33:39<3:59:52, 17.20s/it] 83%|████████▎ | 4164/5000 [20:33:55<3:53:28, 16.76s/it]                                                        {'loss': 16.6624, 'grad_norm': 8.75, 'learning_rate': 5.2151931371857895e-06, 'epoch': 5.29}
 83%|████████▎ | 4164/5000 [20:33:55<3:53:28, 16.76s/it] 83%|████████▎ | 4165/5000 [20:34:16<4:14:46, 18.31s/it]                                                        {'loss': 18.7577, 'grad_norm': 10.3125, 'learning_rate': 5.203042606920198e-06, 'epoch': 5.29}
 83%|████████▎ | 4165/5000 [20:34:16<4:14:46, 18.31s/it] 83%|████████▎ | 4166/5000 [20:34:33<4:07:40, 17.82s/it]                                                        {'loss': 17.4633, 'grad_norm': 19.875, 'learning_rate': 5.190905110856101e-06, 'epoch': 5.29}
 83%|████████▎ | 4166/5000 [20:34:33<4:07:40, 17.82s/it] 83%|████████▎ | 4167/5000 [20:34:52<4:13:07, 18.23s/it]                                                        {'loss': 16.5211, 'grad_norm': 6.25, 'learning_rate': 5.178780654302848e-06, 'epoch': 5.29}
 83%|████████▎ | 4167/5000 [20:34:52<4:13:07, 18.23s/it] 83%|████████▎ | 4168/5000 [20:35:15<4:29:25, 19.43s/it]                                                        {'loss': 19.6583, 'grad_norm': 16.25, 'learning_rate': 5.166669242564078e-06, 'epoch': 5.29}
 83%|████████▎ | 4168/5000 [20:35:15<4:29:25, 19.43s/it] 83%|████████▎ | 4169/5000 [20:35:28<4:04:01, 17.62s/it]                                                        {'loss': 18.2112, 'grad_norm': 22.875, 'learning_rate': 5.15457088093775e-06, 'epoch': 5.29}
 83%|████████▎ | 4169/5000 [20:35:28<4:04:01, 17.62s/it] 83%|████████▎ | 4170/5000 [20:35:45<4:01:52, 17.48s/it]                                                        {'loss': 17.6169, 'grad_norm': 9.75, 'learning_rate': 5.142485574716095e-06, 'epoch': 5.3}
 83%|████████▎ | 4170/5000 [20:35:45<4:01:52, 17.48s/it] 83%|████████▎ | 4171/5000 [20:36:11<4:35:09, 19.92s/it]                                                        {'loss': 17.5124, 'grad_norm': 14.375, 'learning_rate': 5.13041332918562e-06, 'epoch': 5.3}
 83%|████████▎ | 4171/5000 [20:36:11<4:35:09, 19.92s/it] 83%|████████▎ | 4172/5000 [20:36:27<4:21:43, 18.97s/it]                                                        {'loss': 17.4626, 'grad_norm': 9.9375, 'learning_rate': 5.118354149627159e-06, 'epoch': 5.3}
 83%|████████▎ | 4172/5000 [20:36:27<4:21:43, 18.97s/it] 83%|████████▎ | 4173/5000 [20:36:41<3:57:04, 17.20s/it]                                                        {'loss': 19.5023, 'grad_norm': 16.625, 'learning_rate': 5.106308041315788e-06, 'epoch': 5.3}
 83%|████████▎ | 4173/5000 [20:36:41<3:57:04, 17.20s/it] 83%|████████▎ | 4174/5000 [20:36:53<3:37:47, 15.82s/it]                                                        {'loss': 19.237, 'grad_norm': 17.625, 'learning_rate': 5.094275009520897e-06, 'epoch': 5.3}
 83%|████████▎ | 4174/5000 [20:36:53<3:37:47, 15.82s/it] 84%|████████▎ | 4175/5000 [20:37:11<3:44:36, 16.33s/it]                                                        {'loss': 18.3103, 'grad_norm': 15.4375, 'learning_rate': 5.08225505950613e-06, 'epoch': 5.3}
 84%|████████▎ | 4175/5000 [20:37:11<3:44:36, 16.33s/it] 84%|████████▎ | 4176/5000 [20:37:26<3:39:10, 15.96s/it]                                                        {'loss': 18.3586, 'grad_norm': 9.125, 'learning_rate': 5.0702481965294414e-06, 'epoch': 5.3}
 84%|████████▎ | 4176/5000 [20:37:26<3:39:10, 15.96s/it] 84%|████████▎ | 4177/5000 [20:37:46<3:56:51, 17.27s/it]                                                        {'loss': 17.1023, 'grad_norm': 8.75, 'learning_rate': 5.058254425843026e-06, 'epoch': 5.3}
 84%|████████▎ | 4177/5000 [20:37:46<3:56:51, 17.27s/it] 84%|████████▎ | 4178/5000 [20:38:09<4:20:18, 19.00s/it]                                                        {'loss': 16.504, 'grad_norm': 11.125, 'learning_rate': 5.046273752693362e-06, 'epoch': 5.31}
 84%|████████▎ | 4178/5000 [20:38:09<4:20:18, 19.00s/it] 84%|████████▎ | 4179/5000 [20:38:28<4:19:50, 18.99s/it]                                                        {'loss': 17.7433, 'grad_norm': 12.9375, 'learning_rate': 5.034306182321219e-06, 'epoch': 5.31}
 84%|████████▎ | 4179/5000 [20:38:28<4:19:50, 18.99s/it] 84%|████████▎ | 4180/5000 [20:38:53<4:45:05, 20.86s/it]                                                        {'loss': 17.8222, 'grad_norm': 12.4375, 'learning_rate': 5.0223517199616105e-06, 'epoch': 5.31}
 84%|████████▎ | 4180/5000 [20:38:53<4:45:05, 20.86s/it] 84%|████████▎ | 4181/5000 [20:39:13<4:39:46, 20.50s/it]                                                        {'loss': 16.7606, 'grad_norm': 11.125, 'learning_rate': 5.0104103708438155e-06, 'epoch': 5.31}
 84%|████████▎ | 4181/5000 [20:39:13<4:39:46, 20.50s/it] 84%|████████▎ | 4182/5000 [20:39:30<4:24:02, 19.37s/it]                                                        {'loss': 18.4216, 'grad_norm': 10.5625, 'learning_rate': 4.998482140191401e-06, 'epoch': 5.31}
 84%|████████▎ | 4182/5000 [20:39:30<4:24:02, 19.37s/it] 84%|████████▎ | 4183/5000 [20:39:51<4:31:20, 19.93s/it]                                                        {'loss': 17.2332, 'grad_norm': 8.9375, 'learning_rate': 4.986567033222177e-06, 'epoch': 5.31}
 84%|████████▎ | 4183/5000 [20:39:51<4:31:20, 19.93s/it] 84%|████████▎ | 4184/5000 [20:40:11<4:32:23, 20.03s/it]                                                        {'loss': 16.0455, 'grad_norm': 7.53125, 'learning_rate': 4.974665055148199e-06, 'epoch': 5.31}
 84%|████████▎ | 4184/5000 [20:40:11<4:32:23, 20.03s/it] 84%|████████▎ | 4185/5000 [20:40:28<4:19:30, 19.10s/it]                                                        {'loss': 18.3051, 'grad_norm': 13.25, 'learning_rate': 4.962776211175826e-06, 'epoch': 5.31}
 84%|████████▎ | 4185/5000 [20:40:28<4:19:30, 19.10s/it] 84%|████████▎ | 4186/5000 [20:40:56<4:55:36, 21.79s/it]                                                        {'loss': 17.0294, 'grad_norm': 10.75, 'learning_rate': 4.950900506505621e-06, 'epoch': 5.32}
 84%|████████▎ | 4186/5000 [20:40:56<4:55:36, 21.79s/it] 84%|████████▎ | 4187/5000 [20:41:16<4:46:30, 21.15s/it]                                                        {'loss': 17.542, 'grad_norm': 8.125, 'learning_rate': 4.939037946332424e-06, 'epoch': 5.32}
 84%|████████▎ | 4187/5000 [20:41:16<4:46:30, 21.15s/it] 84%|████████▍ | 4188/5000 [20:41:39<4:53:23, 21.68s/it]                                                        {'loss': 17.2375, 'grad_norm': 9.4375, 'learning_rate': 4.9271885358453275e-06, 'epoch': 5.32}
 84%|████████▍ | 4188/5000 [20:41:39<4:53:23, 21.68s/it] 84%|████████▍ | 4189/5000 [20:41:55<4:32:55, 20.19s/it]                                                        {'loss': 18.4892, 'grad_norm': 14.5625, 'learning_rate': 4.915352280227666e-06, 'epoch': 5.32}
 84%|████████▍ | 4189/5000 [20:41:55<4:32:55, 20.19s/it] 84%|████████▍ | 4190/5000 [20:42:14<4:25:08, 19.64s/it]                                                        {'loss': 16.9411, 'grad_norm': 10.5, 'learning_rate': 4.903529184657007e-06, 'epoch': 5.32}
 84%|████████▍ | 4190/5000 [20:42:14<4:25:08, 19.64s/it] 84%|████████▍ | 4191/5000 [20:42:31<4:16:11, 19.00s/it]                                                        {'loss': 17.8676, 'grad_norm': 13.0, 'learning_rate': 4.8917192543051935e-06, 'epoch': 5.32}
 84%|████████▍ | 4191/5000 [20:42:31<4:16:11, 19.00s/it] 84%|████████▍ | 4192/5000 [20:42:47<4:00:59, 17.89s/it]                                                        {'loss': 16.6817, 'grad_norm': 7.5625, 'learning_rate': 4.8799224943382805e-06, 'epoch': 5.32}
 84%|████████▍ | 4192/5000 [20:42:47<4:00:59, 17.89s/it] 84%|████████▍ | 4193/5000 [20:42:59<3:38:49, 16.27s/it]                                                        {'loss': 18.2091, 'grad_norm': 15.625, 'learning_rate': 4.868138909916562e-06, 'epoch': 5.32}
 84%|████████▍ | 4193/5000 [20:42:59<3:38:49, 16.27s/it] 84%|████████▍ | 4194/5000 [20:43:24<4:14:43, 18.96s/it]                                                        {'loss': 16.8944, 'grad_norm': 6.65625, 'learning_rate': 4.856368506194595e-06, 'epoch': 5.33}
 84%|████████▍ | 4194/5000 [20:43:24<4:14:43, 18.96s/it] 84%|████████▍ | 4195/5000 [20:43:42<4:07:34, 18.45s/it]                                                        {'loss': 17.2605, 'grad_norm': 10.0625, 'learning_rate': 4.844611288321154e-06, 'epoch': 5.33}
 84%|████████▍ | 4195/5000 [20:43:42<4:07:34, 18.45s/it] 84%|████████▍ | 4196/5000 [20:44:04<4:20:54, 19.47s/it]                                                        {'loss': 16.0747, 'grad_norm': 8.875, 'learning_rate': 4.832867261439225e-06, 'epoch': 5.33}
 84%|████████▍ | 4196/5000 [20:44:04<4:20:54, 19.47s/it] 84%|████████▍ | 4197/5000 [20:44:18<4:01:14, 18.03s/it]                                                        {'loss': 18.5468, 'grad_norm': 12.75, 'learning_rate': 4.821136430686066e-06, 'epoch': 5.33}
 84%|████████▍ | 4197/5000 [20:44:18<4:01:14, 18.03s/it] 84%|████████▍ | 4198/5000 [20:44:35<3:58:04, 17.81s/it]                                                        {'loss': 18.9327, 'grad_norm': 13.0, 'learning_rate': 4.809418801193124e-06, 'epoch': 5.33}
 84%|████████▍ | 4198/5000 [20:44:35<3:58:04, 17.81s/it] 84%|████████▍ | 4199/5000 [20:44:52<3:53:11, 17.47s/it]                                                        {'loss': 17.5145, 'grad_norm': 9.6875, 'learning_rate': 4.79771437808611e-06, 'epoch': 5.33}
 84%|████████▍ | 4199/5000 [20:44:52<3:53:11, 17.47s/it] 84%|████████▍ | 4200/5000 [20:45:16<4:19:11, 19.44s/it]                                                        {'loss': 17.3555, 'grad_norm': 10.375, 'learning_rate': 4.786023166484913e-06, 'epoch': 5.33}
 84%|████████▍ | 4200/5000 [20:45:16<4:19:11, 19.44s/it] 84%|████████▍ | 4201/5000 [20:45:31<4:02:01, 18.17s/it]                                                        {'loss': 18.3489, 'grad_norm': 11.875, 'learning_rate': 4.774345171503682e-06, 'epoch': 5.33}
 84%|████████▍ | 4201/5000 [20:45:31<4:02:01, 18.17s/it] 84%|████████▍ | 4202/5000 [20:45:47<3:50:19, 17.32s/it]                                                        {'loss': 18.158, 'grad_norm': 8.9375, 'learning_rate': 4.762680398250765e-06, 'epoch': 5.34}
 84%|████████▍ | 4202/5000 [20:45:47<3:50:19, 17.32s/it] 84%|████████▍ | 4203/5000 [20:46:03<3:44:44, 16.92s/it]                                                        {'loss': 17.2447, 'grad_norm': 9.9375, 'learning_rate': 4.751028851828721e-06, 'epoch': 5.34}
 84%|████████▍ | 4203/5000 [20:46:03<3:44:44, 16.92s/it] 84%|████████▍ | 4204/5000 [20:46:18<3:36:43, 16.34s/it]                                                        {'loss': 17.4368, 'grad_norm': 13.5, 'learning_rate': 4.739390537334349e-06, 'epoch': 5.34}
 84%|████████▍ | 4204/5000 [20:46:18<3:36:43, 16.34s/it] 84%|████████▍ | 4205/5000 [20:46:34<3:34:30, 16.19s/it]                                                        {'loss': 18.3458, 'grad_norm': 9.9375, 'learning_rate': 4.727765459858633e-06, 'epoch': 5.34}
 84%|████████▍ | 4205/5000 [20:46:34<3:34:30, 16.19s/it] 84%|████████▍ | 4206/5000 [20:46:52<3:42:28, 16.81s/it]                                                        {'loss': 17.4197, 'grad_norm': 10.1875, 'learning_rate': 4.716153624486768e-06, 'epoch': 5.34}
 84%|████████▍ | 4206/5000 [20:46:52<3:42:28, 16.81s/it] 84%|████████▍ | 4207/5000 [20:47:07<3:37:38, 16.47s/it]                                                        {'loss': 17.8017, 'grad_norm': 16.0, 'learning_rate': 4.704555036298185e-06, 'epoch': 5.34}
 84%|████████▍ | 4207/5000 [20:47:07<3:37:38, 16.47s/it] 84%|████████▍ | 4208/5000 [20:47:22<3:31:41, 16.04s/it]                                                        {'loss': 18.1853, 'grad_norm': 11.4375, 'learning_rate': 4.692969700366486e-06, 'epoch': 5.34}
 84%|████████▍ | 4208/5000 [20:47:22<3:31:41, 16.04s/it] 84%|████████▍ | 4209/5000 [20:47:40<3:38:02, 16.54s/it]                                                        {'loss': 17.7302, 'grad_norm': 16.0, 'learning_rate': 4.681397621759494e-06, 'epoch': 5.34}
 84%|████████▍ | 4209/5000 [20:47:40<3:38:02, 16.54s/it] 84%|████████▍ | 4210/5000 [20:47:58<3:40:55, 16.78s/it]                                                        {'loss': 17.6062, 'grad_norm': 7.9375, 'learning_rate': 4.6698388055392345e-06, 'epoch': 5.35}
 84%|████████▍ | 4210/5000 [20:47:58<3:40:55, 16.78s/it] 84%|████████▍ | 4211/5000 [20:48:15<3:42:51, 16.95s/it]                                                        {'loss': 17.1415, 'grad_norm': 15.0625, 'learning_rate': 4.6582932567619206e-06, 'epoch': 5.35}
 84%|████████▍ | 4211/5000 [20:48:15<3:42:51, 16.95s/it] 84%|████████▍ | 4212/5000 [20:48:40<4:14:10, 19.35s/it]                                                        {'loss': 17.5747, 'grad_norm': 17.125, 'learning_rate': 4.646760980477974e-06, 'epoch': 5.35}
 84%|████████▍ | 4212/5000 [20:48:40<4:14:10, 19.35s/it] 84%|████████▍ | 4213/5000 [20:48:56<3:59:48, 18.28s/it]                                                        {'loss': 18.0372, 'grad_norm': 9.75, 'learning_rate': 4.635241981731987e-06, 'epoch': 5.35}
 84%|████████▍ | 4213/5000 [20:48:56<3:59:48, 18.28s/it] 84%|████████▍ | 4214/5000 [20:49:10<3:45:09, 17.19s/it]                                                        {'loss': 18.9651, 'grad_norm': 12.0625, 'learning_rate': 4.623736265562786e-06, 'epoch': 5.35}
 84%|████████▍ | 4214/5000 [20:49:10<3:45:09, 17.19s/it] 84%|████████▍ | 4215/5000 [20:49:27<3:43:45, 17.10s/it]                                                        {'loss': 19.8507, 'grad_norm': 16.125, 'learning_rate': 4.612243837003347e-06, 'epoch': 5.35}
 84%|████████▍ | 4215/5000 [20:49:27<3:43:45, 17.10s/it] 84%|████████▍ | 4216/5000 [20:49:54<4:21:37, 20.02s/it]                                                        {'loss': 17.5287, 'grad_norm': 15.75, 'learning_rate': 4.60076470108084e-06, 'epoch': 5.35}
 84%|████████▍ | 4216/5000 [20:49:54<4:21:37, 20.02s/it] 84%|████████▍ | 4217/5000 [20:50:09<4:01:58, 18.54s/it]                                                        {'loss': 21.7335, 'grad_norm': 40.25, 'learning_rate': 4.589298862816653e-06, 'epoch': 5.35}
 84%|████████▍ | 4217/5000 [20:50:09<4:01:58, 18.54s/it] 84%|████████▍ | 4218/5000 [20:50:23<3:43:04, 17.12s/it]                                                        {'loss': 17.5973, 'grad_norm': 15.8125, 'learning_rate': 4.57784632722632e-06, 'epoch': 5.36}
 84%|████████▍ | 4218/5000 [20:50:23<3:43:04, 17.12s/it] 84%|████████▍ | 4219/5000 [20:50:40<3:44:05, 17.22s/it]                                                        {'loss': 17.1232, 'grad_norm': 6.90625, 'learning_rate': 4.566407099319559e-06, 'epoch': 5.36}
 84%|████████▍ | 4219/5000 [20:50:40<3:44:05, 17.22s/it] 84%|████████▍ | 4220/5000 [20:51:04<4:09:53, 19.22s/it]                                                        {'loss': 18.786, 'grad_norm': 18.375, 'learning_rate': 4.5549811841003e-06, 'epoch': 5.36}
 84%|████████▍ | 4220/5000 [20:51:04<4:09:53, 19.22s/it] 84%|████████▍ | 4221/5000 [20:51:27<4:23:52, 20.32s/it]                                                        {'loss': 17.363, 'grad_norm': 8.0, 'learning_rate': 4.543568586566601e-06, 'epoch': 5.36}
 84%|████████▍ | 4221/5000 [20:51:27<4:23:52, 20.32s/it] 84%|████████▍ | 4222/5000 [20:51:46<4:19:11, 19.99s/it]                                                        {'loss': 16.6035, 'grad_norm': 8.25, 'learning_rate': 4.532169311710745e-06, 'epoch': 5.36}
 84%|████████▍ | 4222/5000 [20:51:46<4:19:11, 19.99s/it] 84%|████████▍ | 4223/5000 [20:52:11<4:38:21, 21.49s/it]                                                        {'loss': 16.098, 'grad_norm': 9.0, 'learning_rate': 4.520783364519142e-06, 'epoch': 5.36}
 84%|████████▍ | 4223/5000 [20:52:11<4:38:21, 21.49s/it] 84%|████████▍ | 4224/5000 [20:52:30<4:27:29, 20.68s/it]                                                        {'loss': 18.3551, 'grad_norm': 9.75, 'learning_rate': 4.509410749972405e-06, 'epoch': 5.36}
 84%|████████▍ | 4224/5000 [20:52:30<4:27:29, 20.68s/it] 84%|████████▍ | 4225/5000 [20:52:46<4:09:02, 19.28s/it]                                                        {'loss': 16.8872, 'grad_norm': 10.1875, 'learning_rate': 4.498051473045291e-06, 'epoch': 5.37}
 84%|████████▍ | 4225/5000 [20:52:46<4:09:02, 19.28s/it] 85%|████████▍ | 4226/5000 [20:53:04<4:04:52, 18.98s/it]                                                        {'loss': 16.1144, 'grad_norm': 6.25, 'learning_rate': 4.486705538706748e-06, 'epoch': 5.37}
 85%|████████▍ | 4226/5000 [20:53:04<4:04:52, 18.98s/it] 85%|████████▍ | 4227/5000 [20:53:20<3:50:28, 17.89s/it]                                                        {'loss': 18.6073, 'grad_norm': 16.625, 'learning_rate': 4.475372951919864e-06, 'epoch': 5.37}
 85%|████████▍ | 4227/5000 [20:53:20<3:50:28, 17.89s/it] 85%|████████▍ | 4228/5000 [20:53:35<3:41:29, 17.21s/it]                                                        {'loss': 16.8761, 'grad_norm': 12.75, 'learning_rate': 4.464053717641891e-06, 'epoch': 5.37}
 85%|████████▍ | 4228/5000 [20:53:35<3:41:29, 17.21s/it] 85%|████████▍ | 4229/5000 [20:54:01<4:11:43, 19.59s/it]                                                        {'loss': 17.4772, 'grad_norm': 10.9375, 'learning_rate': 4.452747840824265e-06, 'epoch': 5.37}
 85%|████████▍ | 4229/5000 [20:54:01<4:11:43, 19.59s/it] 85%|████████▍ | 4230/5000 [20:54:17<4:00:54, 18.77s/it]                                                        {'loss': 16.8815, 'grad_norm': 8.375, 'learning_rate': 4.4414553264125445e-06, 'epoch': 5.37}
 85%|████████▍ | 4230/5000 [20:54:17<4:00:54, 18.77s/it] 85%|████████▍ | 4231/5000 [20:54:35<3:54:42, 18.31s/it]                                                        {'loss': 16.508, 'grad_norm': 9.875, 'learning_rate': 4.43017617934647e-06, 'epoch': 5.37}
 85%|████████▍ | 4231/5000 [20:54:35<3:54:42, 18.31s/it] 85%|████████▍ | 4232/5000 [20:54:50<3:42:35, 17.39s/it]                                                        {'loss': 19.6424, 'grad_norm': 134.0, 'learning_rate': 4.418910404559905e-06, 'epoch': 5.37}
 85%|████████▍ | 4232/5000 [20:54:50<3:42:35, 17.39s/it] 85%|████████▍ | 4233/5000 [20:55:05<3:32:59, 16.66s/it]                                                        {'loss': 18.4897, 'grad_norm': 10.75, 'learning_rate': 4.407658006980906e-06, 'epoch': 5.38}
 85%|████████▍ | 4233/5000 [20:55:05<3:32:59, 16.66s/it] 85%|████████▍ | 4234/5000 [20:55:19<3:21:18, 15.77s/it]                                                        {'loss': 17.3815, 'grad_norm': 13.4375, 'learning_rate': 4.396418991531639e-06, 'epoch': 5.38}
 85%|████████▍ | 4234/5000 [20:55:19<3:21:18, 15.77s/it] 85%|████████▍ | 4235/5000 [20:55:34<3:18:12, 15.55s/it]                                                        {'loss': 17.9399, 'grad_norm': 11.4375, 'learning_rate': 4.385193363128431e-06, 'epoch': 5.38}
 85%|████████▍ | 4235/5000 [20:55:34<3:18:12, 15.55s/it] 85%|████████▍ | 4236/5000 [20:55:46<3:06:49, 14.67s/it]                                                        {'loss': 18.4244, 'grad_norm': 12.875, 'learning_rate': 4.373981126681766e-06, 'epoch': 5.38}
 85%|████████▍ | 4236/5000 [20:55:46<3:06:49, 14.67s/it] 85%|████████▍ | 4237/5000 [20:56:03<3:13:22, 15.21s/it]                                                        {'loss': 17.1723, 'grad_norm': 7.25, 'learning_rate': 4.362782287096247e-06, 'epoch': 5.38}
 85%|████████▍ | 4237/5000 [20:56:03<3:13:22, 15.21s/it] 85%|████████▍ | 4238/5000 [20:56:25<3:40:39, 17.37s/it]                                                        {'loss': 17.7477, 'grad_norm': 14.6875, 'learning_rate': 4.351596849270618e-06, 'epoch': 5.38}
 85%|████████▍ | 4238/5000 [20:56:25<3:40:39, 17.37s/it] 85%|████████▍ | 4239/5000 [20:56:46<3:54:19, 18.47s/it]                                                        {'loss': 17.7105, 'grad_norm': 9.5, 'learning_rate': 4.340424818097792e-06, 'epoch': 5.38}
 85%|████████▍ | 4239/5000 [20:56:46<3:54:19, 18.47s/it] 85%|████████▍ | 4240/5000 [20:57:02<3:42:48, 17.59s/it]                                                        {'loss': 17.6161, 'grad_norm': 9.3125, 'learning_rate': 4.329266198464782e-06, 'epoch': 5.38}
 85%|████████▍ | 4240/5000 [20:57:02<3:42:48, 17.59s/it] 85%|████████▍ | 4241/5000 [20:57:26<4:06:44, 19.51s/it]                                                        {'loss': 17.2792, 'grad_norm': 9.875, 'learning_rate': 4.318120995252736e-06, 'epoch': 5.39}
 85%|████████▍ | 4241/5000 [20:57:26<4:06:44, 19.51s/it] 85%|████████▍ | 4242/5000 [20:57:41<3:50:13, 18.22s/it]                                                        {'loss': 16.7186, 'grad_norm': 9.5625, 'learning_rate': 4.306989213336972e-06, 'epoch': 5.39}
 85%|████████▍ | 4242/5000 [20:57:41<3:50:13, 18.22s/it] 85%|████████▍ | 4243/5000 [20:57:56<3:37:15, 17.22s/it]                                                        {'loss': 18.3234, 'grad_norm': 11.9375, 'learning_rate': 4.295870857586889e-06, 'epoch': 5.39}
 85%|████████▍ | 4243/5000 [20:57:56<3:37:15, 17.22s/it] 85%|████████▍ | 4244/5000 [20:58:12<3:34:16, 17.01s/it]                                                        {'loss': 17.763, 'grad_norm': 13.0625, 'learning_rate': 4.2847659328660324e-06, 'epoch': 5.39}
 85%|████████▍ | 4244/5000 [20:58:12<3:34:16, 17.01s/it] 85%|████████▍ | 4245/5000 [20:58:30<3:35:53, 17.16s/it]                                                        {'loss': 18.1129, 'grad_norm': 16.5, 'learning_rate': 4.2736744440320885e-06, 'epoch': 5.39}
 85%|████████▍ | 4245/5000 [20:58:30<3:35:53, 17.16s/it] 85%|████████▍ | 4246/5000 [20:58:44<3:25:12, 16.33s/it]                                                        {'loss': 19.4615, 'grad_norm': 15.9375, 'learning_rate': 4.2625963959368365e-06, 'epoch': 5.39}
 85%|████████▍ | 4246/5000 [20:58:44<3:25:12, 16.33s/it] 85%|████████▍ | 4247/5000 [20:59:01<3:25:52, 16.40s/it]                                                        {'loss': 16.7895, 'grad_norm': 7.625, 'learning_rate': 4.251531793426202e-06, 'epoch': 5.39}
 85%|████████▍ | 4247/5000 [20:59:01<3:25:52, 16.40s/it] 85%|████████▍ | 4248/5000 [20:59:18<3:30:30, 16.80s/it]                                                        {'loss': 17.509, 'grad_norm': 8.8125, 'learning_rate': 4.240480641340207e-06, 'epoch': 5.39}
 85%|████████▍ | 4248/5000 [20:59:18<3:30:30, 16.80s/it] 85%|████████▍ | 4249/5000 [20:59:34<3:26:50, 16.53s/it]                                                        {'loss': 19.064, 'grad_norm': 15.125, 'learning_rate': 4.229442944513018e-06, 'epoch': 5.4}
 85%|████████▍ | 4249/5000 [20:59:34<3:26:50, 16.53s/it] 85%|████████▌ | 4250/5000 [20:59:59<3:55:21, 18.83s/it]                                                        {'loss': 20.1106, 'grad_norm': 19.375, 'learning_rate': 4.218418707772886e-06, 'epoch': 5.4}
 85%|████████▌ | 4250/5000 [20:59:59<3:55:21, 18.83s/it] 85%|████████▌ | 4251/5000 [21:00:15<3:44:49, 18.01s/it]                                                        {'loss': 19.2098, 'grad_norm': 13.6875, 'learning_rate': 4.207407935942178e-06, 'epoch': 5.4}
 85%|████████▌ | 4251/5000 [21:00:15<3:44:49, 18.01s/it] 85%|████████▌ | 4252/5000 [21:00:44<4:25:51, 21.33s/it]                                                        {'loss': 16.4626, 'grad_norm': 10.6875, 'learning_rate': 4.196410633837401e-06, 'epoch': 5.4}
 85%|████████▌ | 4252/5000 [21:00:44<4:25:51, 21.33s/it] 85%|████████▌ | 4253/5000 [21:01:11<4:48:07, 23.14s/it]                                                        {'loss': 16.5258, 'grad_norm': 8.375, 'learning_rate': 4.1854268062691395e-06, 'epoch': 5.4}
 85%|████████▌ | 4253/5000 [21:01:11<4:48:07, 23.14s/it] 85%|████████▌ | 4254/5000 [21:01:26<4:18:53, 20.82s/it]                                                        {'loss': 17.9118, 'grad_norm': 10.125, 'learning_rate': 4.174456458042083e-06, 'epoch': 5.4}
 85%|████████▌ | 4254/5000 [21:01:26<4:18:53, 20.82s/it] 85%|████████▌ | 4255/5000 [21:01:41<3:55:30, 18.97s/it]                                                        {'loss': 21.8065, 'grad_norm': 24.625, 'learning_rate': 4.163499593955047e-06, 'epoch': 5.4}
 85%|████████▌ | 4255/5000 [21:01:41<3:55:30, 18.97s/it] 85%|████████▌ | 4256/5000 [21:01:55<3:36:12, 17.44s/it]                                                        {'loss': 18.4099, 'grad_norm': 10.1875, 'learning_rate': 4.152556218800936e-06, 'epoch': 5.4}
 85%|████████▌ | 4256/5000 [21:01:55<3:36:12, 17.44s/it] 85%|████████▌ | 4257/5000 [21:02:20<4:05:25, 19.82s/it]                                                        {'loss': 17.7033, 'grad_norm': 11.4375, 'learning_rate': 4.141626337366739e-06, 'epoch': 5.41}
 85%|████████▌ | 4257/5000 [21:02:20<4:05:25, 19.82s/it] 85%|████████▌ | 4258/5000 [21:02:39<3:59:16, 19.35s/it]                                                        {'loss': 17.2003, 'grad_norm': 11.1875, 'learning_rate': 4.130709954433579e-06, 'epoch': 5.41}
 85%|████████▌ | 4258/5000 [21:02:39<3:59:16, 19.35s/it] 85%|████████▌ | 4259/5000 [21:02:55<3:47:22, 18.41s/it]                                                        {'loss': 20.5875, 'grad_norm': 23.25, 'learning_rate': 4.1198070747766385e-06, 'epoch': 5.41}
 85%|████████▌ | 4259/5000 [21:02:55<3:47:22, 18.41s/it] 85%|████████▌ | 4260/5000 [21:03:09<3:29:53, 17.02s/it]                                                        {'loss': 18.3541, 'grad_norm': 13.0, 'learning_rate': 4.108917703165208e-06, 'epoch': 5.41}
 85%|████████▌ | 4260/5000 [21:03:09<3:29:53, 17.02s/it] 85%|████████▌ | 4261/5000 [21:03:27<3:33:43, 17.35s/it]                                                        {'loss': 17.4671, 'grad_norm': 8.875, 'learning_rate': 4.098041844362674e-06, 'epoch': 5.41}
 85%|████████▌ | 4261/5000 [21:03:27<3:33:43, 17.35s/it] 85%|████████▌ | 4262/5000 [21:03:40<3:18:18, 16.12s/it]                                                        {'loss': 18.6819, 'grad_norm': 12.625, 'learning_rate': 4.087179503126507e-06, 'epoch': 5.41}
 85%|████████▌ | 4262/5000 [21:03:40<3:18:18, 16.12s/it] 85%|████████▌ | 4263/5000 [21:03:56<3:16:18, 15.98s/it]                                                        {'loss': 18.7272, 'grad_norm': 14.8125, 'learning_rate': 4.076330684208257e-06, 'epoch': 5.41}
 85%|████████▌ | 4263/5000 [21:03:56<3:16:18, 15.98s/it] 85%|████████▌ | 4264/5000 [21:04:10<3:11:27, 15.61s/it]                                                        {'loss': 17.97, 'grad_norm': 28.5, 'learning_rate': 4.065495392353574e-06, 'epoch': 5.41}
 85%|████████▌ | 4264/5000 [21:04:10<3:11:27, 15.61s/it] 85%|████████▌ | 4265/5000 [21:04:36<3:48:06, 18.62s/it]                                                        {'loss': 18.1901, 'grad_norm': 8.4375, 'learning_rate': 4.05467363230218e-06, 'epoch': 5.42}
 85%|████████▌ | 4265/5000 [21:04:36<3:48:06, 18.62s/it] 85%|████████▌ | 4266/5000 [21:04:51<3:35:39, 17.63s/it]                                                        {'loss': 16.7891, 'grad_norm': 13.1875, 'learning_rate': 4.043865408787881e-06, 'epoch': 5.42}
 85%|████████▌ | 4266/5000 [21:04:51<3:35:39, 17.63s/it] 85%|████████▌ | 4267/5000 [21:05:08<3:32:41, 17.41s/it]                                                        {'loss': 19.1131, 'grad_norm': 13.0, 'learning_rate': 4.033070726538555e-06, 'epoch': 5.42}
 85%|████████▌ | 4267/5000 [21:05:08<3:32:41, 17.41s/it] 85%|████████▌ | 4268/5000 [21:05:21<3:16:58, 16.15s/it]                                                        {'loss': 20.4416, 'grad_norm': 16.5, 'learning_rate': 4.022289590276164e-06, 'epoch': 5.42}
 85%|████████▌ | 4268/5000 [21:05:21<3:16:58, 16.15s/it] 85%|████████▌ | 4269/5000 [21:05:47<3:51:25, 19.00s/it]                                                        {'loss': 16.8499, 'grad_norm': 9.625, 'learning_rate': 4.011522004716757e-06, 'epoch': 5.42}
 85%|████████▌ | 4269/5000 [21:05:47<3:51:25, 19.00s/it] 85%|████████▌ | 4270/5000 [21:06:00<3:29:48, 17.25s/it]                                                        {'loss': 18.253, 'grad_norm': 9.875, 'learning_rate': 4.000767974570419e-06, 'epoch': 5.42}
 85%|████████▌ | 4270/5000 [21:06:00<3:29:48, 17.25s/it] 85%|████████▌ | 4271/5000 [21:06:27<4:02:42, 19.98s/it]                                                        {'loss': 17.4859, 'grad_norm': 13.5625, 'learning_rate': 3.990027504541355e-06, 'epoch': 5.42}
 85%|████████▌ | 4271/5000 [21:06:27<4:02:42, 19.98s/it] 85%|████████▌ | 4272/5000 [21:06:50<4:15:24, 21.05s/it]                                                        {'loss': 17.7648, 'grad_norm': 8.625, 'learning_rate': 3.979300599327792e-06, 'epoch': 5.42}
 85%|████████▌ | 4272/5000 [21:06:50<4:15:24, 21.05s/it] 85%|████████▌ | 4273/5000 [21:07:06<3:55:16, 19.42s/it]                                                        {'loss': 17.1345, 'grad_norm': 9.375, 'learning_rate': 3.968587263622042e-06, 'epoch': 5.43}
 85%|████████▌ | 4273/5000 [21:07:06<3:55:16, 19.42s/it] 85%|████████▌ | 4274/5000 [21:07:24<3:49:30, 18.97s/it]                                                        {'loss': 18.7179, 'grad_norm': 17.375, 'learning_rate': 3.9578875021104984e-06, 'epoch': 5.43}
 85%|████████▌ | 4274/5000 [21:07:24<3:49:30, 18.97s/it] 86%|████████▌ | 4275/5000 [21:07:50<4:17:04, 21.27s/it]                                                        {'loss': 17.049, 'grad_norm': 12.0, 'learning_rate': 3.947201319473587e-06, 'epoch': 5.43}
 86%|████████▌ | 4275/5000 [21:07:50<4:17:04, 21.27s/it] 86%|████████▌ | 4276/5000 [21:08:07<3:59:02, 19.81s/it]                                                        {'loss': 17.2052, 'grad_norm': 8.3125, 'learning_rate': 3.9365287203858035e-06, 'epoch': 5.43}
 86%|████████▌ | 4276/5000 [21:08:07<3:59:02, 19.81s/it] 86%|████████▌ | 4277/5000 [21:08:21<3:39:48, 18.24s/it]                                                        {'loss': 18.0055, 'grad_norm': 11.5, 'learning_rate': 3.9258697095157195e-06, 'epoch': 5.43}
 86%|████████▌ | 4277/5000 [21:08:21<3:39:48, 18.24s/it] 86%|████████▌ | 4278/5000 [21:08:38<3:34:31, 17.83s/it]                                                        {'loss': 17.2272, 'grad_norm': 12.0625, 'learning_rate': 3.915224291525943e-06, 'epoch': 5.43}
 86%|████████▌ | 4278/5000 [21:08:38<3:34:31, 17.83s/it] 86%|████████▌ | 4279/5000 [21:08:58<3:40:05, 18.32s/it]                                                        {'loss': 17.8632, 'grad_norm': 10.9375, 'learning_rate': 3.904592471073134e-06, 'epoch': 5.43}
 86%|████████▌ | 4279/5000 [21:08:58<3:40:05, 18.32s/it] 86%|████████▌ | 4280/5000 [21:09:22<4:02:11, 20.18s/it]                                                        {'loss': 18.0714, 'grad_norm': 9.375, 'learning_rate': 3.893974252808019e-06, 'epoch': 5.43}
 86%|████████▌ | 4280/5000 [21:09:22<4:02:11, 20.18s/it] 86%|████████▌ | 4281/5000 [21:09:36<3:38:28, 18.23s/it]                                                        {'loss': 18.4538, 'grad_norm': 11.5625, 'learning_rate': 3.8833696413753705e-06, 'epoch': 5.44}
 86%|████████▌ | 4281/5000 [21:09:36<3:38:28, 18.23s/it] 86%|████████▌ | 4282/5000 [21:09:51<3:27:22, 17.33s/it]                                                        {'loss': 17.6504, 'grad_norm': 16.125, 'learning_rate': 3.872778641413998e-06, 'epoch': 5.44}
 86%|████████▌ | 4282/5000 [21:09:51<3:27:22, 17.33s/it] 86%|████████▌ | 4283/5000 [21:10:08<3:26:03, 17.24s/it]                                                        {'loss': 17.7309, 'grad_norm': 8.75, 'learning_rate': 3.862201257556765e-06, 'epoch': 5.44}
 86%|████████▌ | 4283/5000 [21:10:08<3:26:03, 17.24s/it] 86%|████████▌ | 4284/5000 [21:10:26<3:26:19, 17.29s/it]                                                        {'loss': 17.3084, 'grad_norm': 8.0625, 'learning_rate': 3.8516374944305845e-06, 'epoch': 5.44}
 86%|████████▌ | 4284/5000 [21:10:26<3:26:19, 17.29s/it] 86%|████████▌ | 4285/5000 [21:10:54<4:05:59, 20.64s/it]                                                        {'loss': 17.0952, 'grad_norm': 12.875, 'learning_rate': 3.841087356656403e-06, 'epoch': 5.44}
 86%|████████▌ | 4285/5000 [21:10:54<4:05:59, 20.64s/it] 86%|████████▌ | 4286/5000 [21:11:09<3:45:54, 18.98s/it]                                                        {'loss': 17.1737, 'grad_norm': 11.8125, 'learning_rate': 3.830550848849202e-06, 'epoch': 5.44}
 86%|████████▌ | 4286/5000 [21:11:09<3:45:54, 18.98s/it] 86%|████████▌ | 4287/5000 [21:11:21<3:21:53, 16.99s/it]                                                        {'loss': 20.6239, 'grad_norm': 22.875, 'learning_rate': 3.820027975618023e-06, 'epoch': 5.44}
 86%|████████▌ | 4287/5000 [21:11:21<3:21:53, 16.99s/it] 86%|████████▌ | 4288/5000 [21:11:40<3:26:59, 17.44s/it]                                                        {'loss': 18.4415, 'grad_norm': 9.875, 'learning_rate': 3.80951874156592e-06, 'epoch': 5.45}
 86%|████████▌ | 4288/5000 [21:11:40<3:26:59, 17.44s/it] 86%|████████▌ | 4289/5000 [21:11:55<3:19:11, 16.81s/it]                                                        {'loss': 17.9397, 'grad_norm': 9.8125, 'learning_rate': 3.7990231512899836e-06, 'epoch': 5.45}
 86%|████████▌ | 4289/5000 [21:11:55<3:19:11, 16.81s/it] 86%|████████▌ | 4290/5000 [21:12:10<3:09:57, 16.05s/it]                                                        {'loss': 16.1562, 'grad_norm': 12.625, 'learning_rate': 3.78854120938135e-06, 'epoch': 5.45}
 86%|████████▌ | 4290/5000 [21:12:10<3:09:57, 16.05s/it] 86%|████████▌ | 4291/5000 [21:12:37<3:49:25, 19.41s/it]                                                        {'loss': 18.2358, 'grad_norm': 12.4375, 'learning_rate': 3.7780729204251698e-06, 'epoch': 5.45}
 86%|████████▌ | 4291/5000 [21:12:37<3:49:25, 19.41s/it] 86%|████████▌ | 4292/5000 [21:13:01<4:04:36, 20.73s/it]                                                        {'loss': 19.2377, 'grad_norm': 11.0, 'learning_rate': 3.7676182890006373e-06, 'epoch': 5.45}
 86%|████████▌ | 4292/5000 [21:13:01<4:04:36, 20.73s/it] 86%|████████▌ | 4293/5000 [21:13:16<3:44:34, 19.06s/it]                                                        {'loss': 19.2658, 'grad_norm': 24.375, 'learning_rate': 3.757177319680951e-06, 'epoch': 5.45}
 86%|████████▌ | 4293/5000 [21:13:16<3:44:34, 19.06s/it] 86%|████████▌ | 4294/5000 [21:13:31<3:29:48, 17.83s/it]                                                        {'loss': 18.4704, 'grad_norm': 11.9375, 'learning_rate': 3.746750017033357e-06, 'epoch': 5.45}
 86%|████████▌ | 4294/5000 [21:13:31<3:29:48, 17.83s/it] 86%|████████▌ | 4295/5000 [21:13:48<3:26:28, 17.57s/it]                                                        {'loss': 17.7339, 'grad_norm': 9.5625, 'learning_rate': 3.7363363856191043e-06, 'epoch': 5.45}
 86%|████████▌ | 4295/5000 [21:13:48<3:26:28, 17.57s/it] 86%|████████▌ | 4296/5000 [21:14:12<3:51:16, 19.71s/it]                                                        {'loss': 18.3953, 'grad_norm': 12.0, 'learning_rate': 3.725936429993478e-06, 'epoch': 5.46}
 86%|████████▌ | 4296/5000 [21:14:12<3:51:16, 19.71s/it] 86%|████████▌ | 4297/5000 [21:14:28<3:36:16, 18.46s/it]                                                        {'loss': 17.539, 'grad_norm': 11.1875, 'learning_rate': 3.7155501547057676e-06, 'epoch': 5.46}
 86%|████████▌ | 4297/5000 [21:14:28<3:36:16, 18.46s/it] 86%|████████▌ | 4298/5000 [21:14:45<3:30:53, 18.02s/it]                                                        {'loss': 15.701, 'grad_norm': 7.59375, 'learning_rate': 3.705177564299282e-06, 'epoch': 5.46}
 86%|████████▌ | 4298/5000 [21:14:45<3:30:53, 18.02s/it] 86%|████████▌ | 4299/5000 [21:15:00<3:20:42, 17.18s/it]                                                        {'loss': 18.4981, 'grad_norm': 13.125, 'learning_rate': 3.694818663311335e-06, 'epoch': 5.46}
 86%|████████▌ | 4299/5000 [21:15:00<3:20:42, 17.18s/it] 86%|████████▌ | 4300/5000 [21:15:27<3:55:43, 20.21s/it]                                                        {'loss': 16.5283, 'grad_norm': 8.125, 'learning_rate': 3.684473456273278e-06, 'epoch': 5.46}
 86%|████████▌ | 4300/5000 [21:15:27<3:55:43, 20.21s/it] 86%|████████▌ | 4301/5000 [21:15:43<3:39:34, 18.85s/it]                                                        {'loss': 18.5761, 'grad_norm': 12.625, 'learning_rate': 3.6741419477104496e-06, 'epoch': 5.46}
 86%|████████▌ | 4301/5000 [21:15:43<3:39:34, 18.85s/it] 86%|████████▌ | 4302/5000 [21:16:01<3:34:17, 18.42s/it]                                                        {'loss': 17.4201, 'grad_norm': 13.0, 'learning_rate': 3.6638241421421983e-06, 'epoch': 5.46}
 86%|████████▌ | 4302/5000 [21:16:01<3:34:17, 18.42s/it] 86%|████████▌ | 4303/5000 [21:16:17<3:26:05, 17.74s/it]                                                        {'loss': 17.5807, 'grad_norm': 8.0625, 'learning_rate': 3.6535200440818896e-06, 'epoch': 5.46}
 86%|████████▌ | 4303/5000 [21:16:17<3:26:05, 17.74s/it] 86%|████████▌ | 4304/5000 [21:16:30<3:11:35, 16.52s/it]                                                        {'loss': 18.401, 'grad_norm': 17.375, 'learning_rate': 3.6432296580368852e-06, 'epoch': 5.47}
 86%|████████▌ | 4304/5000 [21:16:30<3:11:35, 16.52s/it] 86%|████████▌ | 4305/5000 [21:17:00<3:57:29, 20.50s/it]                                                        {'loss': 16.2389, 'grad_norm': 12.125, 'learning_rate': 3.6329529885085394e-06, 'epoch': 5.47}
 86%|████████▌ | 4305/5000 [21:17:00<3:57:29, 20.50s/it] 86%|████████▌ | 4306/5000 [21:17:14<3:33:52, 18.49s/it]                                                        {'loss': 17.2373, 'grad_norm': 10.375, 'learning_rate': 3.6226900399922305e-06, 'epoch': 5.47}
 86%|████████▌ | 4306/5000 [21:17:14<3:33:52, 18.49s/it] 86%|████████▌ | 4307/5000 [21:17:31<3:27:47, 17.99s/it]                                                        {'loss': 17.2597, 'grad_norm': 10.5625, 'learning_rate': 3.6124408169773165e-06, 'epoch': 5.47}
 86%|████████▌ | 4307/5000 [21:17:31<3:27:47, 17.99s/it] 86%|████████▌ | 4308/5000 [21:17:56<3:51:14, 20.05s/it]                                                        {'loss': 16.9135, 'grad_norm': 9.5, 'learning_rate': 3.602205323947146e-06, 'epoch': 5.47}
 86%|████████▌ | 4308/5000 [21:17:56<3:51:14, 20.05s/it] 86%|████████▌ | 4309/5000 [21:18:14<3:46:09, 19.64s/it]                                                        {'loss': 17.3558, 'grad_norm': 8.5, 'learning_rate': 3.591983565379086e-06, 'epoch': 5.47}
 86%|████████▌ | 4309/5000 [21:18:14<3:46:09, 19.64s/it] 86%|████████▌ | 4310/5000 [21:18:30<3:33:01, 18.52s/it]                                                        {'loss': 18.4165, 'grad_norm': 14.0, 'learning_rate': 3.581775545744476e-06, 'epoch': 5.47}
 86%|████████▌ | 4310/5000 [21:18:30<3:33:01, 18.52s/it] 86%|████████▌ | 4311/5000 [21:18:49<3:34:52, 18.71s/it]                                                        {'loss': 19.3475, 'grad_norm': 11.8125, 'learning_rate': 3.5715812695086432e-06, 'epoch': 5.47}
 86%|████████▌ | 4311/5000 [21:18:49<3:34:52, 18.71s/it] 86%|████████▌ | 4312/5000 [21:19:05<3:24:43, 17.85s/it]                                                        {'loss': 16.2771, 'grad_norm': 9.4375, 'learning_rate': 3.56140074113092e-06, 'epoch': 5.48}
 86%|████████▌ | 4312/5000 [21:19:05<3:24:43, 17.85s/it] 86%|████████▋ | 4313/5000 [21:19:18<3:07:26, 16.37s/it]                                                        {'loss': 18.7982, 'grad_norm': 13.8125, 'learning_rate': 3.5512339650646128e-06, 'epoch': 5.48}
 86%|████████▋ | 4313/5000 [21:19:18<3:07:26, 16.37s/it] 86%|████████▋ | 4314/5000 [21:19:34<3:04:55, 16.17s/it]                                                        {'loss': 19.4501, 'grad_norm': 14.5, 'learning_rate': 3.5410809457570095e-06, 'epoch': 5.48}
 86%|████████▋ | 4314/5000 [21:19:34<3:04:55, 16.17s/it] 86%|████████▋ | 4315/5000 [21:19:50<3:06:00, 16.29s/it]                                                        {'loss': 18.1261, 'grad_norm': 9.5, 'learning_rate': 3.530941687649394e-06, 'epoch': 5.48}
 86%|████████▋ | 4315/5000 [21:19:50<3:06:00, 16.29s/it] 86%|████████▋ | 4316/5000 [21:20:18<3:43:58, 19.65s/it]                                                        {'loss': 16.5062, 'grad_norm': 7.28125, 'learning_rate': 3.520816195177012e-06, 'epoch': 5.48}
 86%|████████▋ | 4316/5000 [21:20:18<3:43:58, 19.65s/it] 86%|████████▋ | 4317/5000 [21:20:43<4:03:42, 21.41s/it]                                                        {'loss': 19.6898, 'grad_norm': 15.5625, 'learning_rate': 3.510704472769113e-06, 'epoch': 5.48}
 86%|████████▋ | 4317/5000 [21:20:43<4:03:42, 21.41s/it] 86%|████████▋ | 4318/5000 [21:21:01<3:49:17, 20.17s/it]                                                        {'loss': 16.9956, 'grad_norm': 12.875, 'learning_rate': 3.500606524848889e-06, 'epoch': 5.48}
 86%|████████▋ | 4318/5000 [21:21:01<3:49:17, 20.17s/it] 86%|████████▋ | 4319/5000 [21:21:20<3:46:52, 19.99s/it]                                                        {'loss': 17.6615, 'grad_norm': 11.875, 'learning_rate': 3.4905223558335437e-06, 'epoch': 5.48}
 86%|████████▋ | 4319/5000 [21:21:20<3:46:52, 19.99s/it] 86%|████████▋ | 4320/5000 [21:21:45<4:02:31, 21.40s/it]                                                        {'loss': 17.1376, 'grad_norm': 8.8125, 'learning_rate': 3.480451970134227e-06, 'epoch': 5.49}
 86%|████████▋ | 4320/5000 [21:21:45<4:02:31, 21.40s/it] 86%|████████▋ | 4321/5000 [21:22:16<4:35:21, 24.33s/it]                                                        {'loss': 16.8917, 'grad_norm': 9.4375, 'learning_rate': 3.4703953721560616e-06, 'epoch': 5.49}
 86%|████████▋ | 4321/5000 [21:22:16<4:35:21, 24.33s/it] 86%|████████▋ | 4322/5000 [21:22:38<4:25:27, 23.49s/it]                                                        {'loss': 16.6924, 'grad_norm': 13.8125, 'learning_rate': 3.4603525662981594e-06, 'epoch': 5.49}
 86%|████████▋ | 4322/5000 [21:22:38<4:25:27, 23.49s/it] 86%|████████▋ | 4323/5000 [21:23:03<4:31:52, 24.09s/it]                                                        {'loss': 18.0675, 'grad_norm': 49.75, 'learning_rate': 3.450323556953574e-06, 'epoch': 5.49}
 86%|████████▋ | 4323/5000 [21:23:03<4:31:52, 24.09s/it] 86%|████████▋ | 4324/5000 [21:23:20<4:06:14, 21.86s/it]                                                        {'loss': 17.4916, 'grad_norm': 12.25, 'learning_rate': 3.4403083485093325e-06, 'epoch': 5.49}
 86%|████████▋ | 4324/5000 [21:23:20<4:06:14, 21.86s/it] 86%|████████▋ | 4325/5000 [21:23:34<3:40:57, 19.64s/it]                                                        {'loss': 18.4655, 'grad_norm': 11.5625, 'learning_rate': 3.4303069453464383e-06, 'epoch': 5.49}
 86%|████████▋ | 4325/5000 [21:23:34<3:40:57, 19.64s/it] 87%|████████▋ | 4326/5000 [21:23:50<3:26:06, 18.35s/it]                                                        {'loss': 18.2758, 'grad_norm': 11.6875, 'learning_rate': 3.4203193518398375e-06, 'epoch': 5.49}
 87%|████████▋ | 4326/5000 [21:23:50<3:26:06, 18.35s/it] 87%|████████▋ | 4327/5000 [21:24:16<3:52:41, 20.75s/it]                                                        {'loss': 17.6171, 'grad_norm': 13.75, 'learning_rate': 3.410345572358442e-06, 'epoch': 5.49}
 87%|████████▋ | 4327/5000 [21:24:16<3:52:41, 20.75s/it] 87%|████████▋ | 4328/5000 [21:24:30<3:29:45, 18.73s/it]                                                        {'loss': 17.8675, 'grad_norm': 10.9375, 'learning_rate': 3.4003856112651273e-06, 'epoch': 5.5}
 87%|████████▋ | 4328/5000 [21:24:30<3:29:45, 18.73s/it] 87%|████████▋ | 4329/5000 [21:24:58<4:02:10, 21.65s/it]                                                        {'loss': 16.5472, 'grad_norm': 9.5, 'learning_rate': 3.3904394729167173e-06, 'epoch': 5.5}
 87%|████████▋ | 4329/5000 [21:24:58<4:02:10, 21.65s/it] 87%|████████▋ | 4330/5000 [21:25:21<4:04:52, 21.93s/it]                                                        {'loss': 18.5184, 'grad_norm': 14.5625, 'learning_rate': 3.38050716166398e-06, 'epoch': 5.5}
 87%|████████▋ | 4330/5000 [21:25:21<4:04:52, 21.93s/it] 87%|████████▋ | 4331/5000 [21:25:37<3:43:55, 20.08s/it]                                                        {'loss': 17.59, 'grad_norm': 16.0, 'learning_rate': 3.3705886818516653e-06, 'epoch': 5.5}
 87%|████████▋ | 4331/5000 [21:25:37<3:43:55, 20.08s/it] 87%|████████▋ | 4332/5000 [21:25:52<3:26:08, 18.52s/it]                                                        {'loss': 16.9356, 'grad_norm': 22.5, 'learning_rate': 3.3606840378184412e-06, 'epoch': 5.5}
 87%|████████▋ | 4332/5000 [21:25:52<3:26:08, 18.52s/it] 87%|████████▋ | 4333/5000 [21:26:07<3:15:29, 17.59s/it]                                                        {'loss': 19.1516, 'grad_norm': 13.1875, 'learning_rate': 3.3507932338969377e-06, 'epoch': 5.5}
 87%|████████▋ | 4333/5000 [21:26:07<3:15:29, 17.59s/it] 87%|████████▋ | 4334/5000 [21:26:24<3:12:03, 17.30s/it]                                                        {'loss': 17.2109, 'grad_norm': 9.4375, 'learning_rate': 3.3409162744137193e-06, 'epoch': 5.5}
 87%|████████▋ | 4334/5000 [21:26:24<3:12:03, 17.30s/it] 87%|████████▋ | 4335/5000 [21:26:41<3:12:33, 17.37s/it]                                                        {'loss': 15.7977, 'grad_norm': 11.125, 'learning_rate': 3.3310531636893224e-06, 'epoch': 5.5}
 87%|████████▋ | 4335/5000 [21:26:41<3:12:33, 17.37s/it] 87%|████████▋ | 4336/5000 [21:26:57<3:06:00, 16.81s/it]                                                        {'loss': 18.4398, 'grad_norm': 12.125, 'learning_rate': 3.321203906038192e-06, 'epoch': 5.51}
 87%|████████▋ | 4336/5000 [21:26:57<3:06:00, 16.81s/it] 87%|████████▋ | 4337/5000 [21:27:25<3:44:13, 20.29s/it]                                                        {'loss': 18.0535, 'grad_norm': 11.1875, 'learning_rate': 3.3113685057687296e-06, 'epoch': 5.51}
 87%|████████▋ | 4337/5000 [21:27:25<3:44:13, 20.29s/it] 87%|████████▋ | 4338/5000 [21:27:39<3:23:34, 18.45s/it]                                                        {'loss': 17.049, 'grad_norm': 11.6875, 'learning_rate': 3.3015469671832834e-06, 'epoch': 5.51}
 87%|████████▋ | 4338/5000 [21:27:39<3:23:34, 18.45s/it] 87%|████████▋ | 4339/5000 [21:27:53<3:07:42, 17.04s/it]                                                        {'loss': 18.587, 'grad_norm': 14.75, 'learning_rate': 3.2917392945781125e-06, 'epoch': 5.51}
 87%|████████▋ | 4339/5000 [21:27:53<3:07:42, 17.04s/it] 87%|████████▋ | 4340/5000 [21:28:08<3:00:14, 16.39s/it]                                                        {'loss': 17.5219, 'grad_norm': 9.6875, 'learning_rate': 3.281945492243441e-06, 'epoch': 5.51}
 87%|████████▋ | 4340/5000 [21:28:08<3:00:14, 16.39s/it] 87%|████████▋ | 4341/5000 [21:28:30<3:20:06, 18.22s/it]                                                        {'loss': 18.9476, 'grad_norm': 16.75, 'learning_rate': 3.2721655644633997e-06, 'epoch': 5.51}
 87%|████████▋ | 4341/5000 [21:28:30<3:20:06, 18.22s/it] 87%|████████▋ | 4342/5000 [21:28:44<3:06:08, 16.97s/it]                                                        {'loss': 19.4882, 'grad_norm': 15.625, 'learning_rate': 3.262399515516071e-06, 'epoch': 5.51}
 87%|████████▋ | 4342/5000 [21:28:44<3:06:08, 16.97s/it] 87%|████████▋ | 4343/5000 [21:29:07<3:24:24, 18.67s/it]                                                        {'loss': 16.5269, 'grad_norm': 8.5625, 'learning_rate': 3.2526473496734464e-06, 'epoch': 5.51}
 87%|████████▋ | 4343/5000 [21:29:07<3:24:24, 18.67s/it] 87%|████████▋ | 4344/5000 [21:29:26<3:24:28, 18.70s/it]                                                        {'loss': 17.492, 'grad_norm': 8.8125, 'learning_rate': 3.2429090712014675e-06, 'epoch': 5.52}
 87%|████████▋ | 4344/5000 [21:29:26<3:24:28, 18.70s/it] 87%|████████▋ | 4345/5000 [21:29:46<3:28:23, 19.09s/it]                                                        {'loss': 17.6726, 'grad_norm': 8.0625, 'learning_rate': 3.233184684359977e-06, 'epoch': 5.52}
 87%|████████▋ | 4345/5000 [21:29:46<3:28:23, 19.09s/it] 87%|████████▋ | 4346/5000 [21:30:02<3:19:37, 18.31s/it]                                                        {'loss': 16.4806, 'grad_norm': 8.125, 'learning_rate': 3.223474193402753e-06, 'epoch': 5.52}
 87%|████████▋ | 4346/5000 [21:30:02<3:19:37, 18.31s/it] 87%|████████▋ | 4347/5000 [21:30:20<3:16:31, 18.06s/it]                                                        {'loss': 17.3889, 'grad_norm': 12.0, 'learning_rate': 3.2137776025774985e-06, 'epoch': 5.52}
 87%|████████▋ | 4347/5000 [21:30:20<3:16:31, 18.06s/it] 87%|████████▋ | 4348/5000 [21:30:46<3:43:37, 20.58s/it]                                                        {'loss': 18.449, 'grad_norm': 19.375, 'learning_rate': 3.2040949161258345e-06, 'epoch': 5.52}
 87%|████████▋ | 4348/5000 [21:30:46<3:43:37, 20.58s/it] 87%|████████▋ | 4349/5000 [21:31:05<3:36:20, 19.94s/it]                                                        {'loss': 17.9181, 'grad_norm': 11.25, 'learning_rate': 3.194426138283281e-06, 'epoch': 5.52}
 87%|████████▋ | 4349/5000 [21:31:05<3:36:20, 19.94s/it] 87%|████████▋ | 4350/5000 [21:31:24<3:32:19, 19.60s/it]                                                        {'loss': 18.438, 'grad_norm': 14.875, 'learning_rate': 3.184771273279312e-06, 'epoch': 5.52}
 87%|████████▋ | 4350/5000 [21:31:24<3:32:19, 19.60s/it] 87%|████████▋ | 4351/5000 [21:31:40<3:21:13, 18.60s/it]                                                        {'loss': 16.543, 'grad_norm': 6.5, 'learning_rate': 3.175130325337281e-06, 'epoch': 5.53}
 87%|████████▋ | 4351/5000 [21:31:40<3:21:13, 18.60s/it] 87%|████████▋ | 4352/5000 [21:31:53<3:04:10, 17.05s/it]                                                        {'loss': 18.6757, 'grad_norm': 14.9375, 'learning_rate': 3.1655032986744694e-06, 'epoch': 5.53}
 87%|████████▋ | 4352/5000 [21:31:53<3:04:10, 17.05s/it] 87%|████████▋ | 4353/5000 [21:32:06<2:50:10, 15.78s/it]                                                        {'loss': 18.8156, 'grad_norm': 12.8125, 'learning_rate': 3.1558901975020564e-06, 'epoch': 5.53}
 87%|████████▋ | 4353/5000 [21:32:06<2:50:10, 15.78s/it] 87%|████████▋ | 4354/5000 [21:32:25<3:01:00, 16.81s/it]                                                        {'loss': 17.3368, 'grad_norm': 14.5625, 'learning_rate': 3.146291026025152e-06, 'epoch': 5.53}
 87%|████████▋ | 4354/5000 [21:32:25<3:01:00, 16.81s/it] 87%|████████▋ | 4355/5000 [21:32:41<2:56:14, 16.39s/it]                                                        {'loss': 18.3113, 'grad_norm': 12.1875, 'learning_rate': 3.1367057884427575e-06, 'epoch': 5.53}
 87%|████████▋ | 4355/5000 [21:32:41<2:56:14, 16.39s/it] 87%|████████▋ | 4356/5000 [21:32:54<2:45:25, 15.41s/it]                                                        {'loss': 18.9952, 'grad_norm': 14.125, 'learning_rate': 3.1271344889477713e-06, 'epoch': 5.53}
 87%|████████▋ | 4356/5000 [21:32:54<2:45:25, 15.41s/it] 87%|████████▋ | 4357/5000 [21:33:08<2:41:50, 15.10s/it]                                                        {'loss': 19.0193, 'grad_norm': 70.5, 'learning_rate': 3.11757713172702e-06, 'epoch': 5.53}
 87%|████████▋ | 4357/5000 [21:33:08<2:41:50, 15.10s/it] 87%|████████▋ | 4358/5000 [21:33:23<2:41:26, 15.09s/it]                                                        {'loss': 18.793, 'grad_norm': 13.25, 'learning_rate': 3.108033720961207e-06, 'epoch': 5.53}
 87%|████████▋ | 4358/5000 [21:33:23<2:41:26, 15.09s/it] 87%|████████▋ | 4359/5000 [21:33:43<2:57:36, 16.62s/it]                                                        {'loss': 17.7417, 'grad_norm': 12.6875, 'learning_rate': 3.09850426082494e-06, 'epoch': 5.54}
 87%|████████▋ | 4359/5000 [21:33:43<2:57:36, 16.62s/it] 87%|████████▋ | 4360/5000 [21:33:58<2:51:26, 16.07s/it]                                                        {'loss': 18.0778, 'grad_norm': 19.25, 'learning_rate': 3.0889887554867436e-06, 'epoch': 5.54}
 87%|████████▋ | 4360/5000 [21:33:58<2:51:26, 16.07s/it] 87%|████████▋ | 4361/5000 [21:34:25<3:25:47, 19.32s/it]                                                        {'loss': 18.2091, 'grad_norm': 10.3125, 'learning_rate': 3.0794872091090123e-06, 'epoch': 5.54}
 87%|████████▋ | 4361/5000 [21:34:25<3:25:47, 19.32s/it] 87%|████████▋ | 4362/5000 [21:34:42<3:17:49, 18.60s/it]                                                        {'loss': 17.5896, 'grad_norm': 13.5625, 'learning_rate': 3.0699996258480427e-06, 'epoch': 5.54}
 87%|████████▋ | 4362/5000 [21:34:42<3:17:49, 18.60s/it] 87%|████████▋ | 4363/5000 [21:34:56<3:02:56, 17.23s/it]                                                        {'loss': 19.1712, 'grad_norm': 13.625, 'learning_rate': 3.0605260098540345e-06, 'epoch': 5.54}
 87%|████████▋ | 4363/5000 [21:34:56<3:02:56, 17.23s/it] 87%|████████▋ | 4364/5000 [21:35:12<2:58:40, 16.86s/it]                                                        {'loss': 17.8483, 'grad_norm': 13.25, 'learning_rate': 3.051066365271061e-06, 'epoch': 5.54}
 87%|████████▋ | 4364/5000 [21:35:12<2:58:40, 16.86s/it] 87%|████████▋ | 4365/5000 [21:35:31<3:03:49, 17.37s/it]                                                        {'loss': 19.1443, 'grad_norm': 13.125, 'learning_rate': 3.0416206962371048e-06, 'epoch': 5.54}
 87%|████████▋ | 4365/5000 [21:35:31<3:03:49, 17.37s/it] 87%|████████▋ | 4366/5000 [21:35:51<3:11:43, 18.14s/it]                                                        {'loss': 17.7212, 'grad_norm': 16.75, 'learning_rate': 3.032189006884006e-06, 'epoch': 5.54}
 87%|████████▋ | 4366/5000 [21:35:51<3:11:43, 18.14s/it] 87%|████████▋ | 4367/5000 [21:36:08<3:08:39, 17.88s/it]                                                        {'loss': 16.0474, 'grad_norm': 6.625, 'learning_rate': 3.0227713013375194e-06, 'epoch': 5.55}
 87%|████████▋ | 4367/5000 [21:36:08<3:08:39, 17.88s/it] 87%|████████▋ | 4368/5000 [21:36:23<2:59:32, 17.04s/it]                                                        {'loss': 18.4865, 'grad_norm': 14.6875, 'learning_rate': 3.0133675837172638e-06, 'epoch': 5.55}
 87%|████████▋ | 4368/5000 [21:36:23<2:59:32, 17.04s/it] 87%|████████▋ | 4369/5000 [21:36:41<3:01:51, 17.29s/it]                                                        {'loss': 17.4608, 'grad_norm': 11.4375, 'learning_rate': 3.0039778581367423e-06, 'epoch': 5.55}
 87%|████████▋ | 4369/5000 [21:36:41<3:01:51, 17.29s/it] 87%|████████▋ | 4370/5000 [21:36:55<2:51:22, 16.32s/it]                                                        {'loss': 17.7669, 'grad_norm': 13.9375, 'learning_rate': 2.9946021287033454e-06, 'epoch': 5.55}
 87%|████████▋ | 4370/5000 [21:36:55<2:51:22, 16.32s/it] 87%|████████▋ | 4371/5000 [21:37:09<2:45:02, 15.74s/it]                                                        {'loss': 16.9796, 'grad_norm': 10.4375, 'learning_rate': 2.985240399518332e-06, 'epoch': 5.55}
 87%|████████▋ | 4371/5000 [21:37:09<2:45:02, 15.74s/it] 87%|████████▋ | 4372/5000 [21:37:24<2:42:24, 15.52s/it]                                                        {'loss': 17.983, 'grad_norm': 17.25, 'learning_rate': 2.975892674676841e-06, 'epoch': 5.55}
 87%|████████▋ | 4372/5000 [21:37:24<2:42:24, 15.52s/it] 87%|████████▋ | 4373/5000 [21:37:39<2:40:45, 15.38s/it]                                                        {'loss': 16.8869, 'grad_norm': 9.75, 'learning_rate': 2.966558958267886e-06, 'epoch': 5.55}
 87%|████████▋ | 4373/5000 [21:37:39<2:40:45, 15.38s/it] 87%|████████▋ | 4374/5000 [21:38:03<3:06:29, 17.87s/it]                                                        {'loss': 17.9256, 'grad_norm': 19.125, 'learning_rate': 2.957239254374351e-06, 'epoch': 5.55}
 87%|████████▋ | 4374/5000 [21:38:03<3:06:29, 17.87s/it] 88%|████████▊ | 4375/5000 [21:38:21<3:07:05, 17.96s/it]                                                        {'loss': 16.5597, 'grad_norm': 9.75, 'learning_rate': 2.947933567072987e-06, 'epoch': 5.56}
 88%|████████▊ | 4375/5000 [21:38:21<3:07:05, 17.96s/it] 88%|████████▊ | 4376/5000 [21:38:47<3:31:57, 20.38s/it]                                                        {'loss': 16.2927, 'grad_norm': 10.875, 'learning_rate': 2.9386419004344284e-06, 'epoch': 5.56}
 88%|████████▊ | 4376/5000 [21:38:47<3:31:57, 20.38s/it] 88%|████████▊ | 4377/5000 [21:39:03<3:16:17, 18.90s/it]                                                        {'loss': 19.422, 'grad_norm': 23.0, 'learning_rate': 2.9293642585231597e-06, 'epoch': 5.56}
 88%|████████▊ | 4377/5000 [21:39:03<3:16:17, 18.90s/it] 88%|████████▊ | 4378/5000 [21:39:19<3:07:29, 18.09s/it]                                                        {'loss': 18.6184, 'grad_norm': 11.75, 'learning_rate': 2.9201006453975286e-06, 'epoch': 5.56}
 88%|████████▊ | 4378/5000 [21:39:19<3:07:29, 18.09s/it] 88%|████████▊ | 4379/5000 [21:39:34<2:57:30, 17.15s/it]                                                        {'loss': 24.6423, 'grad_norm': 208.0, 'learning_rate': 2.9108510651097706e-06, 'epoch': 5.56}
 88%|████████▊ | 4379/5000 [21:39:34<2:57:30, 17.15s/it] 88%|████████▊ | 4380/5000 [21:39:58<3:18:47, 19.24s/it]                                                        {'loss': 17.3189, 'grad_norm': 11.8125, 'learning_rate': 2.901615521705955e-06, 'epoch': 5.56}
 88%|████████▊ | 4380/5000 [21:39:58<3:18:47, 19.24s/it] 88%|████████▊ | 4381/5000 [21:40:12<3:02:56, 17.73s/it]                                                        {'loss': 18.3109, 'grad_norm': 12.4375, 'learning_rate': 2.8923940192260194e-06, 'epoch': 5.56}
 88%|████████▊ | 4381/5000 [21:40:12<3:02:56, 17.73s/it] 88%|████████▊ | 4382/5000 [21:40:26<2:49:14, 16.43s/it]                                                        {'loss': 16.6459, 'grad_norm': 9.8125, 'learning_rate': 2.8831865617037726e-06, 'epoch': 5.56}
 88%|████████▊ | 4382/5000 [21:40:26<2:49:14, 16.43s/it] 88%|████████▊ | 4383/5000 [21:40:41<2:46:51, 16.23s/it]                                                        {'loss': 17.778, 'grad_norm': 10.5625, 'learning_rate': 2.8739931531668655e-06, 'epoch': 5.57}
 88%|████████▊ | 4383/5000 [21:40:41<2:46:51, 16.23s/it] 88%|████████▊ | 4384/5000 [21:41:05<3:10:57, 18.60s/it]                                                        {'loss': 16.6082, 'grad_norm': 7.46875, 'learning_rate': 2.8648137976368007e-06, 'epoch': 5.57}
 88%|████████▊ | 4384/5000 [21:41:05<3:10:57, 18.60s/it] 88%|████████▊ | 4385/5000 [21:41:20<2:58:23, 17.40s/it]                                                        {'loss': 17.9606, 'grad_norm': 14.1875, 'learning_rate': 2.8556484991289503e-06, 'epoch': 5.57}
 88%|████████▊ | 4385/5000 [21:41:20<2:58:23, 17.40s/it] 88%|████████▊ | 4386/5000 [21:41:39<3:01:42, 17.76s/it]                                                        {'loss': 16.8865, 'grad_norm': 9.6875, 'learning_rate': 2.846497261652526e-06, 'epoch': 5.57}
 88%|████████▊ | 4386/5000 [21:41:39<3:01:42, 17.76s/it] 88%|████████▊ | 4387/5000 [21:41:58<3:07:38, 18.37s/it]                                                        {'loss': 16.2292, 'grad_norm': 6.9375, 'learning_rate': 2.837360089210578e-06, 'epoch': 5.57}
 88%|████████▊ | 4387/5000 [21:41:58<3:07:38, 18.37s/it] 88%|████████▊ | 4388/5000 [21:42:28<3:42:45, 21.84s/it]                                                        {'loss': 17.6176, 'grad_norm': 11.4375, 'learning_rate': 2.8282369858000316e-06, 'epoch': 5.57}
 88%|████████▊ | 4388/5000 [21:42:28<3:42:45, 21.84s/it] 88%|████████▊ | 4389/5000 [21:42:44<3:22:47, 19.91s/it]                                                        {'loss': 17.9301, 'grad_norm': 12.25, 'learning_rate': 2.8191279554116298e-06, 'epoch': 5.57}
 88%|████████▊ | 4389/5000 [21:42:44<3:22:47, 19.91s/it] 88%|████████▊ | 4390/5000 [21:43:00<3:10:32, 18.74s/it]                                                        {'loss': 17.7309, 'grad_norm': 13.375, 'learning_rate': 2.8100330020299806e-06, 'epoch': 5.57}
 88%|████████▊ | 4390/5000 [21:43:00<3:10:32, 18.74s/it] 88%|████████▊ | 4391/5000 [21:43:19<3:11:01, 18.82s/it]                                                        {'loss': 17.3351, 'grad_norm': 11.0, 'learning_rate': 2.8009521296335208e-06, 'epoch': 5.58}
 88%|████████▊ | 4391/5000 [21:43:19<3:11:01, 18.82s/it] 88%|████████▊ | 4392/5000 [21:43:40<3:18:54, 19.63s/it]                                                        {'loss': 16.7093, 'grad_norm': 8.5625, 'learning_rate': 2.7918853421945362e-06, 'epoch': 5.58}
 88%|████████▊ | 4392/5000 [21:43:40<3:18:54, 19.63s/it] 88%|████████▊ | 4393/5000 [21:43:54<3:00:54, 17.88s/it]                                                        {'loss': 18.98, 'grad_norm': 12.25, 'learning_rate': 2.7828326436791504e-06, 'epoch': 5.58}
 88%|████████▊ | 4393/5000 [21:43:54<3:00:54, 17.88s/it] 88%|████████▊ | 4394/5000 [21:44:11<2:57:51, 17.61s/it]                                                        {'loss': 17.6861, 'grad_norm': 10.9375, 'learning_rate': 2.7737940380473074e-06, 'epoch': 5.58}
 88%|████████▊ | 4394/5000 [21:44:11<2:57:51, 17.61s/it] 88%|████████▊ | 4395/5000 [21:44:32<3:07:03, 18.55s/it]                                                        {'loss': 16.2109, 'grad_norm': 9.5625, 'learning_rate': 2.7647695292528123e-06, 'epoch': 5.58}
 88%|████████▊ | 4395/5000 [21:44:32<3:07:03, 18.55s/it] 88%|████████▊ | 4396/5000 [21:44:48<2:58:01, 17.69s/it]                                                        {'loss': 17.9723, 'grad_norm': 10.5625, 'learning_rate': 2.7557591212432956e-06, 'epoch': 5.58}
 88%|████████▊ | 4396/5000 [21:44:48<2:58:01, 17.69s/it] 88%|████████▊ | 4397/5000 [21:45:03<2:52:28, 17.16s/it]                                                        {'loss': 17.8321, 'grad_norm': 14.4375, 'learning_rate': 2.746762817960198e-06, 'epoch': 5.58}
 88%|████████▊ | 4397/5000 [21:45:03<2:52:28, 17.16s/it] 88%|████████▊ | 4398/5000 [21:45:20<2:51:45, 17.12s/it]                                                        {'loss': 16.862, 'grad_norm': 7.375, 'learning_rate': 2.7377806233388284e-06, 'epoch': 5.58}
 88%|████████▊ | 4398/5000 [21:45:20<2:51:45, 17.12s/it] 88%|████████▊ | 4399/5000 [21:45:35<2:44:59, 16.47s/it]                                                        {'loss': 17.7839, 'grad_norm': 10.5, 'learning_rate': 2.7288125413082934e-06, 'epoch': 5.59}
 88%|████████▊ | 4399/5000 [21:45:35<2:44:59, 16.47s/it] 88%|████████▊ | 4400/5000 [21:45:58<3:03:38, 18.36s/it]                                                        {'loss': 16.9672, 'grad_norm': 6.90625, 'learning_rate': 2.719858575791534e-06, 'epoch': 5.59}
 88%|████████▊ | 4400/5000 [21:45:58<3:03:38, 18.36s/it] 88%|████████▊ | 4401/5000 [21:46:17<3:05:28, 18.58s/it]                                                        {'loss': 17.7071, 'grad_norm': 10.9375, 'learning_rate': 2.7109187307053266e-06, 'epoch': 5.59}
 88%|████████▊ | 4401/5000 [21:46:17<3:05:28, 18.58s/it] 88%|████████▊ | 4402/5000 [21:46:33<2:55:47, 17.64s/it]                                                        {'loss': 18.9711, 'grad_norm': 11.6875, 'learning_rate': 2.701993009960263e-06, 'epoch': 5.59}
 88%|████████▊ | 4402/5000 [21:46:33<2:55:47, 17.64s/it] 88%|████████▊ | 4403/5000 [21:46:48<2:47:31, 16.84s/it]                                                        {'loss': 16.9495, 'grad_norm': 9.4375, 'learning_rate': 2.693081417460753e-06, 'epoch': 5.59}
 88%|████████▊ | 4403/5000 [21:46:48<2:47:31, 16.84s/it] 88%|████████▊ | 4404/5000 [21:47:03<2:41:39, 16.27s/it]                                                        {'loss': 18.7228, 'grad_norm': 16.375, 'learning_rate': 2.684183957105022e-06, 'epoch': 5.59}
 88%|████████▊ | 4404/5000 [21:47:03<2:41:39, 16.27s/it] 88%|████████▊ | 4405/5000 [21:47:19<2:41:24, 16.28s/it]                                                        {'loss': 18.9847, 'grad_norm': 12.6875, 'learning_rate': 2.6753006327851388e-06, 'epoch': 5.59}
 88%|████████▊ | 4405/5000 [21:47:19<2:41:24, 16.28s/it] 88%|████████▊ | 4406/5000 [21:47:36<2:43:52, 16.55s/it]                                                        {'loss': 17.4378, 'grad_norm': 10.625, 'learning_rate': 2.6664314483869612e-06, 'epoch': 5.59}
 88%|████████▊ | 4406/5000 [21:47:36<2:43:52, 16.55s/it] 88%|████████▊ | 4407/5000 [21:47:50<2:35:18, 15.71s/it]                                                        {'loss': 19.4959, 'grad_norm': 12.1875, 'learning_rate': 2.6575764077901655e-06, 'epoch': 5.6}
 88%|████████▊ | 4407/5000 [21:47:50<2:35:18, 15.71s/it] 88%|████████▊ | 4408/5000 [21:48:04<2:30:35, 15.26s/it]                                                        {'loss': 17.9432, 'grad_norm': 14.0625, 'learning_rate': 2.6487355148682647e-06, 'epoch': 5.6}
 88%|████████▊ | 4408/5000 [21:48:04<2:30:35, 15.26s/it] 88%|████████▊ | 4409/5000 [21:48:17<2:23:11, 14.54s/it]                                                        {'loss': 17.9187, 'grad_norm': 12.0, 'learning_rate': 2.639908773488551e-06, 'epoch': 5.6}
 88%|████████▊ | 4409/5000 [21:48:17<2:23:11, 14.54s/it] 88%|████████▊ | 4410/5000 [21:48:33<2:27:40, 15.02s/it]                                                        {'loss': 18.1906, 'grad_norm': 30.625, 'learning_rate': 2.631096187512142e-06, 'epoch': 5.6}
 88%|████████▊ | 4410/5000 [21:48:33<2:27:40, 15.02s/it] 88%|████████▊ | 4411/5000 [21:48:50<2:32:25, 15.53s/it]                                                        {'loss': 17.8532, 'grad_norm': 8.125, 'learning_rate': 2.622297760793972e-06, 'epoch': 5.6}
 88%|████████▊ | 4411/5000 [21:48:50<2:32:25, 15.53s/it] 88%|████████▊ | 4412/5000 [21:49:06<2:34:30, 15.77s/it]                                                        {'loss': 18.1133, 'grad_norm': 14.625, 'learning_rate': 2.6135134971827603e-06, 'epoch': 5.6}
 88%|████████▊ | 4412/5000 [21:49:06<2:34:30, 15.77s/it] 88%|████████▊ | 4413/5000 [21:49:22<2:33:43, 15.71s/it]                                                        {'loss': 18.5997, 'grad_norm': 39.0, 'learning_rate': 2.6047434005210537e-06, 'epoch': 5.6}
 88%|████████▊ | 4413/5000 [21:49:22<2:33:43, 15.71s/it] 88%|████████▊ | 4414/5000 [21:49:48<3:03:14, 18.76s/it]                                                        {'loss': 17.0707, 'grad_norm': 9.75, 'learning_rate': 2.5959874746451747e-06, 'epoch': 5.61}
 88%|████████▊ | 4414/5000 [21:49:48<3:03:14, 18.76s/it] 88%|████████▊ | 4415/5000 [21:50:02<2:50:05, 17.45s/it]                                                        {'loss': 18.2046, 'grad_norm': 11.0625, 'learning_rate': 2.5872457233852826e-06, 'epoch': 5.61}
 88%|████████▊ | 4415/5000 [21:50:02<2:50:05, 17.45s/it] 88%|████████▊ | 4416/5000 [21:50:18<2:44:17, 16.88s/it]                                                        {'loss': 17.1208, 'grad_norm': 9.9375, 'learning_rate': 2.578518150565301e-06, 'epoch': 5.61}
 88%|████████▊ | 4416/5000 [21:50:18<2:44:17, 16.88s/it] 88%|████████▊ | 4417/5000 [21:50:35<2:46:24, 17.13s/it]                                                        {'loss': 17.2467, 'grad_norm': 11.75, 'learning_rate': 2.569804760002973e-06, 'epoch': 5.61}
 88%|████████▊ | 4417/5000 [21:50:35<2:46:24, 17.13s/it] 88%|████████▊ | 4418/5000 [21:50:52<2:44:39, 16.98s/it]                                                        {'loss': 17.908, 'grad_norm': 20.375, 'learning_rate': 2.561105555509835e-06, 'epoch': 5.61}
 88%|████████▊ | 4418/5000 [21:50:52<2:44:39, 16.98s/it] 88%|████████▊ | 4419/5000 [21:51:09<2:43:36, 16.90s/it]                                                        {'loss': 16.9639, 'grad_norm': 12.8125, 'learning_rate': 2.5524205408912035e-06, 'epoch': 5.61}
 88%|████████▊ | 4419/5000 [21:51:09<2:43:36, 16.90s/it] 88%|████████▊ | 4420/5000 [21:51:24<2:37:58, 16.34s/it]                                                        {'loss': 18.8267, 'grad_norm': 14.25, 'learning_rate': 2.5437497199462107e-06, 'epoch': 5.61}
 88%|████████▊ | 4420/5000 [21:51:24<2:37:58, 16.34s/it] 88%|████████▊ | 4421/5000 [21:51:40<2:37:41, 16.34s/it]                                                        {'loss': 17.7191, 'grad_norm': 9.875, 'learning_rate': 2.5350930964677634e-06, 'epoch': 5.61}
 88%|████████▊ | 4421/5000 [21:51:40<2:37:41, 16.34s/it] 88%|████████▊ | 4422/5000 [21:52:02<2:53:53, 18.05s/it]                                                        {'loss': 21.1491, 'grad_norm': 19.375, 'learning_rate': 2.526450674242562e-06, 'epoch': 5.62}
 88%|████████▊ | 4422/5000 [21:52:02<2:53:53, 18.05s/it] 88%|████████▊ | 4423/5000 [21:52:19<2:50:19, 17.71s/it]                                                        {'loss': 17.8668, 'grad_norm': 12.9375, 'learning_rate': 2.517822457051091e-06, 'epoch': 5.62}
 88%|████████▊ | 4423/5000 [21:52:19<2:50:19, 17.71s/it] 88%|████████▊ | 4424/5000 [21:52:34<2:43:08, 16.99s/it]                                                        {'loss': 16.8438, 'grad_norm': 11.1875, 'learning_rate': 2.5092084486676388e-06, 'epoch': 5.62}
 88%|████████▊ | 4424/5000 [21:52:34<2:43:08, 16.99s/it] 88%|████████▊ | 4425/5000 [21:52:47<2:31:22, 15.80s/it]                                                        {'loss': 18.5101, 'grad_norm': 11.875, 'learning_rate': 2.500608652860256e-06, 'epoch': 5.62}
 88%|████████▊ | 4425/5000 [21:52:47<2:31:22, 15.80s/it] 89%|████████▊ | 4426/5000 [21:53:23<3:28:19, 21.78s/it]                                                        {'loss': 18.5253, 'grad_norm': 11.0, 'learning_rate': 2.4920230733907814e-06, 'epoch': 5.62}
 89%|████████▊ | 4426/5000 [21:53:23<3:28:19, 21.78s/it] 89%|████████▊ | 4427/5000 [21:53:41<3:16:29, 20.57s/it]                                                        {'loss': 18.0452, 'grad_norm': 11.375, 'learning_rate': 2.4834517140148497e-06, 'epoch': 5.62}
 89%|████████▊ | 4427/5000 [21:53:41<3:16:29, 20.57s/it] 89%|████████▊ | 4428/5000 [21:53:57<3:04:38, 19.37s/it]                                                        {'loss': 17.8294, 'grad_norm': 14.25, 'learning_rate': 2.474894578481858e-06, 'epoch': 5.62}
 89%|████████▊ | 4428/5000 [21:53:57<3:04:38, 19.37s/it] 89%|████████▊ | 4429/5000 [21:54:22<3:18:29, 20.86s/it]                                                        {'loss': 19.5165, 'grad_norm': 19.625, 'learning_rate': 2.466351670534987e-06, 'epoch': 5.62}
 89%|████████▊ | 4429/5000 [21:54:22<3:18:29, 20.86s/it] 89%|████████▊ | 4430/5000 [21:54:38<3:04:09, 19.38s/it]                                                        {'loss': 17.5424, 'grad_norm': 22.125, 'learning_rate': 2.4578229939112028e-06, 'epoch': 5.63}
 89%|████████▊ | 4430/5000 [21:54:38<3:04:09, 19.38s/it] 89%|████████▊ | 4431/5000 [21:54:52<2:51:05, 18.04s/it]                                                        {'loss': 20.1239, 'grad_norm': 19.75, 'learning_rate': 2.449308552341232e-06, 'epoch': 5.63}
 89%|████████▊ | 4431/5000 [21:54:52<2:51:05, 18.04s/it] 89%|████████▊ | 4432/5000 [21:55:17<3:10:01, 20.07s/it]                                                        {'loss': 18.7463, 'grad_norm': 16.875, 'learning_rate': 2.44080834954958e-06, 'epoch': 5.63}
 89%|████████▊ | 4432/5000 [21:55:17<3:10:01, 20.07s/it] 89%|████████▊ | 4433/5000 [21:55:37<3:08:59, 20.00s/it]                                                        {'loss': 16.1132, 'grad_norm': 7.4375, 'learning_rate': 2.432322389254527e-06, 'epoch': 5.63}
 89%|████████▊ | 4433/5000 [21:55:37<3:08:59, 20.00s/it] 89%|████████▊ | 4434/5000 [21:55:54<3:00:07, 19.09s/it]                                                        {'loss': 18.7567, 'grad_norm': 13.75, 'learning_rate': 2.4238506751681234e-06, 'epoch': 5.63}
 89%|████████▊ | 4434/5000 [21:55:54<3:00:07, 19.09s/it] 89%|████████▊ | 4435/5000 [21:56:08<2:46:15, 17.66s/it]                                                        {'loss': 18.669, 'grad_norm': 12.625, 'learning_rate': 2.4153932109961733e-06, 'epoch': 5.63}
 89%|████████▊ | 4435/5000 [21:56:08<2:46:15, 17.66s/it] 89%|████████▊ | 4436/5000 [21:56:34<3:07:03, 19.90s/it]                                                        {'loss': 18.0908, 'grad_norm': 33.25, 'learning_rate': 2.406950000438273e-06, 'epoch': 5.63}
 89%|████████▊ | 4436/5000 [21:56:34<3:07:03, 19.90s/it] 89%|████████▊ | 4437/5000 [21:56:49<2:53:51, 18.53s/it]                                                        {'loss': 16.9104, 'grad_norm': 8.9375, 'learning_rate': 2.3985210471877565e-06, 'epoch': 5.63}
 89%|████████▊ | 4437/5000 [21:56:49<2:53:51, 18.53s/it] 89%|████████▉ | 4438/5000 [21:57:05<2:46:39, 17.79s/it]                                                        {'loss': 18.0741, 'grad_norm': 8.0, 'learning_rate': 2.3901063549317436e-06, 'epoch': 5.64}
 89%|████████▉ | 4438/5000 [21:57:05<2:46:39, 17.79s/it] 89%|████████▉ | 4439/5000 [21:57:21<2:40:50, 17.20s/it]                                                        {'loss': 19.5311, 'grad_norm': 13.5, 'learning_rate': 2.381705927351102e-06, 'epoch': 5.64}
 89%|████████▉ | 4439/5000 [21:57:21<2:40:50, 17.20s/it] 89%|████████▉ | 4440/5000 [21:57:36<2:36:21, 16.75s/it]                                                        {'loss': 18.6005, 'grad_norm': 13.125, 'learning_rate': 2.373319768120471e-06, 'epoch': 5.64}
 89%|████████▉ | 4440/5000 [21:57:36<2:36:21, 16.75s/it] 89%|████████▉ | 4441/5000 [21:57:54<2:38:19, 16.99s/it]                                                        {'loss': 16.7503, 'grad_norm': 10.0625, 'learning_rate': 2.3649478809082366e-06, 'epoch': 5.64}
 89%|████████▉ | 4441/5000 [21:57:54<2:38:19, 16.99s/it] 89%|████████▉ | 4442/5000 [21:58:15<2:47:46, 18.04s/it]                                                        {'loss': 17.954, 'grad_norm': 10.125, 'learning_rate': 2.356590269376538e-06, 'epoch': 5.64}
 89%|████████▉ | 4442/5000 [21:58:15<2:47:46, 18.04s/it] 89%|████████▉ | 4443/5000 [21:58:34<2:52:29, 18.58s/it]                                                        {'loss': 17.4292, 'grad_norm': 9.6875, 'learning_rate': 2.348246937181296e-06, 'epoch': 5.64}
 89%|████████▉ | 4443/5000 [21:58:34<2:52:29, 18.58s/it] 89%|████████▉ | 4444/5000 [21:58:51<2:48:12, 18.15s/it]                                                        {'loss': 17.5364, 'grad_norm': 11.9375, 'learning_rate': 2.3399178879721605e-06, 'epoch': 5.64}
 89%|████████▉ | 4444/5000 [21:58:51<2:48:12, 18.15s/it] 89%|████████▉ | 4445/5000 [21:59:08<2:44:39, 17.80s/it]                                                        {'loss': 17.4329, 'grad_norm': 10.1875, 'learning_rate': 2.331603125392528e-06, 'epoch': 5.64}
 89%|████████▉ | 4445/5000 [21:59:08<2:44:39, 17.80s/it] 89%|████████▉ | 4446/5000 [21:59:34<3:05:53, 20.13s/it]                                                        {'loss': 18.7264, 'grad_norm': 15.0625, 'learning_rate': 2.3233026530795784e-06, 'epoch': 5.65}
 89%|████████▉ | 4446/5000 [21:59:34<3:05:53, 20.13s/it] 89%|████████▉ | 4447/5000 [21:59:59<3:18:10, 21.50s/it]                                                        {'loss': 17.2786, 'grad_norm': 13.9375, 'learning_rate': 2.315016474664209e-06, 'epoch': 5.65}
 89%|████████▉ | 4447/5000 [21:59:59<3:18:10, 21.50s/it] 89%|████████▉ | 4448/5000 [22:00:12<2:53:59, 18.91s/it]                                                        {'loss': 18.5318, 'grad_norm': 13.1875, 'learning_rate': 2.306744593771072e-06, 'epoch': 5.65}
 89%|████████▉ | 4448/5000 [22:00:12<2:53:59, 18.91s/it] 89%|████████▉ | 4449/5000 [22:00:27<2:44:13, 17.88s/it]                                                        {'loss': 17.2917, 'grad_norm': 11.625, 'learning_rate': 2.298487014018579e-06, 'epoch': 5.65}
 89%|████████▉ | 4449/5000 [22:00:27<2:44:13, 17.88s/it] 89%|████████▉ | 4450/5000 [22:00:56<3:15:34, 21.34s/it]                                                        {'loss': 17.8189, 'grad_norm': 9.0625, 'learning_rate': 2.2902437390188737e-06, 'epoch': 5.65}
 89%|████████▉ | 4450/5000 [22:00:56<3:15:34, 21.34s/it] 89%|████████▉ | 4451/5000 [22:01:12<2:58:59, 19.56s/it]                                                        {'loss': 17.5117, 'grad_norm': 8.625, 'learning_rate': 2.2820147723778366e-06, 'epoch': 5.65}
 89%|████████▉ | 4451/5000 [22:01:12<2:58:59, 19.56s/it] 89%|████████▉ | 4452/5000 [22:01:34<3:04:27, 20.20s/it]                                                        {'loss': 18.7335, 'grad_norm': 16.375, 'learning_rate': 2.2738001176951066e-06, 'epoch': 5.65}
 89%|████████▉ | 4452/5000 [22:01:34<3:04:27, 20.20s/it] 89%|████████▉ | 4453/5000 [22:01:59<3:17:18, 21.64s/it]                                                        {'loss': 18.3329, 'grad_norm': 11.5625, 'learning_rate': 2.2655997785640517e-06, 'epoch': 5.65}
 89%|████████▉ | 4453/5000 [22:01:59<3:17:18, 21.64s/it] 89%|████████▉ | 4454/5000 [22:02:12<2:55:35, 19.30s/it]                                                        {'loss': 19.449, 'grad_norm': 18.25, 'learning_rate': 2.257413758571776e-06, 'epoch': 5.66}
 89%|████████▉ | 4454/5000 [22:02:12<2:55:35, 19.30s/it] 89%|████████▉ | 4455/5000 [22:02:30<2:49:24, 18.65s/it]                                                        {'loss': 17.0698, 'grad_norm': 7.3125, 'learning_rate': 2.2492420612991157e-06, 'epoch': 5.66}
 89%|████████▉ | 4455/5000 [22:02:30<2:49:24, 18.65s/it] 89%|████████▉ | 4456/5000 [22:02:48<2:49:36, 18.71s/it]                                                        {'loss': 18.7895, 'grad_norm': 15.3125, 'learning_rate': 2.2410846903206627e-06, 'epoch': 5.66}
 89%|████████▉ | 4456/5000 [22:02:48<2:49:36, 18.71s/it] 89%|████████▉ | 4457/5000 [22:03:05<2:42:11, 17.92s/it]                                                        {'loss': 18.1569, 'grad_norm': 11.875, 'learning_rate': 2.2329416492047207e-06, 'epoch': 5.66}
 89%|████████▉ | 4457/5000 [22:03:05<2:42:11, 17.92s/it] 89%|████████▉ | 4458/5000 [22:03:21<2:38:25, 17.54s/it]                                                        {'loss': 17.0253, 'grad_norm': 8.8125, 'learning_rate': 2.2248129415133313e-06, 'epoch': 5.66}
 89%|████████▉ | 4458/5000 [22:03:21<2:38:25, 17.54s/it] 89%|████████▉ | 4459/5000 [22:03:37<2:33:58, 17.08s/it]                                                        {'loss': 16.8129, 'grad_norm': 10.6875, 'learning_rate': 2.216698570802275e-06, 'epoch': 5.66}
 89%|████████▉ | 4459/5000 [22:03:37<2:33:58, 17.08s/it] 89%|████████▉ | 4460/5000 [22:03:56<2:38:23, 17.60s/it]                                                        {'loss': 16.8582, 'grad_norm': 11.1875, 'learning_rate': 2.208598540621041e-06, 'epoch': 5.66}
 89%|████████▉ | 4460/5000 [22:03:56<2:38:23, 17.60s/it] 89%|████████▉ | 4461/5000 [22:04:12<2:33:12, 17.05s/it]                                                        {'loss': 17.0432, 'grad_norm': 8.125, 'learning_rate': 2.200512854512871e-06, 'epoch': 5.66}
 89%|████████▉ | 4461/5000 [22:04:12<2:33:12, 17.05s/it] 89%|████████▉ | 4462/5000 [22:04:26<2:24:17, 16.09s/it]                                                        {'loss': 20.0702, 'grad_norm': 14.625, 'learning_rate': 2.192441516014707e-06, 'epoch': 5.67}
 89%|████████▉ | 4462/5000 [22:04:26<2:24:17, 16.09s/it] 89%|████████▉ | 4463/5000 [22:04:43<2:26:46, 16.40s/it]                                                        {'loss': 19.1204, 'grad_norm': 13.0625, 'learning_rate': 2.1843845286572353e-06, 'epoch': 5.67}
 89%|████████▉ | 4463/5000 [22:04:43<2:26:46, 16.40s/it] 89%|████████▉ | 4464/5000 [22:04:59<2:27:27, 16.51s/it]                                                        {'loss': 15.9652, 'grad_norm': 9.125, 'learning_rate': 2.1763418959648475e-06, 'epoch': 5.67}
 89%|████████▉ | 4464/5000 [22:04:59<2:27:27, 16.51s/it] 89%|████████▉ | 4465/5000 [22:05:17<2:29:55, 16.81s/it]                                                        {'loss': 18.4063, 'grad_norm': 9.5, 'learning_rate': 2.168313621455679e-06, 'epoch': 5.67}
 89%|████████▉ | 4465/5000 [22:05:17<2:29:55, 16.81s/it] 89%|████████▉ | 4466/5000 [22:05:41<2:50:08, 19.12s/it]                                                        {'loss': 18.4302, 'grad_norm': 12.75, 'learning_rate': 2.160299708641558e-06, 'epoch': 5.67}
 89%|████████▉ | 4466/5000 [22:05:41<2:50:08, 19.12s/it] 89%|████████▉ | 4467/5000 [22:05:57<2:39:32, 17.96s/it]                                                        {'loss': 19.8347, 'grad_norm': 17.0, 'learning_rate': 2.152300161028038e-06, 'epoch': 5.67}
 89%|████████▉ | 4467/5000 [22:05:57<2:39:32, 17.96s/it] 89%|████████▉ | 4468/5000 [22:06:11<2:30:38, 16.99s/it]                                                        {'loss': 18.2893, 'grad_norm': 12.8125, 'learning_rate': 2.1443149821144073e-06, 'epoch': 5.67}
 89%|████████▉ | 4468/5000 [22:06:11<2:30:38, 16.99s/it] 89%|████████▉ | 4469/5000 [22:06:28<2:28:02, 16.73s/it]                                                        {'loss': 17.026, 'grad_norm': 7.5, 'learning_rate': 2.1363441753936455e-06, 'epoch': 5.67}
 89%|████████▉ | 4469/5000 [22:06:28<2:28:02, 16.73s/it] 89%|████████▉ | 4470/5000 [22:06:53<2:51:32, 19.42s/it]                                                        {'loss': 17.7189, 'grad_norm': 9.8125, 'learning_rate': 2.1283877443524515e-06, 'epoch': 5.68}
 89%|████████▉ | 4470/5000 [22:06:53<2:51:32, 19.42s/it] 89%|████████▉ | 4471/5000 [22:07:13<2:52:35, 19.57s/it]                                                        {'loss': 17.1043, 'grad_norm': 9.75, 'learning_rate': 2.1204456924712453e-06, 'epoch': 5.68}
 89%|████████▉ | 4471/5000 [22:07:13<2:52:35, 19.57s/it] 89%|████████▉ | 4472/5000 [22:07:29<2:42:03, 18.42s/it]                                                        {'loss': 17.641, 'grad_norm': 10.375, 'learning_rate': 2.1125180232241477e-06, 'epoch': 5.68}
 89%|████████▉ | 4472/5000 [22:07:29<2:42:03, 18.42s/it] 89%|████████▉ | 4473/5000 [22:07:43<2:30:12, 17.10s/it]                                                        {'loss': 17.5163, 'grad_norm': 8.625, 'learning_rate': 2.104604740078993e-06, 'epoch': 5.68}
 89%|████████▉ | 4473/5000 [22:07:43<2:30:12, 17.10s/it] 89%|████████▉ | 4474/5000 [22:08:00<2:30:06, 17.12s/it]                                                        {'loss': 16.9577, 'grad_norm': 9.9375, 'learning_rate': 2.0967058464973113e-06, 'epoch': 5.68}
 89%|████████▉ | 4474/5000 [22:08:00<2:30:06, 17.12s/it] 90%|████████▉ | 4475/5000 [22:08:18<2:30:28, 17.20s/it]                                                        {'loss': 17.8858, 'grad_norm': 16.875, 'learning_rate': 2.0888213459343587e-06, 'epoch': 5.68}
 90%|████████▉ | 4475/5000 [22:08:18<2:30:28, 17.20s/it] 90%|████████▉ | 4476/5000 [22:08:34<2:29:16, 17.09s/it]                                                        {'loss': 16.8179, 'grad_norm': 7.90625, 'learning_rate': 2.080951241839077e-06, 'epoch': 5.68}
 90%|████████▉ | 4476/5000 [22:08:34<2:29:16, 17.09s/it] 90%|████████▉ | 4477/5000 [22:08:48<2:20:18, 16.10s/it]                                                        {'loss': 17.9512, 'grad_norm': 12.8125, 'learning_rate': 2.073095537654111e-06, 'epoch': 5.69}
 90%|████████▉ | 4477/5000 [22:08:48<2:20:18, 16.10s/it] 90%|████████▉ | 4478/5000 [22:09:01<2:11:46, 15.15s/it]                                                        {'loss': 19.7366, 'grad_norm': 17.25, 'learning_rate': 2.065254236815828e-06, 'epoch': 5.69}
 90%|████████▉ | 4478/5000 [22:09:01<2:11:46, 15.15s/it] 90%|████████▉ | 4479/5000 [22:09:16<2:11:22, 15.13s/it]                                                        {'loss': 18.5677, 'grad_norm': 10.6875, 'learning_rate': 2.057427342754269e-06, 'epoch': 5.69}
 90%|████████▉ | 4479/5000 [22:09:16<2:11:22, 15.13s/it] 90%|████████▉ | 4480/5000 [22:09:43<2:40:25, 18.51s/it]                                                        {'loss': 16.7021, 'grad_norm': 8.1875, 'learning_rate': 2.0496148588931837e-06, 'epoch': 5.69}
 90%|████████▉ | 4480/5000 [22:09:43<2:40:25, 18.51s/it] 90%|████████▉ | 4481/5000 [22:09:59<2:35:54, 18.02s/it]                                                        {'loss': 17.9449, 'grad_norm': 14.8125, 'learning_rate': 2.0418167886500224e-06, 'epoch': 5.69}
 90%|████████▉ | 4481/5000 [22:09:59<2:35:54, 18.02s/it] 90%|████████▉ | 4482/5000 [22:10:15<2:29:35, 17.33s/it]                                                        {'loss': 18.034, 'grad_norm': 12.875, 'learning_rate': 2.034033135435926e-06, 'epoch': 5.69}
 90%|████████▉ | 4482/5000 [22:10:15<2:29:35, 17.33s/it] 90%|████████▉ | 4483/5000 [22:10:31<2:24:27, 16.76s/it]                                                        {'loss': 17.6683, 'grad_norm': 10.6875, 'learning_rate': 2.0262639026557154e-06, 'epoch': 5.69}
 90%|████████▉ | 4483/5000 [22:10:31<2:24:27, 16.76s/it] 90%|████████▉ | 4484/5000 [22:10:46<2:20:50, 16.38s/it]                                                        {'loss': 18.6351, 'grad_norm': 11.1875, 'learning_rate': 2.018509093707938e-06, 'epoch': 5.69}
 90%|████████▉ | 4484/5000 [22:10:46<2:20:50, 16.38s/it] 90%|████████▉ | 4485/5000 [22:11:08<2:34:57, 18.05s/it]                                                        {'loss': 18.1169, 'grad_norm': 14.5, 'learning_rate': 2.0107687119847944e-06, 'epoch': 5.7}
 90%|████████▉ | 4485/5000 [22:11:08<2:34:57, 18.05s/it] 90%|████████▉ | 4486/5000 [22:11:24<2:30:07, 17.52s/it]                                                        {'loss': 17.3525, 'grad_norm': 7.84375, 'learning_rate': 2.0030427608722023e-06, 'epoch': 5.7}
 90%|████████▉ | 4486/5000 [22:11:24<2:30:07, 17.52s/it] 90%|████████▉ | 4487/5000 [22:11:38<2:19:22, 16.30s/it]                                                        {'loss': 19.1397, 'grad_norm': 12.0625, 'learning_rate': 1.9953312437497416e-06, 'epoch': 5.7}
 90%|████████▉ | 4487/5000 [22:11:38<2:19:22, 16.30s/it] 90%|████████▉ | 4488/5000 [22:11:58<2:28:40, 17.42s/it]                                                        {'loss': 17.3531, 'grad_norm': 8.9375, 'learning_rate': 1.987634163990709e-06, 'epoch': 5.7}
 90%|████████▉ | 4488/5000 [22:11:58<2:28:40, 17.42s/it] 90%|████████▉ | 4489/5000 [22:12:13<2:21:49, 16.65s/it]                                                        {'loss': 18.6337, 'grad_norm': 12.875, 'learning_rate': 1.979951524962056e-06, 'epoch': 5.7}
 90%|████████▉ | 4489/5000 [22:12:13<2:21:49, 16.65s/it] 90%|████████▉ | 4490/5000 [22:12:26<2:12:44, 15.62s/it]                                                        {'loss': 17.8285, 'grad_norm': 10.875, 'learning_rate': 1.97228333002443e-06, 'epoch': 5.7}
 90%|████████▉ | 4490/5000 [22:12:26<2:12:44, 15.62s/it] 90%|████████▉ | 4491/5000 [22:12:41<2:11:48, 15.54s/it]                                                        {'loss': 17.73, 'grad_norm': 8.75, 'learning_rate': 1.964629582532166e-06, 'epoch': 5.7}
 90%|████████▉ | 4491/5000 [22:12:41<2:11:48, 15.54s/it] 90%|████████▉ | 4492/5000 [22:13:06<2:33:53, 18.18s/it]                                                        {'loss': 18.956, 'grad_norm': 22.875, 'learning_rate': 1.956990285833272e-06, 'epoch': 5.7}
 90%|████████▉ | 4492/5000 [22:13:06<2:33:53, 18.18s/it] 90%|████████▉ | 4493/5000 [22:13:20<2:23:28, 16.98s/it]                                                        {'loss': 18.7719, 'grad_norm': 17.25, 'learning_rate': 1.9493654432694294e-06, 'epoch': 5.71}
 90%|████████▉ | 4493/5000 [22:13:20<2:23:28, 16.98s/it] 90%|████████▉ | 4494/5000 [22:13:46<2:45:46, 19.66s/it]                                                        {'loss': 17.4573, 'grad_norm': 8.3125, 'learning_rate': 1.941755058176014e-06, 'epoch': 5.71}
 90%|████████▉ | 4494/5000 [22:13:46<2:45:46, 19.66s/it] 90%|████████▉ | 4495/5000 [22:14:02<2:36:45, 18.62s/it]                                                        {'loss': 16.717, 'grad_norm': 7.28125, 'learning_rate': 1.9341591338820628e-06, 'epoch': 5.71}
 90%|████████▉ | 4495/5000 [22:14:02<2:36:45, 18.62s/it] 90%|████████▉ | 4496/5000 [22:14:18<2:30:51, 17.96s/it]                                                        {'loss': 17.4858, 'grad_norm': 8.125, 'learning_rate': 1.926577673710284e-06, 'epoch': 5.71}
 90%|████████▉ | 4496/5000 [22:14:18<2:30:51, 17.96s/it] 90%|████████▉ | 4497/5000 [22:14:34<2:26:04, 17.42s/it]                                                        {'loss': 17.4777, 'grad_norm': 9.8125, 'learning_rate': 1.9190106809770745e-06, 'epoch': 5.71}
 90%|████████▉ | 4497/5000 [22:14:34<2:26:04, 17.42s/it] 90%|████████▉ | 4498/5000 [22:14:50<2:20:56, 16.85s/it]                                                        {'loss': 18.1583, 'grad_norm': 10.5, 'learning_rate': 1.911458158992494e-06, 'epoch': 5.71}
 90%|████████▉ | 4498/5000 [22:14:50<2:20:56, 16.85s/it] 90%|████████▉ | 4499/5000 [22:15:05<2:17:24, 16.46s/it]                                                        {'loss': 16.1364, 'grad_norm': 7.65625, 'learning_rate': 1.9039201110602597e-06, 'epoch': 5.71}
 90%|████████▉ | 4499/5000 [22:15:05<2:17:24, 16.46s/it] 90%|█████████ | 4500/5000 [22:15:20<2:12:49, 15.94s/it]                                                        {'loss': 18.4437, 'grad_norm': 9.625, 'learning_rate': 1.8963965404777875e-06, 'epoch': 5.71}
 90%|█████████ | 4500/5000 [22:15:20<2:12:49, 15.94s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:19,  4.41s/it][A
  3%|▎         | 3/88 [00:16<08:03,  5.69s/it][A
  5%|▍         | 4/88 [00:20<07:14,  5.18s/it][A
  6%|▌         | 5/88 [00:24<06:30,  4.70s/it][A
  7%|▋         | 6/88 [00:29<06:45,  4.94s/it][A
  8%|▊         | 7/88 [00:33<06:15,  4.64s/it][A
  9%|▉         | 8/88 [00:37<05:48,  4.36s/it][A
 10%|█         | 9/88 [00:41<05:22,  4.08s/it][A
 11%|█▏        | 10/88 [00:44<04:51,  3.74s/it][A
 12%|█▎        | 11/88 [00:46<04:27,  3.48s/it][A
 14%|█▎        | 12/88 [00:49<03:57,  3.13s/it][A
 15%|█▍        | 13/88 [00:51<03:41,  2.96s/it][A
 16%|█▌        | 14/88 [00:57<04:36,  3.74s/it][A
 17%|█▋        | 15/88 [01:02<05:11,  4.26s/it][A
 18%|█▊        | 16/88 [01:06<04:51,  4.04s/it][A
 19%|█▉        | 17/88 [01:11<05:15,  4.44s/it][A
 20%|██        | 18/88 [01:14<04:44,  4.06s/it][A
 22%|██▏       | 19/88 [01:19<04:44,  4.13s/it][A
 23%|██▎       | 20/88 [01:23<04:34,  4.04s/it][A
 24%|██▍       | 21/88 [01:26<04:15,  3.81s/it][A
 25%|██▌       | 22/88 [01:30<04:12,  3.82s/it][A
 26%|██▌       | 23/88 [01:32<03:45,  3.47s/it][A
 27%|██▋       | 24/88 [01:41<05:22,  5.05s/it][A
 28%|██▊       | 25/88 [01:44<04:47,  4.56s/it][A
 30%|██▉       | 26/88 [01:51<05:18,  5.13s/it][A
 31%|███       | 27/88 [01:55<04:58,  4.89s/it][A
 32%|███▏      | 28/88 [02:04<06:02,  6.04s/it][A
 33%|███▎      | 29/88 [02:09<05:42,  5.80s/it][A
 34%|███▍      | 30/88 [02:14<05:27,  5.65s/it][A
 35%|███▌      | 31/88 [02:19<04:55,  5.19s/it][A
 36%|███▋      | 32/88 [02:28<05:55,  6.34s/it][A
 38%|███▊      | 33/88 [02:32<05:21,  5.84s/it][A
 39%|███▊      | 34/88 [02:41<06:02,  6.72s/it][A
 40%|███▉      | 35/88 [02:44<04:56,  5.60s/it][A
 41%|████      | 36/88 [02:49<04:46,  5.51s/it][A
 42%|████▏     | 37/88 [02:53<04:13,  4.96s/it][A
 43%|████▎     | 38/88 [02:57<03:56,  4.73s/it][A
 44%|████▍     | 39/88 [03:00<03:28,  4.25s/it][A
 45%|████▌     | 40/88 [03:07<03:58,  4.97s/it][A
 47%|████▋     | 41/88 [03:17<04:58,  6.35s/it][A
 48%|████▊     | 42/88 [03:20<04:13,  5.51s/it][A
 49%|████▉     | 43/88 [03:29<04:51,  6.48s/it][A
 50%|█████     | 44/88 [03:32<04:03,  5.53s/it][A
 51%|█████     | 45/88 [03:37<03:45,  5.24s/it][A
 52%|█████▏    | 46/88 [03:41<03:23,  4.84s/it][A
 53%|█████▎    | 47/88 [03:43<02:44,  4.02s/it][A
 55%|█████▍    | 48/88 [03:45<02:20,  3.51s/it][A
 56%|█████▌    | 49/88 [03:50<02:34,  3.96s/it][A
 57%|█████▋    | 50/88 [03:54<02:32,  4.02s/it][A
 58%|█████▊    | 51/88 [03:59<02:31,  4.09s/it][A
 59%|█████▉    | 52/88 [04:04<02:45,  4.61s/it][A
 60%|██████    | 53/88 [04:09<02:45,  4.73s/it][A
 61%|██████▏   | 54/88 [04:19<03:30,  6.18s/it][A
 62%|██████▎   | 55/88 [04:24<03:11,  5.80s/it][A
 64%|██████▎   | 56/88 [04:26<02:34,  4.83s/it][A
 65%|██████▍   | 57/88 [04:30<02:21,  4.56s/it][A
 66%|██████▌   | 58/88 [04:39<02:56,  5.87s/it][A
 67%|██████▋   | 59/88 [04:44<02:41,  5.56s/it][A
 68%|██████▊   | 60/88 [04:47<02:16,  4.87s/it][A
 69%|██████▉   | 61/88 [04:51<02:04,  4.63s/it][A
 70%|███████   | 62/88 [04:54<01:47,  4.14s/it][A
 72%|███████▏  | 63/88 [05:00<01:53,  4.56s/it][A
 73%|███████▎  | 64/88 [05:04<01:45,  4.41s/it][A
 74%|███████▍  | 65/88 [05:08<01:41,  4.41s/it][A
 75%|███████▌  | 66/88 [05:12<01:32,  4.22s/it][A
 76%|███████▌  | 67/88 [05:15<01:21,  3.88s/it][A
 77%|███████▋  | 68/88 [05:20<01:23,  4.18s/it][A
 78%|███████▊  | 69/88 [05:27<01:31,  4.83s/it][A
 80%|███████▉  | 70/88 [05:31<01:22,  4.59s/it][A
 81%|████████  | 71/88 [05:34<01:14,  4.35s/it][A
 82%|████████▏ | 72/88 [05:38<01:07,  4.22s/it][A
 83%|████████▎ | 73/88 [05:41<00:58,  3.88s/it][A
 84%|████████▍ | 74/88 [05:44<00:50,  3.58s/it][A
 85%|████████▌ | 75/88 [05:49<00:51,  3.94s/it][A
 86%|████████▋ | 76/88 [05:53<00:46,  3.91s/it][A
 88%|████████▊ | 77/88 [06:00<00:53,  4.87s/it][A
 89%|████████▊ | 78/88 [06:04<00:45,  4.58s/it][A
 90%|████████▉ | 79/88 [06:09<00:41,  4.63s/it][A
 91%|█████████ | 80/88 [06:12<00:33,  4.13s/it][A
 92%|█████████▏| 81/88 [06:17<00:30,  4.43s/it][A
 93%|█████████▎| 82/88 [06:21<00:25,  4.27s/it][A
 94%|█████████▍| 83/88 [06:24<00:20,  4.14s/it][A
 95%|█████████▌| 84/88 [06:28<00:16,  4.06s/it][A
 97%|█████████▋| 85/88 [06:31<00:11,  3.72s/it][A
 98%|█████████▊| 86/88 [06:34<00:06,  3.46s/it][A
 99%|█████████▉| 87/88 [06:38<00:03,  3.65s/it][A
100%|██████████| 88/88 [06:42<00:00,  3.70s/it][A                                                        
                                               [A{'eval_loss': 17.490787506103516, 'eval_runtime': 406.4473, 'eval_samples_per_second': 6.889, 'eval_steps_per_second': 0.217, 'epoch': 5.71}
 90%|█████████ | 4500/5000 [22:22:07<2:12:49, 15.94s/it]
100%|██████████| 88/88 [06:42<00:00,  3.70s/it][A
                                               [A2024-06-14 07:57:44,737 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-14 07:57:54,771 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
/data0/xjw/anaconda3/envs/llm311/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 90%|█████████ | 4501/5000 [22:22:40<19:50:32, 143.15s/it]                                                          {'loss': 16.4753, 'grad_norm': 9.5625, 'learning_rate': 1.888887450536135e-06, 'epoch': 5.72}
 90%|█████████ | 4501/5000 [22:22:40<19:50:32, 143.15s/it] 90%|█████████ | 4502/5000 [22:23:00<14:39:50, 106.01s/it]                                                          {'loss': 16.2556, 'grad_norm': 25.375, 'learning_rate': 1.8813928445200265e-06, 'epoch': 5.72}
 90%|█████████ | 4502/5000 [22:23:00<14:39:50, 106.01s/it] 90%|█████████ | 4503/5000 [22:23:16<10:56:41, 79.28s/it]                                                          {'loss': 17.6935, 'grad_norm': 11.125, 'learning_rate': 1.8739127257078678e-06, 'epoch': 5.72}
 90%|█████████ | 4503/5000 [22:23:16<10:56:41, 79.28s/it] 90%|█████████ | 4504/5000 [22:23:32<8:16:12, 60.03s/it]                                                         {'loss': 17.9014, 'grad_norm': 11.125, 'learning_rate': 1.8664470973717154e-06, 'epoch': 5.72}
 90%|█████████ | 4504/5000 [22:23:32<8:16:12, 60.03s/it] 90%|█████████ | 4505/5000 [22:23:45<6:19:28, 46.00s/it]                                                        {'loss': 19.6307, 'grad_norm': 16.5, 'learning_rate': 1.8589959627772805e-06, 'epoch': 5.72}
 90%|█████████ | 4505/5000 [22:23:45<6:19:28, 46.00s/it] 90%|█████████ | 4506/5000 [22:24:00<5:02:58, 36.80s/it]                                                        {'loss': 18.8102, 'grad_norm': 15.6875, 'learning_rate': 1.8515593251839518e-06, 'epoch': 5.72}
 90%|█████████ | 4506/5000 [22:24:00<5:02:58, 36.80s/it] 90%|█████████ | 4507/5000 [22:24:22<4:24:54, 32.24s/it]                                                        {'loss': 17.3977, 'grad_norm': 10.4375, 'learning_rate': 1.8441371878447648e-06, 'epoch': 5.72}
 90%|█████████ | 4507/5000 [22:24:22<4:24:54, 32.24s/it] 90%|█████████ | 4508/5000 [22:24:45<4:03:07, 29.65s/it]                                                        {'loss': 18.051, 'grad_norm': 15.0, 'learning_rate': 1.8367295540064097e-06, 'epoch': 5.72}
 90%|█████████ | 4508/5000 [22:24:45<4:03:07, 29.65s/it] 90%|█████████ | 4509/5000 [22:25:01<3:29:13, 25.57s/it]                                                        {'loss': 18.6394, 'grad_norm': 9.875, 'learning_rate': 1.829336426909243e-06, 'epoch': 5.73}
 90%|█████████ | 4509/5000 [22:25:01<3:29:13, 25.57s/it] 90%|█████████ | 4510/5000 [22:25:17<3:03:32, 22.47s/it]                                                        {'loss': 17.5668, 'grad_norm': 8.625, 'learning_rate': 1.821957809787264e-06, 'epoch': 5.73}
 90%|█████████ | 4510/5000 [22:25:17<3:03:32, 22.47s/it] 90%|█████████ | 4511/5000 [22:25:32<2:45:54, 20.36s/it]                                                        {'loss': 17.6956, 'grad_norm': 10.875, 'learning_rate': 1.8145937058681415e-06, 'epoch': 5.73}
 90%|█████████ | 4511/5000 [22:25:32<2:45:54, 20.36s/it] 90%|█████████ | 4512/5000 [22:25:45<2:26:45, 18.04s/it]                                                        {'loss': 18.9593, 'grad_norm': 21.625, 'learning_rate': 1.8072441183731684e-06, 'epoch': 5.73}
 90%|█████████ | 4512/5000 [22:25:45<2:26:45, 18.04s/it] 90%|█████████ | 4513/5000 [22:26:21<3:09:39, 23.37s/it]                                                        {'loss': 16.8731, 'grad_norm': 7.6875, 'learning_rate': 1.7999090505173191e-06, 'epoch': 5.73}
 90%|█████████ | 4513/5000 [22:26:21<3:09:39, 23.37s/it] 90%|█████████ | 4514/5000 [22:26:38<2:54:40, 21.57s/it]                                                        {'loss': 16.9528, 'grad_norm': 9.5, 'learning_rate': 1.7925885055091955e-06, 'epoch': 5.73}
 90%|█████████ | 4514/5000 [22:26:38<2:54:40, 21.57s/it] 90%|█████████ | 4515/5000 [22:27:05<3:07:42, 23.22s/it]                                                        {'loss': 18.2681, 'grad_norm': 12.1875, 'learning_rate': 1.7852824865510496e-06, 'epoch': 5.73}
 90%|█████████ | 4515/5000 [22:27:05<3:07:42, 23.22s/it] 90%|█████████ | 4516/5000 [22:27:18<2:43:34, 20.28s/it]                                                        {'loss': 16.4227, 'grad_norm': 10.4375, 'learning_rate': 1.7779909968387885e-06, 'epoch': 5.73}
 90%|█████████ | 4516/5000 [22:27:18<2:43:34, 20.28s/it] 90%|█████████ | 4517/5000 [22:27:43<2:54:01, 21.62s/it]                                                        {'loss': 17.3184, 'grad_norm': 8.5, 'learning_rate': 1.770714039561954e-06, 'epoch': 5.74}
 90%|█████████ | 4517/5000 [22:27:43<2:54:01, 21.62s/it] 90%|█████████ | 4518/5000 [22:27:57<2:35:22, 19.34s/it]                                                        {'loss': 18.3089, 'grad_norm': 11.875, 'learning_rate': 1.763451617903731e-06, 'epoch': 5.74}
 90%|█████████ | 4518/5000 [22:27:57<2:35:22, 19.34s/it] 90%|█████████ | 4519/5000 [22:28:10<2:20:10, 17.49s/it]                                                        {'loss': 18.7028, 'grad_norm': 12.0, 'learning_rate': 1.7562037350409552e-06, 'epoch': 5.74}
 90%|█████████ | 4519/5000 [22:28:10<2:20:10, 17.49s/it] 90%|█████████ | 4520/5000 [22:28:23<2:08:54, 16.11s/it]                                                        {'loss': 18.9413, 'grad_norm': 11.125, 'learning_rate': 1.748970394144097e-06, 'epoch': 5.74}
 90%|█████████ | 4520/5000 [22:28:23<2:08:54, 16.11s/it] 90%|█████████ | 4521/5000 [22:28:39<2:08:57, 16.15s/it]                                                        {'loss': 17.9603, 'grad_norm': 12.0, 'learning_rate': 1.7417515983772578e-06, 'epoch': 5.74}
 90%|█████████ | 4521/5000 [22:28:39<2:08:57, 16.15s/it] 90%|█████████ | 4522/5000 [22:28:54<2:05:57, 15.81s/it]                                                        {'loss': 18.6596, 'grad_norm': 12.125, 'learning_rate': 1.7345473508981983e-06, 'epoch': 5.74}
 90%|█████████ | 4522/5000 [22:28:54<2:05:57, 15.81s/it] 90%|█████████ | 4523/5000 [22:29:21<2:31:59, 19.12s/it]                                                        {'loss': 17.6505, 'grad_norm': 10.4375, 'learning_rate': 1.7273576548582863e-06, 'epoch': 5.74}
 90%|█████████ | 4523/5000 [22:29:21<2:31:59, 19.12s/it] 90%|█████████ | 4524/5000 [22:29:40<2:30:53, 19.02s/it]                                                        {'loss': 18.0173, 'grad_norm': 8.0625, 'learning_rate': 1.7201825134025482e-06, 'epoch': 5.74}
 90%|█████████ | 4524/5000 [22:29:40<2:30:53, 19.02s/it] 90%|█████████ | 4525/5000 [22:30:01<2:34:23, 19.50s/it]                                                        {'loss': 16.3197, 'grad_norm': 9.4375, 'learning_rate': 1.7130219296696263e-06, 'epoch': 5.75}
 90%|█████████ | 4525/5000 [22:30:01<2:34:23, 19.50s/it] 91%|█████████ | 4526/5000 [22:30:14<2:20:21, 17.77s/it]                                                        {'loss': 18.7248, 'grad_norm': 13.8125, 'learning_rate': 1.7058759067918133e-06, 'epoch': 5.75}
 91%|█████████ | 4526/5000 [22:30:14<2:20:21, 17.77s/it] 91%|█████████ | 4527/5000 [22:30:39<2:35:40, 19.75s/it]                                                        {'loss': 17.3811, 'grad_norm': 13.25, 'learning_rate': 1.6987444478950175e-06, 'epoch': 5.75}
 91%|█████████ | 4527/5000 [22:30:39<2:35:40, 19.75s/it] 91%|█████████ | 4528/5000 [22:30:57<2:32:35, 19.40s/it]                                                        {'loss': 16.7016, 'grad_norm': 6.8125, 'learning_rate': 1.6916275560987747e-06, 'epoch': 5.75}
 91%|█████████ | 4528/5000 [22:30:57<2:32:35, 19.40s/it] 91%|█████████ | 4529/5000 [22:31:15<2:28:34, 18.93s/it]                                                        {'loss': 16.6936, 'grad_norm': 7.53125, 'learning_rate': 1.6845252345162712e-06, 'epoch': 5.75}
 91%|█████████ | 4529/5000 [22:31:15<2:28:34, 18.93s/it] 91%|█████████ | 4530/5000 [22:31:41<2:43:33, 20.88s/it]                                                        {'loss': 16.8096, 'grad_norm': 12.3125, 'learning_rate': 1.67743748625429e-06, 'epoch': 5.75}
 91%|█████████ | 4530/5000 [22:31:41<2:43:33, 20.88s/it] 91%|█████████ | 4531/5000 [22:31:55<2:26:43, 18.77s/it]                                                        {'loss': 19.6873, 'grad_norm': 21.625, 'learning_rate': 1.6703643144132562e-06, 'epoch': 5.75}
 91%|█████████ | 4531/5000 [22:31:55<2:26:43, 18.77s/it] 91%|█████████ | 4532/5000 [22:32:09<2:15:34, 17.38s/it]                                                        {'loss': 17.9249, 'grad_norm': 11.0, 'learning_rate': 1.6633057220872186e-06, 'epoch': 5.75}
 91%|█████████ | 4532/5000 [22:32:09<2:15:34, 17.38s/it] 91%|█████████ | 4533/5000 [22:32:24<2:11:42, 16.92s/it]                                                        {'loss': 19.0423, 'grad_norm': 15.6875, 'learning_rate': 1.6562617123638383e-06, 'epoch': 5.76}
 91%|█████████ | 4533/5000 [22:32:24<2:11:42, 16.92s/it] 91%|█████████ | 4534/5000 [22:32:44<2:18:29, 17.83s/it]                                                        {'loss': 17.809, 'grad_norm': 12.4375, 'learning_rate': 1.6492322883244147e-06, 'epoch': 5.76}
 91%|█████████ | 4534/5000 [22:32:44<2:18:29, 17.83s/it] 91%|█████████ | 4535/5000 [22:33:05<2:23:41, 18.54s/it]                                                        {'loss': 16.57, 'grad_norm': 9.125, 'learning_rate': 1.6422174530438475e-06, 'epoch': 5.76}
 91%|█████████ | 4535/5000 [22:33:05<2:23:41, 18.54s/it] 91%|█████████ | 4536/5000 [22:33:24<2:24:14, 18.65s/it]                                                        {'loss': 17.5622, 'grad_norm': 10.8125, 'learning_rate': 1.6352172095906717e-06, 'epoch': 5.76}
 91%|█████████ | 4536/5000 [22:33:24<2:24:14, 18.65s/it] 91%|█████████ | 4537/5000 [22:33:41<2:20:45, 18.24s/it]                                                        {'loss': 17.295, 'grad_norm': 9.0625, 'learning_rate': 1.6282315610270262e-06, 'epoch': 5.76}
 91%|█████████ | 4537/5000 [22:33:41<2:20:45, 18.24s/it] 91%|█████████ | 4538/5000 [22:33:55<2:11:37, 17.09s/it]                                                        {'loss': 17.0388, 'grad_norm': 15.125, 'learning_rate': 1.6212605104086734e-06, 'epoch': 5.76}
 91%|█████████ | 4538/5000 [22:33:55<2:11:37, 17.09s/it] 91%|█████████ | 4539/5000 [22:34:14<2:16:01, 17.70s/it]                                                        {'loss': 16.2171, 'grad_norm': 8.0625, 'learning_rate': 1.614304060784991e-06, 'epoch': 5.76}
 91%|█████████ | 4539/5000 [22:34:14<2:16:01, 17.70s/it] 91%|█████████ | 4540/5000 [22:34:29<2:07:31, 16.63s/it]                                                        {'loss': 17.3608, 'grad_norm': 10.9375, 'learning_rate': 1.6073622151989573e-06, 'epoch': 5.77}
 91%|█████████ | 4540/5000 [22:34:29<2:07:31, 16.63s/it] 91%|█████████ | 4541/5000 [22:34:46<2:08:18, 16.77s/it]                                                        {'loss': 17.6424, 'grad_norm': 12.75, 'learning_rate': 1.6004349766871777e-06, 'epoch': 5.77}
 91%|█████████ | 4541/5000 [22:34:46<2:08:18, 16.77s/it] 91%|█████████ | 4542/5000 [22:35:00<2:03:36, 16.19s/it]                                                        {'loss': 17.0998, 'grad_norm': 16.25, 'learning_rate': 1.5935223482798614e-06, 'epoch': 5.77}
 91%|█████████ | 4542/5000 [22:35:00<2:03:36, 16.19s/it] 91%|█████████ | 4543/5000 [22:35:22<2:16:19, 17.90s/it]                                                        {'loss': 17.4985, 'grad_norm': 12.25, 'learning_rate': 1.58662433300083e-06, 'epoch': 5.77}
 91%|█████████ | 4543/5000 [22:35:22<2:16:19, 17.90s/it] 91%|█████████ | 4544/5000 [22:35:36<2:06:43, 16.67s/it]                                                        {'loss': 18.0084, 'grad_norm': 14.1875, 'learning_rate': 1.5797409338674967e-06, 'epoch': 5.77}
 91%|█████████ | 4544/5000 [22:35:36<2:06:43, 16.67s/it] 91%|█████████ | 4545/5000 [22:35:51<2:01:30, 16.02s/it]                                                        {'loss': 18.2636, 'grad_norm': 12.9375, 'learning_rate': 1.57287215389091e-06, 'epoch': 5.77}
 91%|█████████ | 4545/5000 [22:35:51<2:01:30, 16.02s/it] 91%|█████████ | 4546/5000 [22:36:08<2:04:49, 16.50s/it]                                                        {'loss': 18.3542, 'grad_norm': 12.0625, 'learning_rate': 1.5660179960756997e-06, 'epoch': 5.77}
 91%|█████████ | 4546/5000 [22:36:08<2:04:49, 16.50s/it] 91%|█████████ | 4547/5000 [22:36:27<2:09:06, 17.10s/it]                                                        {'loss': 18.2285, 'grad_norm': 9.25, 'learning_rate': 1.5591784634201027e-06, 'epoch': 5.77}
 91%|█████████ | 4547/5000 [22:36:27<2:09:06, 17.10s/it] 91%|█████████ | 4548/5000 [22:36:48<2:19:16, 18.49s/it]                                                        {'loss': 16.9991, 'grad_norm': 9.1875, 'learning_rate': 1.5523535589159758e-06, 'epoch': 5.78}
 91%|█████████ | 4548/5000 [22:36:48<2:19:16, 18.49s/it] 91%|█████████ | 4549/5000 [22:37:02<2:07:17, 16.93s/it]                                                        {'loss': 20.8943, 'grad_norm': 89.5, 'learning_rate': 1.5455432855487565e-06, 'epoch': 5.78}
 91%|█████████ | 4549/5000 [22:37:02<2:07:17, 16.93s/it] 91%|█████████ | 4550/5000 [22:37:16<1:59:47, 15.97s/it]                                                        {'loss': 17.9569, 'grad_norm': 9.1875, 'learning_rate': 1.5387476462974824e-06, 'epoch': 5.78}
 91%|█████████ | 4550/5000 [22:37:16<1:59:47, 15.97s/it] 91%|█████████ | 4551/5000 [22:37:30<1:56:12, 15.53s/it]                                                        {'loss': 18.0334, 'grad_norm': 302.0, 'learning_rate': 1.5319666441348106e-06, 'epoch': 5.78}
 91%|█████████ | 4551/5000 [22:37:30<1:56:12, 15.53s/it] 91%|█████████ | 4552/5000 [22:38:05<2:39:38, 21.38s/it]                                                        {'loss': 17.206, 'grad_norm': 10.6875, 'learning_rate': 1.5252002820269788e-06, 'epoch': 5.78}
 91%|█████████ | 4552/5000 [22:38:05<2:39:38, 21.38s/it] 91%|█████████ | 4553/5000 [22:38:22<2:29:24, 20.05s/it]                                                        {'loss': 17.901, 'grad_norm': 11.25, 'learning_rate': 1.5184485629338133e-06, 'epoch': 5.78}
 91%|█████████ | 4553/5000 [22:38:22<2:29:24, 20.05s/it] 91%|█████████ | 4554/5000 [22:38:40<2:23:31, 19.31s/it]                                                        {'loss': 20.4085, 'grad_norm': 23.625, 'learning_rate': 1.5117114898087601e-06, 'epoch': 5.78}
 91%|█████████ | 4554/5000 [22:38:40<2:23:31, 19.31s/it] 91%|█████████ | 4555/5000 [22:38:55<2:14:50, 18.18s/it]                                                        {'loss': 18.76, 'grad_norm': 11.75, 'learning_rate': 1.504989065598834e-06, 'epoch': 5.78}
 91%|█████████ | 4555/5000 [22:38:55<2:14:50, 18.18s/it] 91%|█████████ | 4556/5000 [22:39:12<2:12:13, 17.87s/it]                                                        {'loss': 17.6416, 'grad_norm': 9.75, 'learning_rate': 1.4982812932446535e-06, 'epoch': 5.79}
 91%|█████████ | 4556/5000 [22:39:12<2:12:13, 17.87s/it] 91%|█████████ | 4557/5000 [22:39:29<2:10:08, 17.63s/it]                                                        {'loss': 16.6947, 'grad_norm': 15.625, 'learning_rate': 1.4915881756804227e-06, 'epoch': 5.79}
 91%|█████████ | 4557/5000 [22:39:29<2:10:08, 17.63s/it] 91%|█████████ | 4558/5000 [22:39:45<2:04:26, 16.89s/it]                                                        {'loss': 19.6102, 'grad_norm': 34.25, 'learning_rate': 1.4849097158339524e-06, 'epoch': 5.79}
 91%|█████████ | 4558/5000 [22:39:45<2:04:26, 16.89s/it] 91%|█████████ | 4559/5000 [22:40:02<2:05:26, 17.07s/it]                                                        {'loss': 16.2942, 'grad_norm': 8.875, 'learning_rate': 1.4782459166266153e-06, 'epoch': 5.79}
 91%|█████████ | 4559/5000 [22:40:02<2:05:26, 17.07s/it] 91%|█████████ | 4560/5000 [22:40:17<2:01:05, 16.51s/it]                                                        {'loss': 17.4419, 'grad_norm': 13.4375, 'learning_rate': 1.4715967809733804e-06, 'epoch': 5.79}
 91%|█████████ | 4560/5000 [22:40:17<2:01:05, 16.51s/it] 91%|█████████ | 4561/5000 [22:40:37<2:08:10, 17.52s/it]                                                        {'loss': 15.9118, 'grad_norm': 5.96875, 'learning_rate': 1.4649623117828201e-06, 'epoch': 5.79}
 91%|█████████ | 4561/5000 [22:40:37<2:08:10, 17.52s/it] 91%|█████████ | 4562/5000 [22:40:53<2:03:42, 16.95s/it]                                                        {'loss': 18.3484, 'grad_norm': 19.5, 'learning_rate': 1.458342511957065e-06, 'epoch': 5.79}
 91%|█████████ | 4562/5000 [22:40:53<2:03:42, 16.95s/it] 91%|█████████▏| 4563/5000 [22:41:10<2:05:00, 17.16s/it]                                                        {'loss': 18.3357, 'grad_norm': 16.625, 'learning_rate': 1.4517373843918418e-06, 'epoch': 5.79}
 91%|█████████▏| 4563/5000 [22:41:10<2:05:00, 17.16s/it] 91%|█████████▏| 4564/5000 [22:41:27<2:03:54, 17.05s/it]                                                        {'loss': 16.6947, 'grad_norm': 9.0625, 'learning_rate': 1.4451469319764647e-06, 'epoch': 5.8}
 91%|█████████▏| 4564/5000 [22:41:27<2:03:54, 17.05s/it] 91%|█████████▏| 4565/5000 [22:41:41<1:56:42, 16.10s/it]                                                        {'loss': 20.2715, 'grad_norm': 15.0, 'learning_rate': 1.438571157593814e-06, 'epoch': 5.8}
 91%|█████████▏| 4565/5000 [22:41:41<1:56:42, 16.10s/it] 91%|█████████▏| 4566/5000 [22:41:59<1:59:55, 16.58s/it]                                                        {'loss': 16.0631, 'grad_norm': 9.75, 'learning_rate': 1.4320100641203586e-06, 'epoch': 5.8}
 91%|█████████▏| 4566/5000 [22:41:59<1:59:55, 16.58s/it] 91%|█████████▏| 4567/5000 [22:42:13<1:55:10, 15.96s/it]                                                        {'loss': 19.2416, 'grad_norm': 17.25, 'learning_rate': 1.4254636544261472e-06, 'epoch': 5.8}
 91%|█████████▏| 4567/5000 [22:42:13<1:55:10, 15.96s/it] 91%|█████████▏| 4568/5000 [22:42:26<1:48:54, 15.13s/it]                                                        {'loss': 19.0674, 'grad_norm': 11.625, 'learning_rate': 1.4189319313747983e-06, 'epoch': 5.8}
 91%|█████████▏| 4568/5000 [22:42:26<1:48:54, 15.13s/it] 91%|█████████▏| 4569/5000 [22:42:44<1:54:10, 15.89s/it]                                                        {'loss': 16.814, 'grad_norm': 8.5, 'learning_rate': 1.4124148978235028e-06, 'epoch': 5.8}
 91%|█████████▏| 4569/5000 [22:42:44<1:54:10, 15.89s/it] 91%|█████████▏| 4570/5000 [22:43:00<1:54:12, 15.94s/it]                                                        {'loss': 17.2266, 'grad_norm': 11.6875, 'learning_rate': 1.4059125566230484e-06, 'epoch': 5.8}
 91%|█████████▏| 4570/5000 [22:43:00<1:54:12, 15.94s/it] 91%|█████████▏| 4571/5000 [22:43:15<1:51:38, 15.61s/it]                                                        {'loss': 18.563, 'grad_norm': 14.8125, 'learning_rate': 1.3994249106177719e-06, 'epoch': 5.8}
 91%|█████████▏| 4571/5000 [22:43:15<1:51:38, 15.61s/it] 91%|█████████▏| 4572/5000 [22:43:30<1:51:08, 15.58s/it]                                                        {'loss': 17.1567, 'grad_norm': 10.125, 'learning_rate': 1.3929519626455832e-06, 'epoch': 5.81}
 91%|█████████▏| 4572/5000 [22:43:30<1:51:08, 15.58s/it] 91%|█████████▏| 4573/5000 [22:43:55<2:10:27, 18.33s/it]                                                        {'loss': 19.119, 'grad_norm': 13.3125, 'learning_rate': 1.3864937155379806e-06, 'epoch': 5.81}
 91%|█████████▏| 4573/5000 [22:43:55<2:10:27, 18.33s/it] 91%|█████████▏| 4574/5000 [22:44:14<2:11:07, 18.47s/it]                                                        {'loss': 17.0939, 'grad_norm': 12.0, 'learning_rate': 1.3800501721200201e-06, 'epoch': 5.81}
 91%|█████████▏| 4574/5000 [22:44:14<2:11:07, 18.47s/it] 92%|█████████▏| 4575/5000 [22:44:31<2:07:57, 18.07s/it]                                                        {'loss': 17.383, 'grad_norm': 9.0, 'learning_rate': 1.3736213352103147e-06, 'epoch': 5.81}
 92%|█████████▏| 4575/5000 [22:44:31<2:07:57, 18.07s/it] 92%|█████████▏| 4576/5000 [22:44:46<2:00:46, 17.09s/it]                                                        {'loss': 19.25, 'grad_norm': 13.75, 'learning_rate': 1.36720720762107e-06, 'epoch': 5.81}
 92%|█████████▏| 4576/5000 [22:44:46<2:00:46, 17.09s/it] 92%|█████████▏| 4577/5000 [22:45:05<2:03:45, 17.56s/it]                                                        {'loss': 17.5889, 'grad_norm': 10.6875, 'learning_rate': 1.3608077921580367e-06, 'epoch': 5.81}
 92%|█████████▏| 4577/5000 [22:45:05<2:03:45, 17.56s/it] 92%|█████████▏| 4578/5000 [22:45:25<2:09:00, 18.34s/it]                                                        {'loss': 18.0809, 'grad_norm': 13.0, 'learning_rate': 1.3544230916205356e-06, 'epoch': 5.81}
 92%|█████████▏| 4578/5000 [22:45:25<2:09:00, 18.34s/it] 92%|█████████▏| 4579/5000 [22:45:42<2:05:37, 17.90s/it]                                                        {'loss': 18.292, 'grad_norm': 168.0, 'learning_rate': 1.3480531088014556e-06, 'epoch': 5.81}
 92%|█████████▏| 4579/5000 [22:45:42<2:05:37, 17.90s/it] 92%|█████████▏| 4580/5000 [22:45:55<1:56:37, 16.66s/it]                                                        {'loss': 19.211, 'grad_norm': 13.6875, 'learning_rate': 1.3416978464872358e-06, 'epoch': 5.82}
 92%|█████████▏| 4580/5000 [22:45:55<1:56:37, 16.66s/it] 92%|█████████▏| 4581/5000 [22:46:10<1:52:18, 16.08s/it]                                                        {'loss': 17.6837, 'grad_norm': 12.9375, 'learning_rate': 1.3353573074578954e-06, 'epoch': 5.82}
 92%|█████████▏| 4581/5000 [22:46:10<1:52:18, 16.08s/it] 92%|█████████▏| 4582/5000 [22:46:27<1:53:59, 16.36s/it]                                                        {'loss': 16.8979, 'grad_norm': 8.5, 'learning_rate': 1.3290314944869923e-06, 'epoch': 5.82}
 92%|█████████▏| 4582/5000 [22:46:27<1:53:59, 16.36s/it] 92%|█████████▏| 4583/5000 [22:46:45<1:57:20, 16.88s/it]                                                        {'loss': 17.2494, 'grad_norm': 9.6875, 'learning_rate': 1.3227204103416566e-06, 'epoch': 5.82}
 92%|█████████▏| 4583/5000 [22:46:45<1:57:20, 16.88s/it] 92%|█████████▏| 4584/5000 [22:47:01<1:54:29, 16.51s/it]                                                        {'loss': 18.2372, 'grad_norm': 14.375, 'learning_rate': 1.3164240577825724e-06, 'epoch': 5.82}
 92%|█████████▏| 4584/5000 [22:47:01<1:54:29, 16.51s/it] 92%|█████████▏| 4585/5000 [22:47:16<1:51:33, 16.13s/it]                                                        {'loss': 17.3103, 'grad_norm': 14.3125, 'learning_rate': 1.3101424395639732e-06, 'epoch': 5.82}
 92%|█████████▏| 4585/5000 [22:47:16<1:51:33, 16.13s/it] 92%|█████████▏| 4586/5000 [22:47:30<1:46:59, 15.51s/it]                                                        {'loss': 18.0233, 'grad_norm': 14.375, 'learning_rate': 1.3038755584336535e-06, 'epoch': 5.82}
 92%|█████████▏| 4586/5000 [22:47:30<1:46:59, 15.51s/it] 92%|█████████▏| 4587/5000 [22:47:47<1:49:32, 15.91s/it]                                                        {'loss': 17.845, 'grad_norm': 12.0625, 'learning_rate': 1.2976234171329658e-06, 'epoch': 5.82}
 92%|█████████▏| 4587/5000 [22:47:47<1:49:32, 15.91s/it] 92%|█████████▏| 4588/5000 [22:48:04<1:50:35, 16.10s/it]                                                        {'loss': 31.9199, 'grad_norm': 474.0, 'learning_rate': 1.2913860183967961e-06, 'epoch': 5.83}
 92%|█████████▏| 4588/5000 [22:48:04<1:50:35, 16.10s/it] 92%|█████████▏| 4589/5000 [22:48:22<1:54:33, 16.72s/it]                                                        {'loss': 17.3302, 'grad_norm': 10.125, 'learning_rate': 1.2851633649536114e-06, 'epoch': 5.83}
 92%|█████████▏| 4589/5000 [22:48:22<1:54:33, 16.72s/it] 92%|█████████▏| 4590/5000 [22:48:37<1:51:54, 16.38s/it]                                                        {'loss': 17.5182, 'grad_norm': 15.75, 'learning_rate': 1.2789554595254008e-06, 'epoch': 5.83}
 92%|█████████▏| 4590/5000 [22:48:37<1:51:54, 16.38s/it] 92%|█████████▏| 4591/5000 [22:48:53<1:50:38, 16.23s/it]                                                        {'loss': 17.7228, 'grad_norm': 11.0, 'learning_rate': 1.2727623048277115e-06, 'epoch': 5.83}
 92%|█████████▏| 4591/5000 [22:48:53<1:50:38, 16.23s/it] 92%|█████████▏| 4592/5000 [22:49:06<1:43:54, 15.28s/it]                                                        {'loss': 19.6116, 'grad_norm': 43.5, 'learning_rate': 1.2665839035696468e-06, 'epoch': 5.83}
 92%|█████████▏| 4592/5000 [22:49:06<1:43:54, 15.28s/it] 92%|█████████▏| 4593/5000 [22:49:22<1:44:50, 15.46s/it]                                                        {'loss': 16.6674, 'grad_norm': 9.125, 'learning_rate': 1.2604202584538454e-06, 'epoch': 5.83}
 92%|█████████▏| 4593/5000 [22:49:22<1:44:50, 15.46s/it] 92%|█████████▏| 4594/5000 [22:49:36<1:41:28, 15.00s/it]                                                        {'loss': 19.5749, 'grad_norm': 10.375, 'learning_rate': 1.2542713721764985e-06, 'epoch': 5.83}
 92%|█████████▏| 4594/5000 [22:49:36<1:41:28, 15.00s/it] 92%|█████████▏| 4595/5000 [22:49:52<1:43:40, 15.36s/it]                                                        {'loss': 17.2437, 'grad_norm': 8.9375, 'learning_rate': 1.2481372474273322e-06, 'epoch': 5.83}
 92%|█████████▏| 4595/5000 [22:49:52<1:43:40, 15.36s/it] 92%|█████████▏| 4596/5000 [22:50:07<1:41:59, 15.15s/it]                                                        {'loss': 17.6712, 'grad_norm': 9.8125, 'learning_rate': 1.2420178868896252e-06, 'epoch': 5.84}
 92%|█████████▏| 4596/5000 [22:50:07<1:41:59, 15.15s/it] 92%|█████████▏| 4597/5000 [22:50:23<1:44:20, 15.53s/it]                                                        {'loss': 17.3671, 'grad_norm': 10.25, 'learning_rate': 1.2359132932401912e-06, 'epoch': 5.84}
 92%|█████████▏| 4597/5000 [22:50:23<1:44:20, 15.53s/it] 92%|█████████▏| 4598/5000 [22:50:38<1:41:22, 15.13s/it]                                                        {'loss': 18.2682, 'grad_norm': 13.0625, 'learning_rate': 1.2298234691493886e-06, 'epoch': 5.84}
 92%|█████████▏| 4598/5000 [22:50:38<1:41:22, 15.13s/it] 92%|█████████▏| 4599/5000 [22:50:53<1:42:24, 15.32s/it]                                                        {'loss': 16.9981, 'grad_norm': 10.9375, 'learning_rate': 1.2237484172811146e-06, 'epoch': 5.84}
 92%|█████████▏| 4599/5000 [22:50:53<1:42:24, 15.32s/it] 92%|█████████▏| 4600/5000 [22:51:06<1:36:36, 14.49s/it]                                                        {'loss': 20.1297, 'grad_norm': 16.75, 'learning_rate': 1.2176881402928002e-06, 'epoch': 5.84}
 92%|█████████▏| 4600/5000 [22:51:06<1:36:36, 14.49s/it] 92%|█████████▏| 4601/5000 [22:51:32<1:59:55, 18.03s/it]                                                        {'loss': 19.1323, 'grad_norm': 14.8125, 'learning_rate': 1.211642640835414e-06, 'epoch': 5.84}
 92%|█████████▏| 4601/5000 [22:51:32<1:59:55, 18.03s/it] 92%|█████████▏| 4602/5000 [22:51:54<2:07:06, 19.16s/it]                                                        {'loss': 18.5356, 'grad_norm': 13.3125, 'learning_rate': 1.205611921553475e-06, 'epoch': 5.84}
 92%|█████████▏| 4602/5000 [22:51:54<2:07:06, 19.16s/it] 92%|█████████▏| 4603/5000 [22:52:12<2:03:48, 18.71s/it]                                                        {'loss': 17.8613, 'grad_norm': 10.625, 'learning_rate': 1.1995959850850079e-06, 'epoch': 5.85}
 92%|█████████▏| 4603/5000 [22:52:12<2:03:48, 18.71s/it] 92%|█████████▏| 4604/5000 [22:52:29<2:01:09, 18.36s/it]                                                        {'loss': 18.0504, 'grad_norm': 13.625, 'learning_rate': 1.1935948340616036e-06, 'epoch': 5.85}
 92%|█████████▏| 4604/5000 [22:52:29<2:01:09, 18.36s/it] 92%|█████████▏| 4605/5000 [22:52:44<1:53:55, 17.30s/it]                                                        {'loss': 17.3932, 'grad_norm': 10.5, 'learning_rate': 1.1876084711083556e-06, 'epoch': 5.85}
 92%|█████████▏| 4605/5000 [22:52:44<1:53:55, 17.30s/it] 92%|█████████▏| 4606/5000 [22:53:00<1:51:22, 16.96s/it]                                                        {'loss': 18.2446, 'grad_norm': 15.4375, 'learning_rate': 1.1816368988439144e-06, 'epoch': 5.85}
 92%|█████████▏| 4606/5000 [22:53:00<1:51:22, 16.96s/it] 92%|█████████▏| 4607/5000 [22:53:15<1:47:01, 16.34s/it]                                                        {'loss': 17.3175, 'grad_norm': 39.0, 'learning_rate': 1.1756801198804382e-06, 'epoch': 5.85}
 92%|█████████▏| 4607/5000 [22:53:15<1:47:01, 16.34s/it] 92%|█████████▏| 4608/5000 [22:53:30<1:43:48, 15.89s/it]                                                        {'loss': 18.6125, 'grad_norm': 12.375, 'learning_rate': 1.1697381368236303e-06, 'epoch': 5.85}
 92%|█████████▏| 4608/5000 [22:53:30<1:43:48, 15.89s/it] 92%|█████████▏| 4609/5000 [22:53:47<1:46:05, 16.28s/it]                                                        {'loss': 16.5191, 'grad_norm': 11.875, 'learning_rate': 1.1638109522727202e-06, 'epoch': 5.85}
 92%|█████████▏| 4609/5000 [22:53:47<1:46:05, 16.28s/it] 92%|█████████▏| 4610/5000 [22:54:13<2:04:36, 19.17s/it]                                                        {'loss': 16.6557, 'grad_norm': 9.625, 'learning_rate': 1.1578985688204523e-06, 'epoch': 5.85}
 92%|█████████▏| 4610/5000 [22:54:13<2:04:36, 19.17s/it] 92%|█████████▏| 4611/5000 [22:54:32<2:03:18, 19.02s/it]                                                        {'loss': 17.8078, 'grad_norm': 8.9375, 'learning_rate': 1.1520009890531052e-06, 'epoch': 5.86}
 92%|█████████▏| 4611/5000 [22:54:32<2:03:18, 19.02s/it] 92%|█████████▏| 4612/5000 [22:54:57<2:15:03, 20.89s/it]                                                        {'loss': 18.1881, 'grad_norm': 11.5625, 'learning_rate': 1.1461182155504834e-06, 'epoch': 5.86}
 92%|█████████▏| 4612/5000 [22:54:57<2:15:03, 20.89s/it] 92%|█████████▏| 4613/5000 [22:55:10<1:59:10, 18.48s/it]                                                        {'loss': 18.2247, 'grad_norm': 22.375, 'learning_rate': 1.1402502508859101e-06, 'epoch': 5.86}
 92%|█████████▏| 4613/5000 [22:55:10<1:59:10, 18.48s/it] 92%|█████████▏| 4614/5000 [22:55:26<1:53:58, 17.72s/it]                                                        {'loss': 17.5268, 'grad_norm': 11.625, 'learning_rate': 1.1343970976262274e-06, 'epoch': 5.86}
 92%|█████████▏| 4614/5000 [22:55:26<1:53:58, 17.72s/it] 92%|█████████▏| 4615/5000 [22:55:41<1:49:04, 17.00s/it]                                                        {'loss': 18.4343, 'grad_norm': 12.5625, 'learning_rate': 1.1285587583318146e-06, 'epoch': 5.86}
 92%|█████████▏| 4615/5000 [22:55:41<1:49:04, 17.00s/it] 92%|█████████▏| 4616/5000 [22:55:57<1:47:15, 16.76s/it]                                                        {'loss': 17.552, 'grad_norm': 11.5, 'learning_rate': 1.1227352355565545e-06, 'epoch': 5.86}
 92%|█████████▏| 4616/5000 [22:55:57<1:47:15, 16.76s/it] 92%|█████████▏| 4617/5000 [22:56:15<1:48:42, 17.03s/it]                                                        {'loss': 17.2317, 'grad_norm': 16.5, 'learning_rate': 1.1169265318478483e-06, 'epoch': 5.86}
 92%|█████████▏| 4617/5000 [22:56:15<1:48:42, 17.03s/it] 92%|█████████▏| 4618/5000 [22:56:29<1:42:38, 16.12s/it]                                                        {'loss': 18.7982, 'grad_norm': 13.375, 'learning_rate': 1.1111326497466271e-06, 'epoch': 5.86}
 92%|█████████▏| 4618/5000 [22:56:29<1:42:38, 16.12s/it] 92%|█████████▏| 4619/5000 [22:56:49<1:49:36, 17.26s/it]                                                        {'loss': 17.8622, 'grad_norm': 18.125, 'learning_rate': 1.1053535917873296e-06, 'epoch': 5.87}
 92%|█████████▏| 4619/5000 [22:56:49<1:49:36, 17.26s/it] 92%|█████████▏| 4620/5000 [22:57:02<1:41:23, 16.01s/it]                                                        {'loss': 18.8197, 'grad_norm': 14.3125, 'learning_rate': 1.0995893604979122e-06, 'epoch': 5.87}
 92%|█████████▏| 4620/5000 [22:57:02<1:41:23, 16.01s/it] 92%|█████████▏| 4621/5000 [22:57:19<1:43:33, 16.39s/it]                                                        {'loss': 18.3948, 'grad_norm': 10.9375, 'learning_rate': 1.0938399583998465e-06, 'epoch': 5.87}
 92%|█████████▏| 4621/5000 [22:57:19<1:43:33, 16.39s/it] 92%|█████████▏| 4622/5000 [22:57:37<1:45:02, 16.67s/it]                                                        {'loss': 16.9537, 'grad_norm': 14.125, 'learning_rate': 1.0881053880081142e-06, 'epoch': 5.87}
 92%|█████████▏| 4622/5000 [22:57:37<1:45:02, 16.67s/it] 92%|█████████▏| 4623/5000 [22:57:50<1:39:19, 15.81s/it]                                                        {'loss': 17.7582, 'grad_norm': 10.75, 'learning_rate': 1.0823856518312124e-06, 'epoch': 5.87}
 92%|█████████▏| 4623/5000 [22:57:50<1:39:19, 15.81s/it] 92%|█████████▏| 4624/5000 [22:58:06<1:38:24, 15.70s/it]                                                        {'loss': 18.3206, 'grad_norm': 10.75, 'learning_rate': 1.0766807523711485e-06, 'epoch': 5.87}
 92%|█████████▏| 4624/5000 [22:58:06<1:38:24, 15.70s/it] 92%|█████████▎| 4625/5000 [22:58:24<1:42:29, 16.40s/it]                                                        {'loss': 18.1776, 'grad_norm': 17.625, 'learning_rate': 1.0709906921234367e-06, 'epoch': 5.87}
 92%|█████████▎| 4625/5000 [22:58:24<1:42:29, 16.40s/it] 93%|█████████▎| 4626/5000 [22:58:41<1:43:48, 16.65s/it]                                                        {'loss': 17.8453, 'grad_norm': 9.375, 'learning_rate': 1.0653154735771063e-06, 'epoch': 5.87}
 93%|█████████▎| 4626/5000 [22:58:41<1:43:48, 16.65s/it] 93%|█████████▎| 4627/5000 [22:58:57<1:41:43, 16.36s/it]                                                        {'loss': 16.88, 'grad_norm': 7.0, 'learning_rate': 1.059655099214689e-06, 'epoch': 5.88}
 93%|█████████▎| 4627/5000 [22:58:57<1:41:43, 16.36s/it] 93%|█████████▎| 4628/5000 [22:59:14<1:43:54, 16.76s/it]                                                        {'loss': 16.9732, 'grad_norm': 9.5625, 'learning_rate': 1.0540095715122238e-06, 'epoch': 5.88}
 93%|█████████▎| 4628/5000 [22:59:14<1:43:54, 16.76s/it] 93%|█████████▎| 4629/5000 [22:59:32<1:45:03, 16.99s/it]                                                        {'loss': 18.3418, 'grad_norm': 17.0, 'learning_rate': 1.04837889293926e-06, 'epoch': 5.88}
 93%|█████████▎| 4629/5000 [22:59:32<1:45:03, 16.99s/it] 93%|█████████▎| 4630/5000 [22:59:49<1:45:11, 17.06s/it]                                                        {'loss': 18.05, 'grad_norm': 8.625, 'learning_rate': 1.0427630659588427e-06, 'epoch': 5.88}
 93%|█████████▎| 4630/5000 [22:59:49<1:45:11, 17.06s/it] 93%|█████████▎| 4631/5000 [23:00:04<1:40:26, 16.33s/it]                                                        {'loss': 18.1175, 'grad_norm': 10.875, 'learning_rate': 1.037162093027531e-06, 'epoch': 5.88}
 93%|█████████▎| 4631/5000 [23:00:04<1:40:26, 16.33s/it] 93%|█████████▎| 4632/5000 [23:00:17<1:34:34, 15.42s/it]                                                        {'loss': 18.1193, 'grad_norm': 14.0625, 'learning_rate': 1.0315759765953757e-06, 'epoch': 5.88}
 93%|█████████▎| 4632/5000 [23:00:17<1:34:34, 15.42s/it] 93%|█████████▎| 4633/5000 [23:00:39<1:46:52, 17.47s/it]                                                        {'loss': 17.162, 'grad_norm': 17.0, 'learning_rate': 1.0260047191059345e-06, 'epoch': 5.88}
 93%|█████████▎| 4633/5000 [23:00:39<1:46:52, 17.47s/it] 93%|█████████▎| 4634/5000 [23:00:55<1:42:34, 16.81s/it]                                                        {'loss': 17.2464, 'grad_norm': 9.4375, 'learning_rate': 1.0204483229962678e-06, 'epoch': 5.88}
 93%|█████████▎| 4634/5000 [23:00:55<1:42:34, 16.81s/it] 93%|█████████▎| 4635/5000 [23:01:12<1:43:26, 17.00s/it]                                                        {'loss': 17.0756, 'grad_norm': 8.6875, 'learning_rate': 1.0149067906969316e-06, 'epoch': 5.89}
 93%|█████████▎| 4635/5000 [23:01:12<1:43:26, 17.00s/it] 93%|█████████▎| 4636/5000 [23:01:26<1:36:37, 15.93s/it]                                                        {'loss': 17.63, 'grad_norm': 16.5, 'learning_rate': 1.0093801246319765e-06, 'epoch': 5.89}
 93%|█████████▎| 4636/5000 [23:01:26<1:36:37, 15.93s/it] 93%|█████████▎| 4637/5000 [23:01:40<1:33:32, 15.46s/it]                                                        {'loss': 19.7832, 'grad_norm': 16.875, 'learning_rate': 1.0038683272189608e-06, 'epoch': 5.89}
 93%|█████████▎| 4637/5000 [23:01:40<1:33:32, 15.46s/it] 93%|█████████▎| 4638/5000 [23:02:02<1:45:16, 17.45s/it]                                                        {'loss': 17.3504, 'grad_norm': 8.625, 'learning_rate': 9.983714008689252e-07, 'epoch': 5.89}
 93%|█████████▎| 4638/5000 [23:02:02<1:45:16, 17.45s/it] 93%|█████████▎| 4639/5000 [23:02:18<1:42:11, 16.98s/it]                                                        {'loss': 17.4009, 'grad_norm': 8.8125, 'learning_rate': 9.928893479864108e-07, 'epoch': 5.89}
 93%|█████████▎| 4639/5000 [23:02:18<1:42:11, 16.98s/it] 93%|█████████▎| 4640/5000 [23:02:32<1:36:24, 16.07s/it]                                                        {'loss': 17.8442, 'grad_norm': 9.75, 'learning_rate': 9.874221709694646e-07, 'epoch': 5.89}
 93%|█████████▎| 4640/5000 [23:02:32<1:36:24, 16.07s/it] 93%|█████████▎| 4641/5000 [23:02:49<1:38:33, 16.47s/it]                                                        {'loss': 16.9317, 'grad_norm': 7.90625, 'learning_rate': 9.819698722096058e-07, 'epoch': 5.89}
 93%|█████████▎| 4641/5000 [23:02:49<1:38:33, 16.47s/it] 93%|█████████▎| 4642/5000 [23:03:07<1:41:17, 16.98s/it]                                                        {'loss': 17.1539, 'grad_norm': 11.0625, 'learning_rate': 9.76532454091853e-07, 'epoch': 5.89}
 93%|█████████▎| 4642/5000 [23:03:07<1:41:17, 16.98s/it] 93%|█████████▎| 4643/5000 [23:03:21<1:34:35, 15.90s/it]                                                        {'loss': 18.2481, 'grad_norm': 11.25, 'learning_rate': 9.711099189947308e-07, 'epoch': 5.9}
 93%|█████████▎| 4643/5000 [23:03:21<1:34:35, 15.90s/it] 93%|█████████▎| 4644/5000 [23:03:41<1:41:32, 17.11s/it]                                                        {'loss': 18.1764, 'grad_norm': 9.4375, 'learning_rate': 9.657022692902288e-07, 'epoch': 5.9}
 93%|█████████▎| 4644/5000 [23:03:41<1:41:32, 17.11s/it] 93%|█████████▎| 4645/5000 [23:03:53<1:33:19, 15.77s/it]                                                        {'loss': 19.9639, 'grad_norm': 15.4375, 'learning_rate': 9.603095073438395e-07, 'epoch': 5.9}
 93%|█████████▎| 4645/5000 [23:03:53<1:33:19, 15.77s/it] 93%|█████████▎| 4646/5000 [23:04:09<1:32:22, 15.66s/it]                                                        {'loss': 18.4342, 'grad_norm': 12.3125, 'learning_rate': 9.54931635514538e-07, 'epoch': 5.9}
 93%|█████████▎| 4646/5000 [23:04:09<1:32:22, 15.66s/it] 93%|█████████▎| 4647/5000 [23:04:24<1:30:57, 15.46s/it]                                                        {'loss': 19.9734, 'grad_norm': 18.25, 'learning_rate': 9.495686561547922e-07, 'epoch': 5.9}
 93%|█████████▎| 4647/5000 [23:04:24<1:30:57, 15.46s/it] 93%|█████████▎| 4648/5000 [23:04:44<1:38:45, 16.83s/it]                                                        {'loss': 17.5516, 'grad_norm': 9.375, 'learning_rate': 9.442205716105522e-07, 'epoch': 5.9}
 93%|█████████▎| 4648/5000 [23:04:44<1:38:45, 16.83s/it] 93%|█████████▎| 4649/5000 [23:04:59<1:35:03, 16.25s/it]                                                        {'loss': 16.9475, 'grad_norm': 11.5, 'learning_rate': 9.388873842212485e-07, 'epoch': 5.9}
 93%|█████████▎| 4649/5000 [23:04:59<1:35:03, 16.25s/it] 93%|█████████▎| 4650/5000 [23:05:13<1:32:09, 15.80s/it]                                                        {'loss': 17.7689, 'grad_norm': 8.5, 'learning_rate': 9.33569096319799e-07, 'epoch': 5.9}
 93%|█████████▎| 4650/5000 [23:05:13<1:32:09, 15.80s/it] 93%|█████████▎| 4651/5000 [23:05:27<1:28:42, 15.25s/it]                                                        {'loss': 17.9122, 'grad_norm': 11.25, 'learning_rate': 9.282657102326007e-07, 'epoch': 5.91}
 93%|█████████▎| 4651/5000 [23:05:27<1:28:42, 15.25s/it] 93%|█████████▎| 4652/5000 [23:05:41<1:26:16, 14.87s/it]                                                        {'loss': 17.5302, 'grad_norm': 12.875, 'learning_rate': 9.229772282795428e-07, 'epoch': 5.91}
 93%|█████████▎| 4652/5000 [23:05:41<1:26:16, 14.87s/it] 93%|█████████▎| 4653/5000 [23:05:59<1:30:09, 15.59s/it]                                                        {'loss': 17.9663, 'grad_norm': 17.5, 'learning_rate': 9.177036527739817e-07, 'epoch': 5.91}
 93%|█████████▎| 4653/5000 [23:05:59<1:30:09, 15.59s/it] 93%|█████████▎| 4654/5000 [23:06:12<1:25:36, 14.85s/it]                                                        {'loss': 18.1434, 'grad_norm': 12.3125, 'learning_rate': 9.124449860227578e-07, 'epoch': 5.91}
 93%|█████████▎| 4654/5000 [23:06:12<1:25:36, 14.85s/it] 93%|█████████▎| 4655/5000 [23:06:31<1:32:43, 16.13s/it]                                                        {'loss': 17.5544, 'grad_norm': 10.8125, 'learning_rate': 9.072012303261911e-07, 'epoch': 5.91}
 93%|█████████▎| 4655/5000 [23:06:31<1:32:43, 16.13s/it] 93%|█████████▎| 4656/5000 [23:06:45<1:29:51, 15.67s/it]                                                        {'loss': 17.7806, 'grad_norm': 11.0625, 'learning_rate': 9.019723879780849e-07, 'epoch': 5.91}
 93%|█████████▎| 4656/5000 [23:06:45<1:29:51, 15.67s/it] 93%|█████████▎| 4657/5000 [23:07:02<1:30:18, 15.80s/it]                                                        {'loss': 18.9741, 'grad_norm': 13.9375, 'learning_rate': 8.96758461265703e-07, 'epoch': 5.91}
 93%|█████████▎| 4657/5000 [23:07:02<1:30:18, 15.80s/it] 93%|█████████▎| 4658/5000 [23:07:22<1:38:45, 17.33s/it]                                                        {'loss': 17.6654, 'grad_norm': 11.125, 'learning_rate': 8.915594524697966e-07, 'epoch': 5.91}
 93%|█████████▎| 4658/5000 [23:07:22<1:38:45, 17.33s/it] 93%|█████████▎| 4659/5000 [23:07:43<1:43:40, 18.24s/it]                                                        {'loss': 18.3857, 'grad_norm': 11.75, 'learning_rate': 8.863753638645965e-07, 'epoch': 5.92}
 93%|█████████▎| 4659/5000 [23:07:43<1:43:40, 18.24s/it] 93%|█████████▎| 4660/5000 [23:07:59<1:40:11, 17.68s/it]                                                        {'loss': 17.4352, 'grad_norm': 10.8125, 'learning_rate': 8.812061977177898e-07, 'epoch': 5.92}
 93%|█████████▎| 4660/5000 [23:07:59<1:40:11, 17.68s/it] 93%|█████████▎| 4661/5000 [23:08:24<1:51:31, 19.74s/it]                                                        {'loss': 18.6367, 'grad_norm': 21.125, 'learning_rate': 8.760519562905472e-07, 'epoch': 5.92}
 93%|█████████▎| 4661/5000 [23:08:24<1:51:31, 19.74s/it] 93%|█████████▎| 4662/5000 [23:08:40<1:44:38, 18.58s/it]                                                        {'loss': 17.0478, 'grad_norm': 12.75, 'learning_rate': 8.709126418375112e-07, 'epoch': 5.92}
 93%|█████████▎| 4662/5000 [23:08:40<1:44:38, 18.58s/it] 93%|█████████▎| 4663/5000 [23:08:54<1:37:50, 17.42s/it]                                                        {'loss': 18.4986, 'grad_norm': 14.625, 'learning_rate': 8.657882566067925e-07, 'epoch': 5.92}
 93%|█████████▎| 4663/5000 [23:08:54<1:37:50, 17.42s/it] 93%|█████████▎| 4664/5000 [23:09:12<1:37:06, 17.34s/it]                                                        {'loss': 17.3902, 'grad_norm': 11.8125, 'learning_rate': 8.606788028399697e-07, 'epoch': 5.92}
 93%|█████████▎| 4664/5000 [23:09:12<1:37:06, 17.34s/it] 93%|█████████▎| 4665/5000 [23:09:30<1:38:01, 17.56s/it]                                                        {'loss': 17.2884, 'grad_norm': 9.125, 'learning_rate': 8.555842827720893e-07, 'epoch': 5.92}
 93%|█████████▎| 4665/5000 [23:09:30<1:38:01, 17.56s/it] 93%|█████████▎| 4666/5000 [23:09:42<1:29:06, 16.01s/it]                                                        {'loss': 22.4681, 'grad_norm': 22.25, 'learning_rate': 8.505046986316738e-07, 'epoch': 5.93}
 93%|█████████▎| 4666/5000 [23:09:42<1:29:06, 16.01s/it] 93%|█████████▎| 4667/5000 [23:10:03<1:36:41, 17.42s/it]                                                        {'loss': 19.5213, 'grad_norm': 25.0, 'learning_rate': 8.454400526407057e-07, 'epoch': 5.93}
 93%|█████████▎| 4667/5000 [23:10:03<1:36:41, 17.42s/it] 93%|█████████▎| 4668/5000 [23:10:28<1:49:32, 19.80s/it]                                                        {'loss': 16.8653, 'grad_norm': 11.0, 'learning_rate': 8.40390347014624e-07, 'epoch': 5.93}
 93%|█████████▎| 4668/5000 [23:10:28<1:49:32, 19.80s/it] 93%|█████████▎| 4669/5000 [23:10:43<1:40:24, 18.20s/it]                                                        {'loss': 17.3787, 'grad_norm': 10.875, 'learning_rate': 8.35355583962351e-07, 'epoch': 5.93}
 93%|█████████▎| 4669/5000 [23:10:43<1:40:24, 18.20s/it] 93%|█████████▎| 4670/5000 [23:11:00<1:38:26, 17.90s/it]                                                        {'loss': 17.5592, 'grad_norm': 12.125, 'learning_rate': 8.30335765686258e-07, 'epoch': 5.93}
 93%|█████████▎| 4670/5000 [23:11:00<1:38:26, 17.90s/it] 93%|█████████▎| 4671/5000 [23:11:15<1:33:10, 16.99s/it]                                                        {'loss': 16.818, 'grad_norm': 10.375, 'learning_rate': 8.25330894382184e-07, 'epoch': 5.93}
 93%|█████████▎| 4671/5000 [23:11:15<1:33:10, 16.99s/it] 93%|█████████▎| 4672/5000 [23:11:39<1:44:54, 19.19s/it]                                                        {'loss': 19.4383, 'grad_norm': 29.25, 'learning_rate': 8.203409722394361e-07, 'epoch': 5.93}
 93%|█████████▎| 4672/5000 [23:11:39<1:44:54, 19.19s/it] 93%|█████████▎| 4673/5000 [23:11:54<1:37:16, 17.85s/it]                                                        {'loss': 17.3799, 'grad_norm': 10.1875, 'learning_rate': 8.153660014407698e-07, 'epoch': 5.93}
 93%|█████████▎| 4673/5000 [23:11:54<1:37:16, 17.85s/it] 93%|█████████▎| 4674/5000 [23:12:09<1:32:57, 17.11s/it]                                                        {'loss': 19.4825, 'grad_norm': 16.25, 'learning_rate': 8.10405984162405e-07, 'epoch': 5.94}
 93%|█████████▎| 4674/5000 [23:12:09<1:32:57, 17.11s/it] 94%|█████████▎| 4675/5000 [23:12:23<1:26:55, 16.05s/it]                                                        {'loss': 18.112, 'grad_norm': 14.625, 'learning_rate': 8.054609225740255e-07, 'epoch': 5.94}
 94%|█████████▎| 4675/5000 [23:12:23<1:26:55, 16.05s/it] 94%|█████████▎| 4676/5000 [23:12:38<1:25:58, 15.92s/it]                                                        {'loss': 16.0898, 'grad_norm': 8.375, 'learning_rate': 8.005308188387677e-07, 'epoch': 5.94}
 94%|█████████▎| 4676/5000 [23:12:38<1:25:58, 15.92s/it] 94%|█████████▎| 4677/5000 [23:12:55<1:27:44, 16.30s/it]                                                        {'loss': 18.2105, 'grad_norm': 10.5625, 'learning_rate': 7.956156751132281e-07, 'epoch': 5.94}
 94%|█████████▎| 4677/5000 [23:12:55<1:27:44, 16.30s/it] 94%|█████████▎| 4678/5000 [23:13:10<1:25:29, 15.93s/it]                                                        {'loss': 18.4105, 'grad_norm': 53.25, 'learning_rate': 7.907154935474558e-07, 'epoch': 5.94}
 94%|█████████▎| 4678/5000 [23:13:10<1:25:29, 15.93s/it] 94%|█████████▎| 4679/5000 [23:13:27<1:26:12, 16.11s/it]                                                        {'loss': 19.317, 'grad_norm': 13.75, 'learning_rate': 7.8583027628496e-07, 'epoch': 5.94}
 94%|█████████▎| 4679/5000 [23:13:27<1:26:12, 16.11s/it] 94%|█████████▎| 4680/5000 [23:13:42<1:24:56, 15.93s/it]                                                        {'loss': 19.9365, 'grad_norm': 22.25, 'learning_rate': 7.809600254626947e-07, 'epoch': 5.94}
 94%|█████████▎| 4680/5000 [23:13:42<1:24:56, 15.93s/it] 94%|█████████▎| 4681/5000 [23:13:59<1:25:45, 16.13s/it]                                                        {'loss': 18.9793, 'grad_norm': 11.0, 'learning_rate': 7.761047432110779e-07, 'epoch': 5.94}
 94%|█████████▎| 4681/5000 [23:13:59<1:25:45, 16.13s/it] 94%|█████████▎| 4682/5000 [23:14:22<1:36:56, 18.29s/it]                                                        {'loss': 16.9819, 'grad_norm': 8.375, 'learning_rate': 7.712644316539723e-07, 'epoch': 5.95}
 94%|█████████▎| 4682/5000 [23:14:22<1:36:56, 18.29s/it] 94%|█████████▎| 4683/5000 [23:14:36<1:29:12, 16.89s/it]                                                        {'loss': 20.0452, 'grad_norm': 16.5, 'learning_rate': 7.664390929086972e-07, 'epoch': 5.95}
 94%|█████████▎| 4683/5000 [23:14:36<1:29:12, 16.89s/it] 94%|█████████▎| 4684/5000 [23:14:51<1:25:59, 16.33s/it]                                                        {'loss': 17.8169, 'grad_norm': 11.0625, 'learning_rate': 7.616287290860163e-07, 'epoch': 5.95}
 94%|█████████▎| 4684/5000 [23:14:51<1:25:59, 16.33s/it] 94%|█████████▎| 4685/5000 [23:15:08<1:26:00, 16.38s/it]                                                        {'loss': 17.985, 'grad_norm': 13.5, 'learning_rate': 7.568333422901535e-07, 'epoch': 5.95}
 94%|█████████▎| 4685/5000 [23:15:08<1:26:00, 16.38s/it] 94%|█████████▎| 4686/5000 [23:15:33<1:39:12, 18.96s/it]                                                        {'loss': 16.9044, 'grad_norm': 12.75, 'learning_rate': 7.52052934618766e-07, 'epoch': 5.95}
 94%|█████████▎| 4686/5000 [23:15:33<1:39:12, 18.96s/it] 94%|█████████▎| 4687/5000 [23:15:53<1:42:02, 19.56s/it]                                                        {'loss': 16.2571, 'grad_norm': 7.21875, 'learning_rate': 7.472875081629631e-07, 'epoch': 5.95}
 94%|█████████▎| 4687/5000 [23:15:53<1:42:02, 19.56s/it] 94%|█████████▍| 4688/5000 [23:16:14<1:42:59, 19.81s/it]                                                        {'loss': 16.3633, 'grad_norm': 9.0625, 'learning_rate': 7.425370650073187e-07, 'epoch': 5.95}
 94%|█████████▍| 4688/5000 [23:16:14<1:42:59, 19.81s/it] 94%|█████████▍| 4689/5000 [23:16:41<1:53:28, 21.89s/it]                                                        {'loss': 15.895, 'grad_norm': 7.6875, 'learning_rate': 7.378016072298276e-07, 'epoch': 5.95}
 94%|█████████▍| 4689/5000 [23:16:41<1:53:28, 21.89s/it] 94%|█████████▍| 4690/5000 [23:16:56<1:42:35, 19.86s/it]                                                        {'loss': 19.4984, 'grad_norm': 29.5, 'learning_rate': 7.330811369019374e-07, 'epoch': 5.96}
 94%|█████████▍| 4690/5000 [23:16:56<1:42:35, 19.86s/it] 94%|█████████▍| 4691/5000 [23:17:10<1:33:52, 18.23s/it]                                                        {'loss': 18.7313, 'grad_norm': 14.3125, 'learning_rate': 7.283756560885479e-07, 'epoch': 5.96}
 94%|█████████▍| 4691/5000 [23:17:10<1:33:52, 18.23s/it] 94%|█████████▍| 4692/5000 [23:17:24<1:27:01, 16.95s/it]                                                        {'loss': 18.1864, 'grad_norm': 8.5625, 'learning_rate': 7.236851668479999e-07, 'epoch': 5.96}
 94%|█████████▍| 4692/5000 [23:17:24<1:27:01, 16.95s/it] 94%|█████████▍| 4693/5000 [23:17:51<1:41:25, 19.82s/it]                                                        {'loss': 18.2735, 'grad_norm': 11.5625, 'learning_rate': 7.19009671232063e-07, 'epoch': 5.96}
 94%|█████████▍| 4693/5000 [23:17:51<1:41:25, 19.82s/it] 94%|█████████▍| 4694/5000 [23:18:06<1:34:18, 18.49s/it]                                                        {'loss': 18.9559, 'grad_norm': 14.1875, 'learning_rate': 7.143491712859639e-07, 'epoch': 5.96}
 94%|█████████▍| 4694/5000 [23:18:06<1:34:18, 18.49s/it] 94%|█████████▍| 4695/5000 [23:18:22<1:30:15, 17.76s/it]                                                        {'loss': 17.7765, 'grad_norm': 7.09375, 'learning_rate': 7.097036690483654e-07, 'epoch': 5.96}
 94%|█████████▍| 4695/5000 [23:18:22<1:30:15, 17.76s/it] 94%|█████████▍| 4696/5000 [23:18:38<1:26:42, 17.11s/it]                                                        {'loss': 18.4339, 'grad_norm': 10.6875, 'learning_rate': 7.050731665513598e-07, 'epoch': 5.96}
 94%|█████████▍| 4696/5000 [23:18:38<1:26:42, 17.11s/it] 94%|█████████▍| 4697/5000 [23:18:57<1:29:09, 17.66s/it]                                                        {'loss': 17.2205, 'grad_norm': 8.0625, 'learning_rate': 7.004576658204958e-07, 'epoch': 5.96}
 94%|█████████▍| 4697/5000 [23:18:57<1:29:09, 17.66s/it] 94%|█████████▍| 4698/5000 [23:19:19<1:36:01, 19.08s/it]                                                        {'loss': 19.257, 'grad_norm': 16.5, 'learning_rate': 6.958571688747433e-07, 'epoch': 5.97}
 94%|█████████▍| 4698/5000 [23:19:19<1:36:01, 19.08s/it] 94%|█████████▍| 4699/5000 [23:19:36<1:32:47, 18.50s/it]                                                        {'loss': 18.5275, 'grad_norm': 10.6875, 'learning_rate': 6.912716777265209e-07, 'epoch': 5.97}
 94%|█████████▍| 4699/5000 [23:19:36<1:32:47, 18.50s/it] 94%|█████████▍| 4700/5000 [23:19:53<1:30:35, 18.12s/it]                                                        {'loss': 17.6922, 'grad_norm': 10.8125, 'learning_rate': 6.867011943816724e-07, 'epoch': 5.97}
 94%|█████████▍| 4700/5000 [23:19:53<1:30:35, 18.12s/it] 94%|█████████▍| 4701/5000 [23:20:13<1:32:37, 18.59s/it]                                                        {'loss': 18.9026, 'grad_norm': 14.25, 'learning_rate': 6.821457208394865e-07, 'epoch': 5.97}
 94%|█████████▍| 4701/5000 [23:20:13<1:32:37, 18.59s/it] 94%|█████████▍| 4702/5000 [23:20:28<1:26:19, 17.38s/it]                                                        {'loss': 19.2576, 'grad_norm': 11.0625, 'learning_rate': 6.776052590926845e-07, 'epoch': 5.97}
 94%|█████████▍| 4702/5000 [23:20:28<1:26:19, 17.38s/it] 94%|█████████▍| 4703/5000 [23:20:43<1:23:27, 16.86s/it]                                                        {'loss': 17.3474, 'grad_norm': 9.5, 'learning_rate': 6.730798111274132e-07, 'epoch': 5.97}
 94%|█████████▍| 4703/5000 [23:20:43<1:23:27, 16.86s/it] 94%|█████████▍| 4704/5000 [23:21:08<1:34:29, 19.15s/it]                                                        {'loss': 17.4282, 'grad_norm': 13.4375, 'learning_rate': 6.685693789232604e-07, 'epoch': 5.97}
 94%|█████████▍| 4704/5000 [23:21:08<1:34:29, 19.15s/it] 94%|█████████▍| 4705/5000 [23:21:24<1:29:09, 18.13s/it]                                                        {'loss': 17.5205, 'grad_norm': 9.75, 'learning_rate': 6.640739644532467e-07, 'epoch': 5.97}
 94%|█████████▍| 4705/5000 [23:21:24<1:29:09, 18.13s/it] 94%|█████████▍| 4706/5000 [23:21:37<1:22:32, 16.85s/it]                                                        {'loss': 17.3535, 'grad_norm': 9.3125, 'learning_rate': 6.595935696838062e-07, 'epoch': 5.98}
 94%|█████████▍| 4706/5000 [23:21:37<1:22:32, 16.85s/it] 94%|█████████▍| 4707/5000 [23:21:53<1:21:08, 16.62s/it]                                                        {'loss': 18.5653, 'grad_norm': 15.25, 'learning_rate': 6.551281965748295e-07, 'epoch': 5.98}
 94%|█████████▍| 4707/5000 [23:21:53<1:21:08, 16.62s/it] 94%|█████████▍| 4708/5000 [23:22:20<1:35:53, 19.70s/it]                                                        {'loss': 18.648, 'grad_norm': 13.375, 'learning_rate': 6.506778470796131e-07, 'epoch': 5.98}
 94%|█████████▍| 4708/5000 [23:22:20<1:35:53, 19.70s/it] 94%|█████████▍| 4709/5000 [23:22:37<1:31:48, 18.93s/it]                                                        {'loss': 17.6775, 'grad_norm': 17.625, 'learning_rate': 6.462425231448942e-07, 'epoch': 5.98}
 94%|█████████▍| 4709/5000 [23:22:37<1:31:48, 18.93s/it] 94%|█████████▍| 4710/5000 [23:22:59<1:34:42, 19.60s/it]                                                        {'loss': 17.9788, 'grad_norm': 11.5, 'learning_rate': 6.418222267108391e-07, 'epoch': 5.98}
 94%|█████████▍| 4710/5000 [23:22:59<1:34:42, 19.60s/it] 94%|█████████▍| 4711/5000 [23:23:16<1:30:30, 18.79s/it]                                                        {'loss': 16.7181, 'grad_norm': 7.21875, 'learning_rate': 6.374169597110318e-07, 'epoch': 5.98}
 94%|█████████▍| 4711/5000 [23:23:16<1:30:30, 18.79s/it] 94%|█████████▍| 4712/5000 [23:23:31<1:25:03, 17.72s/it]                                                        {'loss': 18.6803, 'grad_norm': 30.375, 'learning_rate': 6.330267240724773e-07, 'epoch': 5.98}
 94%|█████████▍| 4712/5000 [23:23:31<1:25:03, 17.72s/it] 94%|█████████▍| 4713/5000 [23:23:47<1:22:26, 17.24s/it]                                                        {'loss': 17.7892, 'grad_norm': 15.8125, 'learning_rate': 6.286515217156296e-07, 'epoch': 5.98}
 94%|█████████▍| 4713/5000 [23:23:47<1:22:26, 17.24s/it] 94%|█████████▍| 4714/5000 [23:24:00<1:16:13, 15.99s/it]                                                        {'loss': 18.6486, 'grad_norm': 14.4375, 'learning_rate': 6.242913545543405e-07, 'epoch': 5.99}
 94%|█████████▍| 4714/5000 [23:24:00<1:16:13, 15.99s/it] 94%|█████████▍| 4715/5000 [23:24:16<1:15:53, 15.98s/it]                                                        {'loss': 18.475, 'grad_norm': 9.6875, 'learning_rate': 6.199462244958986e-07, 'epoch': 5.99}
 94%|█████████▍| 4715/5000 [23:24:16<1:15:53, 15.98s/it] 94%|█████████▍| 4716/5000 [23:24:29<1:11:47, 15.17s/it]                                                        {'loss': 18.9342, 'grad_norm': 13.875, 'learning_rate': 6.156161334410065e-07, 'epoch': 5.99}
 94%|█████████▍| 4716/5000 [23:24:29<1:11:47, 15.17s/it] 94%|█████████▍| 4717/5000 [23:24:43<1:09:06, 14.65s/it]                                                        {'loss': 18.825, 'grad_norm': 11.8125, 'learning_rate': 6.113010832838034e-07, 'epoch': 5.99}
 94%|█████████▍| 4717/5000 [23:24:43<1:09:06, 14.65s/it] 94%|█████████▍| 4718/5000 [23:24:55<1:06:19, 14.11s/it]                                                        {'loss': 18.0603, 'grad_norm': 11.75, 'learning_rate': 6.070010759118266e-07, 'epoch': 5.99}
 94%|█████████▍| 4718/5000 [23:24:55<1:06:19, 14.11s/it] 94%|█████████▍| 4719/5000 [23:25:11<1:08:21, 14.60s/it]                                                        {'loss': 17.7752, 'grad_norm': 11.3125, 'learning_rate': 6.027161132060543e-07, 'epoch': 5.99}
 94%|█████████▍| 4719/5000 [23:25:11<1:08:21, 14.60s/it] 94%|█████████▍| 4720/5000 [23:25:33<1:18:22, 16.79s/it]                                                        {'loss': 17.0083, 'grad_norm': 11.6875, 'learning_rate': 5.984461970408705e-07, 'epoch': 5.99}
 94%|█████████▍| 4720/5000 [23:25:33<1:18:22, 16.79s/it] 94%|█████████▍| 4721/5000 [23:25:50<1:18:38, 16.91s/it]                                                        {'loss': 16.68, 'grad_norm': 23.375, 'learning_rate': 5.941913292840883e-07, 'epoch': 5.99}
 94%|█████████▍| 4721/5000 [23:25:50<1:18:38, 16.91s/it] 94%|█████████▍| 4722/5000 [23:26:06<1:16:13, 16.45s/it]                                                        {'loss': 17.3731, 'grad_norm': 10.3125, 'learning_rate': 5.899515117969189e-07, 'epoch': 6.0}
 94%|█████████▍| 4722/5000 [23:26:06<1:16:13, 16.45s/it] 94%|█████████▍| 4723/5000 [23:26:24<1:18:02, 16.90s/it]                                                        {'loss': 16.8035, 'grad_norm': 9.8125, 'learning_rate': 5.857267464340143e-07, 'epoch': 6.0}
 94%|█████████▍| 4723/5000 [23:26:24<1:18:02, 16.90s/it] 94%|█████████▍| 4724/5000 [23:26:38<1:14:45, 16.25s/it]                                                        {'loss': 18.1267, 'grad_norm': 13.875, 'learning_rate': 5.815170350434206e-07, 'epoch': 6.0}
 94%|█████████▍| 4724/5000 [23:26:38<1:14:45, 16.25s/it] 94%|█████████▍| 4725/5000 [23:27:02<1:25:00, 18.55s/it]                                                        {'loss': 17.4867, 'grad_norm': 11.125, 'learning_rate': 5.77322379466617e-07, 'epoch': 6.0}
 94%|█████████▍| 4725/5000 [23:27:02<1:25:00, 18.55s/it] 95%|█████████▍| 4726/5000 [23:27:18<1:20:26, 17.61s/it]                                                        {'loss': 18.0199, 'grad_norm': 21.375, 'learning_rate': 5.731427815384848e-07, 'epoch': 6.0}
 95%|█████████▍| 4726/5000 [23:27:18<1:20:26, 17.61s/it] 95%|█████████▍| 4727/5000 [23:27:33<1:16:27, 16.81s/it]                                                        {'loss': 19.1276, 'grad_norm': 15.375, 'learning_rate': 5.689782430873263e-07, 'epoch': 6.0}
 95%|█████████▍| 4727/5000 [23:27:33<1:16:27, 16.81s/it] 95%|█████████▍| 4728/5000 [23:27:46<1:11:31, 15.78s/it]                                                        {'loss': 17.7304, 'grad_norm': 14.5625, 'learning_rate': 5.648287659348421e-07, 'epoch': 6.0}
 95%|█████████▍| 4728/5000 [23:27:46<1:11:31, 15.78s/it] 95%|█████████▍| 4729/5000 [23:28:00<1:09:19, 15.35s/it]                                                        {'loss': 18.5367, 'grad_norm': 9.9375, 'learning_rate': 5.60694351896166e-07, 'epoch': 6.01}
 95%|█████████▍| 4729/5000 [23:28:00<1:09:19, 15.35s/it] 95%|█████████▍| 4730/5000 [23:28:18<1:12:34, 16.13s/it]                                                        {'loss': 16.914, 'grad_norm': 7.90625, 'learning_rate': 5.565750027798216e-07, 'epoch': 6.01}
 95%|█████████▍| 4730/5000 [23:28:18<1:12:34, 16.13s/it] 95%|█████████▍| 4731/5000 [23:28:34<1:11:26, 15.93s/it]                                                        {'loss': 18.4607, 'grad_norm': 13.5625, 'learning_rate': 5.524707203877582e-07, 'epoch': 6.01}
 95%|█████████▍| 4731/5000 [23:28:34<1:11:26, 15.93s/it] 95%|█████████▍| 4732/5000 [23:28:54<1:16:55, 17.22s/it]                                                        {'loss': 18.3767, 'grad_norm': 12.0625, 'learning_rate': 5.483815065153269e-07, 'epoch': 6.01}
 95%|█████████▍| 4732/5000 [23:28:54<1:16:55, 17.22s/it] 95%|█████████▍| 4733/5000 [23:29:11<1:16:23, 17.17s/it]                                                        {'loss': 18.4834, 'grad_norm': 11.1875, 'learning_rate': 5.443073629512885e-07, 'epoch': 6.01}
 95%|█████████▍| 4733/5000 [23:29:11<1:16:23, 17.17s/it] 95%|█████████▍| 4734/5000 [23:29:26<1:13:13, 16.52s/it]                                                        {'loss': 17.7459, 'grad_norm': 13.0, 'learning_rate': 5.402482914778134e-07, 'epoch': 6.01}
 95%|█████████▍| 4734/5000 [23:29:26<1:13:13, 16.52s/it] 95%|█████████▍| 4735/5000 [23:29:43<1:13:06, 16.55s/it]                                                        {'loss': 18.4698, 'grad_norm': 12.8125, 'learning_rate': 5.362042938704703e-07, 'epoch': 6.01}
 95%|█████████▍| 4735/5000 [23:29:43<1:13:06, 16.55s/it] 95%|█████████▍| 4736/5000 [23:30:01<1:14:41, 16.98s/it]                                                        {'loss': 17.705, 'grad_norm': 11.6875, 'learning_rate': 5.32175371898253e-07, 'epoch': 6.01}
 95%|█████████▍| 4736/5000 [23:30:01<1:14:41, 16.98s/it] 95%|█████████▍| 4737/5000 [23:30:15<1:11:23, 16.29s/it]                                                        {'loss': 17.2157, 'grad_norm': 10.0625, 'learning_rate': 5.281615273235418e-07, 'epoch': 6.02}
 95%|█████████▍| 4737/5000 [23:30:15<1:11:23, 16.29s/it] 95%|█████████▍| 4738/5000 [23:30:35<1:15:08, 17.21s/it]                                                        {'loss': 17.4552, 'grad_norm': 10.4375, 'learning_rate': 5.241627619021227e-07, 'epoch': 6.02}
 95%|█████████▍| 4738/5000 [23:30:35<1:15:08, 17.21s/it] 95%|█████████▍| 4739/5000 [23:30:48<1:10:00, 16.09s/it]                                                        {'loss': 19.5128, 'grad_norm': 13.5, 'learning_rate': 5.201790773832032e-07, 'epoch': 6.02}
 95%|█████████▍| 4739/5000 [23:30:48<1:10:00, 16.09s/it] 95%|█████████▍| 4740/5000 [23:31:10<1:17:27, 17.87s/it]                                                        {'loss': 16.1786, 'grad_norm': 5.90625, 'learning_rate': 5.162104755093772e-07, 'epoch': 6.02}
 95%|█████████▍| 4740/5000 [23:31:10<1:17:27, 17.87s/it] 95%|█████████▍| 4741/5000 [23:31:34<1:24:28, 19.57s/it]                                                        {'loss': 18.0879, 'grad_norm': 11.375, 'learning_rate': 5.122569580166441e-07, 'epoch': 6.02}
 95%|█████████▍| 4741/5000 [23:31:34<1:24:28, 19.57s/it] 95%|█████████▍| 4742/5000 [23:32:00<1:33:12, 21.67s/it]                                                        {'loss': 17.4622, 'grad_norm': 10.375, 'learning_rate': 5.083185266344097e-07, 'epoch': 6.02}
 95%|█████████▍| 4742/5000 [23:32:00<1:33:12, 21.67s/it] 95%|█████████▍| 4743/5000 [23:32:15<1:24:09, 19.65s/it]                                                        {'loss': 18.2187, 'grad_norm': 11.1875, 'learning_rate': 5.043951830854776e-07, 'epoch': 6.02}
 95%|█████████▍| 4743/5000 [23:32:15<1:24:09, 19.65s/it] 95%|█████████▍| 4744/5000 [23:32:34<1:22:02, 19.23s/it]                                                        {'loss': 16.6336, 'grad_norm': 6.875, 'learning_rate': 5.004869290860492e-07, 'epoch': 6.02}
 95%|█████████▍| 4744/5000 [23:32:34<1:22:02, 19.23s/it] 95%|█████████▍| 4745/5000 [23:32:49<1:16:30, 18.00s/it]                                                        {'loss': 17.1197, 'grad_norm': 9.9375, 'learning_rate': 4.965937663457281e-07, 'epoch': 6.03}
 95%|█████████▍| 4745/5000 [23:32:49<1:16:30, 18.00s/it] 95%|█████████▍| 4746/5000 [23:33:14<1:25:09, 20.12s/it]                                                        {'loss': 17.0809, 'grad_norm': 12.0, 'learning_rate': 4.9271569656752e-07, 'epoch': 6.03}
 95%|█████████▍| 4746/5000 [23:33:14<1:25:09, 20.12s/it] 95%|█████████▍| 4747/5000 [23:33:30<1:19:38, 18.89s/it]                                                        {'loss': 18.5864, 'grad_norm': 11.75, 'learning_rate': 4.888527214478166e-07, 'epoch': 6.03}
 95%|█████████▍| 4747/5000 [23:33:30<1:19:38, 18.89s/it] 95%|█████████▍| 4748/5000 [23:33:45<1:14:52, 17.83s/it]                                                        {'loss': 16.7151, 'grad_norm': 17.125, 'learning_rate': 4.850048426764236e-07, 'epoch': 6.03}
 95%|█████████▍| 4748/5000 [23:33:45<1:14:52, 17.83s/it] 95%|█████████▍| 4749/5000 [23:34:04<1:15:36, 18.07s/it]                                                        {'loss': 17.5633, 'grad_norm': 11.6875, 'learning_rate': 4.811720619365251e-07, 'epoch': 6.03}
 95%|█████████▍| 4749/5000 [23:34:04<1:15:36, 18.07s/it] 95%|█████████▌| 4750/5000 [23:34:21<1:14:46, 17.95s/it]                                                        {'loss': 17.5556, 'grad_norm': 8.9375, 'learning_rate': 4.773543809047186e-07, 'epoch': 6.03}
 95%|█████████▌| 4750/5000 [23:34:21<1:14:46, 17.95s/it] 95%|█████████▌| 4751/5000 [23:34:36<1:09:51, 16.83s/it]                                                        {'loss': 16.9963, 'grad_norm': 11.9375, 'learning_rate': 4.7355180125097705e-07, 'epoch': 6.03}
 95%|█████████▌| 4751/5000 [23:34:36<1:09:51, 16.83s/it] 95%|█████████▌| 4752/5000 [23:34:51<1:08:21, 16.54s/it]                                                        {'loss': 18.5505, 'grad_norm': 13.9375, 'learning_rate': 4.6976432463869006e-07, 'epoch': 6.03}
 95%|█████████▌| 4752/5000 [23:34:51<1:08:21, 16.54s/it] 95%|█████████▌| 4753/5000 [23:35:07<1:07:05, 16.30s/it]                                                        {'loss': 18.416, 'grad_norm': 11.6875, 'learning_rate': 4.6599195272461863e-07, 'epoch': 6.04}
 95%|█████████▌| 4753/5000 [23:35:07<1:07:05, 16.30s/it] 95%|█████████▌| 4754/5000 [23:35:20<1:02:55, 15.35s/it]                                                        {'loss': 18.0711, 'grad_norm': 12.625, 'learning_rate': 4.622346871589294e-07, 'epoch': 6.04}
 95%|█████████▌| 4754/5000 [23:35:20<1:02:55, 15.35s/it] 95%|█████████▌| 4755/5000 [23:35:35<1:01:26, 15.05s/it]                                                        {'loss': 19.0422, 'grad_norm': 33.25, 'learning_rate': 4.5849252958517937e-07, 'epoch': 6.04}
 95%|█████████▌| 4755/5000 [23:35:35<1:01:26, 15.05s/it] 95%|█████████▌| 4756/5000 [23:35:48<59:26, 14.62s/it]                                                        {'loss': 17.958, 'grad_norm': 12.3125, 'learning_rate': 4.547654816403157e-07, 'epoch': 6.04}
 95%|█████████▌| 4756/5000 [23:35:48<59:26, 14.62s/it] 95%|█████████▌| 4757/5000 [23:36:05<1:02:15, 15.37s/it]                                                        {'loss': 17.7329, 'grad_norm': 13.8125, 'learning_rate': 4.5105354495467204e-07, 'epoch': 6.04}
 95%|█████████▌| 4757/5000 [23:36:05<1:02:15, 15.37s/it] 95%|█████████▌| 4758/5000 [23:36:21<1:02:47, 15.57s/it]                                                        {'loss': 19.8497, 'grad_norm': 16.625, 'learning_rate': 4.473567211519802e-07, 'epoch': 6.04}
 95%|█████████▌| 4758/5000 [23:36:21<1:02:47, 15.57s/it] 95%|█████████▌| 4759/5000 [23:36:40<1:05:49, 16.39s/it]                                                        {'loss': 16.4448, 'grad_norm': 10.0625, 'learning_rate': 4.4367501184935426e-07, 'epoch': 6.04}
 95%|█████████▌| 4759/5000 [23:36:40<1:05:49, 16.39s/it] 95%|█████████▌| 4760/5000 [23:36:55<1:04:33, 16.14s/it]                                                        {'loss': 18.3506, 'grad_norm': 12.375, 'learning_rate': 4.4000841865729885e-07, 'epoch': 6.04}
 95%|█████████▌| 4760/5000 [23:36:55<1:04:33, 16.14s/it] 95%|█████████▌| 4761/5000 [23:37:10<1:02:54, 15.79s/it]                                                        {'loss': 18.1367, 'grad_norm': 10.0, 'learning_rate': 4.3635694317970873e-07, 'epoch': 6.05}
 95%|█████████▌| 4761/5000 [23:37:10<1:02:54, 15.79s/it] 95%|█████████▌| 4762/5000 [23:37:28<1:04:52, 16.36s/it]                                                        {'loss': 17.1431, 'grad_norm': 15.25, 'learning_rate': 4.3272058701386115e-07, 'epoch': 6.05}
 95%|█████████▌| 4762/5000 [23:37:28<1:04:52, 16.36s/it] 95%|█████████▌| 4763/5000 [23:37:43<1:02:53, 15.92s/it]                                                        {'loss': 18.7585, 'grad_norm': 14.5625, 'learning_rate': 4.2909935175041987e-07, 'epoch': 6.05}
 95%|█████████▌| 4763/5000 [23:37:43<1:02:53, 15.92s/it] 95%|█████████▌| 4764/5000 [23:37:57<1:00:37, 15.41s/it]                                                        {'loss': 17.4077, 'grad_norm': 13.6875, 'learning_rate': 4.254932389734389e-07, 'epoch': 6.05}
 95%|█████████▌| 4764/5000 [23:37:57<1:00:37, 15.41s/it] 95%|█████████▌| 4765/5000 [23:38:21<1:10:07, 17.90s/it]                                                        {'loss': 19.9026, 'grad_norm': 18.5, 'learning_rate': 4.2190225026035474e-07, 'epoch': 6.05}
 95%|█████████▌| 4765/5000 [23:38:21<1:10:07, 17.90s/it] 95%|█████████▌| 4766/5000 [23:38:36<1:07:09, 17.22s/it]                                                        {'loss': 17.9099, 'grad_norm': 11.1875, 'learning_rate': 4.183263871819864e-07, 'epoch': 6.05}
 95%|█████████▌| 4766/5000 [23:38:36<1:07:09, 17.22s/it] 95%|█████████▌| 4767/5000 [23:38:51<1:03:32, 16.36s/it]                                                        {'loss': 18.0436, 'grad_norm': 14.5, 'learning_rate': 4.1476565130253936e-07, 'epoch': 6.05}
 95%|█████████▌| 4767/5000 [23:38:51<1:03:32, 16.36s/it] 95%|█████████▌| 4768/5000 [23:39:03<58:59, 15.26s/it]                                                        {'loss': 17.5609, 'grad_norm': 13.0625, 'learning_rate': 4.1122004417959767e-07, 'epoch': 6.05}
 95%|█████████▌| 4768/5000 [23:39:03<58:59, 15.26s/it] 95%|█████████▌| 4769/5000 [23:39:19<59:11, 15.37s/it]                                                      {'loss': 16.8336, 'grad_norm': 7.71875, 'learning_rate': 4.076895673641356e-07, 'epoch': 6.06}
 95%|█████████▌| 4769/5000 [23:39:19<59:11, 15.37s/it] 95%|█████████▌| 4770/5000 [23:39:45<1:11:05, 18.54s/it]                                                        {'loss': 15.6372, 'grad_norm': 7.78125, 'learning_rate': 4.041742224004985e-07, 'epoch': 6.06}
 95%|█████████▌| 4770/5000 [23:39:45<1:11:05, 18.54s/it] 95%|█████████▌| 4771/5000 [23:40:00<1:06:24, 17.40s/it]                                                        {'loss': 18.4785, 'grad_norm': 68.0, 'learning_rate': 4.006740108264217e-07, 'epoch': 6.06}
 95%|█████████▌| 4771/5000 [23:40:00<1:06:24, 17.40s/it] 95%|█████████▌| 4772/5000 [23:40:21<1:10:33, 18.57s/it]                                                        {'loss': 17.0959, 'grad_norm': 10.5, 'learning_rate': 3.971889341730117e-07, 'epoch': 6.06}
 95%|█████████▌| 4772/5000 [23:40:21<1:10:33, 18.57s/it] 95%|█████████▌| 4773/5000 [23:40:35<1:05:17, 17.26s/it]                                                        {'loss': 17.8907, 'grad_norm': 14.3125, 'learning_rate': 3.9371899396476116e-07, 'epoch': 6.06}
 95%|█████████▌| 4773/5000 [23:40:35<1:05:17, 17.26s/it] 95%|█████████▌| 4774/5000 [23:40:50<1:01:38, 16.37s/it]                                                        {'loss': 19.1903, 'grad_norm': 19.75, 'learning_rate': 3.902641917195415e-07, 'epoch': 6.06}
 95%|█████████▌| 4774/5000 [23:40:50<1:01:38, 16.37s/it] 96%|█████████▌| 4775/5000 [23:41:07<1:02:21, 16.63s/it]                                                        {'loss': 16.7664, 'grad_norm': 9.6875, 'learning_rate': 3.868245289486027e-07, 'epoch': 6.06}
 96%|█████████▌| 4775/5000 [23:41:07<1:02:21, 16.63s/it] 96%|█████████▌| 4776/5000 [23:41:25<1:04:09, 17.19s/it]                                                        {'loss': 17.6431, 'grad_norm': 12.9375, 'learning_rate': 3.834000071565657e-07, 'epoch': 6.06}
 96%|█████████▌| 4776/5000 [23:41:25<1:04:09, 17.19s/it] 96%|█████████▌| 4777/5000 [23:41:40<1:01:25, 16.53s/it]                                                        {'loss': 19.8617, 'grad_norm': 16.375, 'learning_rate': 3.7999062784143777e-07, 'epoch': 6.07}
 96%|█████████▌| 4777/5000 [23:41:40<1:01:25, 16.53s/it] 96%|█████████▌| 4778/5000 [23:41:57<1:01:41, 16.67s/it]                                                        {'loss': 17.0973, 'grad_norm': 9.3125, 'learning_rate': 3.7659639249458917e-07, 'epoch': 6.07}
 96%|█████████▌| 4778/5000 [23:41:57<1:01:41, 16.67s/it] 96%|█████████▌| 4779/5000 [23:42:13<59:48, 16.24s/it]                                                        {'loss': 17.4753, 'grad_norm': 14.9375, 'learning_rate': 3.7321730260078063e-07, 'epoch': 6.07}
 96%|█████████▌| 4779/5000 [23:42:13<59:48, 16.24s/it] 96%|█████████▌| 4780/5000 [23:42:30<1:00:40, 16.55s/it]                                                        {'loss': 17.9604, 'grad_norm': 18.875, 'learning_rate': 3.698533596381398e-07, 'epoch': 6.07}
 96%|█████████▌| 4780/5000 [23:42:30<1:00:40, 16.55s/it] 96%|█████████▌| 4781/5000 [23:42:47<1:00:51, 16.67s/it]                                                        {'loss': 19.0126, 'grad_norm': 11.6875, 'learning_rate': 3.6650456507816896e-07, 'epoch': 6.07}
 96%|█████████▌| 4781/5000 [23:42:47<1:00:51, 16.67s/it] 96%|█████████▌| 4782/5000 [23:43:11<1:09:10, 19.04s/it]                                                        {'loss': 18.6053, 'grad_norm': 11.75, 'learning_rate': 3.6317092038574134e-07, 'epoch': 6.07}
 96%|█████████▌| 4782/5000 [23:43:11<1:09:10, 19.04s/it] 96%|█████████▌| 4783/5000 [23:43:27<1:04:53, 17.94s/it]                                                        {'loss': 17.4038, 'grad_norm': 10.875, 'learning_rate': 3.5985242701911277e-07, 'epoch': 6.07}
 96%|█████████▌| 4783/5000 [23:43:27<1:04:53, 17.94s/it] 96%|█████████▌| 4784/5000 [23:43:46<1:05:45, 18.27s/it]                                                        {'loss': 19.7672, 'grad_norm': 18.75, 'learning_rate': 3.565490864298981e-07, 'epoch': 6.07}
 96%|█████████▌| 4784/5000 [23:43:46<1:05:45, 18.27s/it] 96%|█████████▌| 4785/5000 [23:44:00<1:01:37, 17.20s/it]                                                        {'loss': 17.729, 'grad_norm': 14.6875, 'learning_rate': 3.532609000630987e-07, 'epoch': 6.08}
 96%|█████████▌| 4785/5000 [23:44:00<1:01:37, 17.20s/it] 96%|█████████▌| 4786/5000 [23:44:17<1:01:08, 17.14s/it]                                                        {'loss': 18.9737, 'grad_norm': 13.375, 'learning_rate': 3.4998786935706735e-07, 'epoch': 6.08}
 96%|█████████▌| 4786/5000 [23:44:17<1:01:08, 17.14s/it] 96%|█████████▌| 4787/5000 [23:44:33<58:51, 16.58s/it]                                                        {'loss': 17.9609, 'grad_norm': 11.5625, 'learning_rate': 3.46729995743551e-07, 'epoch': 6.08}
 96%|█████████▌| 4787/5000 [23:44:33<58:51, 16.58s/it] 96%|█████████▌| 4788/5000 [23:44:48<57:26, 16.26s/it]                                                      {'loss': 16.5932, 'grad_norm': 7.03125, 'learning_rate': 3.4348728064765187e-07, 'epoch': 6.08}
 96%|█████████▌| 4788/5000 [23:44:48<57:26, 16.26s/it] 96%|█████████▌| 4789/5000 [23:45:21<1:14:17, 21.13s/it]                                                        {'loss': 16.9181, 'grad_norm': 6.5625, 'learning_rate': 3.402597254878353e-07, 'epoch': 6.08}
 96%|█████████▌| 4789/5000 [23:45:21<1:14:17, 21.13s/it] 96%|█████████▌| 4790/5000 [23:45:46<1:18:43, 22.49s/it]                                                        {'loss': 16.9343, 'grad_norm': 11.8125, 'learning_rate': 3.370473316759531e-07, 'epoch': 6.08}
 96%|█████████▌| 4790/5000 [23:45:46<1:18:43, 22.49s/it] 96%|█████████▌| 4791/5000 [23:46:04<1:12:58, 20.95s/it]                                                        {'loss': 17.0001, 'grad_norm': 10.3125, 'learning_rate': 3.3385010061720843e-07, 'epoch': 6.08}
 96%|█████████▌| 4791/5000 [23:46:04<1:12:58, 20.95s/it] 96%|█████████▌| 4792/5000 [23:46:20<1:07:46, 19.55s/it]                                                        {'loss': 18.0501, 'grad_norm': 9.1875, 'learning_rate': 3.30668033710183e-07, 'epoch': 6.09}
 96%|█████████▌| 4792/5000 [23:46:20<1:07:46, 19.55s/it] 96%|█████████▌| 4793/5000 [23:46:33<1:00:08, 17.43s/it]                                                        {'loss': 20.0198, 'grad_norm': 11.625, 'learning_rate': 3.2750113234682184e-07, 'epoch': 6.09}
 96%|█████████▌| 4793/5000 [23:46:33<1:00:08, 17.43s/it] 96%|█████████▌| 4794/5000 [23:46:48<57:21, 16.71s/it]                                                        {'loss': 18.2134, 'grad_norm': 13.875, 'learning_rate': 3.2434939791243284e-07, 'epoch': 6.09}
 96%|█████████▌| 4794/5000 [23:46:48<57:21, 16.71s/it] 96%|█████████▌| 4795/5000 [23:47:04<57:19, 16.78s/it]                                                      {'loss': 17.2805, 'grad_norm': 10.4375, 'learning_rate': 3.2121283178569103e-07, 'epoch': 6.09}
 96%|█████████▌| 4795/5000 [23:47:04<57:19, 16.78s/it] 96%|█████████▌| 4796/5000 [23:47:19<54:20, 15.98s/it]                                                      {'loss': 19.262, 'grad_norm': 12.4375, 'learning_rate': 3.1809143533863824e-07, 'epoch': 6.09}
 96%|█████████▌| 4796/5000 [23:47:19<54:20, 15.98s/it] 96%|█████████▌| 4797/5000 [23:47:32<51:32, 15.23s/it]                                                      {'loss': 18.1528, 'grad_norm': 11.25, 'learning_rate': 3.1498520993667955e-07, 'epoch': 6.09}
 96%|█████████▌| 4797/5000 [23:47:32<51:32, 15.23s/it] 96%|█████████▌| 4798/5000 [23:47:46<49:38, 14.75s/it]                                                      {'loss': 18.9817, 'grad_norm': 12.1875, 'learning_rate': 3.11894156938583e-07, 'epoch': 6.09}
 96%|█████████▌| 4798/5000 [23:47:46<49:38, 14.75s/it] 96%|█████████▌| 4799/5000 [23:48:00<48:47, 14.57s/it]                                                      {'loss': 18.2886, 'grad_norm': 8.3125, 'learning_rate': 3.088182776964798e-07, 'epoch': 6.09}
 96%|█████████▌| 4799/5000 [23:48:00<48:47, 14.57s/it] 96%|█████████▌| 4800/5000 [23:48:20<54:17, 16.29s/it]                                                      {'loss': 17.9125, 'grad_norm': 13.3125, 'learning_rate': 3.0575757355586817e-07, 'epoch': 6.1}
 96%|█████████▌| 4800/5000 [23:48:20<54:17, 16.29s/it] 96%|█████████▌| 4801/5000 [23:48:46<1:03:10, 19.05s/it]                                                        {'loss': 18.5093, 'grad_norm': 13.375, 'learning_rate': 3.027120458555976e-07, 'epoch': 6.1}
 96%|█████████▌| 4801/5000 [23:48:46<1:03:10, 19.05s/it] 96%|█████████▌| 4802/5000 [23:48:59<57:07, 17.31s/it]                                                        {'loss': 20.1069, 'grad_norm': 13.1875, 'learning_rate': 2.996816959278886e-07, 'epoch': 6.1}
 96%|█████████▌| 4802/5000 [23:48:59<57:07, 17.31s/it] 96%|█████████▌| 4803/5000 [23:49:15<55:21, 16.86s/it]                                                      {'loss': 18.9355, 'grad_norm': 15.9375, 'learning_rate': 2.966665250983208e-07, 'epoch': 6.1}
 96%|█████████▌| 4803/5000 [23:49:15<55:21, 16.86s/it] 96%|█████████▌| 4804/5000 [23:49:29<52:30, 16.08s/it]                                                      {'loss': 19.0549, 'grad_norm': 13.0625, 'learning_rate': 2.936665346858291e-07, 'epoch': 6.1}
 96%|█████████▌| 4804/5000 [23:49:29<52:30, 16.08s/it] 96%|█████████▌| 4805/5000 [23:49:43<49:51, 15.34s/it]                                                      {'loss': 18.9037, 'grad_norm': 13.1875, 'learning_rate': 2.906817260027117e-07, 'epoch': 6.1}
 96%|█████████▌| 4805/5000 [23:49:43<49:51, 15.34s/it] 96%|█████████▌| 4806/5000 [23:50:03<54:51, 16.96s/it]                                                      {'loss': 17.7232, 'grad_norm': 12.875, 'learning_rate': 2.8771210035462566e-07, 'epoch': 6.1}
 96%|█████████▌| 4806/5000 [23:50:03<54:51, 16.96s/it] 96%|█████████▌| 4807/5000 [23:50:19<53:36, 16.66s/it]                                                      {'loss': 16.8525, 'grad_norm': 8.5625, 'learning_rate': 2.8475765904059146e-07, 'epoch': 6.1}
 96%|█████████▌| 4807/5000 [23:50:19<53:36, 16.66s/it] 96%|█████████▌| 4808/5000 [23:50:41<58:05, 18.16s/it]                                                      {'loss': 18.1252, 'grad_norm': 10.1875, 'learning_rate': 2.81818403352973e-07, 'epoch': 6.11}
 96%|█████████▌| 4808/5000 [23:50:41<58:05, 18.16s/it] 96%|█████████▌| 4809/5000 [23:50:56<55:15, 17.36s/it]                                                      {'loss': 17.853, 'grad_norm': 11.4375, 'learning_rate': 2.788943345775052e-07, 'epoch': 6.11}
 96%|█████████▌| 4809/5000 [23:50:56<55:15, 17.36s/it] 96%|█████████▌| 4810/5000 [23:51:12<53:43, 16.97s/it]                                                      {'loss': 17.8859, 'grad_norm': 10.8125, 'learning_rate': 2.759854539932782e-07, 'epoch': 6.11}
 96%|█████████▌| 4810/5000 [23:51:12<53:43, 16.97s/it] 96%|█████████▌| 4811/5000 [23:51:40<1:03:02, 20.01s/it]                                                        {'loss': 17.9756, 'grad_norm': 14.5625, 'learning_rate': 2.7309176287272584e-07, 'epoch': 6.11}
 96%|█████████▌| 4811/5000 [23:51:40<1:03:02, 20.01s/it] 96%|█████████▌| 4812/5000 [23:51:58<1:00:51, 19.42s/it]                                                        {'loss': 16.434, 'grad_norm': 8.0625, 'learning_rate': 2.7021326248165667e-07, 'epoch': 6.11}
 96%|█████████▌| 4812/5000 [23:51:58<1:00:51, 19.42s/it] 96%|█████████▋| 4813/5000 [23:52:13<57:10, 18.34s/it]                                                        {'loss': 17.5223, 'grad_norm': 11.5, 'learning_rate': 2.673499540792229e-07, 'epoch': 6.11}
 96%|█████████▋| 4813/5000 [23:52:13<57:10, 18.34s/it] 96%|█████████▋| 4814/5000 [23:52:29<54:36, 17.62s/it]                                                      {'loss': 17.3604, 'grad_norm': 9.125, 'learning_rate': 2.645018389179282e-07, 'epoch': 6.11}
 96%|█████████▋| 4814/5000 [23:52:29<54:36, 17.62s/it] 96%|█████████▋| 4815/5000 [23:52:50<57:03, 18.51s/it]                                                      {'loss': 18.7169, 'grad_norm': 12.8125, 'learning_rate': 2.6166891824363545e-07, 'epoch': 6.11}
 96%|█████████▋| 4815/5000 [23:52:50<57:03, 18.51s/it] 96%|█████████▋| 4816/5000 [23:53:04<52:18, 17.06s/it]                                                      {'loss': 18.9649, 'grad_norm': 8.75, 'learning_rate': 2.588511932955667e-07, 'epoch': 6.12}
 96%|█████████▋| 4816/5000 [23:53:04<52:18, 17.06s/it] 96%|█████████▋| 4817/5000 [23:53:17<48:11, 15.80s/it]                                                      {'loss': 18.3104, 'grad_norm': 13.1875, 'learning_rate': 2.5604866530628377e-07, 'epoch': 6.12}
 96%|█████████▋| 4817/5000 [23:53:17<48:11, 15.80s/it] 96%|█████████▋| 4818/5000 [23:53:31<46:38, 15.38s/it]                                                      {'loss': 18.12, 'grad_norm': 13.0625, 'learning_rate': 2.532613355017116e-07, 'epoch': 6.12}
 96%|█████████▋| 4818/5000 [23:53:31<46:38, 15.38s/it] 96%|█████████▋| 4819/5000 [23:53:51<50:18, 16.67s/it]                                                      {'loss': 17.9829, 'grad_norm': 10.875, 'learning_rate': 2.504892051011226e-07, 'epoch': 6.12}
 96%|█████████▋| 4819/5000 [23:53:51<50:18, 16.67s/it] 96%|█████████▋| 4820/5000 [23:54:04<47:29, 15.83s/it]                                                      {'loss': 17.8629, 'grad_norm': 13.375, 'learning_rate': 2.477322753171329e-07, 'epoch': 6.12}
 96%|█████████▋| 4820/5000 [23:54:04<47:29, 15.83s/it] 96%|█████████▋| 4821/5000 [23:54:22<49:05, 16.45s/it]                                                      {'loss': 18.9169, 'grad_norm': 14.125, 'learning_rate': 2.4499054735572954e-07, 'epoch': 6.12}
 96%|█████████▋| 4821/5000 [23:54:22<49:05, 16.45s/it] 96%|█████████▋| 4822/5000 [23:54:47<55:57, 18.86s/it]                                                      {'loss': 16.746, 'grad_norm': 10.8125, 'learning_rate': 2.4226402241623144e-07, 'epoch': 6.12}
 96%|█████████▋| 4822/5000 [23:54:47<55:57, 18.86s/it] 96%|█████████▋| 4823/5000 [23:55:03<52:49, 17.90s/it]                                                      {'loss': 17.8804, 'grad_norm': 8.375, 'learning_rate': 2.3955270169131293e-07, 'epoch': 6.12}
 96%|█████████▋| 4823/5000 [23:55:03<52:49, 17.90s/it] 96%|█████████▋| 4824/5000 [23:55:29<59:58, 20.45s/it]                                                      {'loss': 17.8054, 'grad_norm': 10.25, 'learning_rate': 2.3685658636699968e-07, 'epoch': 6.13}
 96%|█████████▋| 4824/5000 [23:55:29<59:58, 20.45s/it] 96%|█████████▋| 4825/5000 [23:55:46<56:57, 19.53s/it]                                                      {'loss': 17.9593, 'grad_norm': 12.8125, 'learning_rate': 2.3417567762266497e-07, 'epoch': 6.13}
 96%|█████████▋| 4825/5000 [23:55:46<56:57, 19.53s/it] 97%|█████████▋| 4826/5000 [23:56:01<52:09, 17.99s/it]                                                      {'loss': 17.6402, 'grad_norm': 10.3125, 'learning_rate': 2.3150997663102576e-07, 'epoch': 6.13}
 97%|█████████▋| 4826/5000 [23:56:01<52:09, 17.99s/it] 97%|█████████▋| 4827/5000 [23:56:17<50:06, 17.38s/it]                                                      {'loss': 17.3673, 'grad_norm': 11.1875, 'learning_rate': 2.288594845581504e-07, 'epoch': 6.13}
 97%|█████████▋| 4827/5000 [23:56:17<50:06, 17.38s/it] 97%|█████████▋| 4828/5000 [23:56:33<48:33, 16.94s/it]                                                      {'loss': 16.6705, 'grad_norm': 9.0625, 'learning_rate': 2.2622420256345475e-07, 'epoch': 6.13}
 97%|█████████▋| 4828/5000 [23:56:33<48:33, 16.94s/it] 97%|█████████▋| 4829/5000 [23:56:50<48:23, 16.98s/it]                                                      {'loss': 17.9789, 'grad_norm': 10.125, 'learning_rate': 2.2360413179970617e-07, 'epoch': 6.13}
 97%|█████████▋| 4829/5000 [23:56:50<48:23, 16.98s/it] 97%|█████████▋| 4830/5000 [23:57:04<46:15, 16.32s/it]                                                      {'loss': 19.2835, 'grad_norm': 13.625, 'learning_rate': 2.2099927341300394e-07, 'epoch': 6.13}
 97%|█████████▋| 4830/5000 [23:57:04<46:15, 16.32s/it] 97%|█████████▋| 4831/5000 [23:57:19<44:50, 15.92s/it]                                                      {'loss': 19.31, 'grad_norm': 14.625, 'learning_rate': 2.184096285428105e-07, 'epoch': 6.13}
 97%|█████████▋| 4831/5000 [23:57:19<44:50, 15.92s/it] 97%|█████████▋| 4832/5000 [23:57:38<46:52, 16.74s/it]                                                      {'loss': 17.7129, 'grad_norm': 10.0625, 'learning_rate': 2.1583519832191644e-07, 'epoch': 6.14}
 97%|█████████▋| 4832/5000 [23:57:38<46:52, 16.74s/it] 97%|█████████▋| 4833/5000 [23:58:11<1:00:09, 21.61s/it]                                                        {'loss': 18.6495, 'grad_norm': 13.5, 'learning_rate': 2.132759838764675e-07, 'epoch': 6.14}
 97%|█████████▋| 4833/5000 [23:58:11<1:00:09, 21.61s/it] 97%|█████████▋| 4834/5000 [23:58:26<54:16, 19.62s/it]                                                        {'loss': 19.1465, 'grad_norm': 14.375, 'learning_rate': 2.1073198632595322e-07, 'epoch': 6.14}
 97%|█████████▋| 4834/5000 [23:58:26<54:16, 19.62s/it] 97%|█████████▋| 4835/5000 [23:58:51<58:00, 21.09s/it]                                                      {'loss': 18.1043, 'grad_norm': 11.875, 'learning_rate': 2.082032067832068e-07, 'epoch': 6.14}
 97%|█████████▋| 4835/5000 [23:58:51<58:00, 21.09s/it] 97%|█████████▋| 4836/5000 [23:59:05<51:57, 19.01s/it]                                                      {'loss': 32.2132, 'grad_norm': 836.0, 'learning_rate': 2.0568964635439334e-07, 'epoch': 6.14}
 97%|█████████▋| 4836/5000 [23:59:05<51:57, 19.01s/it] 97%|█████████▋| 4837/5000 [23:59:27<54:09, 19.93s/it]                                                      {'loss': 16.2114, 'grad_norm': 8.375, 'learning_rate': 2.0319130613903724e-07, 'epoch': 6.14}
 97%|█████████▋| 4837/5000 [23:59:27<54:09, 19.93s/it] 97%|█████████▋| 4838/5000 [23:59:45<52:13, 19.35s/it]                                                      {'loss': 16.5719, 'grad_norm': 9.375, 'learning_rate': 2.0070818722999482e-07, 'epoch': 6.14}
 97%|█████████▋| 4838/5000 [23:59:45<52:13, 19.35s/it] 97%|█████████▋| 4839/5000 [23:59:59<47:46, 17.80s/it]                                                      {'loss': 17.8188, 'grad_norm': 11.625, 'learning_rate': 1.9824029071347004e-07, 'epoch': 6.14}
 97%|█████████▋| 4839/5000 [23:59:59<47:46, 17.80s/it] 97%|█████████▋| 4840/5000 [24:00:16<47:05, 17.66s/it]                                                      {'loss': 17.3947, 'grad_norm': 12.0625, 'learning_rate': 1.9578761766899487e-07, 'epoch': 6.15}
 97%|█████████▋| 4840/5000 [24:00:16<47:05, 17.66s/it] 97%|█████████▋| 4841/5000 [24:00:44<54:31, 20.58s/it]                                                      {'loss': 17.6333, 'grad_norm': 10.25, 'learning_rate': 1.9335016916946445e-07, 'epoch': 6.15}
 97%|█████████▋| 4841/5000 [24:00:44<54:31, 20.58s/it] 97%|█████████▋| 4842/5000 [24:00:59<50:02, 19.00s/it]                                                      {'loss': 17.9475, 'grad_norm': 15.3125, 'learning_rate': 1.909279462810942e-07, 'epoch': 6.15}
 97%|█████████▋| 4842/5000 [24:00:59<50:02, 19.00s/it] 97%|█████████▋| 4843/5000 [24:01:35<1:03:08, 24.13s/it]                                                        {'loss': 17.6573, 'grad_norm': 11.0625, 'learning_rate': 1.8852095006344714e-07, 'epoch': 6.15}
 97%|█████████▋| 4843/5000 [24:01:35<1:03:08, 24.13s/it] 97%|█████████▋| 4844/5000 [24:01:50<55:35, 21.38s/it]                                                        {'loss': 18.128, 'grad_norm': 10.5, 'learning_rate': 1.8612918156942603e-07, 'epoch': 6.15}
 97%|█████████▋| 4844/5000 [24:01:50<55:35, 21.38s/it] 97%|█████████▋| 4845/5000 [24:02:06<50:52, 19.69s/it]                                                      {'loss': 18.3143, 'grad_norm': 13.75, 'learning_rate': 1.8375264184527338e-07, 'epoch': 6.15}
 97%|█████████▋| 4845/5000 [24:02:06<50:52, 19.69s/it] 97%|█████████▋| 4846/5000 [24:02:27<51:20, 20.00s/it]                                                      {'loss': 16.4588, 'grad_norm': 9.875, 'learning_rate': 1.8139133193056376e-07, 'epoch': 6.15}
 97%|█████████▋| 4846/5000 [24:02:27<51:20, 20.00s/it] 97%|█████████▋| 4847/5000 [24:02:45<50:04, 19.64s/it]                                                      {'loss': 18.2308, 'grad_norm': 21.625, 'learning_rate': 1.7904525285821925e-07, 'epoch': 6.15}
 97%|█████████▋| 4847/5000 [24:02:45<50:04, 19.64s/it] 97%|█████████▋| 4848/5000 [24:03:10<53:19, 21.05s/it]                                                      {'loss': 18.0978, 'grad_norm': 12.0625, 'learning_rate': 1.767144056544939e-07, 'epoch': 6.16}
 97%|█████████▋| 4848/5000 [24:03:10<53:19, 21.05s/it] 97%|█████████▋| 4849/5000 [24:03:28<51:01, 20.27s/it]                                                      {'loss': 17.5119, 'grad_norm': 170.0, 'learning_rate': 1.7439879133897772e-07, 'epoch': 6.16}
 97%|█████████▋| 4849/5000 [24:03:28<51:01, 20.27s/it] 97%|█████████▋| 4850/5000 [24:03:53<54:21, 21.74s/it]                                                      {'loss': 19.6619, 'grad_norm': 25.125, 'learning_rate': 1.7209841092460043e-07, 'epoch': 6.16}
 97%|█████████▋| 4850/5000 [24:03:53<54:21, 21.74s/it] 97%|█████████▋| 4851/5000 [24:04:07<48:21, 19.47s/it]                                                      {'loss': 17.7721, 'grad_norm': 11.8125, 'learning_rate': 1.6981326541762763e-07, 'epoch': 6.16}
 97%|█████████▋| 4851/5000 [24:04:07<48:21, 19.47s/it] 97%|█████████▋| 4852/5000 [24:04:22<44:28, 18.03s/it]                                                      {'loss': 17.8749, 'grad_norm': 12.8125, 'learning_rate': 1.675433558176531e-07, 'epoch': 6.16}
 97%|█████████▋| 4852/5000 [24:04:22<44:28, 18.03s/it] 97%|█████████▋| 4853/5000 [24:04:37<42:08, 17.20s/it]                                                      {'loss': 16.9908, 'grad_norm': 10.3125, 'learning_rate': 1.652886831176181e-07, 'epoch': 6.16}
 97%|█████████▋| 4853/5000 [24:04:37<42:08, 17.20s/it] 97%|█████████▋| 4854/5000 [24:04:52<40:10, 16.51s/it]                                                      {'loss': 18.9136, 'grad_norm': 11.4375, 'learning_rate': 1.6304924830379596e-07, 'epoch': 6.16}
 97%|█████████▋| 4854/5000 [24:04:52<40:10, 16.51s/it] 97%|█████████▋| 4855/5000 [24:05:08<39:16, 16.25s/it]                                                      {'loss': 17.2564, 'grad_norm': 8.9375, 'learning_rate': 1.6082505235578414e-07, 'epoch': 6.17}
 97%|█████████▋| 4855/5000 [24:05:08<39:16, 16.25s/it] 97%|█████████▋| 4856/5000 [24:05:32<44:52, 18.70s/it]                                                      {'loss': 18.8397, 'grad_norm': 14.3125, 'learning_rate': 1.5861609624652772e-07, 'epoch': 6.17}
 97%|█████████▋| 4856/5000 [24:05:32<44:52, 18.70s/it] 97%|█████████▋| 4857/5000 [24:05:50<43:58, 18.45s/it]                                                      {'loss': 18.4051, 'grad_norm': 18.125, 'learning_rate': 1.56422380942296e-07, 'epoch': 6.17}
 97%|█████████▋| 4857/5000 [24:05:50<43:58, 18.45s/it] 97%|█████████▋| 4858/5000 [24:06:15<48:29, 20.49s/it]                                                      {'loss': 18.1903, 'grad_norm': 12.5625, 'learning_rate': 1.542439074026941e-07, 'epoch': 6.17}
 97%|█████████▋| 4858/5000 [24:06:15<48:29, 20.49s/it] 97%|█████████▋| 4859/5000 [24:06:30<44:01, 18.74s/it]                                                      {'loss': 17.5971, 'grad_norm': 10.625, 'learning_rate': 1.5208067658065926e-07, 'epoch': 6.17}
 97%|█████████▋| 4859/5000 [24:06:30<44:01, 18.74s/it] 97%|█████████▋| 4860/5000 [24:06:53<46:42, 20.02s/it]                                                      {'loss': 17.9584, 'grad_norm': 10.125, 'learning_rate': 1.4993268942246838e-07, 'epoch': 6.17}
 97%|█████████▋| 4860/5000 [24:06:53<46:42, 20.02s/it] 97%|█████████▋| 4861/5000 [24:07:07<42:03, 18.15s/it]                                                      {'loss': 18.9316, 'grad_norm': 14.0625, 'learning_rate': 1.477999468677188e-07, 'epoch': 6.17}
 97%|█████████▋| 4861/5000 [24:07:07<42:03, 18.15s/it] 97%|█████████▋| 4862/5000 [24:07:22<39:56, 17.37s/it]                                                      {'loss': 17.8541, 'grad_norm': 16.375, 'learning_rate': 1.4568244984934363e-07, 'epoch': 6.17}
 97%|█████████▋| 4862/5000 [24:07:22<39:56, 17.37s/it] 97%|█████████▋| 4863/5000 [24:07:35<36:30, 15.99s/it]                                                      {'loss': 17.7357, 'grad_norm': 16.0, 'learning_rate': 1.4358019929361198e-07, 'epoch': 6.18}
 97%|█████████▋| 4863/5000 [24:07:35<36:30, 15.99s/it] 97%|█████████▋| 4864/5000 [24:07:59<41:47, 18.44s/it]                                                      {'loss': 17.2756, 'grad_norm': 9.875, 'learning_rate': 1.414931961201171e-07, 'epoch': 6.18}
 97%|█████████▋| 4864/5000 [24:07:59<41:47, 18.44s/it] 97%|█████████▋| 4865/5000 [24:08:16<40:32, 18.02s/it]                                                      {'loss': 18.5779, 'grad_norm': 12.5, 'learning_rate': 1.3942144124178434e-07, 'epoch': 6.18}
 97%|█████████▋| 4865/5000 [24:08:16<40:32, 18.02s/it] 97%|█████████▋| 4866/5000 [24:08:30<37:19, 16.71s/it]                                                      {'loss': 19.2089, 'grad_norm': 11.6875, 'learning_rate': 1.3736493556487094e-07, 'epoch': 6.18}
 97%|█████████▋| 4866/5000 [24:08:30<37:19, 16.71s/it] 97%|█████████▋| 4867/5000 [24:08:46<36:30, 16.47s/it]                                                      {'loss': 18.8531, 'grad_norm': 17.875, 'learning_rate': 1.3532367998896232e-07, 'epoch': 6.18}
 97%|█████████▋| 4867/5000 [24:08:46<36:30, 16.47s/it] 97%|█████████▋| 4868/5000 [24:09:02<35:44, 16.25s/it]                                                      {'loss': 17.2118, 'grad_norm': 9.9375, 'learning_rate': 1.332976754069759e-07, 'epoch': 6.18}
 97%|█████████▋| 4868/5000 [24:09:02<35:44, 16.25s/it] 97%|█████████▋| 4869/5000 [24:09:18<35:10, 16.11s/it]                                                      {'loss': 18.1232, 'grad_norm': 12.4375, 'learning_rate': 1.312869227051494e-07, 'epoch': 6.18}
 97%|█████████▋| 4869/5000 [24:09:18<35:10, 16.11s/it] 97%|█████████▋| 4870/5000 [24:09:31<33:17, 15.37s/it]                                                      {'loss': 17.5183, 'grad_norm': 10.625, 'learning_rate': 1.2929142276306036e-07, 'epoch': 6.18}
 97%|█████████▋| 4870/5000 [24:09:31<33:17, 15.37s/it] 97%|█████████▋| 4871/5000 [24:09:46<32:42, 15.21s/it]                                                      {'loss': 18.7044, 'grad_norm': 12.75, 'learning_rate': 1.273111764536028e-07, 'epoch': 6.19}
 97%|█████████▋| 4871/5000 [24:09:46<32:42, 15.21s/it] 97%|█████████▋| 4872/5000 [24:10:16<41:46, 19.58s/it]                                                      {'loss': 17.8949, 'grad_norm': 10.25, 'learning_rate': 1.253461846430065e-07, 'epoch': 6.19}
 97%|█████████▋| 4872/5000 [24:10:16<41:46, 19.58s/it] 97%|█████████▋| 4873/5000 [24:10:32<39:27, 18.64s/it]                                                      {'loss': 18.213, 'grad_norm': 13.3125, 'learning_rate': 1.233964481908256e-07, 'epoch': 6.19}
 97%|█████████▋| 4873/5000 [24:10:32<39:27, 18.64s/it] 97%|█████████▋| 4874/5000 [24:10:59<44:15, 21.08s/it]                                                      {'loss': 17.4902, 'grad_norm': 9.8125, 'learning_rate': 1.2146196794993844e-07, 'epoch': 6.19}
 97%|█████████▋| 4874/5000 [24:10:59<44:15, 21.08s/it] 98%|█████████▊| 4875/5000 [24:11:17<41:58, 20.14s/it]                                                      {'loss': 17.256, 'grad_norm': 11.0, 'learning_rate': 1.1954274476655534e-07, 'epoch': 6.19}
 98%|█████████▊| 4875/5000 [24:11:17<41:58, 20.14s/it] 98%|█████████▊| 4876/5000 [24:11:35<40:25, 19.56s/it]                                                      {'loss': 19.0148, 'grad_norm': 18.0, 'learning_rate': 1.1763877948021083e-07, 'epoch': 6.19}
 98%|█████████▊| 4876/5000 [24:11:35<40:25, 19.56s/it] 98%|█████████▊| 4877/5000 [24:11:52<38:15, 18.66s/it]                                                      {'loss': 17.3563, 'grad_norm': 12.875, 'learning_rate': 1.1575007292375982e-07, 'epoch': 6.19}
 98%|█████████▊| 4877/5000 [24:11:52<38:15, 18.66s/it] 98%|█████████▊| 4878/5000 [24:12:07<36:10, 17.79s/it]                                                      {'loss': 17.6159, 'grad_norm': 8.125, 'learning_rate': 1.1387662592338919e-07, 'epoch': 6.19}
 98%|█████████▊| 4878/5000 [24:12:07<36:10, 17.79s/it] 98%|█████████▊| 4879/5000 [24:12:25<35:40, 17.69s/it]                                                      {'loss': 17.9833, 'grad_norm': 11.625, 'learning_rate': 1.1201843929861009e-07, 'epoch': 6.2}
 98%|█████████▊| 4879/5000 [24:12:25<35:40, 17.69s/it] 98%|█████████▊| 4880/5000 [24:12:42<35:02, 17.52s/it]                                                      {'loss': 17.8068, 'grad_norm': 9.6875, 'learning_rate': 1.1017551386225009e-07, 'epoch': 6.2}
 98%|█████████▊| 4880/5000 [24:12:42<35:02, 17.52s/it] 98%|█████████▊| 4881/5000 [24:12:58<34:03, 17.17s/it]                                                      {'loss': 17.5016, 'grad_norm': 11.8125, 'learning_rate': 1.0834785042047656e-07, 'epoch': 6.2}
 98%|█████████▊| 4881/5000 [24:12:58<34:03, 17.17s/it] 98%|█████████▊| 4882/5000 [24:13:15<33:26, 17.01s/it]                                                      {'loss': 17.0018, 'grad_norm': 12.4375, 'learning_rate': 1.0653544977276552e-07, 'epoch': 6.2}
 98%|█████████▊| 4882/5000 [24:13:15<33:26, 17.01s/it] 98%|█████████▊| 4883/5000 [24:13:28<30:42, 15.75s/it]                                                      {'loss': 19.4257, 'grad_norm': 25.25, 'learning_rate': 1.0473831271192112e-07, 'epoch': 6.2}
 98%|█████████▊| 4883/5000 [24:13:28<30:42, 15.75s/it] 98%|█████████▊| 4884/5000 [24:13:50<33:54, 17.54s/it]                                                      {'loss': 16.8245, 'grad_norm': 11.9375, 'learning_rate': 1.0295644002407566e-07, 'epoch': 6.2}
 98%|█████████▊| 4884/5000 [24:13:50<33:54, 17.54s/it] 98%|█████████▊| 4885/5000 [24:14:16<38:55, 20.31s/it]                                                      {'loss': 18.0433, 'grad_norm': 9.8125, 'learning_rate': 1.0118983248868173e-07, 'epoch': 6.2}
 98%|█████████▊| 4885/5000 [24:14:16<38:55, 20.31s/it] 98%|█████████▊| 4886/5000 [24:14:30<34:53, 18.37s/it]                                                      {'loss': 19.5054, 'grad_norm': 14.5625, 'learning_rate': 9.94384908785123e-08, 'epoch': 6.2}
 98%|█████████▊| 4886/5000 [24:14:30<34:53, 18.37s/it] 98%|█████████▊| 4887/5000 [24:14:47<33:56, 18.03s/it]                                                      {'loss': 17.5559, 'grad_norm': 13.125, 'learning_rate': 9.770241595966066e-08, 'epoch': 6.21}
 98%|█████████▊| 4887/5000 [24:14:47<33:56, 18.03s/it] 98%|█████████▊| 4888/5000 [24:15:03<32:26, 17.38s/it]                                                      {'loss': 17.7775, 'grad_norm': 7.875, 'learning_rate': 9.598160849155212e-08, 'epoch': 6.21}
 98%|█████████▊| 4888/5000 [24:15:03<32:26, 17.38s/it] 98%|█████████▊| 4889/5000 [24:15:23<33:35, 18.16s/it]                                                      {'loss': 17.2061, 'grad_norm': 8.125, 'learning_rate': 9.427606922691677e-08, 'epoch': 6.21}
 98%|█████████▊| 4889/5000 [24:15:23<33:35, 18.16s/it] 98%|█████████▊| 4890/5000 [24:15:37<30:57, 16.89s/it]                                                      {'loss': 17.6064, 'grad_norm': 11.375, 'learning_rate': 9.25857989118245e-08, 'epoch': 6.21}
 98%|█████████▊| 4890/5000 [24:15:37<30:57, 16.89s/it] 98%|█████████▊| 4891/5000 [24:15:59<33:34, 18.48s/it]                                                      {'loss': 18.4172, 'grad_norm': 15.5, 'learning_rate': 9.091079828564996e-08, 'epoch': 6.21}
 98%|█████████▊| 4891/5000 [24:15:59<33:34, 18.48s/it] 98%|█████████▊| 4892/5000 [24:16:13<30:34, 16.99s/it]                                                      {'loss': 18.0357, 'grad_norm': 9.75, 'learning_rate': 8.925106808109594e-08, 'epoch': 6.21}
 98%|█████████▊| 4892/5000 [24:16:13<30:34, 16.99s/it] 98%|█████████▊| 4893/5000 [24:16:31<30:42, 17.22s/it]                                                      {'loss': 18.2866, 'grad_norm': 11.9375, 'learning_rate': 8.760660902419336e-08, 'epoch': 6.21}
 98%|█████████▊| 4893/5000 [24:16:31<30:42, 17.22s/it] 98%|█████████▊| 4894/5000 [24:16:57<35:26, 20.06s/it]                                                      {'loss': 18.4046, 'grad_norm': 10.3125, 'learning_rate': 8.597742183427403e-08, 'epoch': 6.21}
 98%|█████████▊| 4894/5000 [24:16:57<35:26, 20.06s/it] 98%|█████████▊| 4895/5000 [24:17:13<32:39, 18.66s/it]                                                      {'loss': 19.1197, 'grad_norm': 15.5, 'learning_rate': 8.436350722400175e-08, 'epoch': 6.22}
 98%|█████████▊| 4895/5000 [24:17:13<32:39, 18.66s/it] 98%|█████████▊| 4896/5000 [24:17:29<31:12, 18.00s/it]                                                      {'loss': 17.368, 'grad_norm': 31.75, 'learning_rate': 8.27648658993646e-08, 'epoch': 6.22}
 98%|█████████▊| 4896/5000 [24:17:29<31:12, 18.00s/it] 98%|█████████▊| 4897/5000 [24:17:54<34:09, 19.90s/it]                                                      {'loss': 18.5843, 'grad_norm': 13.4375, 'learning_rate': 8.118149855965539e-08, 'epoch': 6.22}
 98%|█████████▊| 4897/5000 [24:17:54<34:09, 19.90s/it] 98%|█████████▊| 4898/5000 [24:18:10<32:13, 18.95s/it]                                                      {'loss': 16.3719, 'grad_norm': 8.5625, 'learning_rate': 7.96134058974951e-08, 'epoch': 6.22}
 98%|█████████▊| 4898/5000 [24:18:10<32:13, 18.95s/it] 98%|█████████▊| 4899/5000 [24:18:34<34:19, 20.39s/it]                                                      {'loss': 17.8203, 'grad_norm': 14.1875, 'learning_rate': 7.806058859882503e-08, 'epoch': 6.22}
 98%|█████████▊| 4899/5000 [24:18:34<34:19, 20.39s/it] 98%|█████████▊| 4900/5000 [24:18:59<36:09, 21.69s/it]                                                      {'loss': 16.6387, 'grad_norm': 7.6875, 'learning_rate': 7.652304734289127e-08, 'epoch': 6.22}
 98%|█████████▊| 4900/5000 [24:18:59<36:09, 21.69s/it] 98%|█████████▊| 4901/5000 [24:19:15<33:07, 20.07s/it]                                                      {'loss': 17.4895, 'grad_norm': 11.0625, 'learning_rate': 7.500078280227196e-08, 'epoch': 6.22}
 98%|█████████▊| 4901/5000 [24:19:15<33:07, 20.07s/it] 98%|█████████▊| 4902/5000 [24:19:31<30:46, 18.84s/it]                                                      {'loss': 16.7066, 'grad_norm': 8.75, 'learning_rate': 7.349379564285773e-08, 'epoch': 6.22}
 98%|█████████▊| 4902/5000 [24:19:31<30:46, 18.84s/it] 98%|█████████▊| 4903/5000 [24:19:51<31:08, 19.26s/it]                                                      {'loss': 16.2694, 'grad_norm': 8.375, 'learning_rate': 7.200208652385186e-08, 'epoch': 6.23}
 98%|█████████▊| 4903/5000 [24:19:51<31:08, 19.26s/it] 98%|█████████▊| 4904/5000 [24:20:07<29:20, 18.34s/it]                                                      {'loss': 19.2064, 'grad_norm': 14.25, 'learning_rate': 7.052565609778571e-08, 'epoch': 6.23}
 98%|█████████▊| 4904/5000 [24:20:07<29:20, 18.34s/it] 98%|█████████▊| 4905/5000 [24:20:23<27:49, 17.58s/it]                                                      {'loss': 16.7766, 'grad_norm': 9.0625, 'learning_rate': 6.906450501049543e-08, 'epoch': 6.23}
 98%|█████████▊| 4905/5000 [24:20:23<27:49, 17.58s/it] 98%|█████████▊| 4906/5000 [24:20:41<27:25, 17.50s/it]                                                      {'loss': 15.97, 'grad_norm': 9.4375, 'learning_rate': 6.761863390113753e-08, 'epoch': 6.23}
 98%|█████████▊| 4906/5000 [24:20:41<27:25, 17.50s/it] 98%|█████████▊| 4907/5000 [24:20:56<26:12, 16.91s/it]                                                      {'loss': 17.4058, 'grad_norm': 10.5, 'learning_rate': 6.618804340218887e-08, 'epoch': 6.23}
 98%|█████████▊| 4907/5000 [24:20:56<26:12, 16.91s/it] 98%|█████████▊| 4908/5000 [24:21:11<25:02, 16.33s/it]                                                      {'loss': 18.6275, 'grad_norm': 12.0, 'learning_rate': 6.477273413943496e-08, 'epoch': 6.23}
 98%|█████████▊| 4908/5000 [24:21:11<25:02, 16.33s/it] 98%|█████████▊| 4909/5000 [24:21:25<23:38, 15.59s/it]                                                      {'loss': 19.4461, 'grad_norm': 18.75, 'learning_rate': 6.33727067319778e-08, 'epoch': 6.23}
 98%|█████████▊| 4909/5000 [24:21:25<23:38, 15.59s/it] 98%|█████████▊| 4910/5000 [24:21:41<23:38, 15.76s/it]                                                      {'loss': 18.973, 'grad_norm': 27.625, 'learning_rate': 6.198796179224363e-08, 'epoch': 6.23}
 98%|█████████▊| 4910/5000 [24:21:41<23:38, 15.76s/it] 98%|█████████▊| 4911/5000 [24:21:59<24:14, 16.35s/it]                                                      {'loss': 17.3327, 'grad_norm': 9.5, 'learning_rate': 6.061849992596347e-08, 'epoch': 6.24}
 98%|█████████▊| 4911/5000 [24:21:59<24:14, 16.35s/it] 98%|█████████▊| 4912/5000 [24:22:21<26:26, 18.02s/it]                                                      {'loss': 17.6456, 'grad_norm': 8.6875, 'learning_rate': 5.926432173218476e-08, 'epoch': 6.24}
 98%|█████████▊| 4912/5000 [24:22:21<26:26, 18.02s/it] 98%|█████████▊| 4913/5000 [24:22:34<24:08, 16.64s/it]                                                      {'loss': 18.5515, 'grad_norm': 10.375, 'learning_rate': 5.792542780327147e-08, 'epoch': 6.24}
 98%|█████████▊| 4913/5000 [24:22:34<24:08, 16.64s/it] 98%|█████████▊| 4914/5000 [24:22:54<25:22, 17.70s/it]                                                      {'loss': 18.3878, 'grad_norm': 17.25, 'learning_rate': 5.660181872490788e-08, 'epoch': 6.24}
 98%|█████████▊| 4914/5000 [24:22:54<25:22, 17.70s/it] 98%|█████████▊| 4915/5000 [24:23:09<23:47, 16.79s/it]                                                      {'loss': 19.1584, 'grad_norm': 17.375, 'learning_rate': 5.529349507607528e-08, 'epoch': 6.24}
 98%|█████████▊| 4915/5000 [24:23:09<23:47, 16.79s/it] 98%|█████████▊| 4916/5000 [24:23:22<22:06, 15.80s/it]                                                      {'loss': 17.6986, 'grad_norm': 10.875, 'learning_rate': 5.400045742908698e-08, 'epoch': 6.24}
 98%|█████████▊| 4916/5000 [24:23:22<22:06, 15.80s/it] 98%|█████████▊| 4917/5000 [24:23:40<22:36, 16.35s/it]                                                      {'loss': 16.9919, 'grad_norm': 9.1875, 'learning_rate': 5.272270634955722e-08, 'epoch': 6.24}
 98%|█████████▊| 4917/5000 [24:23:40<22:36, 16.35s/it] 98%|█████████▊| 4918/5000 [24:23:53<20:44, 15.18s/it]                                                      {'loss': 19.845, 'grad_norm': 18.125, 'learning_rate': 5.146024239642055e-08, 'epoch': 6.25}
 98%|█████████▊| 4918/5000 [24:23:53<20:44, 15.18s/it] 98%|█████████▊| 4919/5000 [24:24:09<20:58, 15.54s/it]                                                      {'loss': 16.9186, 'grad_norm': 8.75, 'learning_rate': 5.021306612191633e-08, 'epoch': 6.25}
 98%|█████████▊| 4919/5000 [24:24:09<20:58, 15.54s/it] 98%|█████████▊| 4920/5000 [24:24:24<20:24, 15.31s/it]                                                      {'loss': 16.9644, 'grad_norm': 18.375, 'learning_rate': 4.8981178071608176e-08, 'epoch': 6.25}
 98%|█████████▊| 4920/5000 [24:24:24<20:24, 15.31s/it] 98%|█████████▊| 4921/5000 [24:24:39<19:57, 15.16s/it]                                                      {'loss': 17.6703, 'grad_norm': 11.5, 'learning_rate': 4.776457878436446e-08, 'epoch': 6.25}
 98%|█████████▊| 4921/5000 [24:24:39<19:57, 15.16s/it] 98%|█████████▊| 4922/5000 [24:24:55<20:03, 15.43s/it]                                                      {'loss': 17.7246, 'grad_norm': 15.25, 'learning_rate': 4.656326879236616e-08, 'epoch': 6.25}
 98%|█████████▊| 4922/5000 [24:24:55<20:03, 15.43s/it] 98%|█████████▊| 4923/5000 [24:25:10<19:39, 15.31s/it]                                                      {'loss': 16.2032, 'grad_norm': 8.5, 'learning_rate': 4.5377248621102926e-08, 'epoch': 6.25}
 98%|█████████▊| 4923/5000 [24:25:10<19:39, 15.31s/it] 98%|█████████▊| 4924/5000 [24:25:25<19:26, 15.35s/it]                                                      {'loss': 17.6948, 'grad_norm': 15.0625, 'learning_rate': 4.4206518789388635e-08, 'epoch': 6.25}
 98%|█████████▊| 4924/5000 [24:25:25<19:26, 15.35s/it] 98%|█████████▊| 4925/5000 [24:25:40<18:55, 15.14s/it]                                                      {'loss': 18.1748, 'grad_norm': 13.625, 'learning_rate': 4.30510798093342e-08, 'epoch': 6.25}
 98%|█████████▊| 4925/5000 [24:25:40<18:55, 15.14s/it] 99%|█████████▊| 4926/5000 [24:25:59<20:22, 16.52s/it]                                                      {'loss': 18.7642, 'grad_norm': 13.4375, 'learning_rate': 4.1910932186366984e-08, 'epoch': 6.26}
 99%|█████████▊| 4926/5000 [24:25:59<20:22, 16.52s/it] 99%|█████████▊| 4927/5000 [24:26:16<19:58, 16.42s/it]                                                      {'loss': 16.7285, 'grad_norm': 15.6875, 'learning_rate': 4.0786076419230814e-08, 'epoch': 6.26}
 99%|█████████▊| 4927/5000 [24:26:16<19:58, 16.42s/it] 99%|█████████▊| 4928/5000 [24:26:33<19:57, 16.63s/it]                                                      {'loss': 18.6328, 'grad_norm': 13.8125, 'learning_rate': 3.9676512999974317e-08, 'epoch': 6.26}
 99%|█████████▊| 4928/5000 [24:26:33<19:57, 16.63s/it] 99%|█████████▊| 4929/5000 [24:26:54<21:18, 18.01s/it]                                                      {'loss': 18.4033, 'grad_norm': 14.4375, 'learning_rate': 3.85822424139548e-08, 'epoch': 6.26}
 99%|█████████▊| 4929/5000 [24:26:54<21:18, 18.01s/it] 99%|█████████▊| 4930/5000 [24:27:07<19:25, 16.64s/it]                                                      {'loss': 19.7553, 'grad_norm': 11.75, 'learning_rate': 3.750326513984603e-08, 'epoch': 6.26}
 99%|█████████▊| 4930/5000 [24:27:07<19:25, 16.64s/it] 99%|█████████▊| 4931/5000 [24:27:23<18:44, 16.30s/it]                                                      {'loss': 17.3028, 'grad_norm': 7.8125, 'learning_rate': 3.6439581649634344e-08, 'epoch': 6.26}
 99%|█████████▊| 4931/5000 [24:27:23<18:44, 16.30s/it] 99%|█████████▊| 4932/5000 [24:27:38<18:06, 15.98s/it]                                                      {'loss': 17.3216, 'grad_norm': 9.125, 'learning_rate': 3.5391192408603106e-08, 'epoch': 6.26}
 99%|█████████▊| 4932/5000 [24:27:38<18:06, 15.98s/it] 99%|█████████▊| 4933/5000 [24:27:51<16:49, 15.07s/it]                                                      {'loss': 19.7985, 'grad_norm': 13.5625, 'learning_rate': 3.435809787535604e-08, 'epoch': 6.26}
 99%|█████████▊| 4933/5000 [24:27:51<16:49, 15.07s/it] 99%|█████████▊| 4934/5000 [24:28:16<19:44, 17.94s/it]                                                      {'loss': 18.4158, 'grad_norm': 10.25, 'learning_rate': 3.334029850180553e-08, 'epoch': 6.27}
 99%|█████████▊| 4934/5000 [24:28:16<19:44, 17.94s/it] 99%|█████████▊| 4935/5000 [24:28:32<18:45, 17.31s/it]                                                      {'loss': 17.1193, 'grad_norm': 11.8125, 'learning_rate': 3.233779473316878e-08, 'epoch': 6.27}
 99%|█████████▊| 4935/5000 [24:28:32<18:45, 17.31s/it] 99%|█████████▊| 4936/5000 [24:28:48<18:18, 17.16s/it]                                                      {'loss': 18.1261, 'grad_norm': 9.8125, 'learning_rate': 3.135058700797555e-08, 'epoch': 6.27}
 99%|█████████▊| 4936/5000 [24:28:48<18:18, 17.16s/it] 99%|█████████▊| 4937/5000 [24:29:04<17:39, 16.81s/it]                                                      {'loss': 17.554, 'grad_norm': 10.3125, 'learning_rate': 3.0378675758068186e-08, 'epoch': 6.27}
 99%|█████████▊| 4937/5000 [24:29:04<17:39, 16.81s/it] 99%|█████████▉| 4938/5000 [24:29:21<17:23, 16.83s/it]                                                      {'loss': 18.5323, 'grad_norm': 9.6875, 'learning_rate': 2.9422061408589937e-08, 'epoch': 6.27}
 99%|█████████▉| 4938/5000 [24:29:21<17:23, 16.83s/it] 99%|█████████▉| 4939/5000 [24:29:36<16:24, 16.15s/it]                                                      {'loss': 17.6834, 'grad_norm': 11.25, 'learning_rate': 2.8480744377996635e-08, 'epoch': 6.27}
 99%|█████████▉| 4939/5000 [24:29:36<16:24, 16.15s/it] 99%|█████████▉| 4940/5000 [24:29:54<16:46, 16.78s/it]                                                      {'loss': 17.5784, 'grad_norm': 12.3125, 'learning_rate': 2.75547250780489e-08, 'epoch': 6.27}
 99%|█████████▉| 4940/5000 [24:29:54<16:46, 16.78s/it] 99%|█████████▉| 4941/5000 [24:30:07<15:28, 15.74s/it]                                                      {'loss': 21.3793, 'grad_norm': 20.25, 'learning_rate': 2.6644003913827703e-08, 'epoch': 6.27}
 99%|█████████▉| 4941/5000 [24:30:07<15:28, 15.74s/it] 99%|█████████▉| 4942/5000 [24:30:22<14:56, 15.45s/it]                                                      {'loss': 18.4122, 'grad_norm': 12.25, 'learning_rate': 2.574858128370716e-08, 'epoch': 6.28}
 99%|█████████▉| 4942/5000 [24:30:22<14:56, 15.45s/it] 99%|█████████▉| 4943/5000 [24:30:51<18:35, 19.57s/it]                                                      {'loss': 17.7755, 'grad_norm': 9.3125, 'learning_rate': 2.4868457579377833e-08, 'epoch': 6.28}
 99%|█████████▉| 4943/5000 [24:30:51<18:35, 19.57s/it] 99%|█████████▉| 4944/5000 [24:31:07<17:06, 18.33s/it]                                                      {'loss': 20.0524, 'grad_norm': 17.75, 'learning_rate': 2.4003633185831207e-08, 'epoch': 6.28}
 99%|█████████▉| 4944/5000 [24:31:07<17:06, 18.33s/it] 99%|█████████▉| 4945/5000 [24:31:24<16:29, 18.00s/it]                                                      {'loss': 18.638, 'grad_norm': 11.625, 'learning_rate': 2.315410848137911e-08, 'epoch': 6.28}
 99%|█████████▉| 4945/5000 [24:31:24<16:29, 18.00s/it] 99%|█████████▉| 4946/5000 [24:31:40<15:43, 17.48s/it]                                                      {'loss': 18.3666, 'grad_norm': 10.875, 'learning_rate': 2.2319883837626507e-08, 'epoch': 6.28}
 99%|█████████▉| 4946/5000 [24:31:40<15:43, 17.48s/it] 99%|█████████▉| 4947/5000 [24:32:04<17:03, 19.30s/it]                                                      {'loss': 19.2724, 'grad_norm': 15.1875, 'learning_rate': 2.150095961949094e-08, 'epoch': 6.28}
 99%|█████████▉| 4947/5000 [24:32:04<17:03, 19.30s/it] 99%|█████████▉| 4948/5000 [24:32:17<15:05, 17.42s/it]                                                      {'loss': 19.3187, 'grad_norm': 14.875, 'learning_rate': 2.0697336185198643e-08, 'epoch': 6.28}
 99%|█████████▉| 4948/5000 [24:32:17<15:05, 17.42s/it] 99%|█████████▉| 4949/5000 [24:32:32<14:13, 16.73s/it]                                                      {'loss': 18.4227, 'grad_norm': 12.9375, 'learning_rate': 1.9909013886288405e-08, 'epoch': 6.28}
 99%|█████████▉| 4949/5000 [24:32:32<14:13, 16.73s/it] 99%|█████████▉| 4950/5000 [24:32:54<15:20, 18.42s/it]                                                      {'loss': 16.7686, 'grad_norm': 7.71875, 'learning_rate': 1.9135993067588284e-08, 'epoch': 6.29}
 99%|█████████▉| 4950/5000 [24:32:54<15:20, 18.42s/it] 99%|█████████▉| 4951/5000 [24:33:11<14:34, 17.84s/it]                                                      {'loss': 20.4693, 'grad_norm': 16.125, 'learning_rate': 1.837827406725445e-08, 'epoch': 6.29}
 99%|█████████▉| 4951/5000 [24:33:11<14:34, 17.84s/it] 99%|█████████▉| 4952/5000 [24:33:27<13:58, 17.47s/it]                                                      {'loss': 18.7123, 'grad_norm': 42.5, 'learning_rate': 1.7635857216728445e-08, 'epoch': 6.29}
 99%|█████████▉| 4952/5000 [24:33:27<13:58, 17.47s/it] 99%|█████████▉| 4953/5000 [24:33:45<13:39, 17.44s/it]                                                      {'loss': 16.7145, 'grad_norm': 8.0, 'learning_rate': 1.6908742840776034e-08, 'epoch': 6.29}
 99%|█████████▉| 4953/5000 [24:33:45<13:39, 17.44s/it] 99%|█████████▉| 4954/5000 [24:34:00<12:56, 16.88s/it]                                                      {'loss': 17.6045, 'grad_norm': 10.6875, 'learning_rate': 1.6196931257460022e-08, 'epoch': 6.29}
 99%|█████████▉| 4954/5000 [24:34:00<12:56, 16.88s/it] 99%|█████████▉| 4955/5000 [24:34:14<11:59, 15.99s/it]                                                      {'loss': 18.8701, 'grad_norm': 18.875, 'learning_rate': 1.550042277815189e-08, 'epoch': 6.29}
 99%|█████████▉| 4955/5000 [24:34:14<11:59, 15.99s/it] 99%|█████████▉| 4956/5000 [24:34:30<11:39, 15.89s/it]                                                      {'loss': 17.4555, 'grad_norm': 13.125, 'learning_rate': 1.4819217707524033e-08, 'epoch': 6.29}
 99%|█████████▉| 4956/5000 [24:34:30<11:39, 15.89s/it] 99%|█████████▉| 4957/5000 [24:34:48<11:50, 16.52s/it]                                                      {'loss': 18.645, 'grad_norm': 15.8125, 'learning_rate': 1.4153316343561426e-08, 'epoch': 6.29}
 99%|█████████▉| 4957/5000 [24:34:48<11:50, 16.52s/it] 99%|█████████▉| 4958/5000 [24:35:06<11:50, 16.92s/it]                                                      {'loss': 15.7919, 'grad_norm': 7.5, 'learning_rate': 1.3502718977553839e-08, 'epoch': 6.3}
 99%|█████████▉| 4958/5000 [24:35:06<11:50, 16.92s/it] 99%|█████████▉| 4959/5000 [24:35:21<11:15, 16.47s/it]                                                      {'loss': 18.4788, 'grad_norm': 13.4375, 'learning_rate': 1.2867425894091954e-08, 'epoch': 6.3}
 99%|█████████▉| 4959/5000 [24:35:21<11:15, 16.47s/it] 99%|█████████▉| 4960/5000 [24:35:48<12:58, 19.47s/it]                                                      {'loss': 19.5894, 'grad_norm': 19.5, 'learning_rate': 1.2247437371075141e-08, 'epoch': 6.3}
 99%|█████████▉| 4960/5000 [24:35:48<12:58, 19.47s/it] 99%|█████████▉| 4961/5000 [24:36:01<11:29, 17.68s/it]                                                      {'loss': 18.5284, 'grad_norm': 10.5, 'learning_rate': 1.1642753679707573e-08, 'epoch': 6.3}
 99%|█████████▉| 4961/5000 [24:36:01<11:29, 17.68s/it] 99%|█████████▉| 4962/5000 [24:36:19<11:12, 17.71s/it]                                                      {'loss': 15.2797, 'grad_norm': 8.5, 'learning_rate': 1.1053375084502103e-08, 'epoch': 6.3}
 99%|█████████▉| 4962/5000 [24:36:19<11:12, 17.71s/it] 99%|█████████▉| 4963/5000 [24:36:33<10:18, 16.72s/it]                                                      {'loss': 18.0069, 'grad_norm': 16.125, 'learning_rate': 1.0479301843264732e-08, 'epoch': 6.3}
 99%|█████████▉| 4963/5000 [24:36:33<10:18, 16.72s/it] 99%|█████████▉| 4964/5000 [24:36:49<09:53, 16.49s/it]                                                      {'loss': 17.1798, 'grad_norm': 11.4375, 'learning_rate': 9.9205342071218e-09, 'epoch': 6.3}
 99%|█████████▉| 4964/5000 [24:36:49<09:53, 16.49s/it] 99%|█████████▉| 4965/5000 [24:37:03<09:11, 15.76s/it]                                                      {'loss': 18.2793, 'grad_norm': 11.625, 'learning_rate': 9.377072420492793e-09, 'epoch': 6.3}
 99%|█████████▉| 4965/5000 [24:37:03<09:11, 15.76s/it] 99%|█████████▉| 4966/5000 [24:37:18<08:40, 15.31s/it]                                                      {'loss': 19.5144, 'grad_norm': 14.125, 'learning_rate': 8.848916721109767e-09, 'epoch': 6.31}
 99%|█████████▉| 4966/5000 [24:37:18<08:40, 15.31s/it] 99%|█████████▉| 4967/5000 [24:37:32<08:17, 15.06s/it]                                                      {'loss': 18.8708, 'grad_norm': 17.375, 'learning_rate': 8.336067340001807e-09, 'epoch': 6.31}
 99%|█████████▉| 4967/5000 [24:37:32<08:17, 15.06s/it] 99%|█████████▉| 4968/5000 [24:37:48<08:06, 15.20s/it]                                                      {'loss': 17.7267, 'grad_norm': 26.25, 'learning_rate': 7.838524501514454e-09, 'epoch': 6.31}
 99%|█████████▉| 4968/5000 [24:37:48<08:06, 15.20s/it] 99%|█████████▉| 4969/5000 [24:38:07<08:34, 16.58s/it]                                                      {'loss': 16.7659, 'grad_norm': 9.0625, 'learning_rate': 7.356288423282508e-09, 'epoch': 6.31}
 99%|█████████▉| 4969/5000 [24:38:07<08:34, 16.58s/it] 99%|█████████▉| 4970/5000 [24:38:28<08:50, 17.67s/it]                                                      {'loss': 18.0715, 'grad_norm': 9.1875, 'learning_rate': 6.88935931625334e-09, 'epoch': 6.31}
 99%|█████████▉| 4970/5000 [24:38:28<08:50, 17.67s/it] 99%|█████████▉| 4971/5000 [24:38:49<09:07, 18.87s/it]                                                      {'loss': 17.811, 'grad_norm': 11.75, 'learning_rate': 6.4377373846791205e-09, 'epoch': 6.31}
 99%|█████████▉| 4971/5000 [24:38:49<09:07, 18.87s/it] 99%|█████████▉| 4972/5000 [24:39:04<08:14, 17.66s/it]                                                      {'loss': 16.9304, 'grad_norm': 9.8125, 'learning_rate': 6.001422826112934e-09, 'epoch': 6.31}
 99%|█████████▉| 4972/5000 [24:39:04<08:14, 17.66s/it] 99%|█████████▉| 4973/5000 [24:39:17<07:19, 16.29s/it]                                                      {'loss': 18.1234, 'grad_norm': 11.375, 'learning_rate': 5.580415831416552e-09, 'epoch': 6.31}
 99%|█████████▉| 4973/5000 [24:39:17<07:19, 16.29s/it] 99%|█████████▉| 4974/5000 [24:39:33<07:01, 16.19s/it]                                                      {'loss': 18.3292, 'grad_norm': 11.6875, 'learning_rate': 5.174716584752658e-09, 'epoch': 6.32}
 99%|█████████▉| 4974/5000 [24:39:33<07:01, 16.19s/it]100%|█████████▉| 4975/5000 [24:39:48<06:37, 15.89s/it]                                                      {'loss': 17.5451, 'grad_norm': 13.4375, 'learning_rate': 4.784325263584854e-09, 'epoch': 6.32}
100%|█████████▉| 4975/5000 [24:39:48<06:37, 15.89s/it]100%|█████████▉| 4976/5000 [24:40:11<07:07, 17.81s/it]                                                      {'loss': 17.4307, 'grad_norm': 12.4375, 'learning_rate': 4.409242038689309e-09, 'epoch': 6.32}
100%|█████████▉| 4976/5000 [24:40:11<07:07, 17.81s/it]100%|█████████▉| 4977/5000 [24:40:24<06:16, 16.35s/it]                                                      {'loss': 19.3939, 'grad_norm': 12.4375, 'learning_rate': 4.049467074131452e-09, 'epoch': 6.32}
100%|█████████▉| 4977/5000 [24:40:24<06:16, 16.35s/it]100%|█████████▉| 4978/5000 [24:40:41<06:02, 16.49s/it]                                                      {'loss': 17.3995, 'grad_norm': 11.625, 'learning_rate': 3.7050005272931673e-09, 'epoch': 6.32}
100%|█████████▉| 4978/5000 [24:40:41<06:02, 16.49s/it]100%|█████████▉| 4979/5000 [24:40:55<05:31, 15.79s/it]                                                      {'loss': 17.1852, 'grad_norm': 10.3125, 'learning_rate': 3.3758425488572547e-09, 'epoch': 6.32}
100%|█████████▉| 4979/5000 [24:40:55<05:31, 15.79s/it]100%|█████████▉| 4980/5000 [24:41:10<05:14, 15.73s/it]                                                      {'loss': 16.4957, 'grad_norm': 8.6875, 'learning_rate': 3.0619932828113146e-09, 'epoch': 6.32}
100%|█████████▉| 4980/5000 [24:41:10<05:14, 15.73s/it]100%|█████████▉| 4981/5000 [24:41:23<04:41, 14.82s/it]                                                      {'loss': 18.4402, 'grad_norm': 12.625, 'learning_rate': 2.7634528664360888e-09, 'epoch': 6.33}
100%|█████████▉| 4981/5000 [24:41:23<04:41, 14.82s/it]100%|█████████▉| 4982/5000 [24:41:40<04:38, 15.46s/it]                                                      {'loss': 18.8582, 'grad_norm': 14.8125, 'learning_rate': 2.480221430328777e-09, 'epoch': 6.33}
100%|█████████▉| 4982/5000 [24:41:40<04:38, 15.46s/it]100%|█████████▉| 4983/5000 [24:41:55<04:21, 15.40s/it]                                                      {'loss': 18.2484, 'grad_norm': 11.5, 'learning_rate': 2.2122990983836076e-09, 'epoch': 6.33}
100%|█████████▉| 4983/5000 [24:41:55<04:21, 15.40s/it]100%|█████████▉| 4984/5000 [24:42:09<04:01, 15.07s/it]                                                      {'loss': 17.1358, 'grad_norm': 10.6875, 'learning_rate': 1.959685987795723e-09, 'epoch': 6.33}
100%|█████████▉| 4984/5000 [24:42:09<04:01, 15.07s/it]100%|█████████▉| 4985/5000 [24:42:25<03:47, 15.15s/it]                                                      {'loss': 16.2204, 'grad_norm': 9.6875, 'learning_rate': 1.722382209068951e-09, 'epoch': 6.33}
100%|█████████▉| 4985/5000 [24:42:25<03:47, 15.15s/it]100%|█████████▉| 4986/5000 [24:42:39<03:26, 14.72s/it]                                                      {'loss': 18.5339, 'grad_norm': 12.375, 'learning_rate': 1.5003878660119206e-09, 'epoch': 6.33}
100%|█████████▉| 4986/5000 [24:42:39<03:26, 14.72s/it]100%|█████████▉| 4987/5000 [24:42:54<03:12, 14.81s/it]                                                      {'loss': 17.6601, 'grad_norm': 9.1875, 'learning_rate': 1.2937030557264028e-09, 'epoch': 6.33}
100%|█████████▉| 4987/5000 [24:42:54<03:12, 14.81s/it]100%|█████████▉| 4988/5000 [24:43:18<03:32, 17.67s/it]                                                      {'loss': 17.4163, 'grad_norm': 9.9375, 'learning_rate': 1.10232786862674e-09, 'epoch': 6.33}
100%|█████████▉| 4988/5000 [24:43:18<03:32, 17.67s/it]100%|█████████▉| 4989/5000 [24:43:48<03:56, 21.50s/it]                                                      {'loss': 17.5508, 'grad_norm': 13.625, 'learning_rate': 9.262623884243037e-10, 'epoch': 6.34}
100%|█████████▉| 4989/5000 [24:43:48<03:56, 21.50s/it]100%|█████████▉| 4990/5000 [24:44:02<03:12, 19.28s/it]                                                      {'loss': 19.7571, 'grad_norm': 11.5625, 'learning_rate': 7.655066921391506e-10, 'epoch': 6.34}
100%|█████████▉| 4990/5000 [24:44:02<03:12, 19.28s/it]100%|█████████▉| 4991/5000 [24:44:17<02:41, 17.99s/it]                                                      {'loss': 18.7447, 'grad_norm': 17.875, 'learning_rate': 6.200608500883663e-10, 'epoch': 6.34}
100%|█████████▉| 4991/5000 [24:44:17<02:41, 17.99s/it]100%|█████████▉| 4992/5000 [24:44:33<02:17, 17.23s/it]                                                      {'loss': 18.7025, 'grad_norm': 11.9375, 'learning_rate': 4.899249258977223e-10, 'epoch': 6.34}
100%|█████████▉| 4992/5000 [24:44:33<02:17, 17.23s/it]100%|█████████▉| 4993/5000 [24:44:48<01:56, 16.58s/it]                                                      {'loss': 18.6885, 'grad_norm': 12.75, 'learning_rate': 3.750989764900181e-10, 'epoch': 6.34}
100%|█████████▉| 4993/5000 [24:44:48<01:56, 16.58s/it]100%|█████████▉| 4994/5000 [24:45:10<01:48, 18.13s/it]                                                      {'loss': 17.5375, 'grad_norm': 10.5625, 'learning_rate': 2.7558305210062525e-10, 'epoch': 6.34}
100%|█████████▉| 4994/5000 [24:45:10<01:48, 18.13s/it]100%|█████████▉| 4995/5000 [24:45:28<01:30, 18.15s/it]                                                      {'loss': 17.9829, 'grad_norm': 11.1875, 'learning_rate': 1.9137719625028637e-10, 'epoch': 6.34}
100%|█████████▉| 4995/5000 [24:45:28<01:30, 18.15s/it]100%|█████████▉| 4996/5000 [24:45:44<01:10, 17.67s/it]                                                      {'loss': 16.8077, 'grad_norm': 8.375, 'learning_rate': 1.2248144578397289e-10, 'epoch': 6.34}
100%|█████████▉| 4996/5000 [24:45:44<01:10, 17.67s/it]100%|█████████▉| 4997/5000 [24:46:03<00:53, 17.99s/it]                                                      {'loss': 17.4133, 'grad_norm': 9.375, 'learning_rate': 6.889583083202754e-11, 'epoch': 6.35}
100%|█████████▉| 4997/5000 [24:46:03<00:53, 17.99s/it]100%|█████████▉| 4998/5000 [24:46:18<00:34, 17.14s/it]                                                      {'loss': 18.9942, 'grad_norm': 16.5, 'learning_rate': 3.062037484125035e-11, 'epoch': 6.35}
100%|█████████▉| 4998/5000 [24:46:18<00:34, 17.14s/it]100%|█████████▉| 4999/5000 [24:46:37<00:17, 17.52s/it]                                                      {'loss': 17.2745, 'grad_norm': 8.625, 'learning_rate': 7.655094547698303e-12, 'epoch': 6.35}
100%|█████████▉| 4999/5000 [24:46:37<00:17, 17.52s/it]100%|██████████| 5000/5000 [24:46:51<00:00, 16.68s/it]                                                      {'loss': 16.7948, 'grad_norm': 8.125, 'learning_rate': 0.0, 'epoch': 6.35}
100%|██████████| 5000/5000 [24:46:51<00:00, 16.68s/it]
  0%|          | 0/88 [00:00<?, ?it/s][A
  2%|▏         | 2/88 [00:08<06:19,  4.41s/it][A
  3%|▎         | 3/88 [00:16<08:05,  5.71s/it][A
  5%|▍         | 4/88 [00:20<07:16,  5.19s/it][A
  6%|▌         | 5/88 [00:24<06:30,  4.71s/it][A
  7%|▋         | 6/88 [00:29<06:42,  4.91s/it][A
  8%|▊         | 7/88 [00:33<06:13,  4.62s/it][A
  9%|▉         | 8/88 [00:37<05:47,  4.34s/it][A
 10%|█         | 9/88 [00:40<05:21,  4.07s/it][A
 11%|█▏        | 10/88 [00:43<04:50,  3.72s/it][A
 12%|█▎        | 11/88 [00:46<04:27,  3.47s/it][A
 14%|█▎        | 12/88 [00:49<03:57,  3.13s/it][A
 15%|█▍        | 13/88 [00:51<03:41,  2.95s/it][A
 16%|█▌        | 14/88 [00:57<04:36,  3.73s/it][A
 17%|█▋        | 15/88 [01:02<05:10,  4.26s/it][A
 18%|█▊        | 16/88 [01:06<04:47,  4.00s/it][A
 19%|█▉        | 17/88 [01:11<05:12,  4.41s/it][A
 20%|██        | 18/88 [01:14<04:42,  4.04s/it][A
 22%|██▏       | 19/88 [01:18<04:43,  4.12s/it][A
 23%|██▎       | 20/88 [01:22<04:34,  4.03s/it][A
 24%|██▍       | 21/88 [01:26<04:14,  3.80s/it][A
 25%|██▌       | 22/88 [01:29<04:11,  3.82s/it][A
 26%|██▌       | 23/88 [01:32<03:45,  3.47s/it][A
 27%|██▋       | 24/88 [01:41<05:21,  5.02s/it][A
 28%|██▊       | 25/88 [01:44<04:46,  4.55s/it][A
 30%|██▉       | 26/88 [01:51<05:17,  5.13s/it][A
 31%|███       | 27/88 [01:55<04:57,  4.88s/it][A
 32%|███▏      | 28/88 [02:04<06:02,  6.04s/it][A
 33%|███▎      | 29/88 [02:09<05:41,  5.79s/it][A
 34%|███▍      | 30/88 [02:14<05:25,  5.62s/it][A
 35%|███▌      | 31/88 [02:18<04:54,  5.17s/it][A
 36%|███▋      | 32/88 [02:27<05:54,  6.32s/it][A
 38%|███▊      | 33/88 [02:32<05:20,  5.83s/it][A
 39%|███▊      | 34/88 [02:41<06:01,  6.70s/it][A
 40%|███▉      | 35/88 [02:44<04:56,  5.59s/it][A
 41%|████      | 36/88 [02:49<04:44,  5.47s/it][A
 42%|████▏     | 37/88 [02:53<04:11,  4.93s/it][A
 43%|████▎     | 38/88 [02:57<03:55,  4.70s/it][A
 44%|████▍     | 39/88 [03:00<03:27,  4.23s/it][A
 45%|████▌     | 40/88 [03:06<03:57,  4.95s/it][A
 47%|████▋     | 41/88 [03:16<04:58,  6.35s/it][A
 48%|████▊     | 42/88 [03:20<04:14,  5.54s/it][A
 49%|████▉     | 43/88 [03:28<04:50,  6.46s/it][A
 50%|█████     | 44/88 [03:32<04:02,  5.51s/it][A
 51%|█████     | 45/88 [03:36<03:44,  5.22s/it][A
 52%|█████▏    | 46/88 [03:40<03:23,  4.83s/it][A
 53%|█████▎    | 47/88 [03:42<02:47,  4.08s/it][A
 55%|█████▍    | 48/88 [03:45<02:22,  3.55s/it][A
 56%|█████▌    | 49/88 [03:50<02:35,  3.99s/it][A
 57%|█████▋    | 50/88 [03:54<02:33,  4.04s/it][A
 58%|█████▊    | 51/88 [03:58<02:30,  4.08s/it][A
 59%|█████▉    | 52/88 [04:04<02:45,  4.61s/it][A
 60%|██████    | 53/88 [04:09<02:45,  4.73s/it][A
 61%|██████▏   | 54/88 [04:19<03:31,  6.21s/it][A
 62%|██████▎   | 55/88 [04:24<03:15,  5.91s/it][A
 64%|██████▎   | 56/88 [04:26<02:37,  4.92s/it][A
 65%|██████▍   | 57/88 [04:30<02:23,  4.62s/it][A
 66%|██████▌   | 58/88 [04:39<02:56,  5.89s/it][A
 67%|██████▋   | 59/88 [04:44<02:41,  5.57s/it][A
 68%|██████▊   | 60/88 [04:47<02:16,  4.87s/it][A
 69%|██████▉   | 61/88 [04:51<02:05,  4.63s/it][A
 70%|███████   | 62/88 [04:54<01:47,  4.14s/it][A
 72%|███████▏  | 63/88 [05:00<01:53,  4.55s/it][A
 73%|███████▎  | 64/88 [05:04<01:45,  4.40s/it][A
 74%|███████▍  | 65/88 [05:08<01:41,  4.40s/it][A
 75%|███████▌  | 66/88 [05:12<01:32,  4.21s/it][A
 76%|███████▌  | 67/88 [05:15<01:21,  3.88s/it][A
 77%|███████▋  | 68/88 [05:20<01:23,  4.18s/it][A
 78%|███████▊  | 69/88 [05:26<01:31,  4.82s/it][A
 80%|███████▉  | 70/88 [05:30<01:22,  4.57s/it][A
 81%|████████  | 71/88 [05:34<01:13,  4.35s/it][A
 82%|████████▏ | 72/88 [05:38<01:07,  4.23s/it][A
 83%|████████▎ | 73/88 [05:41<00:58,  3.88s/it][A
 84%|████████▍ | 74/88 [05:44<00:49,  3.56s/it][A
 85%|████████▌ | 75/88 [05:49<00:50,  3.92s/it][A
 86%|████████▋ | 76/88 [05:53<00:46,  3.89s/it][A
 88%|████████▊ | 77/88 [06:00<00:54,  4.97s/it][A
 89%|████████▊ | 78/88 [06:04<00:46,  4.65s/it][A
 90%|████████▉ | 79/88 [06:09<00:41,  4.67s/it][A
 91%|█████████ | 80/88 [06:12<00:33,  4.15s/it][A
 92%|█████████▏| 81/88 [06:17<00:30,  4.41s/it][A
 93%|█████████▎| 82/88 [06:21<00:25,  4.25s/it][A
 94%|█████████▍| 83/88 [06:24<00:20,  4.13s/it][A
 95%|█████████▌| 84/88 [06:28<00:16,  4.06s/it][A
 97%|█████████▋| 85/88 [06:31<00:11,  3.71s/it][A
 98%|█████████▊| 86/88 [06:34<00:06,  3.45s/it][A
 99%|█████████▉| 87/88 [06:38<00:03,  3.64s/it][A
100%|██████████| 88/88 [06:42<00:00,  3.69s/it][A                                                      
                                               [A{'eval_loss': 17.48978042602539, 'eval_runtime': 406.3362, 'eval_samples_per_second': 6.891, 'eval_steps_per_second': 0.217, 'epoch': 6.35}
100%|██████████| 5000/5000 [24:53:38<00:00, 16.68s/it]
100%|██████████| 88/88 [06:42<00:00,  3.69s/it][A
                                               [A2024-06-14 10:29:15,423 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
2024-06-14 10:29:25,779 _dedup_tensors.py:44 INFO p:MainProcess t:MainThread: Duplicate keys to remove: {}
                                                      {'train_runtime': 89639.3744, 'train_samples_per_second': 1.785, 'train_steps_per_second': 0.056, 'train_loss': 70.77398354530334, 'epoch': 6.35}
100%|██████████| 5000/5000 [24:53:51<00:00, 16.68s/it]100%|██████████| 5000/5000 [24:53:51<00:00, 17.93s/it]
Model saved to /data2/xjw/llama-meteor-data/train_gate_and_loras
wandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.051 MB of 0.438 MB uploadedwandb: | 0.057 MB of 0.438 MB uploadedwandb: / 0.057 MB of 0.438 MB uploadedwandb: - 0.057 MB of 0.438 MB uploadedwandb: \ 0.057 MB of 0.438 MB uploadedwandb: | 0.057 MB of 0.438 MB uploadedwandb: / 0.057 MB of 0.438 MB uploadedwandb: - 0.069 MB of 0.438 MB uploadedwandb: \ 0.079 MB of 0.438 MB uploadedwandb: | 0.079 MB of 0.438 MB uploadedwandb: / 0.079 MB of 0.438 MB uploadedwandb: - 0.079 MB of 0.438 MB uploadedwandb: \ 0.079 MB of 0.438 MB uploadedwandb: | 0.079 MB of 0.438 MB uploadedwandb: / 0.079 MB of 0.438 MB uploadedwandb: - 0.079 MB of 0.438 MB uploadedwandb: \ 0.079 MB of 0.438 MB uploadedwandb: | 0.079 MB of 0.438 MB uploadedwandb: / 0.079 MB of 0.438 MB uploadedwandb: - 0.079 MB of 0.438 MB uploadedwandb: \ 0.079 MB of 0.438 MB uploadedwandb: | 0.079 MB of 0.438 MB uploadedwandb: / 0.079 MB of 0.438 MB uploadedwandb: - 0.079 MB of 0.438 MB uploadedwandb: \ 0.079 MB of 0.438 MB uploadedwandb: | 0.079 MB of 0.438 MB uploadedwandb: / 0.079 MB of 0.438 MB uploadedwandb: - 0.079 MB of 0.438 MB uploadedwandb: \ 0.438 MB of 0.438 MB uploadedwandb: | 0.438 MB of 0.438 MB uploadedwandb: / 0.438 MB of 0.438 MB uploadedwandb: - 0.438 MB of 0.438 MB uploadedwandb: \ 0.438 MB of 0.438 MB uploadedwandb: | 0.438 MB of 0.438 MB uploadedwandb: / 0.438 MB of 0.438 MB uploadedwandb: - 0.438 MB of 0.438 MB uploadedwandb: \ 0.438 MB of 0.438 MB uploadedwandb: | 0.438 MB of 0.438 MB uploadedwandb: / 0.438 MB of 0.438 MB uploadedwandb: - 0.438 MB of 0.438 MB uploadedwandb: \ 0.438 MB of 0.438 MB uploadedwandb: | 0.438 MB of 0.438 MB uploadedwandb: / 0.438 MB of 0.438 MB uploadedwandb: - 0.438 MB of 0.438 MB uploadedwandb: \ 0.438 MB of 0.438 MB uploadedwandb: | 0.438 MB of 0.438 MB uploadedwandb: / 0.438 MB of 0.438 MB uploadedwandb: - 0.438 MB of 0.438 MB uploadedwandb: \ 0.438 MB of 0.438 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss █▃▂▁▁▁▁▁▁▁
wandb:            eval/runtime ▁▃███▇█▇██
wandb: eval/samples_per_second █▆▁▁▁▂▁▂▁▁
wandb:   eval/steps_per_second █▅▁▁▂▂▂▂▂▂
wandb:             train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:       train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:         train/grad_norm ▆█▆▆▂▁▁▁▁▁▁▁▂▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train/learning_rate ▃▅███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:              train/loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                eval/loss 17.48978
wandb:             eval/runtime 406.3362
wandb:  eval/samples_per_second 6.891
wandb:    eval/steps_per_second 0.217
wandb:               total_flos 1.014431018330882e+18
wandb:              train/epoch 6.34921
wandb:        train/global_step 5000
wandb:          train/grad_norm 8.125
wandb:      train/learning_rate 0.0
wandb:               train/loss 16.7948
wandb:               train_loss 70.77398
wandb:            train_runtime 89639.3744
wandb: train_samples_per_second 1.785
wandb:   train_steps_per_second 0.056
wandb: 
wandb: 🚀 View run trim-capybara-386 at: https://wandb.ai/nju-ics/huggingface/runs/6bkieii5
wandb: ️⚡ View job at https://wandb.ai/nju-ics/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE4OTkyNTg4MA==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240613_093529-6bkieii5/logs
